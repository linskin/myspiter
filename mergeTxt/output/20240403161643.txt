
软件学报 ISSN 1000-9825, CODEN RUXUEW
Journal of Software, [doi: 10.13328/j.cnki.jos.007104]
©中国科学院软件研究所版权所有.

E-mail: jos@iscas.ac.cn
http://www.jos.org.cn
Tel: +86-10-62562563

AmazeMap: 一种基于多层次影响图的微服务故障定位方法

∗

李亚晓 1, 李青山 1, 王璐 1, 姜宇轩 1
1

(西安电子科技大学 计算机科学与技术学院,陕西 西安

710071)

通讯作者: 王璐, E-mail: wanglu@xidian.edu.cn

摘 要:

微服务软件系统由于其具有大量复杂的服务依赖关系和组件化模块,一个服务发生故障往往造成与之相

关的一个或多个服务发生故障,导致故障定位的难度不断提高.因此,如何有效检测系统故障,快速准确定位故障根
因问题,是当前微服务领域研究的重点.现有研究一般通过分析故障对服务、指标的作用关系,构建故障关系模型,
但存在运维数据利用不充分、故障信息建模不全面、根因定位粒度粗等问题.因此,本文提出了 AmazeMap,该方法
设计了多层次故障影响图建模方法以及基于多层次故障影响图的微服务故障定位方法.其中,多层次故障影响图建
模方法通过挖掘系统运行时指标时序数据与链路数据,考虑不同层次间的相互关系,能够较全面地建模故障信息;基
于多层次故障影响图的微服务故障定位方法通过缩小故障影响范围,从服务实例和指标两个方面发现根因,输出最
有可能的故障根因节点和指标序列.本文基于开源基准微服务系统和 AIOps 挑战赛数据集,从有效性和效率两个方
面设计了微服务软件故障定位实验,并与现有方法进行对比,实验结果验证了 AmazeMap 的有效性、准确性和效率.
关键词: 微服务; 故障定位; 多层次故障影响图
中图法分类号: TP311
中 文 引 用 格 式 : 李 亚 晓 ,李 青 山 , 王 璐 ,姜 宇 轩 . AmazeMap:一 种 基 于 多 层 次 影 响 图 的 微 服 务 故 障 定 位 方 法 . 软 件 学 报 .
http://www.jos.org.cn/1000-9825/7104.htm
英文引用格式: Li YX, Li QS, Wang L, Jiang YX. AmazeMap: A Microservices Fault Localization Method Based on Multi-Level
Impact Graph. Ruan Jian Xue Bao/Journal of Software (in Chinese). http://www.jos.org.cn/ 1000-9825/7104.htm

AmazeMap: A Microservices Fault Localization Method Based on Multi-Level Impact Graph
LI Ya-Xiao1, LI Qing-Shan1, WANG Lu1, JIANG Yu-Xuan1
1

(School of Computer Science and Technology, Xidian University, Xi’an 710071, China)

Abstract:

Due to the large number of complex service dependencies and componentized modules, a failure in one service often causes

one or more related services to fail, making it increasingly difficult to locate the cause of the failure. Therefore, how to effectively detect
system faults and locate the root cause of faults quickly and accurately is the focus of current research in the field of microservices.
Existing research generally builds a failure relationship model by analyzing the relationship between failures and services and metrics, but
there are problems such as insufficient utilization of operation and maintenance data, incomplete modeling of fault information, and
coarse granularity of root cause localization, etc. Therefore, this paper proposes AmazeMap, for which we design a multi-level fault
impact graph modeling method and a microservice fault localization method based on the fault impact graph. Specifically, the multi-level
fault impact graph modeling method can comprehensively model the fault information by mining the collected temporal metric data and
trace data while system running and considering the interrelationships between different levels; the fault localization method narrows the
scope of fault impact, discovers the root cause from service instances and metrics, and finally outputs the most probable root cause of
fault and metrics sequence. Based on an open-source benchmark microservice system and the AIOps contest dataset, this paper designs
experiments to validate AmazeMap’s effectiveness, accuracy, and efficiency, and compares AmazeMap with the existing methods.
∗

基金项目: 国家自然科学基金(62372351,U21B2015);陕西省科协青年人才托举计划项目(20220113)
收稿时间: 2023-09-08; 修改时间: 2023-10-30; 采用时间: 2023-12-14; jos 在线出版时间: 2024-01-05

李亚晓 等: 一种基于多层次影响图的微服务故障定位方法

Key words:

2205

microservice; fault localization; multi-level fault impact graph

微服务架构因增强了软件的敏捷性、弹性和可维护性,已被广泛应用于我国航空航天领域(如航空工业集
团制造云)、国防领域(如中电集团 SaaS 云平台)、电力领域(如国家电网 PKI 证书平台)和通信领域(如中国移
动磐基 PaaS 平台)等重要支柱行业中.如果不能准确、快速地对微服务故障进行定位,发现系统缺陷,就会导致
微服务系统运行状态持续恶化,引发告警风暴,造成严重后果.例如,2021 年 Facebook 出现大规模宕机,全球无法
使用近 7 个小时,影响了超 8000 万用户.微服务架构具有高内聚、低耦合和去中心化的特点,将系统拆分为一个
个微服务,而每个微服务均可进行多实例动态部署,产生了大量不同状态的服务实例,服务实例间异步通信产生
的海量运维数据对系统分析提出挑战.例如,字节跳动公司的在线微服务数量超过 10 万,服务实例部署数量超
过 1000 万,在巨大规模的压力下,仅凭人工已无法从大量运维数据中快速发现和诊断异常情况[1].
微服务软件故障定位的研究工作大体分为故障检测和根因定位.其中,故障检测依赖于识别微服务软件运
行过程中的异常 [2].根因定位则是在故障检测结果的基础上,通过相关算法展开进一步分析来确定故障发生的
根本原因[3].而故障往往导致用户最终体验差,即为软件缺陷.目前,微服务软件运行环境复杂多变、未知变化频
发,对建立一种能够保障微服务系统稳定运行的方法带来了以下两个难点:
(1)多源异构运维数据难以时空融合.指标数据通常是时间序列的形式,链路数据通常是以服务间的调用关
系的形式呈现,反映微服务架构软件的空间信息,且多源异构数据会随着时间的变化而动态变化.同时,多源异
构数据之间是相互关联的.因此,从时间和空间角度融合这两类数据是必要的,同时也是亟需解决的难点之一.
(2)微服务架构软件故障难以细粒度定位.大型微服务架构软件多实例容器化部署和独特的通信机制,导致
了监控数据种类和数据量的不断增加,极大地影响了微服务故障检测与定位的精度,定位粒度停留在服务级别.
然而该定位粒度并不能高效地帮助运维人员处理故障,因此,如何更细粒度定位故障是运维工作面临的难题.
目前研究工作针对上述两类难点问题展开研究,但仍存在改进空间.现有方法主要针对一类运维数据获取
故障 信息,未考 虑运 维数 据时 空融 合蕴 含的 故障 信息,导致 故障 的查 全率 低,且 检测 与定 位准 确度 较低.例
如,Nandi 团队提出的 OASIS 方法[4] 只关注于日志数据,忽略系统链路信息和度量指标的影响,且其未考虑数据
时间特性.此外,现有研究在进行故障定位时大多采用基于图的定位方法,主要包括调用图[5] 、因果图[6]和影响图
[7,8]

.调用图和因果图针对服务调用关系和故障因果关系对系统建模,不能全面且准确的定位根因.而影响图能

有效结合调用图和因果图的特点,从故障表征、故障影响因素和故障根因的关系出发,能够较为全面的描述故
障相关信息,提高定位的准确性和全面性.
基于此,针对微服务软件故障定位面临的运维数据利用不充分、故障信息建模不全面、根因定位粒度粗等
真实问题,本文研究了基于多层次影响图的微服务故障定位方法,提出 AmazeMap,该方法通过研究指标时序信
息、指标因果信息和链路调用信息从时间和空间维度建模故障信息,充分挖掘了运维数据的时间和空间特点,
实现多层次故障影响图构建,并据此实现细粒度定位故障,以达到高效定位故障并保证系统平稳运行的目的.本
文的主要贡献总结如下:
(1)基于运维数据的多层次故障影响图建模方法.现有的研究工作主要关注于某一类运维数据与故障间的
映射关系,从而设计出故障关系模型,未从时间和空间维度建模故障信息.相比于现有方法,AmazeMap 首次提出
三层影响图模型,针对海量运维数据挖掘服务调用层、指标因果层和指标时序层之间的联系,从时空角度全面
建模故障信息,实现度量指标和链路信息的时空融合;从时间和空间维度解决了运维数据单一导致查全率低的
问题,为微服务故障定位提供模型基础,提高软件故障检测和根因定位的准确性.
(2)基于多层次故障影响图的故障定位方法.现有方法忽略了细粒度指标参数间的复杂关系会引起系统状
态变化,且大多工作的定位粒度为服务级别.AmazeMap 提出了基于动态阈值的软件故障检测方法,结合异常分
数和动态阈值有效检测故障发生范围,精化故障影响图;在此基础上,设计了基于多层次影响图的故障根因定位
方法,分别从服务实例节点间调用关系和时序度量指标间因果关系两个层次逐步筛选故障节点,精准定位故障
节点位置和故障指标根因,实现更细粒度的故障根因定位.

2206

Journal of Software 软件学报 Vol.X, No.X, X X

(3)基于实验结果向运维人员提供优化建议.通过在典型案例系统和数据集上开展实验,通过与基于因果关
系模型和基于影响关系模型的基准算法进行对比实验,本文验证了 AmazeMap 的有效性和效率.本方法可提醒
软件运维人员某些故障容易诱发微服务软件缺陷.软件运维人员可采取相应措施降低这些关键故障的发生概
率,从而使得微服务软件能够可靠、平稳地运行.
本文第 1 节介绍相关定义,并简述方法框架.第 2 节阐述多层影响图建模方法,详细分析层与层之间的关联
关系.第 3 节介绍基于多层影响图模型的故障检测和根因分析算法.第 4 节为实验分析,验证算法的有效性和效
率.第 5 节介绍相关工作.第 6 节内容总结与研究展望.

1 方法概览
定义 1 软件故障:指软件无法正常运行或者严重影响软件业务正常执行的事件[9] .本文中主要讨论微服务
软件故障,其故障类型根据故障表征分为了功能失效故障和性能缺失故障[10].
定义 2 软件监控:指动态地获取相关软件运行指标,作为软件运维治理的重要组成部分,主要用于反映软
件运行状态.而微服务软件监控包括了链路监控,度量指标监控和日志监控三个部分.
定义 3 故障影响关系:指在软件故障发生过程中,不同服务实例和节点的指标间存在关联关系,包括指标
因果关系、指标时序关系和故障传播关系等.
定义 4 微服务软件故障类型:根据软件故障的定义,本文将微服务软件故障划分为功能失效故障和性能
缺失故障,其区别在于微服务软件系统的一部分业务功能是否失效.在软件故障根源方面,传统的单体软件故障
和分布式软件故障根源分别集中在内部代码逻辑问题和分布式集群部署问题,而微服务软件由于其去中心化
的表现形式和轻量级的通信机制,导致微服务软件故障根源的类型尤为复杂.本文主要针对微服务软件特点,并
结合传统软件故障根源开展讨论,将其分为内部缺陷故障,集群部署故障以及服务依赖故障,如表 1 所示.
Table 1

Table of microservice software fault types
表1

微服务软件故障类型划分表

类别

功能失效故障

性能缺失故障

内部缺陷

内存泄漏
堆栈溢出

I/O 操作时长过高
代码时间复杂度过高

集群部署

节点宕机
网络异常

负载过大
Docker 配置异常

服务依赖

版本不兼容
数据库连接阻塞

响应超时
丢包率过高

定义 5 基于调用图的故障根因定位方法：该方法主要是通过采集软件运行时服务间相关调用的执行轨
迹构建关系模型,并利用轨迹差异对比[5] 或树编辑距离等方法完成故障根因定位.
定义 6 基于因果图的故障定位方法：通常利用度量指标与对应服务模型的相关性,通过量化计算不同指
标间的因果关系,大多采用 PC 算法[11] 完成因果图建模过程.
定义 7 基于影响图的故障定位方法 [7,8] ：通过考虑故障传播,依赖关系和指标波动等对目标系统的影响,
分析量化不同故障表征、故障影响因素和故障根因间的关联关系,进而构建故障影响图.
其次,给出基于多层次影响图的微服务故障定位方法工作过程.如图 1 所示,其主要包含 3 部分工作:①运维
数据采集;②多层次影响图模型构建;③故障定位,即故障检测及根因分析.其中,工作①为工作②和③的基础,负
责从系统运行状态中获取运维数据,提供数据基础.工作②为工作③的基础,也是本文核心之处,全面建模故障
有关信息.

2207

李亚晓 等: 一种基于多层次影响图的微服务故障定位方法

t1

t2

t3

t4

I1
I2
I3

(

Ch = Pi , I j , C ( P ) , CI ( P ) j

(Si
(Sj

Ij ,

(Sk

Ik ,

(

)

I tp

Ii ,

111

回果关系

I tp

,

Fig. 1
图1
①

I tp

Overview of AmazeMap
AmazeMap 方法概览

运维数据采集.针对软件故障表征与多层影响图数据特点,利用软件监控手段,从微服务的度量指标数据
(时间)和可执行链路数据(空间)两个方面对微服务进行全面故障数据采集,主要包括了对于服务调用信
息、服务和节点部署信息、业务质量指标和性能度量指标等数据的获取.

②

多层次影响图模型构建.AmazeMap 创新性地从时空两个维度与指标因果、指标时序和故障传播关系三个
层次对故障影响图节点进行构建和关联,一般的两变量格兰杰因果关系检验方法

错误 !未找到引用源。

无法有效地应

用在复杂环境中存在的伪因果关系问题的两个以上相互作用的变量上,因此提出基于多变量格兰杰因果
关系检验方法对时序度量指标权重关系的量化计算方法,深入分析不同指标间的作用关系,最终输出一个
有向带权图模型,提高后续软件故障检测和根因定位的准确性.
③ 故障定位(故障检测和根因分析).基于上述多层次影响图模型,首先提出基于动态阈值的软件故障检测方
法,其次,提出基于影响图的故障根因定位方法,精确定位故障节点位置和故障指标根因.

2 基于运维数据的多层次故障影响图建模方法
2.1 运维数据采集
为了全面准确地度量微服务软件运行状态,本文从时间和空间角度对微服务系统进行全面故障数据采集,
包括度量指标数据和可执行链路数据两个方面.由于微服务架构的去中心化和轻量级通信的特点及其在基础
设施层和应用服务层与传统软件存在较大差异,给微服务的链路数据采集过程造成极大的阻碍.在基础设施层,

2208

Journal of Software 软件学报 Vol.X, No.X, X X

一个服务组件存在多个实例部署,并且部署在不同的物理节点上,难以有效检测在一条请求链路中是哪个节点
的哪个服务实例存在故障.在应用服务层,每个微服务组件独立承担一部分业务功能,且同一微服务的种类和服
务实例繁多,相互调用错综复杂,很难有效获取清晰的请求调用链路.本文结合了微服务软件在基础设施层和应
用服务层的特点,通过动态插桩方法采集服务部署和交互信息,构建跨节点的可执行链路.该方法能够将侵入代
码限制在软件堆栈的一个足够低的级别,保证了方法的应用程序透明性,从而实现链路数据的全面部署和持续
采集,最终达到微服务可执行链路数据采集的目的.
该方法主要分为设计链路模型,刻画服务调用信息,采集链路数据并完成模型构建三个部分.首先,通过动
态插桩方法采集服务部署和交互信息,构建跨节点的可执行链路,以实现链路数据地采集.首先,本文通过对微
服务软件系统内部相关服务的研究,同时,本文结合了 Dapper 跟踪树的理论和时间跨度,将微服务间服务调用关
系视作一个跟踪树的结构.其次,本文通过多元组和有向边分别表示服务组件实例和服务调用信息.最后,在设
计链路模型和刻画服务调用信息的基础上,利用动态插桩的思想,通过 Kafka 和 HBase 等组件实现对服务调用
信息的采集.采用基于宽度优先搜索方法,对父子调用关系进行分析,最终完成了可执行链路的构建.
随着微服务架构的广泛应用,多实例容器化部署和独特的通信机制导致了监控数据种类和数据量的不断
增加,同时,相关指标的生命周期也逐渐缩短,传统的软件监控工具已无法满足微服务软件度量指标采集的要
求.因此,本文提出一种基于时序数据库的多维度度量指标采集方法,在 USE 方法(Utilization Saturation and
Errors Method)的理论原则下结合 Prometheus 以及 Kubernetes 容器实践,提出了针对实际物理资源,服务部署和
运行资源的度量指标监控方法.该方法首先通过明确划分微服务软件系统的不同维度的指标,然后利用 HTTP
协议持续性抓取被监控组件的状态,不需要任何 SDK 或其他集成方式,适用于微服务虚拟化环境的要求,完成
了对于微服务软件系统在基础设施层、服务应用层以及网络层等多个层次在一定时间周期内的业务质量指标
和性能度量指标,从而能够有效甄别资源瓶颈,反映微服务软件系统下的故障问题.
该方法将时序度量指标监控过程分为目标系统层,数据采集层和指标聚合层三个层次.首先,在目标系统
层,将目标微服务软件系统部署于 Kubernetes 平台上,通过服务网格化的形式设置 Envoy 动态代理,实现对微服
务间的交互进行流量拦截,并通过 Kubernetes 平台自带的监控组件实时获取系统基础设施(包括 CPU、内存、
磁盘、网络和线程等相关维度)的监控指标.然后,在数据采集层,构建基于时序数据库的数据采集器,通过被监控
系统暴露 HTTP 服务接口的方式,周期性地抓取监控数据,使得被监控系统完全独立于监控系统之外,无需感知
监控模块,进一步地降低耦合.在此基础上,针对时序数据库特点,完成了环境部署指标,中间关联指标以及用户
感应指标的多元组表示进行持久化存储.最后,在指标聚合层,本文利用 ECharts 组件对度量指标进行聚合统计,
有效反映微服务系统的运行状态,提高软件运维效率.
2.2 多层次影响图建模方法
现有的研究工作主要关注于分析链路、服务性能指标变化与故障间的映射关系,忽略了细粒度指标参数间
的复杂相互作用会引起系统状态变化,在一定程度上影响了故障表征,从而导致故障映射关系存在误差,影响后
续故障定位的准确性.因此,本文设计了多层次故障影响图模型,如图 2 所示,从服务调用层、指标时序层和指标
因果层三个层次对故障影响图节点进行构建和关联,然后提出基于多变量格兰杰因果关系检验方法,对时序度
量指标权重关系进行量化计算,深入分析不同指标间的作用关系,最终输出一个有向带权图模型,为后续故障定
位提供模型支撑.

李亚晓 等: 一种基于多层次影响图的微服务故障定位方法

h1

I1

2209

h2

S1

S2

S2

S4

S3

S5

t1 t 2 t 3 t 4

I2
I3

Fig. 2

Multi-level fault influence diagram model
图2

多层次故障影响图模型

2.2.1 多层次影响图模型
(1)服务调用层
AmazeMap 通过分析特定时刻下的可执行链路和服务部署状态,挖掘链路数据空间信息,以服务实例作为
故障影响图的图节点,不同服务实例之间用服务调用关系和部署关系作为连接边,而每个服务实例对应于不同
类型的服务组件和部署机器节点.同机器节点下的不同服务实例节点存在多个相同数据的度量指标,即 CPU 使
用率,内存占用率和网络带宽等与机器节点相关的度量指标.服务调用层能够有效解决在同一条可执行链路、
不同可执行链路、同服务部署机器节点下以及跨节点状态下的软件故障传播问题和故障事件覆盖率不足的问
题.例如,实例 S1 和实例 S2 同时部署于 h1 节点,实例 S1 归属于服务调用链路 1,实例 S2 归属于链路 2,当实例
S1 存在资源受限异常,链路 1 会发生异常事件,同时有可能导致 h1 故障或宕机,从而链路 2 也会由于实例 S2 的
运行异常而产生异常事件.
(2)指标时序层
由于软件故障的传播并非是一个瞬时的事件,单纯某一时刻的服务调用信息和度量指标数据无法精确建
模故障的传播和影响过程.因此,AmazeMap 主要考虑软件故障时序关系和传播延迟的影响,结合指标的历史数
据和实时数据,确定不同指标间的时序关系.该时间序列为最大的度量指标影响时间范围,确保涵盖了所有因度
量指标变化而引起的其他指标波动.然后,通过对比分析不同时刻下指标的波动情况,进而量化计算出指标间因
果关系,挖掘出在一段时间内不同度量指标间的故障传播方向,从而解决了由于微服务轻量级通信导致的故障
传播时延问题,同时能够细粒度地判断微服务软件系统由于软件故障所引发的内部指标变化趋势,为后续故障
检测和细粒度根因定位提供数据支撑.
(3)指标因果层
本文通过分析时序度量指标间的因果关系,在指标时序关系的基础上,将每个图节点表示为某种度量指标
在一段时间内的持续监控数据.同时,将度量指标间的因果关系作为故障影响图中每个度量指标节点的带权有
向边,用于表示某种指标受另一种指标的影响程度,从而能够深入了解软件运行过程中不同度量指标变化的关
联性.由于用户感应指标能够切实反映软件系统业务的执行状态,并且一个用户感应指标的异常通常是由若干
环境部署指标和中间关联指标共同作用的最终结果,因此,AmazeMap 将用户感应指标作为微服务软件系统中
不同度量指标相互作用导致的最终结果,即故障表征.例如,用户每秒请求量的增加导致了 CPU 使用率,CPU 负

2210

Journal of Software 软件学报 Vol.X, No.X, X X

载,内存占用率和数据库操作数等指标的变化.同时,CPU 负载在一定程度上也影响了 CPU 使用率,数据库操作
数的增加也导致了线程数的增加,最终导致了服务响应时间的延长,引发了微服务软件故障.
2.2.2 节点构建与关联
在节点构建方面,首先,将不同时间段微服务软件系统的可执行链路和时序度量指标数据集表示为

I it , t = 0,1,..., T , i = 1, 2,..., N ,其中,T 为软件故障数据种类, t 表示不同时刻下监控数据的时间序列.然后,结合时序

度量指标,服务调用和基础设施部署状态生成一个完全有向图,用 G (V , E , I ) 表示某一时刻的微服务软件运行状

态.其中, V 表示图节点,用 Vi , i = 1,..., N 表示,包括服务实例节点和时序度量指标节点; E 是故障影响图的边集
合,表示服务调用关系和指标因果关系; I 则代表多维度的度量指标集合,包括用户感应指标,中间关联指标和环
境部署指标三类.
针对服务实例节点,主要以可执行链路信息和服务部署信息作为服务实例节点的关键数据,每一个服务实
例节点都包含了少量的额外信息,例如实例部署节点,服务实例种类等,用于准确定位该服务实例的位置.此外,
不同服务实例节点间的边用服务调用关系和服务同一机器节点部署关系表示.根据上述定义,能够输出一个反
映服务调用信息的有向图 GT .
算法 1:时序度量指标的因果关系图建模方法
输入:时序度量指标数据集 metricSet,时序时间窗口大小为 timeWindow
输出:时序度量指标因果关系图(带权有向图)
S = constraintsByTime( metricSet, timeWindow)
1
M = constraintsByFaultOrChangeEvent(S)
2
len = S.size()
3
I = init(S)
4
Gm =initGraph(M, len)
5
6
for i←1 to len do
7
for j←1 to len do
8

p = calculateCausalByGC( I i , I j )

9

if

10
11
12

p = 0 then
deleteGraphEdge( Gm ,

i,

j)

//基于时间窗口获取数据集
//初步筛选故障影响的指标集合
//得到数据集中指标种类数
//得到所有时序度量指标值
//构建完全有向图 Gm

//通过 GC 算法计算因果关系
//删除从 I i 到 I j 的边

else
if I i ∈ M && I j ∈ M then

//判断 I i 和 I j 是否故障指标

13

t1 = getFaultOrChangeTime(F, i)

//获取 I i 发生故障的时间

14

t 2 =getFaultOrChangeTime(F, j)

//获取 I j 发生故障的时间

15

if t1 < t 2 then

16

value = calPearsonValue( I i , I j )

//量化因果关系

17

insertValueofEdge( Gm , i, j, value)

//将因果值赋给 I i 到 I j 的边

deleteGraphEdge( Gm , i, j)

//删除从 I i 到 I j 的边

18

else

19
20
21
22
23

endif
endif
endfor
endfor

时序度量指标节点代表一段时间内持续的指标监控数据,主要以度量指标的实时数据和一段时间内的指
标波动情况作为关键数据.本文通过设置滑动窗口获取度量指标数据样本.每个时序度量指标节点表示为
I i = I it −t ,..., I it .其中包括了当前时刻的瞬时数据和在时间窗口 τ 内的监控数据,即故障影响的最大传播时间,保
证了故障传播局限在这一段时间内.同时,标注出由于软件故障发生造成的对应度量指标波动过大的时间节点
t1 t 2
和数据,根据不同度量指标的变化顺序反映指标时序关系,表示为 TR ( I i , I j ) .此外,不同度量指标节点间的边表

李亚晓 等: 一种基于多层次影响图的微服务故障定位方法

2211

示时序和因果关系,并通过量化因果关系的手段计算不同度量指标节点间的影响关系,以此作为有向边的带权
t1 t 2
t1 t 2
t1 t 2
值 P ( Si , I i , I j ) .如果 P ( Si , I i , I j ) 在时序关系上无法满足 t1 < t 2 ,或在因果关系上无法满足 P ( Si , I i , I j ) > 0 的条
t2
件,代表 I it1 对 I j 间不存在影响关系,则从给定完全有向图中去除从 I i 指向 I j 的有向边.不断循环该过程,最终

输出一个反映不同指标间时序因果关系的带权有向图 Gm ,并作为故障影响图的重要组成部分,具体的度量指
标的时序因果图 Gm 的建模算法 1 所示.
在完成多层次故障影响图模型节点的构建和关联之后,本文提出了时序度量指标因果关系量化计算方法.
本文采用基于多变量格兰杰因果关系检验方法判断度量指标节点间的因果关系作为影响关系;然后通过皮尔
逊相关系数对影响图进行加权处理,以此量化软件故障在指标节点上的影响关系,即故障影响图中不同指标节
点间的带权有向边的权重值.
一般的两变量格兰杰因果关系检验方法仅适用于二元时间序列,无法有效地应用在存在两个以上相互作
用的变量复杂环境中存在的伪因果关系问题,主要分为两种情况.其一,在三个变量 X、Y 和 Z 同时存在的前提下,
变量 X 是变量 Y 的原因变量,变量 Z 又是变量 X 的原因变量,然而传统的格兰杰因果关系检验没有考虑第三种
变量 Z 在相互作用过程中的影响,无法判断变量 X 与变量 Y 之间存在的是直接因果关系或间接因果关系,从而
引发检验结果错误,导致三个变量之间的因果关系混淆.其二,当变量 X 是变量 Y 和 Z 的原因变量,可知变量 Y 与
变量 Z 同源.当计算变量 Y 与变量 Z 之间的格兰杰因果关系的过程中,由于变量 Y 与变量 Z 的数据变化都来自
同一个原因变量,错误判定变量 Y 与 Z 间存在因果关系.上述两种情况都违背了基础假设,即不同原因变量间存
在相互关联,从而引发检验结果出错.此外,在现实场景下,也无法控制所有与该相互作用过程相关的所有剩余
变量,极大地影响了格兰杰因果关系检验方法的准确性.因此,为了完成多层次故障影响图建模,需要针对复杂
大规模时序度量指标进行因果关系量化计算,本文提出通过引入条件变量的方式开展多变量格兰杰因果关系
检验,在一定程度上解决了伪因果关系问题,提高故障影响图边权重的准确性和有效性.
针对变量 X,Y 和 Z 的时间序列,通过构建三变量时间序列的回归方程,包括基准方程和比较方程两类,分析
出变量 Y 与变量 Z 间的格兰杰因果关系.其中,构建的基准方程如式(1)所示,构建比较方程如式(2)所示.
p
p

 X (t ) = ∑ α i xt − i + ∑ β j yt − j + ε1 (t )
i 1 =j 1
=


p
p
Y (t ) = χ x + δ y + ε (t )
∑
∑
i
t
i
j t− j
−
2

i 1 =j 1
=


p
p
p

 X (t ) = ∑ α i xt − i + ∑ β j yt − j + ∑η k zt − k + ε 3 (t )
i 1 =j 1 =
k 1
=


p
p
p
Y (t ) = χ x + δ y + µ z + ε (t )
∑ i t −i ∑ j t − j ∑ k t − k 4

i 1 =j 1 =
k 1
=


(1)

(2)

然后,本文将变量 X 作为条件变量,为分析变量 Z 是否为变量 Y 的原因变量,构建因果统计量方程如式(3)所
示.
Fz → y x =

( RSS0 − RSS1 ) p
RSS1 ( N − 2 p − 1)

(3)

在此基础上,针对传统格兰杰因果关系检验在多变量环境下产生的两类伪因果关系问题展开讨论.当变量
X,Y 与 Z 的关系符合直接与间接因果关系时,即只有变量 X 作为条件变量与变量 Y 与 Z 存在直接因果关系.此
时, zt − k 中不包括额外的因果关系作用信息,对 Y (t ) 没有影响.而当变量 X,Y 与 Z 的关系符合同源关系时,变量 Y
与变量 Z 同时依赖于原因变量 X.此时, xt − i 中已经包含了所有的因果作用关系信息,而 zt − k 没有提供额外的信

息,对 Y (t ) 没有影响.综合以上两种情况,可以得出,式(1)中的 ε 2 ( t ) 应该等于式(2)中的 ε 4 ( t ) ,进而导致 Fz → y x = 0 ;
倘 若 变 量 Z 与 变 量 Y 之 间 存 在 直 接 因 果 关 系 ,则 表 示 zt − k 中 包 含 了 一 部 分 变 量 Y 因 果 作 用 信 息 ,使 得

2212

Journal of Software 软件学报 Vol.X, No.X, X X

ε 2 (t ) > ε 4 (t ) ,从而导致 Fz → y x > 0 .此外,本文通过将复杂多变量的场景拆分为三变量的场景,通过划分三个变量
的结构逐步解决伪因果关系.
此外,由于时序度量指标时间序列数据的波动性和随机性,当变量之间不存在格兰杰因果关系状态时,也
会发生 Fz → y | x > 0 的情况,与原假设不符.因此,为了排除数据随机干扰的问题,本文采用基于蒙特卡罗模拟的方
法,设置不同情况下因果统计量的置信阈值.以(2)举例,首先将已有的 zt 时间序列随机打乱顺序,输出得到新的
时间序列 zˆt ,从而破坏了变量 Z 与变量 Y 之间原有的因果关系,确保新的时间序列 zt 对于变量 Y 无作用关系,假
定满足原假设 Fz → y x = 0 .然后,通过重复打乱时间序列 zˆt ,基于式(2)计算因果统计量,将计算输出结果进行升序
排列.最终,根据统计学原理,选择满足 95%的分位数作为该因果统计量的置信阈值.最终,针对存在因果关系的
两个时序度量指标,采用皮尔逊相关性系数量化计算其相关性,以此作为因果关系边的权重值.
本文重点关注微服务软件故障的传播形式,分别从跨指标传播和跨服务传播两个角度展开分析,有效关联
服务调用,服务部署,指标时序以及指标因果关系,完成对不同层次的故障影响图模型间的转换.在跨指标传播
角度,由于不同度量指标间存在冗余信息,即双向关联关系,单纯依赖指标数值波动难以有效判断不同指标间的
因果关系和传播情况.因此,本文结合指标时序关系,在因果关系量化计算的基础上,通过不同指标发生异常波
动的前后次序来判断不同指标间的指向关系,从而保证两个度量指标间为单向带权边.在跨服务传播角度,根据
服务实例在基础设施层和服务应用层所担任的角色,本文将服务调用可执行链路和服务部署状态作为微服务
软件故障跨服务传播的主要途径.其中,服务调用可执行链路主要利用用户感应指标反映不同服务实例节点的
运行状态,以此确定软件故障链路;服务部署状态则通过环境部署指标的变化来影响同部署机器节点上不同服
务实例间的故障传播状态.

3 基于多层次故障影响图的故障定位方法
3.1 基于动态阈值的软件故障检测方法
（1） 软件异常分数评估
本文从可执行链路耗时评估和服务指标评估两部分对微服务软件异常分数进行评估.具体如图 3 所示,首
先通过对可执行链路耗时进行异常分析评估,分析故障链路集合;在此基础上根据该集合中存在的服务调用关
系进行遍历,初步筛选出故障服务集合;最后,对故障服务集合中的微服务的用户感应指标进行异常分数评估,
最终筛选出引起软件异常的关键服务实例,从而缩小故障检测范围.
可执行链路
链路
耗时

生成

执行
成败

故障链路集合

异常分数评估

用户感应指标

生成

响应
时间

吞吐量

Fig. 3

错误率

故障服务集合

Software anomaly score evaluation structure diagram
图3

软件异常分数评估结构图

异常分数设计.当微服务软件系统具体某一条可执行链路发生故障时,势必会导致链路耗时的异常波动.本
文结合了自回归模型和加权移动平均模型的特点构建回归模型,通过量化计算历史数据与当前数据间的关联
关系，并解决历史数据中不同时刻对于当前数据的影响程度不同的问题,对链路耗时的当前值进行预测.其中,
移动平均过程消除随机波动对预测值的影响,从而确保该模型的预测效果更为准确和高效.在此基础上,获得该
链路耗时的预测值.将得到与实际链路耗时的差值与预测值比较得到预测偏差比.预测偏差比值作为该链路的

2213

李亚晓 等: 一种基于多层次影响图的微服务故障定位方法

异常分数,当该预测耗时与当前时刻的实际链路耗时有较大差距时,即异常分数超过阈值时,则判定为该链路发
生了故障.本文通过异常分数评估方法评估可执行链路的异常程度,并将异常分数过大的链路和执行失败的链
路加入故障链路集合 TS 中.
故障服务集合构建.本文设置了故障服务集合 FS ,通过遍历故障链路的服务调用轨迹,并将经过的所有服
务实例加入故障服务集合 FS 中;考虑到微服务软件故障在物理资源层面的故障传播问题,本文将与故障服务
集合 FS 所有的服务实例部署在同一物理节点上的其他服务实例加入到故障服务集合 FS 中,确保该集合中涵
盖了某特点故障的故障表征和故障根因.
精化影响图.本文利用异常分数评估方法依次对故障服务集合 FS 中的所有服务实例的用户感应指标进行
异常评估,包括方法响应时间,吞吐量以及错误率三类度量指标.将异常分数未达到阈值的服务实例从故障服务
集合 FS 中筛除,精化故障服务集合.如图 4 所示,在第三章多层次故障影响图的基础上,根据故障服务集合筛选
出软件故障影响范围,以此精化故障影响图,减少后续故障根因定位过程的工作量,加快故障根因定位速度.

I tp

I tp
h2
S1

S2

S2

I tp

S4

S5

S3

h1

I tp
I tp

I tp

Fig. 4

Refine the fault impact diagram
图4

精化故障影响图

（2） 基于累积分布函数的动态阈值计算
本文结合了累积分布函数用于描述异常分数的分布情况,将离散数据进行平滑处理,输出连续曲线,并参考
连续函数曲率的定义分析异常分数增长缓慢的位置作为动态阈值.已知,异常分数阈值越高,被判定为故障服务
实例节点的数量越少,有可能会导致故障节点作为正常节点被剔除出故障服务集合 FS ,从而影响故障定位的
准确性.因此,本文将故障服务集合 FS 中的服务实例在每个时刻的用户感知指标作为基于曲率的动态阈值计
算方法的输入.首先,基于对应指标的异常分数生成对应的离散数据,得到不同指标的预测值和实际值.然后根
据加权移动对离散数据进行平滑处理,得到定义平滑曲线 Ds .最终生成一个以预测偏差值为 X 轴,被判定为正
常指标节点占所有节点的比值为 Y 轴的 CDF 图.

=
Ds

{( xi , yi ) xi , yi ≥ 0}

(4)

其次,为了确保基于累积分布函数曲率的动态阈值计算方法的通用性,不受用户感应指标变化的干扰,如
式(5)所示,对该平滑曲线 Ds 进行归一化处理.

2214

Journal of Software 软件学报 Vol.X, No.X, X X

( xi − xmin ) ( xmax − xmin )
 xi =

y
=
 i ( yi − ymin ) ( ymax − ymin )

(5)

然后,如式(6)所示,用 y − x 取代 y 值,获得新的平滑曲线,用 Dd 表示,该方法能够有效去除预测偏差比与正
常节点率按线性变化的冗余数据.

 Dd ={( xdi , ydi ) ∈ R 2 xdi , ydi ≥ 0}

 xdi = xi
 y= ( y − x )
i
i
 di

(6)

在此基础上,为了找到最优阈值点,本文在归一化后的连续函数上进行局部最大值计算.如式(7)所示,生成
以预测误差比为 X 轴,正常节点率为 Y 轴的连续平滑曲线 Dm ,将局部最大点作为最优阈值点的候补节点,用于
表示增长率开始下降的位置.

 Dm ={( xmi , ymi ) ∈ R 2 xmi , ymi ≥ 0}

 xmi = xdi
y
 mi = ydi y di −1 < ydi , ydi +1 < ydi

(7)

此外,在得到候补阈值点之后,需要定义一个唯一阈值,作为 Dm 连续值与灵敏参数 ϑ 间的平均差值,具体计
算过程如式(8)所示.灵敏参数 ϑ 用于调整计算动态阈值的速度.当灵敏参数 ϑ 值越大时,动态阈值的计算时间越
慢;反之,则越快.本文中该灵敏参数默认设置为 ϑ = 1 .
n −1

Tmi = ymi − ϑ ⋅

∑ ( xi +1 − xi )
i =1

n −1

(8)

最后,本文通过候选阈值与上述 Tmi 相对比,筛选出符合条件的阈值.当存在 ( xdj , ydj ) , j > i ,且该值在 Tmi 之下
时,此刻对应的局部最大值为最优阈值 x = xmi .此外,该动态阈值持续到下一个超过 Tmi 的局部最大值点.
3.2 基于影响图的故障根因定位方法
在上述精化后的多层次故障影响图的基础上,本节设计了基于影响图的故障根因定位方法.首先对该故障
根因定位方法所涉及的问题进行阐述,并完成符号定义.
Table 2

Microservice fault root cause location symbol definition table
表2

微服务故障根因定位符号定义表

符号

定义

Si

第 i 个服务实例

I ( Si )

某个服务实例节点 S i 的监控指标集合

I ( Si ) j

某个服务实例节点 S i 的监控指标集合第 j 个指标

F ( Si )

第 i 个服务实例节点的异常分数

FI ( Si ) j

第 i 个服务实例节点的第 j 个用户感应指标的异常分数

(

P Si , I i , I j

Ci

)

第 i 个服务实例节点上指标 j 和指标 k 间的因果关系
故障根因集合中第 i 个根因(以多元组表示)

（1） 基于调用关系的故障服务节点定位

2215

李亚晓 等: 一种基于多层次影响图的微服务故障定位方法

为了从多层次故障影响图中筛选出可能是本次故障影响的根因故障节点,本节提出了基于服务调用关系
的故障节点定位方法,用于计算不同服务实例节点的异常程度,并筛选出可能导致软件故障的根因节点位置集
合.本文采用了基于特征向量中心性的 PageRank 方法,该方法考虑到了不同服务实例节点的重要程度对其他相
邻节点的影响.具体过程如下:
首先,对服务实例节点的用户感应指标的异常分数进行评估,包括方法响应时间,吞吐量以及错误率三类度
量指标.并将异常度量指标的值归一化到[0,1]区间内.该异常分数表示为对应指标当前时刻实际值与预测值间
的残差值.
其次,将三种用户感应指标归一化后的异常分数之和作为服务实例节点 Si 的异常分数的初始值.
3

F ( Si ) = ∑ FI ( Si ) j

(9)

j =1

然后,构建一个 n × 1 的矩阵向量 v ,用于表示每个服务实例节点的初始异常分数,如式(10)所示.
=
v

[ F ( S1 )，F ( S2 )，⋅ ⋅⋅, F ( Sn )]

T

(10)

此外,在多层次故障影响图模型的基础上,根据服务实例间的调用关系和同一机器节点部署关系构建一个
n × n 的关联矩阵 M ,其中 M = ( M ij ) .在本文中采用皮尔逊相关性系数计算方法,对存在影响关系的不同服务实
例节点的响应时间序列做相关性计算,并将量化后的每个服务实例节点的相关性占与该节点相关的所有节点
的相关性之和的比值作为不同服务实例节点间的影响概率.具体如式(11)所示.其中, M ij 表示为第 i 个服务实例
对第 j 个服务实例节点的影响概率, P ( Si , S j ) 表示服务实例节点 Si 对 S j 的影响关系量化值, k ∈ M ( i ) 表示服务

实例 Si 会对 S k 产生影响.如果 M ij 的值大于 0,表示服务实例 Si 与服务实例 S j 之间存在影响关系;如果值为 0,
则表示两个服务实例不存在相互作用关系.
M ij =

P ( Si , S j )

∑ k∈M (i ) P ( Si , Sk )

(11)

然后,将上述向量 v 和矩阵 M 带入迭代方程,如式(12)所示,采用幂次迭代算法计算每个服务实例节点的异
常分数,直至该数值趋于稳定,表示迭代结束.
=
v′ dMv +

1− d
U
N

(12)

其中, N 表示为所有服务实例节点的数量; d 是阻尼系数,在本方法中,默认 d = 0.85 ,而 (1 − d ) N 表示为某
一服务实例节点被随机访问的概率.需要注意的是,通过反复迭代趋于稳定状态需要满足约束条件 v′ - v < ψ ,表
示迭代结束.其中, v′ 表示为第 k 次迭代后计算得到的异常分数,而 v 表示前一次的异常分数, ψ 为迭代停止,异
常分数趋于稳定的阈值,在本文中,默认 ψ = 0.01 .
最后,通过对每个服务实例节点的稳定异常分数排序,并筛选出异常分数最高的前 10 个故障实例节点,加
入故障服务实例节点集合 H 中.最终基于用户感应指标的服务实例节点的异常分数进行排序,筛选出前 10 个
异常分数最高的异常服务实例节点集合作为微服务软件系统的故障根因节点集合.
（2） 基于因果关系的故障指标根因定位
由于不同的度量指标间存在复杂的因果关系,度量指标之间会相互影响,从而导致更大的软件故障.为提高
软件运维人员的故障排查效率,本文提出了基于因果关系的故障指标根因定位方法,通过分析不同时序度量指
标间的量化因果关系以及不同异常指标在指标间的故障传播情况,得到最有可能是导致服务实例节点故障的
指标根因集合序列.
为了提高指标根因定位的准确性,并解决软件故障在度量指标间的传播问题,本文采用了基于特征向量中
心性改进的 PageRank 算法,将指标节点的异常分数作为输入,同时考虑了影响图中的边的方向及其相关性权
重,将影响关系转换为跨指标节点的故障传播概率,最终得出每个指标节点的最终异常分数.基于因果关系的故

2216

Journal of Software 软件学报 Vol.X, No.X, X X

障指标根因定位方法如图 5 所示,分为三个阶段.
…

故障服务实例
节点集合H

…

Si

…

Sj

…

Sn

格兰杰因果关系检验

时序度量指标

X

Y
Z

X

Y

因果关系值

PageRank迭代计算

Z

因果分析

异常分数评估
链路耗时

异常分数初始值

用户感应
指标

异
常
分
数
最
终
值

执行成败

…

故障根因序列

Fig. 5

Si

…

Ii

Sj

…

Ij

Sn

In

…

Diagram of the method for locating the root cause of fault indicators based on causality
图5

基于因果关系的故障指标根因定位方法图

首先,利用多层次影响图中的指标因果层量化指标间的相互作用关系.同时为了后续故障根因定位,需要对
指标间因果关系进行回溯,将有向边指向相反方向.根据度量指标间的量化因果关系构建一个 n × n 的关联矩
阵,作为度量指标间的关联权重.本文以故障节点 S 举例,如式(13)所示.

P ( S , Ii , I j )
 M ij = m

∑ P ( S, I j , Ik )

k =1

M =  M 
 ij 


(13)

然后,对每个服务实例节点的时序指标的异常分数进行计算,并进行归一化,得到异常分数初始值,以此构
建一个 n × 1 的矩阵向量 v ,如式(14)所示.

=
v  FI ( S )1 ，FI ( S )2 ，
⋅ ⋅⋅, FI ( S )m 

T

(14)

在此基础上,结合 PageRank 算法对每个时序度量指标的异常分数初始值进行计算,如式(15)所示.
=
v′ dMv +

1− d
U
N

(15)

此外,本文通过幂次迭代算法将上述 PageRank 算法进行迭代计算,并对结果进行排序,得到对应于每个故
障节点的前 10 个故障指标及其异常分数.
最终,将可能引发微服务软件故障的故障节点和故障指标以多元组的形式表示,如式(16)所示,并依照异常
分数的大小有序加入故障根因集合.

Ck

=

( S , I , F ( S ) , FI ( S ) )
i

j

j

(16)

具体的,由于微服务软件故障的表征,即服务节点故障和软件用户感应指标的故障,主要是源自于故障指标
根因的故障,即异常分数过大的故障指标越有可能是引发故障表征的原因.因此,本文将故障指标的最终异常分
数作为判断故障根因的依据,并将上述故障根因集合通过对故障指标的异常分数大小进行自高到低排序.一个
故障指标的最终异常分数越大,则该指标越有可能是最终故障根因.该故障根因集合旨在辅助软件运维人员开
展根因定位工作,节省故障排查的时间,提高故障诊断的效率.

李亚晓 等: 一种基于多层次影响图的微服务故障定位方法

2217

4 实验分析
本节介绍本文对多层次影响图故障定位方法开展的实验及结果分析工作.首先介绍实验的具体设计方法,
然后给出实验的实际结果并进行分析讨论.
4.1 实验设计
基准方法的选择.本文综合分析了现有的微服务软件故障定位方法,选取了三种不同的基准方法,主要包括
P. Chen 提出的基于因果关系模型的 CauseInfer 方法[12],L. Mariani 和 M. Kim 分别提出的基于影响关系模型的
Loud 方法[13] 和基于随机游走的 MonitorRank 方法[14].上述方法介绍以及选取上述方法的主要原因总结如下.


CauseInfer[12]是针对微服务领域性能问题的根因定位方法,其主要考虑物理资源、逻辑资源(锁)异常

引起的微服务异常问题,可以自动构造一个两层的层次因果图,并通过一系列的统计方法,沿着图中的因
果路径推断性能问题的原因.故障关系模型主要分为调用图、因果图和影响图三类.因果图和影响图是目
前应用最广泛的,用于微服务故障定位的故障关系模型,在故障根因定位的准确性和性能上有较大优势,
因此,本文选择了基于故障因果图和影响图的故障定位方法 CauseInfer 进行对比.


Loud[13] 将机器学习与图中心性算法相结合,该方法依赖于关键性能指标(kpi),即通常在运行系统上

收集的指标,利用机器学习来检测 kpi 中的异常并揭示它们之间的因果关系,利用基于中心性指标的算法
来补充机器学习,以定位产生和传播异常的故障资源.本文方法相比于 Loud 更细粒度地分析了度量指标
之间的因果关系,排除了伪因果问题对故障定位结果的影响,同时对故障影响图模型做出了一定改进.


MonitorRank[14] 是最早使用随机游走的策略定位故障根因服务的方法,该方法从任何一个怀疑的节

点开始入手,进一步查看其依赖的其他高相关性的节点,被访问越多的节点越可能是根因.


PageRank 方法 [11] 是一种基于网络连接计算节点中心性分数的有效方法,该算法的基本原理主要通

构建随机游走模型的方式,表示有向图中节点的重要性沿着有向边向周围扩散的现象.PageRank 方法是在
随机游走的基础上进行的改进,作为典型的图中心性计算方法,相比于随机游走算法更能满足微服务软件
复杂结构和不确定环境的要求.
综上,在软件故障定位领域,基于迭代深度优先搜索的 CauseInfer[12] 、采用 PageRank 算法的 Loud[13]和基于
随机游走的 MonitorRank[14] 三种方法在模型建模、定位过程等方面具有代表性,并且能够符合本文的研究目标,
实现细粒度的故障根因定位.因此,本文将上述三种方法作为本文的对比基准方法,能够验证 AmazeMap 的有效
性和性能,并保证实验结果的可靠性.
研究问题.为验证多层次影响图故障定位方法的有效性,本文尝试回答以下研究问题.
RQ1.本文故障定位有效性如何？能否检测出软件故障的发生,并定位故障的节点和指标根因？
RQ2.本文故障定位方法的准确性和效率如何？与现有方法相比,故障定位的准确性是否得到提高,所花费
的时间开销是否有所减少？
实验数据集.本文根据如下三个原则对数据集进行筛选.首先,实验数据集必须来源于微服务系统,同时包
括了部署机器、服务实例和度量指标三个层面的实验数据;其次,基准系统或实验数据集必须具有一定的适用
性和应用性,在针对微服务故障的相关研究工作中被广泛引用;最后,为满足对故障数据采集方法有效性和故障
定位方法准确性验证的要求,选择的基准系统必须支持人为故障注入和数据采集工作,从而能够针对不同类型
的微服务软件故障进行重复试验和在同场景下针对现有方法进行对比实验.因此,本文选择了基于 Sock-shop 开
源基准系统[46]的实验数据集和 AIOps 挑战赛发布的数据集进行实验.
Sock-shop[46]是一个典型的微服务软件基准系统,模拟电子商务网站的袜子销售流程,广泛应用于微服务相
关研究的测试环节.不同功能模块用不同编程语言编写,支持独立开发、部署和数据库存储,能够有效反映微服
务软件系统的特点,得到了相关研究学者的广泛认同.同时,该系统支持故障注入操作,能够有效反映微服务软
件系统在发生软件故障时的真实状态.该系统采用 SpringBoot[47]、GO Kit[48] 和 Node.js[49] 进行构建,主要由 13
个异构技术实现的微服务模块组成,并通过 HTTP 上的 REST 轻量级通信协议完成服务间交互.具体如图 6 所示.

2218

Journal of Software 软件学报 Vol.X, No.X, X X
Node.JS
Front-end

Go

Go

JAVA

Payment

Order

Go
User

Mongo

Mongo
RabbitMQ

JAVA
Shipping

Fig. 6

JAVA
Catalogue

Cart
MySQL

Mongo

JAVA

Queue

QueueMaster

System architecture diagram for the Sock-Shop benchmark
图6

面向 Sock-Shop 基准系统架构图

本文中设置为每 5 秒采集一次度量指标数据,并存储在相应数据库中.然后,采用开源的 AMS 框架对相应功
能模块的字节码进行增强,通过动态插装代理的方式采集可执行链路信息.本文采用 Chaos Mesh[50]混沌工程测
试工具对 Sock-shop 系统进行服务内存泄漏、CPU 占用和网络时延三种类型的故障注入,从网络、容器和 Pod
层面模拟真实场景下的软件故障.最终,本文针对每个故障类型分别随机注入到四个特定的功能模块的服务实
例中,重复试验 5 次,得到 60 组试验数据集,如表 3 所示.
Table 3

Based on ChaosMesh tools for the Sock-shop fault injection data set
表3

基于 ChaosMesh 工具对 Sock-shop 故障注入数据集

故障类型

符号表示

解释

注入次数

内存泄漏

memory leak

模拟注入内存泄漏故障

20

CPU 占用

CPU fault

模拟容器 CPU 压力

20

网络延迟

network delay

模拟注入网络延迟故障

20

然后,通过人工标注的方式获取验证数据集,记录每次故障注入测试的时间,获取到基于 Sock-shop 开源基
准系统数据集.详细数据集指标如表 4 所示.
Table 4

The Sock-shop fault injection index table data sets
表4

维度

容器

符号

定义

I reum

container_cpu_used 容器 CPU 使用率

I ctup

container_thread_used_pct 容器线程使用率

I ocup

CPU_util_pct 系统 CPU 使用率

I omup

Memory_used_pct 系统内存使用率

I odiu
内存

网络

业务

Sock-shop 故障注入数据集指标表

I reum

I rumr

Disk_io_util 系统 IO 繁忙度
used_memory
used_memory_rss

Redis 内存使用量
Redis 分配的内存总量

I rtcr

total_connections_received Redis 连接请求数

I nsp

Sent_queue 网络发送队列数

I nrep

Received_errors_packets 网络接收错误包数

I rt

服务请求响应时间

I tp

系统在单位时间内处理的请求数-吞吐量

I err

一段时间服务调用失败占总调用数的比值

2219

李亚晓 等: 一种基于多层次影响图的微服务故障定位方法

AIOps 挑战赛数据集是针对某大型的电子商城在私有云环境下的微服务软件运行数据,是专门用于微服
务软件系统故障发现和根因定位的实验数据集.该微服务软件系统部署搭建在 22 个机器节点,并集成了包括
Oracle[51]、Redis[52]和 Docker[53]组件,每天的平均数据量约为 1.2GB,所产生的数据集提供了包括业务功能、调
用链、容器、中间件、主机节点和数据集的指标数据,以及相应微服务软件系统的部署文档.该数据集有如下
三个特点:其一,该数据集包括了服务调用指标、业务功能指标和平台性能指标(包括数据库、中间件、容器等),
有效指标达 100 个以上,能够全面反映微服务软件系统的运行状态,满足了本文所提的故障定位方法中对于度
量指标和可执行链路信息的数据需求.其二,AIOps 挑战赛官方通过分批次实时故障注入的方式获取故障数据,
注入时间为每天的 0:00—6:00,共计 6 小时,对故障注入的时刻、持续时间进行记录,并确保任意时刻只存在唯
一故障.其三,该数据集来源于大规模微服务软件系统在真实场景下的故障注入测试数据,故障的类型多样,数
据规模庞大,在学术界具有一定的影响力.
Table 5

Table of fault types for AIOps Challenge dataset

表5

面向 AIOps 挑战赛数据集故障类型表

故障类型

符号表示

解释

CPU 占用

CPU fault

模拟容器 CPU 压力

网络延迟

network delay

模拟注入网络延迟故障

网络丢包

network loss

模拟注入网络延迟故障

数据库连接失败

db connection limit

模拟数据库连接限制问题

本文从 2020 年 4 月 11 日到 2020 年 5 月 31 日期间随机抽取了 15 日软件运行数据,并将 CPU 占用、网络
延迟、网络丢包以及数据库连接失败四种故障类型作为本文的故障定位方法对比实验的故障类型,如表 5 所示,
最终得到了 15 组实验数据集.此外,由于一个大型微服务软件系统涉及到大量的组件和技术栈,导致该数据集
中存在大量的指标.为了降低故障定位方法的时间和资源开销,本文从 AIOps 数据集中筛选出了 31 个重要指标,
具体包括用户感应指标、中间关联指标、环境部署指标三种类型,涵盖了操作系统、容器、数据库、Redis 中
间件等多个层次的指标,其中一部分重要指标如表 6 所示.
评测指标.


精确率( Precision ):在某时刻 T 下,故障定位根因集合中正确定位率,即查准率



召回率( Recall ):正确定位出故障根因的已检测故障事件占所有检测出故障事件的比例,即查全率




F1 值:精确率和召回率的调和平均,可以用来综合评价故障定位方法
Recall of Top-k( PR @ K ):用于表示故障根因降序集合中前 K 个根因结果包含真实根本原因的概率



定位效率:表示从故障发生时刻到定位出具体的故障根因之间的耗时.

运行环境配置.本文的实验环境由集成配置信息、Sock-shop 系统集群硬件部署环境、集成的软件配置信
息组成.其中,实验环境的集成配置主要采用的编程语言为 JAVA,IDE 版本为 IDEA 2018.3.6.系统集群部署环境
由 1 台 master 机器节点(i5-7500,16G)和 4 台工作节点(i5-7500,8G)组成.

2220

Journal of Software 软件学报 Vol.X, No.X, X X
Table 6

Table of metrics for the AIOps Challenge dataset
表6

类型

维度

符号

容器
环境

操作

部署指标

面向 AIOps 挑战赛数据集指标表
定义

I reum

container_cpu_used 容器 CPU 使用率

I ctup

container_thread_used_pct 容器线程使用率

I omup

Memory_used_pct 系统内存使用率

I odiu

Disk_io_util 系统 IO 繁忙度

系统

I ocup

CPU_util_pct 系统 CPU 使用率

I nsp

Sent_queue 网络发送队列数

网络

I nrep

Received_errors_packets 网络接收错误包数

I pup

Proc_Used_Pct 数据库连接使用百分比

I trt

tnsping_result_time 数据库响应时间

I dbps

TPS_Per_Sec 数据库每秒事务数

数据库
中间
关联指标

I dbsc

Sess_Connect 数据库连接会话数

I reum

used_memory

I rumr

中间件

used_memory_rss

I rtcr

业务

Redis 分配的内存总量

total_connections_received Redis 连接请求数

I rt

服务请求响应时间

I tp

系统在单位时间内处理的请求数-吞吐量

I err

一段时间服务调用失败占总调用数的比值

用户
感应指标

Redis 内存使用量

4.2 算法有效性测试
(1)定位有效性测试(RQ1)
本文针对 Sock-shop 系统数据集对 AmazeMap 提出的故障检测和根因定位进行了有效性验证,首先根据
数据集中的故障数据,构建多层次故障影响图,然后计算出每条执行链路和服务实例节点的异常分数,并根据动
态阈值检测出软件故障,最后采用基于影响图的故障定位方法实现对故障服务节点和指标节点的根因定位.
首先,本文根据 Sock-shop 系统数据集中的执行链路数据、时序度量指标数据和系统部署信息完成多层
次故障影响图建模.本文通过多次实验,将故障数据采集的时序时间窗口限制在 5 分钟(其中故障注入的持续时
间为 60s,采集故障注入前后各 2 分钟的软件运行数据),采集间隔为 1s,从中筛选采集到共计可执行链路 79 条,
服务实例总数 32 个,对应于每条链路的链路耗时以及对应每个服务实例节点的服务响应时间、CPU 使用率等
31 个时序度量指标的数据变化.
Table 7

Metrics causality values corresponding table
表7
I bps

I bps (网络传输速率)

I dbrt

I cpu

I tps

I tp

I rt

0.813

0.639

0.607

0.846

0.919

I dbrt (数据库响应时间)

0.315

0

0.326

0.453

0.463

0.445

I cpu (容器 CPU 使用率)

0.120

0.237

0

0.727

0.694

0.879

0.534

I tps (数据库每秒事务数)

0

度量指标因果关系值对应表

0

0.432

0

I tp (单位时间处理请求数)

0.433

0

0.598

0.621

0.219

I rt (服务响应时间)

0.511

0.257

0

0

0
0.833

0.624
0.796
0

2221

李亚晓 等: 一种基于多层次影响图的微服务故障定位方法

然后,本文通过时序度量指标因果关系量化计算方法,计算在时间窗口下时序度量指标间的关联关系,本文
以在注入网络延迟后 45s 后特定故障服务实例节点 Order2 为例展开介绍,并对产生明显波动的重要指标间的量
化因果关系值进行展示,具体如表 7 所示.
在此基础上,综合分析服务部署状态、服务调用信息、时序度量指标及影响关系,构建多层次故障影响图
模型.以上述故障注入后 45s 时刻下涉及服务实例 Order2 的执行链路为例,在服务实例节点 Order2 涉及到 3 条
可执行链路,其中网络传输速率、网络发送队列数、容器 CPU 使用率等指标均有可能是导致服务实例节点
Order2 响应时间异常的根本原因.
然后,为了准确检测出软件故障,并缩小后续故障根因定位范围,本文针对当前采集到 79 条执行链路的链
路耗时,所在故障链路上和与故障节点部署在同一机器节点上的服务实例的用户感应指标,即吞吐量、错误率
和响应时间进行异常分数评估,并根据不同指标的预测偏差值计算判定故障的动态阈值,从而完成故障检测过
程,进而对故障影响图进行精化.本文以服务实例节点 Order2 和 Catalogue1 以及其所在的链路 Trace41 的故障
状态为例展示故障检测结果,具体如表 8 所示.
Table 8

Trace and service instance node fault detection decision table
表8

链路和服务实例节点故障检测判定表

所属位置

指标

异常分数

阈值

是否故障

Trace41

链路耗时

0.562

0.179

是

响应时间

0.771

0.223

是

吞吐量

0.514

0.235

是

错误率

0.102

0.192

否

Order2

Catalogue1

响应时间

0.452

0.159

是

吞吐量

0.317

0.244

是

错误率

0.096

0.181

否

Fig. 7

Graph of dynamic threshold calculation
图7

动态阈值计算曲线图

其中,服务实例 Order2 响应时间的动态阈值计算如图 7 所示,正常节点率在 0.3 以下时随阈值提高而提高,
阈值达到一定界限之后逐渐趋于稳定;最大距离表示的是正常节点率到 y = x 直线的垂直距离,用于找出最大
阈值点.该曲线的局部最大值为最优动态阈值,即 x = 0.233 ,此时正确节点率曲线达到最大曲率值,能够有效检
测出故障服务节点.
最后,在精化故障影响图的基础上,本文针对服务内存泄漏、CPU 占用和网络时延三类故障进行实验,通过

2222

Journal of Software 软件学报 Vol.X, No.X, X X

基于 PageRank 的根因定位方法实现对故障节点和指标根因进行定位,输出根因概率最大的前 10 个根因序列集
合.同时,将该根因序列集合与故障注入的故障类型和故障服务节点位置进行对比,并选取精确值、召回率和 F1
评价指标作为故障定位方法的有效性验证,具体如图 8 所示.

Fig. 8

Effectiveness evaluation results of fault localization methods on Sock-shop dataset
图8

面向 Sock-shop 数据集的故障定位方法有效性评估结果

本文所提的故障定位方法针对 Sock-shop 数据集上的三种故障都有较好的定位效果.其中,定位结果的精
确率和 F1 值均在 0.9 以上,并通过根因定位方法有效定位故障服务实例节点和指标根因.而在召回率方面,由于
微服务实例的内存泄漏故障涉及到容器、操作系统等多个层面的影响,内部影响关系对比 CPU 占用和网络时
延故障而言更为复杂,因此在召回率方面仅达到 0.897,但其误差在可接受范围之内,仍能够支持微服务故障定
位输出正确根因结果.
综上,通过上述实验步骤,可以判断本文提出的基于故障影响图的软件故障定位方法能够有效定位微服
务软件故障,从而帮助微服务软件运维人员快速准确地定位软件故障问题.
(2)故障定位方法准确性和效率验证(RQ2)
本文方法在 Sock-shop 数据集以及 AIOps 挑战赛数据集上与现有方法的准确性和效率进行验证.选用
PR@K 和定位耗时作为故障定位方法准确性和效率的评价指标.此外,由于 MonitorRank 方法中对于微服务软
件故障的检测效果并不好,而本节的对比实验旨在比较根本原因定位的效果,因此,为了提高本次对比实验的合
理性,本文将基于本文故障检测结果的改进 MonitorRank 方法也作为对比方法进行实验.
首先,本文基于 Sock-shop 开源基准系统数据集中关于内存泄漏、CPU 占用和网络时延三类故障的数据开
展故障定位方法准确性对比验证,得到每个方法在 PR@1、PR@3 和 PR@5 的平均性能指标.具体结果如图 9 所
示,文中选取的故障定位方法在 Sock-shop 数据集上均能够准确定位故障根因,且随着评价指标中 K 值的提高,
故障定位的准确性也逐渐提高.与 Loud 方法相比,本文方法在 PR@5 上一致,在 PR@1 和 PR@3 上分别提高了
12%和 5%,这是由于本文方法对于服务和指标故障节点的定位排除了伪因果问题的影响,在 K 值较低的情况下
的准确性优于 Loud 方法.而与其他基准方法相比,本文方法在 PR@1、PR@3 和 PR@5 的定位结果分别提高了
14%、14%和 11%.

李亚晓 等: 一种基于多层次影响图的微服务故障定位方法

Fig. 9

2223

Accuracy evaluation of fault localization methods based on the Sock-shop dataset
图9

基于 Sock-shop 数据集的故障定位方法准确性评估

同时,在 PR@1 方面,本文参与实验方法输出的排名第一的根因序列与真实根因均存在一定误差,故障定
位的准确性都不高,尤其是 CauseInfer 方法的 PR@1 值仅为 19%,这是由于仅通过 PC 算法分析故障指标间关联
关系,从而构建故障因果图模型的性能较差.改进的基于随机游走的 MonitorRank 方法相较于原 MonitorRank 方
法,在 PR@1、PR@3 和 PR@5 值分别提高了 6%、4%、3%,但是,该方法最终故障定位效果还是有所欠缺,这是
由于 MonitorRank 方法过于依赖故障图模型,当某个正常微服务出现在多条故障链路中,会提高该微服务的异
常分数,从而导致故障定位的失败.相反,本文所提方法与 Loud 算法在此次实验中均具有出色表现,这是由于这
两种方法都通过格兰杰因果关系检验,进一步筛查了可能的故障根因,排除了相关正常微服务的影响.此外,本
文方法通过对伪因果关系的排查和对故障服务和指标节点的筛查,在故障定位的准确性上更为出色.
然后,本文基于 AIOps 挑战赛数据集中关于 CPU 占用、网络延迟、网络丢包以及数据库连接失败四种故
障类型的数据进行对比实验,具体结果如图 10 所示.可以看出,本文方法和对比基准方法在 AIOps 挑战赛数据集
中同样具有良好的故障定位效果.其中,本文方法与其他基准方法相比,在 PR@1、PR@3 和 PR@5 三类评价指
标上分别提高了 12%、6%和 1%,所选用的实验方法随 K 值的变化趋势和基于 Sock-shop 数据集的故障定位对
比实验变化趋势相似.

Fig. 10

Accuracy evaluation of fault localization methods based on AIOps dataset
图 10

基于 AIOps 数据集的故障定位方法准确性评估

此外,通过对比上述两个基准系统的对比实验结果,可以发现,由于 AIOps 数据集涉及的度量指标更多,基
准微服务系统更为复杂,故障类型更为多样,导致基于 AIOps 的故障定位方法的准确性均有一定程度的降低.通
过对比可以看出,相比于其他方法,本文方法面向不同微服务系统的效果稳定性更好,在 PR@1、PR@3 和 PR@5
三类评价指标上分别仅降低了 3%、1%和 1%.由于微服务软件系统自身复杂的结构和不确定性环境因素,本文

2224

Journal of Software 软件学报 Vol.X, No.X, X X

认为本文方法针对复杂系统的误差仍在可接受范围之内.
最后,在故障定位准确性实验的基础上,本文测试了故障定位准确性较好的 MonitorRank、Loud 和本文方
法的故障定位耗时,通过计算故障注入时刻到获得故障定位结果时刻的差值,作为该方法的定位耗时,并以此用
于评估故障定位方法的效率,具体如图 11 所示.
根据上述效率对比实验的评估结果,基于 AIOps 数据集的故障定位耗时普遍高于基于 Sock-shop 数据集的
故障定位耗时,这是由于 AIOps 数据集所选取的微服务基准系统规模庞大,结构复杂,考虑的度量指标全面所导
致的.同时,针对不同的故障类型,故障定位耗时也不同.可以看出,由于 MonitorRank 方法仅考虑服务间的因果依
赖关系,在方法耗时上最少,但方法的准确性较差.而对比于在 PR@5 准确性上相似的 Loud 方法,本文的故障定
位方法耗时更少,这是由于本文方法在故障检测和故障节点根因定位环节做了两次筛选,缩小了故障定位的范
围,从而在一定程度上降低了定位耗时,提高了故障定位的效率.

(a)基于 Sock-shop 数据集效率对比实验评估结果

(b)基于 AIOps 挑战赛数据集效率对比实验评估结果

Fig. 11

Comparative experimental evaluation of the efficiency of fault localization methods
图 11

故障定位方法效率对比实验评估

综上,本文针对两个数据集开展故障定位方法的准确性和效率验证,并进行了多次实验.该实验结果说明
AmazeMap 相比于其他现有基准方法,在准确性和效率有更好的表现,能够快速准确的定位故障服务和指标根
因,有效帮助运维人员维护微服务软件系统的正常运行,快速定位故障.
4.3 消融实验设置
为验证 AmazeMap 中每个组件的贡献以及重要程度,分别构建两种 AmazeMap 的变体,并设计一系列实验
来比较其性能,分别是(1)AmazeMap w/o 指标时序层.为了验证指标时序层在细粒度根因定位过程中的重要性,
将指标时序层从 AmazeMap 中删除.(2)AmazeMap w/o 指标因果层.为衡量指标因果层在细粒度根因定位过程

李亚晓 等: 一种基于多层次影响图的微服务故障定位方法

2225

中发挥的作用,将指标因果层从 AmazeMap 中去除.因服务调用层为细粒度根因定位基础,在确定服务集合后进
一步定位指标级根因,故上述实验设置未针对服务调用层进行设计.
实验结果如图 12 所示,分别列出上述两种变体在两种数据集上的 PR@1, PR@3, PR@5.当指标时序层被删
除后,PR@1, PR@3, PR@5 均有不同程度降低,实验结果表明,对于指标这类时间序列,指标时序层能够建模指
标时序关系,捕捉指标随时间变化的趋势,从而减少误报.当指标因果层被删除后,各项指标都呈现不同程度的
下降趋势,且相较于去除指标因果层下降趋势更大,表明在 AmazeMap 中,指标因果层相较于指标时序层更重要,
指标时序更多从时间维度考虑,而指标因果关系能够较好地描述不同指标间的关联关系.此外,由于 AIOps 挑战
赛数据集相比于 Sock-shop 数据集更加复杂,故在 Sock-shop 数据集上的结果总体优于 AIOps 挑战赛数据集的
结果.

Fig. 12

The PR@1, PR@3, PR@5 of AmazeMap and different model variants

图 12

AmazeMap 与不同模型变体的 PR@1、PR@3、PR@5

5 相关工作
现有的故障定位方法大部分应用于单体软件和分布式软件,关注于在运行环境稳定状态下的软件系统的
程序内部逻辑故障,无法适用于在不确定环境下的微服务软件系统的复杂故障定位[13,15].此外,现有方法较少从
时空融合角度处理运维数据,忽略微服务系统的时间和空间特性.目前,针对微服务软件故障定位的方法大体分
为故障检测和根因定位两个部分.针对故障检测和根因定位的有关工作较为分散,两者无法进行有效结合,从而
导致微服务软件故障定位过程更为复杂,造成了极大的阻碍.因此,本文对国内外故障检测和根因定位方法展开
调研与分析,旨在分析当前在微服务故障定位领域所面临的问题和挑战.
5.1 微服务故障检测方法现状
本文根据国内外微服务故障检测方法所使用的输入数据的类型差别,将其分为基于日志的故障检测、基于
链路的故障检测和基于度量指标故障检测三类.基于日志的故障检测方法通常采用无监督机器学习算法,主要

2226

Journal of Software 软件学报 Vol.X, No.X, X X

通过学习采集到的无故障状态下的软件运行日志,通过判断在线状态下新日志信息是否偏离基准模型来检测
是否发生软件故障[16,17].上述方法立足于以相似顺序能够记录相似的事件.然而,微服务软件频繁变更的软件需
求可能会通过引入新服务来取代旧服务来完成功能的变更[18] .因此,上述故障检测方法无法保证准确性.
基于链路的故障检测方法主要是通过采集到的微服务软件可执行链路,利用机器学习 [19,20] 或链路对比技
术

[21,22]

对故障进行检测.与上述机器学习方法不同的是,复旦大学彭鑫教授团队提出一种 MEPFL 模型[23],通过

将一个微服务应用程序与一组模拟用户请求的自动化测试用例作为输入,并通过监督学习算法分析不同类型
的 故 障 对 应 的 故 障 表 征 来 完 成 在 线 故 障 检 测 . 张 攀 等 [24] 提 出 一 种 基 于 自 然 语 言 处 理 与 双 向 长 短 期 记 忆
(BiLSTM)网络的微服务调用链异常检测方法 MicroTrace,利用事件进行检测.
基于度量指标故障检测方法大部分是通过安装代理的方法实时采集软件运行时指标,并采用机器学习算
法对相关指标进行模型训练[13] ,以此来确定微服务软件在应用级和服务级的故障问题.然而,上述都是在离线状
态下训练基准模型,不会随着时间变化而改变,无法很好地适用于微服务软件系统.而 CloudRanger 方法[8] 能够
通过分析历史监测数据,预测当前响应时间.与上述方法不同的是,Hora 模型[25,26]通过将贝叶斯网络模型关联到
一个数据采集器来完成在线故障检测.此外,CauseInfer 方法[7,12]通过比较 KPI 指标与软件系统的 SLO 来检测软
件性能故障;X.Zang 则是结合心跳检测机制检测服务级功能故障[27].MicroRCA 方法[2,28] 针对特定的 KPI 指标,
应用于 Kubernetes 部署的微服务软件系统,并结合 Istio[29] 和 Prometheus 组件完成指标采集.
综上所述,现有大部分基于机器学习的故障检测方法都依赖于微服务软件系统在运行过程采集到的日志、
链路和度量指标数据,通过训练构建基准模型作为判断软件故障的依据.由于微服务软件系统运行条件和服务
模块频繁变更的需求,上述方法难以解决软件变化导致基准模型不准确的问题.同时,还有部分方法只单独针对
性能故障或服务故障,在故障检测的粒度上存在局限性.
5.2 微服务根因定位方法现状
现有根因定位方法主要是在传统方法的基础上,针对微服务软件特点,通过将日志、链路和度量指标作为
输入,构建相关模型的方式对此类输入数据进行分析,如调用图和影响图等,从而完成根因定位过程.本文将针
对此过程中构建的不同故障相关图模型类型展开分析.
基于调用图的故障根因定位方法主要是通过采集软件运行时服务间相关调用的执行轨迹构建关系模型,
并利用轨迹差异对比 [5] 或树编辑距离等方法完成故障根因定位.例如,CloudDiag 工具 [30] 利用变异系数 [31] 和基
于稳健主成分分析[32] 识别出故障根本原因.MicroRank 方法[33]采用 PageRank 算法计算每个服务在整个调用图
模型中的中心性,获得每个服务实例的异常分数.然而,由于基于调用图的故障根因定位仅能定位服务级故障,
无法细粒度定位软故障根因,MA 团队[6]通过深入挖掘服务间的依赖关系,构建故障依赖图模型.
基于因果图的故障定位方法通常是利用度量指标与对应服务模型的相关性,通过量化计算不同指标间的
因果关系,大多采用 PC 算法[34]完成因果图建模过程.在此基础上结合随机游走[35–38] 或优先遍历方法[3,7,12,39]访
问因果图,从而确定软件故障的根本原因.此外,Yuan Meng 团队[40] 结合软件故障传播时延构建因果图,通过随机
遍历的方式确定故障指标的根因.
基于影响图的故障定位方法则是通过考虑故障传播,依赖关系和指标波动等对目标系统的影响,分析量化
不同故障表征、故障影响因素和故障根因间的关联关系,进而构建故障影响图.例如,CloudRanger 工具[8]用数据
驱动的影响图代替调用图定位故障的传播模型.MonitorRank 方法[14] 和 MicroHECL 方法[41] 都考虑了基于服务
交互的故障传播问题.MicroRCA[33]和 MicroDiag 方法[42]通过不断采集并检测 SLO 指标判断故障,并将故障与
度量指标相关联.此外,还有一部分故障根因定位方法是通过统计分析方法[43–45] 直接处理微服务监控数据,从而
定位故障根因.
大部分针对微服务软件的根因定位方法都是通过利用服务调用信息,性能指标,微服务部署等数据构建调
用图、因果图和影响图模型,并采用随机游走算法,统计分析算法等进行故障根因定位,最终输出多个可能的根
本原因.可以看出,上述方法严重依赖于故障模型的质量,对于故障模型的全面性和科学性有很高的要求.其中,
调用图模型和因果图模型分别针对服务调用关系和故障因果关系对微服务软件系统进行故障模型构建,但不

李亚晓 等: 一种基于多层次影响图的微服务故障定位方法

2227

能全面且准确的定位服务故障和性能故障的根因位置.而故障影响图模型作为目前较为热门的故障根因定位
模型,能够有效结合调用图和因果图模型的特点,从故障表征、故障影响因素和故障根因的关系出发,忽略复杂
的系统业务逻辑,提高微服务软件故障根因定位的准确性和全面性.

6 结论与展望
在不确定环境下,能够准确定位软件故障是确保大规模微服务软件系统稳定运行的关键因素之一.因此,本
文创新性地提出了 AmazeMap,包括多层次故障影响图建模方法和基于多层次故障影响图的微服务故障定位方
法.AmazeMap 通过关联服务调用层、基础设施层、时序指标层,从时间和空间角度挖掘数据间关联关系,并根
据微服务架构软件实际特点设计了节点构建方法和基于格兰杰因果关系检验方法量化计算指标间的作用关
系,整合上述所有信息构成多层次故障影响图,为故障定位提供模型基础.在多层次故障影响图模型的支持下,
利用动态阈值计算和图中心性方法快速准确地检测和细粒度定位软件故障,从故障链路、故障服务和指标根因
三个层次对故障进行层层筛查,缩小故障定位的范围,减少时间开销,最终从时序度量指标中细粒度定位故障根
因.此外,通过对比验证了本文方法在故障定位的准确性和效率上有一定优势,能够帮助运维人员快速进行故障
定位,提高运维质量.
为进一步提高微服务故障定位的准确性,本文仍存在值得继续探讨的工作:(1)本文后续考虑对跨服务节点
上的度量指标作用关系进行建模,并协调好故障关系建模时间开销和后续故障定位的准确性.(2)当前复杂微服
务软件系统不确定性增强,如何计算故障服务节点和异常分数间的真实关联关系,是未来研究工作的重心.(3)目
前微服务数据集较少,希望能够从开源项目和商业项目中搜集更多的数据集,以验证本文所得出的实验结论是
否具有一般性.
References
[1]

Bogner J, Zimmermann A. Towards integrating microservices with adaptable enterprise architecture. 2016 IEEE 20th International
Enterprise Distributed Object Computing Workshop (EDOCW). 2016: 1–6. doi: 10.1109/EDOCW.2016.7584392.

[2]

Wu L, Bogatinovski J, Nedelkoski S, Tordsson J, Kao O. Performance diagnosis in cloud microservices using deep learning. Hacid
H, Outay F, Paik H, Alloum A, Petrocchi M, Bouadjenek M R, Beheshti A, Liu X, Maaradji A. Service-Oriented Computing –
ICSOC 2020 Workshops. Cham: Springer International Publishing, 2021: 85–96.

[3]

Lin J, Chen P, Zheng Z. Microscope: pinpoint performance issues with causal graphs in micro-service environments. Pahl C,
Vukovic M, Yin J, Yu Q. Service-Oriented Computing. Cham: Springer International Publishing, 2018: 3–20.

[4]

Nandi A, Mandal A, Atreja S, Dasgupta GB, Bhattacharya S. Anomaly detection using program control flow graph mining from
execution logs. Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. New
York, NY, USA: Association for Computing Machinery, 2016: 215–224. doi: 10.1145/2939672.2939712

[5]

WANG Zi-Yong CN-J WANG Tao, ZHANG Wen-Bo, Chun ZUO. Fault diagnosis for microservices with execution trace
monitoring. Journal of Software, 2017, 28(6): 1435.

[6]

Ma S-P, Fan C-Y, Chuang Y, Lee W-T, Lee S-J, Hsueh N-L. Using service dependency graph to analyze and test microservices. 2018
IEEE 42nd Annual Computer Software and Applications Conference (COMPSAC). 2018, 02: 81–86.

[7]

Chen P, Qi Y, Zheng P, Hou D. CauseInfer: automatic and distributed performance diagnosis with hierarchical causality graph in

[8]

Wang P, Xu J, Ma M, Lin W, Pan D, Wang Y, Chen P. CloudRanger: root cause identification for cloud native systems. 2018 18th

large distributed systems. IEEE INFOCOM 2014 - IEEE Conference on Computer Communications. 2014: 1887–1895.
IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing (CCGRID). 2018: 492–502.
[9]

Gill SS, Buyya R. Failure management for reliable cloud computing: a taxonomy, model, and future directions. Computing in
Science & Engineering, 2020, 22(3): 52–63.

[10] Aguilera MK, Chen W, Toueg S. Failure detection and consensus in the crash-recovery model. Distributed Computing, 2000, 13(2):

2228

Journal of Software 软件学报 Vol.X, No.X, X X

99–125.
[11] Langville AN, Meyer CD. A survey of eigenvector methods for web information retrieval. SIAM Review, 2005, 47(1): 135–161.
Doi: 10.1137/S0036144503424786
[12] Chen P, Qi Y, Hou D. CauseInfer: automated end-to-end performance diagnosis with hierarchical causality graph in cloud
environment. IEEE Transactions on Services Computing, 2019, 12(2): 214–230.
[13] Mariani L, Monni C, Pezzé M, Riganelli O, Xin R. Localizing faults in cloud systems. 2018 IEEE 11th International Conference on
Software Testing, Verification and Validation (ICST). 2018: 262–273.
[14] Kim M, Sumbaly R, Shah S. Root cause detection in a service-oriented architecture. Proceedings of the ACM
SIGMETRICS/International Conference on Measurement and Modeling of Computer Systems. New York, NY, USA: Association
for Computing Machinery, 2013: 93–104.
[15] Granger CWJ. Some properties of time series data and their use in econometric model specification. Journal of Econometrics, 1981,
16(1): 121–130. doi:10.1016/0304-4076(81)90079-8
[16] Jia T, Chen P, Yang L, Li Y, Meng F, Xu J. An approach for anomaly diagnosis based on hybrid graph model with logs for
distributed services. 2017 IEEE International Conference on Web Services (ICWS). 2017: 25–32.
[17] Jia T, Yang L, Chen P, Li Y, Meng F, Xu J. LogSed: anomaly diagnosis through mining time-weighted control flow graph in logs.
2017 IEEE 10th International Conference on Cloud Computing (CLOUD). 2017: 447–455. doi: 10.1109/CLOUD.2017.64.
[18] Soldani J, Tamburri DA, Heuvel W-JVD. The pains and gains of microservices: a systematic grey literature review. Journal of
Systems and Software, 2018, 146: 215–232.
[19] Rezende DJ, Mohamed S. Variational inference with normalizing flows. Proceedings of the 32nd International Conference on
International Conference on Machine Learning - Volume 37. Lille, France: JMLR.org, 2015: 1530–1538.
[20] Jin M, Lv A, Zhu Y, Wen Z, Zhong Y, Zhao Z, Wu J, Li H, He H, Chen F. An anomaly detection algorithm for microservice
architecture

based

on

robust

principal

component

analysis.

IEEE

Access,

2020,

8:

226397–226408.

doi:

10.1109/ACCESS.2020.3044610.
[21] Meng L, Ji F, Sun Y, Wang T. Detecting anomalies in microservices with execution trace comparison. Future Generation Computer
Systems, 2021, 116: 291–301. doi: https://doi.org/10.1016/j.future.2020.10.040
[22] Wang T, Zhang W, Xu J, Gu Z. Workflow-aware automatic fault diagnosis for microservice-based applications with statistics. IEEE
Transactions on Network and Service Management, 2020, 17(4): 2350–2363. doi: 10.1109/TNSM.2020.3022028.
[23] Zhou X, Peng X, Xie T, Sun J, Ji C, Liu D, Xiang Q, He C. Latent error prediction and fault localization for microservice
applications by learning from system trace logs. Proceedings of the 2019 27th ACM Joint Meeting on European Software
Engineering Conference and Symposium on the Foundations of Software Engineering. New York, NY, USA: Association for
Computing Machinery, 2019: 683–694.
[24] Zhang P, Gao F, Zhou Y, Rao H-Y, Mao D, Li J. An online real-time anomaly detection method for microservice call chains.
COMPUTER ENGINEERING, 2022, 48: 161–169. doi: 10.19678/j.issn.1000-3428.0063817
[25] Pitakrat T, Okanovic D, Van Hoorn A, Grunske L. An architecture-aware approach to hierarchical online failure prediction. 2016
12th International ACM SIGSOFT Conference on Quality of Software Architectures (QoSA). 2016: 60–69.

Doi:

10.1109/QoSA.2016.16
[26] Pitakrat T, Okanović D, Hoorn A van, Grunske L. Hora: architecture-aware online failure prediction. Journal of Systems and
Software, 2018, 137: 669–685. doi: https://doi.org/10.1016/j.jss.2017.02.041
[27] Zang X, Chen W, Zou J, Zhou S, Lisong H, Ruigang L. A fault diagnosis method for microservices based on multi-factor
self-adaptive heartbeat detection algorithm. 2018 2nd IEEE Conference on Energy Internet and Energy System Integration (EI2).
2018: 1–6.
[28] Wu L, Tordsson J, Elmroth E, Kao O. MicroRCA: root cause localization of performance issues in microservices. NOMS 2020 -

2229

李亚晓 等: 一种基于多层次影响图的微服务故障定位方法

2020 IEEE/IFIP Network Operations and Management Symposium. 2020: 1–9. doi: 10.1109/NOMS47738.2020.9110353.
[29] Wu F-B, Li X-Y, Pu R-Q, Zhang L, Yue H-J. Istio: research on service governance upgrade of microservice architecture. Network
Security Technology and Application, 2022: 1–2.
[30] Mi H, Wang H, Zhou Y, Lyu MR-T, Cai H. Toward fine-grained, unsupervised, scalable performance diagnosis for production cloud
computing systems. IEEE Transactions on Parallel and Distributed Systems, 2013, 24(6): 1245–1255.
[31] Coefficient of variation. The Concise Encyclopedia of Statistics. New York, NY: Springer New York, 2008: 95–96.
[32] Candès EJ, Li X, Ma Y, Wright J. Robust principal component analysis? J. ACM, New York, NY, USA: Association for Computing
Machinery, 2011, 58(3).
[33] Yu G, Chen P, Chen H, Guan Z, Huang Z, Jing L, Weng T, Sun X, Li X. MicroRank: end-to-end latency issue localization with
extended spectrum analysis in microservice environments. Proceedings of the Web Conference 2021. New York, NY, USA:
Association for Computing Machinery, 2021: 3087–3098.
[34] Spirtes P, Glymour C, Scheines R. Causation, prediction, and search. The MIT Press, 2001. doi: 10.1007/978-1-4612-2748-9
[35] Aggarwal P, Gupta A, Mohapatra P, Nagar S, Mandal A, Wang Q, Paradkar A. Localization of operational faults in cloud
applications by mining causal dependencies in logs using golden signals. Hacid H, Outay F, Paik H, Alloum A, Petrocchi M,
Bouadjenek M R, Beheshti A, Liu X, Maaradji A. Service-Oriented Computing – ICSOC 2020 Workshops. Cham: Springer
International Publishing, 2021: 137–149.
[36] Ma M, Lin W, Pan D, Wang P. MS-rank: multi-metric and self-adaptive root cause diagnosis for microservice applications. 2019
IEEE International Conference on Web Services (ICWS). 2019: 60–67.
[37] Ma M, Lin W, Pan D, Wang P. Self-adaptive root cause diagnosis for large-scale microservice architecture. IEEE Transactions on
Services Computing, 2022, 15(3): 1399–1410.
[38] Ma M, Xu J, Wang Y, Chen P, Zhang Z, Wang P. AutoMAP: diagnose your microservice-based web applications automatically.
Proceedings of The Web Conference 2020. New York, NY, USA: Association for Computing Machinery, 2020: 246–258.doi:
10.1145/3366423.3380111
[39] Qiu J, Du Q, Yin K, Zhang S, Qian C. A causality mining and knowledge graph based method of root cause diagnosis for
performance anomaly in cloud applications. Applied Sciences, 2020.
[40] Meng Y, Zhang S, Sun Y, Zhang R, Hu Z, Zhang Y, Jia C, Wang Z, Pei D. Localizing failure root causes in a microservice through
causality inference. 2020 IEEE/ACM 28th International Symposium on Quality of Service (IWQoS). 2020: 1–10.doi:
10.1109/IWQoS49365.2020.9213058.
[41] Liu D, He C, Peng X, Lin F, Zhang C, Gong S, Li Z, Ou J, Wu Z. MicroHECL: high-efficient root cause localization in large-scale
microservice systems. Proceedings of the 43rd International Conference on Software Engineering: Software Engineering in Practice.
Virtual Event, Spain: IEEE Press, 2021: 338–347.
[42] Wu L, Tordsson J, Bogatinovski J, Elmroth E, Kao O. MicroDiag: fine-grained performance diagnosis for microservice systems.
2021

IEEE/ACM

International

Workshop

on

Cloud

Intelligence

(CloudIntelligence).

2021:

31–36.

doi:

10.1109/CloudIntelligence52565.2021.00015.
[43] Shan H, Chen Y, Liu H, Zhang Y, Xiao X, He X, Li M, Ding W. ??-Diagnosis: unsupervised and real-time diagnosis of smallwindow long-tail latency in large-scale microservice platforms. The World Wide Web Conference. New York, NY, USA: Association
for Computing Machinery, 2019: 3215–3222.
[44] Wang L, Zhao N, Chen J, Li P, Zhang W, Sui K. Root-cause metric location for microservice systems via log anomaly detection.
2020 IEEE International Conference on Web Services (ICWS). 2020: 142–150. doi: 10.1109/ICWS49710.2020.00026.
[45] Kaldor J, Mace J, Bejda M, Gao E, Kuropatwa W, O’Neill J, Ong KW, Schaller B, Shan P, Viscomi B, Venkataraman V,
Veeraraghavan K, Song YJ. Canopy: an end-to-end performance tracing and analysis system. Proceedings of the 26th Symposium
on Operating Systems Principles. New York, NY, USA: Association for Computing Machinery, 2017: 34–50.

2230
[46]

https://github.com/microservices-demo/microservices-demo

[47]

https://spring.io/projects/spring-boot

[48]

https://github.com/go-kit/kit

[49]

https://nodejs.org/en

[50]

https://chaos-mesh.org/website-zh/

[51]

https://www.oracle.com/cn/

[52]

https://github.com/redis/redis

[53]

https://www.docker.com/

Journal of Software 软件学报 Vol.X, No.X, X X

附中文参考文献:
[5]

王子勇, 王焘, 张文博, 等. 一种基于执行轨迹监测的微服务故障诊断方法. 软件学报, 2017, 28(06): 1435-1454

[24]

张攀, 高丰, 周逸, 饶涵宇, 毛东, 李静. 一种在线实时微服务调用链异常检测方法. 计算机工程, 2022, 48: 161–169.

[29]

吴封斌,李笑瑜,蒲睿强等.Istio:微服务架构服务治理升级研究[J].网络安全技术与应用,2022,No.259(07):1-2.

[30]

赵建涛, 黄立松. 微服务故障诊断相关技术研究探讨. 网络新媒体技术, 2020, 9: 57–64.


软件学报 ISSN 1000-9825, CODEN RUXUEW
Journal of Software,2024,35(3):11731193 [doi: 10.13328/j.cnki.jos.007077]
©中国科学院软件研究所版权所有.

Apache IoTDB 中的多模态数据编码压缩

E-mail: jos@iscas.ac.cn
http://www.jos.org.cn
Tel: +86-10-62562563



贺文迪 1, 夏天睿 1, 宋韶旭 1,2,3, 黄向东 1,2,3, 王建民 1,2,3
1

(清华大学 软件学院, 北京 100084)

2

(大数据系统软件国家工程研究中心(清华大学), 北京 100084)

3

(北京信息科学与技术国家研究中心(清华大学), 北京 100084)

通信作者: 宋韶旭, E-mail: sxsong@tsinghua.edu.cn

摘

要: 时间序列数据在工业制造、气象、船舶、电力、车辆、金融等领域都有着广泛的应用, 促进了时间序列

数据库管理系统的蓬勃发展. 面对愈加庞大的数据规模和多样的数据模态, 高效的数据存储和管理方式十分关键,
而数据的编码压缩愈发成为一个具有重要意义和价值的问题. 现有的编码方法和相关系统未能充分考虑不同模态
的数据特点, 或者未把一些时序数据的处理方法应用于数据编码问题中. 全面阐述了 Apache IoTDB 时序数据库
系统中的多模态数据编码压缩方法及其系统实现, 特别是面向工业物联网等应用场景. 该编码方法较为全面地考
虑包括时间戳数据、数值数据、布尔值数据、频域数据、文本数据等多个不同模态的数据, 充分挖掘和利用各自
模态数据的特点, 特别是包括时间戳模态中时间戳序列间隔近似的特点等, 进行有针对性的编码方案设计. 同时,
将实际应用场景中可能出现的数据质量问题因素纳入编码算法的考量中. 在多个数据集上的编码算法层面和系统
层面的实验评估和分析, 验证了该编码压缩方法及其系统实现的效果.
关键词: 数据编码; 时间序列数据; 数据库; 工业物联网; 多模态
中图法分类号: TP311
中文引用格式: 贺文迪, 夏天睿, 宋韶旭, 黄向东, 王建民. Apache IoTDB 中的多模态数据编码压缩. 软件学报, 2024,
35(3): 1173–1193. http://www.jos.org.cn/1000-9825/7077.htm
英文引用格式: He WD, Xia TR, Song SX, Huang XD, Wang JM. Multimodal Data Encoding and Compression in Apache IoTDB.
Ruan Jian Xue Bao/Journal of Software, 2024, 35(3): 11731193 (in Chinese). http://www.jos.org.cn/1000-9825/7077.htm

Multimodal Data Encoding and Compression in Apache IoTDB
HE Wen-Di1, XIA Tian-Rui1, SONG Shao-Xu1,2,3, HUANG Xiang-Dong1,2,3, WANG Jian-Min1,2,3
1

(School of Software, Tsinghua University, Beijing 100084, China)

2

(National Engineering Research Center for Big Data Software (Tsinghua University), Beijing 100084, China)

3

(Beijing National Research Center for Information Science and Technology (Tsinghua University), Beijing 100084, China)

Abstract: Time-series data are widely used in industrial manufacturing, meteorology, ships, electric power, vehicles, finance, and other
fields, which promotes the booming development of time-series database management systems. Faced with larger data scales and more
diverse data modalities, efficiently storing and managing the data is very critical, and data encoding and compression become more and
more important and are worth studying. Existing data encoding methods and systems fail to consider the characteristics of data in different
modalities thoroughly, and some methods of time-series data analysis have not been applied to the scenario of data encoding. This study
comprehensively introduces the multimodal data encoding methods and their system implementation in the Apache IoTDB time-series



基金项目: 国家重点研发计划(2021YFB3300500); 国家自然科学基金(62232005, 62021002, 62072265, 92267203); 北京信息
科学与技术国家研究中心青年创新基金(BNR2022RC01011)
本文由“面向多模态数据的新型数据库技术”专题特约编辑彭智勇教授、高云君教授、李国良教授、许建秋教授推荐.
收稿时间: 2023-07-17; 修改时间: 2023-09-05; 采用时间: 2023-10-24; jos 在线出版时间: 2023-11-08
CNKI 网络首发时间: 2023-12-25

1174

软件学报 2024 年第 35 卷第 3 期

database system, especially for the industrial Internet of Things application scenarios. In the proposed encoding methods, data are
comprehensively considered in multiple modals including timestamp data, numerical data, Boolean data, frequency domain data, text data,
etc., and the characteristics of the corresponding modal of data fully are explored and utilized, especially the characteristics of timestamp
intervals approximation in timestamp modality, to carry out targeted data encoding design. At the same time, the data quality issue that
may occur in practical applications has been taken into consideration in the coding algorithm. Experimental evaluation and analysis on the
encoding algorithm level and the system level over multiple datasets validate the effectiveness of the proposed encoding method and its
system implementation
Key words: data encoding; time-series data; database; industrial Internet of Things; multimodal

时间序列数据是某些物理量在不同时间点上的采集值所组成的数据序列. 在当今这个大数据的时代, 人
们在各个应用场景中所面对的数据的规模也越来越庞大. 特别是在工业物联网领域 [1], 从车辆到船舶, 从风
电到核电, 工业传感器每时每刻都在产生着海量的数据. 同时, 数据类型和数据格式也越来越多样化, 例如结
构化关系表、半结构化 XML、非结构化文本、图数据、流数据和时序数据等不同的形式. 越来越多的应用场
景要求数据库系统能够同时高效管理多种不同类型的数据, 因此, 多模态数据的管理与分析成为一个非常具
有现实意义的问题.
伴随着数据模态的日益多样和数据量的不断增加, 数据库系统在数据存储方面面临着更多的挑战 [2]. 于
是, 高效的编码压缩算法就变得非常关键. 如果不能通过一定的方法很好地存储这些数据, 将会造成大量的
空间占用和消耗. 而从另一方面来讲, 编码压缩算法和系统实现上的细小的突破或提升, 都将有助于空间存
储开销的降低, 带来巨大的价值和意义. 因此, 充分挖掘不同数据的规律和特点, 并能利用这些特点对数据进
行编码压缩, 是数据能够高效存储的关键.
针对时序数据的巨大功能需求和广阔应用前景, 以 Apache IoTDB[3]为代表的时序数据库, 为管理时序数
据提供了有力的支撑. Apache IoTDB 时序数据库是一个集成了数据收集、存储、管理与分析功能的物联网时
序数据软件系统, 可以满足工业物联网领域的海量存储、高速读取和复杂分析需求[4]. 特别是在如气象、船舶、
电力、车辆等应用场景中都有着非常广泛的应用, 并且取得了十分不错的效果.
现有的一些数据库系统在多模态数据的编码方法存在如下问题.


针对时间戳模态, 通用的数据编码方法通常把时间戳数据当作一般的数值数据来编码和存储, 未能
充分利用时间戳序列的特点, 并且一些针对时间戳序列的处理方法没有能实际运用到编码算法中,
去借助这些方法来有针对性地解决时间戳序列编码问题; 此外, 因为实际应用场景中时间戳间隔并
不是严格相等的, 而是存在很多波动, 这种波动既导致现有的编码效果不理想, 也对设计时间戳编码
算法带来了一定的挑战.



对于小数模态和频域数据模态, 现有的一些编码方法保留了过高但是在实际中可能并不必要的精度,



对于文本模态, 并非所有的信息都需要按照一般的普通字符进行存储, 特别是日志文件中具有一定

造成了空间上不必要的消耗.
规律的时间戳和数值信息等.
同时, 现有的编码方法未能充分考虑时序数据中可能存在的数据质量问题, 来有针对性地设计包括时间
戳编码在内的数据编码方法. 特别是在工业应用场景中 [5], 传感器采集和传输的时间序列数据, 往往可能存
在包括数据缺失、数据重复、数据延迟在内的数据质量问题, 这些数据质量问题对数据的编码压缩带来了很
多问题和挑战, 现有的编码方法通常未能充分将数据质量问题纳入编码设计的考量中, 导致数据的存储效率
下降. 于是, 如何更好地在编码设计中考虑和处理这些数据质量问题, 具有重要的现实意义.
因此, 现有数据库系统中的编码压缩存在的主要挑战可以被总结为如下几个方面.
(1)

如上文概括的几点, 不同模态的数据特点未被充分挖掘, 或者未将这些特点充分应用于数据编码算
法中, 特别是像时间戳模态数据, 现有的系统缺乏对时间戳序列更有针对性的编码算法, 一些时间
序列数据的处理方法没有被运用到编码问题中.

(2)

实际应用场景中, 时间序列数据存在数据缺失、数据重复、数据延迟等数据质量问题, 这些数据质

贺文迪 等: Apache IoTDB 中的多模态数据编码压缩

1175

量问题严重影响数据编码的效果.
(3)

未能把面向不同模态数据的有针对性的编码压缩算法充分地纳入数据库系统的设计和实现中.

针对上述问题, 我们希望全面地阐述 Apache IoTDB 时序数据库系统的编码方法. 在我们的编码设计中,
主要包括如下编码技术点: 在时间戳模态中, 利用时间戳间隔的特点, 实现对时间戳序列的编码; 在数值模
态中, 基于小数的数值精确度特点, 实现针对小数的编码算法, 并利用某些整数数据波动较小、取值域有限、
重复数值多等特点, 实现针对整数的编码算法; 在布尔值模态中, 利用布尔值有效信息的特点, 实现针对布
尔值的编码; 在频域模态中, 利用频域数据的时频变换和幅值特点, 实现针对频域数据的编码; 在文本模态
中, 提取和保存日志中含有的数值信息, 实现针对日志型文本数据的编码; 同时, 在编码算法中, 将数据缺
失、数据重复、数据延迟等数据质量问题纳入编码算法的设计考量中. 本文的主要贡献有: (1) 充分挖掘不同
模态数据的特点, 将这些特点和相关的方法应用于针对此类模态数据的编码算法设计中, 特别是比如时间戳
模态中时间戳序列具有的特点等, 将相关的时间序列处理方法实际应用于时间戳的编码算法之中; (2) 在编码
算法的设计过程中, 将数据质量问题的因素纳入编码算法的考量中; (3) 我们的编码方法及其系统实现可以有
针对性地考虑不同模态的数据, 将面向多种模态数据的编码压缩算法充分地纳入 Apache IoTDB 数据库系统
的设计和实现中.
本文第 1 节介绍在数据库领域其他系统的数据编码压缩相关工作. 第 2 节主要介绍部分基本概念. 第 3
节详细阐释 Apache IoTDB 中的多模态数据编码压缩方案. 第 4 节阐述 Apache IoTDB 的系统实现. 第 5 节通
过实验验证方法的效果. 最后, 第 6 节总结全文.

1

相关工作
数据的编码压缩是数据库系统存储的一个重要问题, 围绕此问题也产生了很多相关工作. 在国内外的数

据库系统和相关研究中, 有很多不同的编码压缩算法和系统实现方式. 其中: 有些编码算法是有损的, 会在
数值的准确性上造成一些损失; 有些编码压缩方法是无损的, 可以完整地保留全部的原始数值信息. 接下来,
我们将大致概括部分在领域中广泛使用的其他编码压缩方法和数据库系统.
在编码压缩算法层面, 存在很多其他的相关编码算法. 比如, SPRINTZ 编码[6]是一个可以用作整数编码的
算法, 主要包括预测、位压缩、游程编码、熵编码这 4 个步骤: 首先, 通过使用一些预测函数来估计接下来的
数值, 然后对实际值和预测值的差异进行编码, 通常情况下, 这可以起到缩小待编码数值的绝对值的效果;
接下来, 对第 1 步中得到的残差块进行位压缩, 并基于数据块中最大数值的有效位作为编码位宽, 将数据写入
系统中; 此外, 运用游程编码和熵编码来减少冗余, 其中, 游程编码是通过记录连续重复数值的次数来压缩
连续的数值, 而熵编码则是通过对哈夫曼编码形式的字节进行编码来达到压缩的目的. 同时, 还有 RAKE 编
码[7]、哈夫曼编码[8]、Simple8B 编码[9]和其他不同的编码方法[10].
此外, 还有一些其他混合型编码算法, 这些编码算法由多个混合的方法来组成一个综合的编码算法 [11].
例如, 整数形式的 RLBE 编码[12]结合了差分编码、游程编码、斐波那契编码的思路. 它的计算步骤可分为差
分编码、二进制编码、游程编码、斐波那契编码. 具体来说, 首先对原始数据应用差分编码, 并计算每个差分
值的长度; 然后, 进一步运用游程编码, 前几位负责存储二进制字的长度, 后几位是长度码的重复次数的斐
波那契编码码字, 接下来的若干位负责表示具有相同长度的差值的二进制位.
与此同时, 也产生了越来越多的通过使用混合技术或机器学习技术 [13]来解决时序数据编码压缩问题的算
法. 比如: 基于机器学习的两阶段压缩算法 [14] 通过引入一个两阶段模型, 为每个单独的点选择压缩方案, 这
有助于解决时间序列中数据的多样性问题. 该算法由一个两阶段模型压缩的框架构成, 包含了多个主要模式
来对典型的模式进行分类, 并通过定义一些参数来帮助构建子模型压缩方案. 这种方法通过一个具有强化学
习功能的神经网络结构, 来自动调整参数的取值.
在数据库系统层面, 除了 Apache IoTDB 时序数据库之外, 也有许多其他的时序数据库系统. 总的来说,
这些时序数据库区别于一般的关系型数据库, 主要面向物联网或其他时序数据应用场景, 针对时序数据的数

1176

软件学报 2024 年第 35 卷第 3 期

据属性、规模海量、写入负载高等特点进行系统实现, 这些系统在存储方案的设计上既有一定的相似之处, 也
有各自不同的具体设计和实现.
InfluxDB 时序数据库[15]的逻辑存储架构包括数据库、物理量、标签和字段, 这些组件共同构成了数据的
组织和存储方式. 其中: 数据库是指在逻辑层面上, 对数据进行分组和分类; 物理量表示一组数据点, 包含多
个标签和字段; 标签是用于标识和过滤数据的键值对, 用于描述物理量的元数据; 字段是实际存储的数据值,
以键值对的形式记录; 数据保留时间指定了数据会在数据库中留存的时间范围. 存储引擎方面, InfluxDB 使
用 TSM 树结构作为存储引擎, 通过将数据分为多个时间段, 并在每个时间段内创建索引和压缩数据来提供高
效的存储和查询性能. 数据被存储在磁盘上的文件中, 而索引则被存储在内存中. 索引结构方面, InfluxDB 使
用 B+树索引结构来支持快速的数据查询, 这是一种常见的平衡树结构, 索引按照时间序列组织, 根据时间戳
和标签来进行索引.
DolphinDB 时序数据库[16]是一种分布式时序数据库, 内置流式数据处理引擎以及并行和分布式计算的功
能, 并提供分布式文件系统, 支持集群扩展, 是使用 C++编写的. 总体上采用类 HDFS 分布式文件系统, 由名
称节点统一管理元数据, 并自动管理分区数据. 在金融、物联网等多种不同应用领域中, 在数据分析建模、实
时流数据处理、海量传感器数据处理与分析等任务场景中具有一定优势. DolphinDB 的架构拥有多个数据节
点, 在数据库层面不存在领导节点, 每个数据节点拥有本地的存储设备, 相互之间通过 DFS 交互, 从而可全
局优化, 实现共享存储, 让数据均匀地分布在各节点上, 充分地利用集群资源.
TDEngine 时序数据库[17]在数据存储时, 结合物联网的应用场景, 设计了独有的存储和查询引擎以及存储
结构. TDengine 存储的数据包括采集的时序数据、库或表相关的元数据、标签数据 3 个部分: 时序数据存放
于 V 节点里, 由数据、数据头和数据尾组成, 采用一个采集点一张表的模型, 连续存储一个时间段内的数据,
可以通过简单的追加操作, 对单张表进行写入; 标签数据存放于 V 节点里的元文件, 支持增删改查操作; 元数
据存放于 M 节点里, 包含系统节点、用户、数据库、数据表格式等信息, 支持增删改查操作. TDengine 通过
列式存储和专有的压缩算法实现数据压缩,包括不压缩、一阶段压缩、二阶段压缩 3 种形式: 一阶段压缩使用
专有算法压缩; 二阶段压缩是在专有算法压缩的基础上, 再使用通用算法额外压缩一次.
各种不同的编码压缩算法和数据库系统分别具有各自的特点和优势, 也有一定的不足和适用性问题. 因
此, 更重要的是如何针对数据的特点和不同的数据模态, 运用合适的数据编码压缩算法构建合适的系统数据
存储方案. 通过总结其中有益的经验, 分析现有编码方法的主要问题, 有助于对编码压缩算法的进一步研究
进和比较, 进而帮助我们对多模态数据设计合适的编码算法.

2

基本概念

2.1 IoTDB的数据存储形式
在 Apache IoTDB 系统中, 数据通过 TsFile 文件进行存储. 这种文件格式在结构上的设计, 使其能够有效
地适应于时序数据的查询和存储等方面的功能需求 [18]. 如图 1 所示, 图的左半部分展示了数据在 Apache
IoTDB 系统中所具有的树状结构的关系. 可以看到: 从顶部的根目录开始, 自上而下依次为根目录、若干个实
体设备层、物理量层, 从根节点到叶子节点相连命名得到的路径, 如 root.filed1.wf1.device1.status, 即表示一
个时间序列, 可能描述的实际含义是某个区域内的某个系统的某个设备的状态值信息, 不同区域、不同系统、
不同设备的不同物理量的信息共同组成了一个庞大的树状结构. 当然, 这里的具体含义是由用户进行设置的.
图的右半部分展示了 TsFile 的主要结构, 呈现了数据块和数据页面之间的关系, 也反映了数据存储在 Apache
IoTDB 系统中的最基本的存储形式.
在 TsFile 中, 一个 TsFile 会包含多个数据块存储组. 数据块是由一个元信息和若干数据页面组成. 与数据
页面不同, 数据块的大小是可变的. 元信息包括该组数据的数据类型、编码压缩方式等基本信息, 使得系统可
以根据元信息中的内容, 使用相应的编码压缩算法来压缩和解压缩不同的时间序列. 在一个数据块中, 会包
含多个数据页面. 数据页面是在磁盘上存储时间序列数据的基本单位. 一个数据页面中的每个时间序列都是

贺文迪 等: Apache IoTDB 中的多模态数据编码压缩

1177

按时间升序排序的, 一共包括两列: 一列是时间戳列, 另一列是数值列. 并且时间戳和数值是分开存储和编
码压缩的, 因为这样的列式存储方式非常有助于压缩时间序列数据.

图1

Apache IoTDB 系统数据模型结构图

2.2 数据编码
数据编码问题是数据存储中的一个重要问题, 是在保证数据完整情况下, 减小数据存储占用的有利方法.
在数据写入的过程中, 对数据进行编码, 从而减少磁盘空间的占用量, 提高数据的存储效率. 即按照一定的处
理方法和转换形式将数据转换为字节流, 而转换后的字节流占用的空间通常比数据原本占用的空间小, 以此
来达到压缩空间的效果.
与编码相对应的过程是解码. 编码算法可以将输入的原始数据按照某种算法进行编码, 在编码的过程中,
对数据按照一定的方法进行处理和转化, 从而达到节省数据存储空间的作用; 而对应的解码算法可以将经过
压缩后的字节流, 经过对应的还原方法来解码出原始的数据. 而在某些情况下, 还原得到的数据可能与原始
数据不完全相等, 这类编码算法属于有损编码压缩方法; 而对于能够确保还原得到的数据和原始数据相等的,
属于无损编码压缩方法.
在 Apache IoTDB 系统的编码压缩中, 由于 TsFile 文件结构使得时间戳列与数值列是分开单独存储的, 因
此, 我们可以对时间戳列和数值列采用不一样的编码压缩方法. 其中: 对于时间戳列, 考虑到物联网场景下
传感器周期性采集下的数据的特点, 可以使用基于差分或者针对时间戳的编码方法; 对于数值列, 则可以根
据实际数据的特征使用合适的编码方法. 此外, 在 TsFile 中, 可以只在数据页面层面上编码和解码数据, 这样
就不需要为了读取一部分数据而解码整个文件, 从而有利于编码和解码的时间性能.
2.3 数据质量问题
在很多应用场景中, 时间序列数据源源不断地在被收集、存储和分析, 以便于对设备和系统的状态进行
监控和管理. 但是在实际应用中, 特别是在一些工业应用场景中, 采集得到的时间序列数据通常在不同程度
上具有一定程度的数据质量问题. 因为在时序数据从被传感器采集到被存储至时序数据库的过程中, 可能会
出现传感器故障、传输丢包、网络延迟等若干问题. 这些数据质量问题除了会影响对数据的分析以外, 也会
影响到数据编码以及数据存储的性能.
总的来看, 数据质量问题可以主要归纳为数据缺失、数据重复、数据延迟这 3 种情况.
(1)

数据缺失: 出现一个或多个数据点丢失, 这可能是由于某个或者某段数据未能成功采集或者成功传
输等原因导致的.

(2)

数据重复: 在一个数据点周围, 出现一个或多个非常接近的点, 这可能是比如传输中误将同一个数
据传输了多次等因素导致的.

(3)

数据延迟: 一个数据点, 比原本该点预期的时间有所延迟, 这可能是比如网络延迟等因素导致的.

如图 2 的例子所示, 这是数据质量问题的示意图, 3 个子图分别表示数据延迟、数据缺失、数据重复的数
据质量问题, 图中箭头所指的位置是出现数据质量问题的地方.

1178

软件学报 2024 年第 35 卷第 3 期

图2

3

数据质量问题示意图

Apache IoTDB 中的多模态数据编码压缩实现方案
时间序列数据反映了观测对象在不同时间点采集得到的一系列测量值, 随着工业物联网的蓬勃发展和广

泛应用, 各种设备的海量数据被实时采集并通过网络共享, 被用于监测运行状态、突发事件响应、预测变化
规律、预测发展趋势等多样的任务需求中. 这些实际的应用场景, 也正是 Apache IoTDB 时序数据库被广泛应
用和发挥其优势的地方.
面对海量的数据以及多样的数据模态, 数据的存储面临着更大的挑战. 在 Apache IoTDB 中, 多模态数据
主要包括时间戳模态、数值模态、布尔值模态、文本模态、频域模态等. 不同的编码方式, 擅长处理不同模
态的数据. 在 Apache IoTDB 时序数据库中, 有若干种不同的编码压缩算法, 包括 RLE 编码、TS2DIFF 编码、
GORILLA 编码、字典编码、频域编码等. 针对不同模态的数据, 有不同的默认编码方式, 同时也支持用户根
据自己的需要进行自由的选择. 接下来, 我们将分别详细介绍各个模态的数据编码压缩方案.
3.1 时间戳数据模态
时间序列数据由一组时间戳序列和一组数值序列构成. 当进行数据编码压缩时, 在每个时间戳序列和数
值序列中, 分别进行编码压缩. 如图 3 的示意图所示, 针对时序数据中的时间戳数据列进行时间戳编码, 最终
得到占用空间更小的时间戳数据编码字节流.

图3

时间戳编码示意图

现有的一些时间戳编码方法, 比如基于差分的编码方法, 虽然也利用了时间戳序列通常是递增的特点,
但是这种差分的方法一方面对时间戳序列特点利用的不够充分, 另一方面没有考虑在实际应用场景中时序数
据可能存在的数据质量问题. 当数据中存在这些数据质量问题时, 差分的方法可能会造成很多极端的结果,
导致最终的空间性能不佳. 如果能针对其中的时间戳序列去设计一种更有针对性的时间戳序列编码算法, 就
可以有效地降低时间戳序列占用的空间, 进而达到提高整个时间序列数据的空间性能的目的.
首先, 通过对时间戳数据的分析可以发现, 时间戳序列通常具有一定的特点和规律. 特别是一些时间戳
序列的间隔, 因为在工业物联网应用场景中, 数据是通过固定的采集频率获得的, 因此往往呈现出间隔近似
的特点. 一般而言, 严格等间隔的时间戳是指一组时间戳序列前后相邻两个时间戳的差值严格等于一个固定
值. 但是从应用中来看, 这类情况是一种比较理想的情况, 对数据质量有比较高的要求. 实际上, 在工业物联

贺文迪 等: Apache IoTDB 中的多模态数据编码压缩

1179

网领域等实际应用场景中来看, 时间戳序列从整体的规律上来看间隔往往是近似的, 但是会有一些误差和波
动, 并不是严格等于一个固定的值. 同时, 在数据的某些地方, 通常还伴随着数据缺失、数据重复、数据延迟
等数据异常情况. 于是, 我们可以充分利用时间戳序列的特点来设计一种更有针对性的编码方法, 以优化其
存储时的空间性能.
观察如图 4 所示的时间戳序列经过一阶差分后的数值频次直方分布示意图, 可以看到: 该例子中的数据
有一个明显的峰, 绝大多数数值都位于峰的左右, 说明数据的间隔基本固定, 但是并不是严格相等; 同时, 在
峰的两侧有一些频次比较低的数值, 表示数据并非严格等间隔, 而是会存在一些上下的波动; 并且还有一些
数据相隔的较远, 表示数据存在一定程度的数据质量问题. 我们所研究的近似间隔时间戳序列正是这类整体
上具有时间戳间隔近似、但并不严格相等的特点, 并且在某些地方存在一些数据质量问题的时间戳序列.

图4

时间戳序列差值频次直方分布示意图

在方晨光等研究者在 VLDB22 的时间戳修复工作[19]中, 根据时间戳间隔的特性, 提出了一种时间戳数据
修复的算法. 这种算法针对时间戳数据异常情况, 通过动态规划的算法, 进行时间戳的匹配和修复. 受到这种
算法的启发, 我们希望将这种时间戳修复算法中的思路, 应用到时间戳的编码问题中. 这需要我们将数据修
复问题转换为数据编码问题, 将数据修复和数据处理的方法应用于编码方法. 同时, 结合编码中遇到的问题,
比如考虑到编码的时间复杂度, 需要采用尽量简便的判断和设计方法, 以保证编码的时间性能. 接下来, 我们
将详细阐述利用时间戳特点设计本文所提出的针对时间戳数据的编码方法.
时间戳编码算法具体包括如下步骤: 首先对数据进行分块, 即通过指定一个固定的数据块大小, 顺序地
将全体数据划分成大小固定的数据块; 然后, 以每个数据块为单位, 分别在各个数据块上进行时间戳编码.
经过编码后, 每个数据块都可以得到一个二进制字节流, 这时再把这些字节流顺序地连接起来, 即可得到最
终经过时间戳编码后的二进制流. 解码时, 整个字节流会再一一对应的分成多段, 每段对应一个数据块. 通过
元信息和解码算法, 即可还原每个数据块上的原始数据, 然后顺序地将其相连在一起, 即可解码出原始数据.
数据块大小的选取需要面临一个平衡: 如果数据块的大小过小, 那么就会导致数据块的数量过多, 随之
产生和需要保存的元信息也就更多, 这会削弱编码的空间性能; 但是如果数据块的大小过大, 那么就会导致
一个数据块里包含的数据点过多, 有时可能数据组织在一起的规律性就会减弱, 甚至单个的异常就会拖累整
个数据块的性能, 同样也会导致编码的效果不佳. 所以, 数据块大小过大或过小都可能损害空间性能. 而在具
体实现中, 考虑到编码对时间性能的要求, 找到最优的参数将花费大量额外的时间开销. 因此在实际应用中,
Apache IotDB 系统将数据块大小的取值设置为一个默认的参数, 同时也支持用户直接人为地指定一个合适的
参数, 而如果用户未直接指定, 那么系统将使用原有的默认参数.
接下来, 确定时间戳间隔值. 对于一组具有间隔近似特点的时间戳序列, 首先就需要确定出该组数据的
间隔值. 找寻一个合适的时间戳间隔值, 也是接下来时间戳分解的奠基性的一步, 更影响着后续编码的效果.
为了确定出效果最好的时间戳间隔值, 可以采取如下的一种精确的时间戳间隔值计算方法: 首先列出时间戳
间隔的所有可能候选集, 通过计算时间戳前后差值, 然后找到这些差值的最小值和最大值, 那么候选集的范
围就是位于最小值与 2 倍最大值之间的所有值. 然后分别进行计算, 最后选取使得编码位宽最小的时间戳间

1180

软件学报 2024 年第 35 卷第 3 期

隔作为最终结果. 这种计算方法将基于差分的编码作为一种特殊的情况, 由此, 把基于差分的编码自然地统
一起来. 可以从理论角度证明, 这样的方法将会优于或者不弱于基于差分的编码方法.
为适应编码对时间性能的要求, 可以在这一环节中通过近似算法来确定时间戳间隔值. 估计一组间隔近
似的时间戳序列的间隔大小可以采用多种方法, 通俗地来说, 就是尽量缩小搜索的范围, 在尽量不影响最终
效果的前提下, 在候选集中筛选出更小的搜索范围. 更直接地, 其中一种最直观的方法就是利用中位数来估
计时间戳序列的间隔大小. 由此, 我们提出一种近似算法. 近似算法的具体计算步骤是: 对时间戳序列计算
前后相邻两个数值的差值, 然后取差值的中位数, 即可将这个中位数作为时间戳的间隔值. 并且在此之前, 结
合差分值频次直方分布图, 增加对间隔分布情况的判断, 在分布的离散情况达到预设阈值时, 就采用固有的
编码方法. 因为对于时间戳序列来说, 大多数数据的间隔是近似的, 通常围绕着这个间隔值上下小幅浮动, 于
是, 通过计算差值的中位数的方法, 就可以近似地估计出这组时间戳序列的间隔值.
确定间隔值后, 对时间戳进行分解. 具体步骤是, 将一个时间戳分解为间隔值和残差的某种组合的形式.
其实, 时间戳的分解可以有很多种方案, 在这里, 我们给出一种可行的时间戳分解方案, 其具体计算步骤是:
首先, 将时间戳减去上一个时间戳的值后除以间隔值并取整, 得到一组间隔列; 然后计算剩余的残差部分,
作为偏差值, 由此组成偏差列. 对于得到的偏差列和间隔列这两条序列, 可进一步对其进行处理. 由于时间戳
数据的特点, 对于多数数据来说, 间隔列的数值为 1 的频率较高. 于是, 为了更好地提升空间性能, 可以对间
隔列进行进一步处理, 只保留其中数值不是 1 的间隔值, 然后记录非 1 间隔值的位置和非 1 间隔值的数值.
最后, 通过位压缩的方法, 将序列数据转化为字节流. 针对目前处理后的数据序列, 可在数值上进行进一
步编码. 这里采用了位压缩方法的思路 [20], 对各个数据块的各部分数据, 按照各自的有效长度进行编码. 因
为原始的数值转换成二进制字节流后, 由于正数和字节转换的方法, 会导致前面有很多前置的 0. 对一组数据
来说, 假如所有数值对应的二进制字节中的前 m 位均为 0, 那么其实这 m 个比特是没有意义的, 因为它们不表
示任何信息. 因此, 我们只需要找到一组数据中最大的数, 计算它所需的位宽, 作为整组数据的编码位宽.于
是, 通过位压缩的方式, 空间性能得到了更好的优化. 一般形式的位压缩编码位宽的公式为
EncodeWidth( X )  max Width( X i )
1≤i≤n

(1)

值得注意的是: 当一组数据中出现负数时, 这个数转换成字节流后的首位将是 1, 这意味着它没有前置 0,
这将导致整组数据都无法通过上述的方式进行优化. 于是, 我们需要对数据进行一定的处理, 将所有数据都
转化为非负数. 常用的转化方法有 Zigzag 法、减最小值法等: Zigzag 法是指通过一个函数映射, 将负数映射为
正数; 减最小值法是指将一组数据都减去这组数据的最小值, 使数据中不再有负数. 在我们的设计中, 采取
了减最小值法作为位压缩编码的预处理方法.
总结来看, 根据位压缩的编码思路, 对各个数据块的数据按照各自的有效长度进行编码. 经过数值编码
处理后, 将数据变为非负数, 然后统计数据块中所需编码位宽最大的数值, 记录该数值所需的编码位宽为整
个数据块的最大编码位宽. 对该数据块上的所有数据, 以最大编码位宽的位宽值进行编码, 加入结果字节流
中. 时间戳编码的大致算法见算法 1.
算法 1. 时间戳编码算法.
输入: 待编码的时间戳数据 data.
输出: 编码后的时间戳数据字节流 buffer.
1:

block=len(data)/size;

2:

for i1 to block do

3:

tl=data[iblock,(i+1)block];

4:

gl=gap(tl);

5:

flag=isDiff(gl)

6:

if flag==True then

7:

grid=getgrid(tl);

贺文迪 等: Apache IoTDB 中的多模态数据编码压缩

8:

1181

gl=calgrid(tlt,grid);

9:

dl=caldiff(tl,grid);

10:

end;

11:

if flag==True then

12:

buffer.append(bitpacking(gl));

13:

buffer.append(bitpacking(tl));

14:
15:
16:

else
buffer.append(bitpacking(dl));
end;

17: return buffer;
接下来, 我们对时间戳编码算法进行一个大致的复杂度分析. 从空间复杂度的角度来看, 虽然时间戳分
解产生的间隔列和相应的参数需要占用额外的空间, 但是额外空间是有限的, 其复杂度仍限于 O(n); 同时,
在整体上可以带来空间的改善. 并且从实际效果来看, 下文的实验结果表明, 时间戳编码在多个数据集上使
空间性能得到了提升. 时间戳编码的各个步骤, 包括确定间隔值、时间戳分解、位压缩等, 时间复杂度都是
O(n). 因此从时间复杂度的角度来看, 时间戳编码算法的时间复杂度是 O(n). 由于时间戳编码的多个步骤, 综
合来看在时间上需要花费更多, 不过由此换取了空间性能的改善.
3.2 数值数据模态
数值模态是时序数据库中最常见的一种模态, 是最基本的一种数据形式. 更进一步地, 数值模态又可分
为整数和浮点数两种类型. 因为整数和浮点数这两种数据类型本身在存储时就有所不同, 因此它们所对应的
编码压缩方式也稍有差异. Xiao 等研究者在 VLDB22 的时序数据编码工作[21]中, 介绍了不同种类的基本数值
编码压缩算法.
3.2.1

整数编码
整数这一数据类型可以非常方便地通过二进制的形式进行转换, 并且一般可以通过 1 个符号位来表示正

负. 此外, 整数又可分为整型 INT32 类型和长整型 INT64 类型. 对于这两种数据类型, 在编码方式的实现细节
上也会略微有所不同. 比如, 在编码位宽的设计中: 对于 INT32 类型, 基准位宽会以 32 为单位; 而对于长整型
INT64 类型, 基准位宽会以 64 为单位.
对于整数形式的 RLE 游程编码[22], 其具体方法是: 对待编码的整数序列, 统计连续出现的整数的数值和
频次; 然后, 对数值列和频次列分别进行编码压缩. 相应地, 在 RLE 编码的解码时, 根据 RLE 编码的解码方
式, 将数值列和频次列重新还原回整数列, 解码出原始的数据. 这种游程编码的方式, 比较擅长处理具有连续
相同元素数值的数据. RLE 编码的编码位宽的具体计算公式如下.
EncodeWidth( X )  max Width( RLE_Ri )  max Width( RLE_Vi )
1≤i≤n

1≤i≤n

(2)

如图 5 的例子所示, 比如一组数据 1, 1, 1, 1, 6, 4, 4, 4, 因为这组数据依次由 4 个 1、1 个 6、3 个 4 组成, 于
是, 经过游程编码可以将数据压缩为(4,1), (1,6), (3,4), 减少了存储所需的信息. 可以看到: 如果连续重复的数
据越多, 那么游程编码就更能发挥出它的优势; 但是如果数据中连续重复的不多, 那么游程编码的效果就会
变差, 甚至反而会使得空间占用变大.
此外, 整数形式的基于差分的编码也是一类常用的编码方式 [23], 其具体方法是: 通过计算前后相邻两个
时间戳的差值, 然后对差值进行编码, 同时需要记录初始值. 一阶差分的思想比较直观, 存储前后相邻两个时
间戳的差值以及起始值即可. 而二阶差分编码是在一阶差分编码的基础上再做一次差分, 然后再去保存差分
值和相应的初始值信息.
对于整数形式的 TS2DIFF 差分编码, 首先对整数序列做一阶差分, 并记录初始值, 然后对得到的序列统
一再减去最小值, 最后对差分列进行编码压缩, 并保存起始值等信息. 相应地, TS2DIFF 差分编码在解码时,

1182

软件学报 2024 年第 35 卷第 3 期

首先对序列加上最小值, 还原回原始的差分列, 然后结合初始值信息, 将差分列还原回整数列, 解码出原始的
数据.

图5

游程编码示意图

TS2DIFF 编码主要擅于处理数值前后变化范围不大的数据. 如图 6 的例子所示: 如果一组数据是 10, 20,
30, 40, 45, 60, 那么第 1 次前后两两作差得到 10, 10, 10, 5, 15 的差分序列, 其中最小值是 5; 第 2 次减去该最
小值 5 后得到 5, 5, 5, 0, 10 的序列, 同时, 编码时需要记录最开始的起始值 10 和最小值 5. 这种基于差分的编
码算法比较适合于数据波动不太大的序列; 相反, 如果序列本身波动较大, 那么最终的效果可能反而会不太
理想.

图6

TS2DIFF 编码示意图

TS2DIFF 编码的编码位宽的具体计算公式如下.
Diffi=Xi+1Xi
TS 2 DIFFi  Diff i  max Diffi

(3)
(4)

EncodeWidth( X )  max Width(TS 2 DIFFi )

(5)

1≤i≤n

1≤i≤n

整数形式的 Gorrila 编码

[24]

是针对数值存储格式特征设计的一种编码算法, 异或操作 XOR 是该编码方法

的关键. 其具体步骤是: 序列的首值不动, 后面的值是该值与第 1 个值取异或的结果. 如果结果相同, 仅需存
储一个 0; 如果结果不同, 存储异或后的结果. 并且利用游程编码的思想, 进一步压缩首尾的 0 值. 不过, XOR
编码受数据波动影响较大, 如果数据波动较大, 那么编码的效果可能就会比较差, 因为这样异或结果的首尾
将产生大量的 0. 因此, 这种编码方法比较适合编码前后数值比较接近的数据序列, 不适合编码前后数值波动
比较大的数据序列.
3.2.2

浮点数编码
浮点数在进行数据存储时, 通常会根据 IEEE754 标准, 部分区间用于表示指数部分, 部分区间用于表示

尾数部分; 同时, 再通过 1 个符号位来表示浮点数的正负. 此外, 浮点数又可分为单精度 Float 类型和双精度
Double 类型. 对于这两种数据类型, 主要区别是用来表示指数部分的区间和表示尾数部分的区间的长度不同.

贺文迪 等: Apache IoTDB 中的多模态数据编码压缩

1183

Double 类型在这两部分的长度都要大于 Float 类型, 这意味着它将具有更高的准确度; 但同时, 也意味着这将
占用更多的空间. 针对浮点数类型的数据, 一般也可以使用如前文所介绍的一些编码方法进行编码压缩.
为了进一步提升编码的空间性能, 还可以针对浮点数使用如下的编码方法, 即基于精度的幂变换方法.
Liu 等研究者在 VLDB2021 的研究工作中[25], 也曾使用过类似的思路. 由于在很多应用场景中, 往往对浮点数
并不需要非常高的数据精度, 也即很多编码位数实际上是有冗余的, 因此, 如果能适当地舍弃一些编码位数,
就可以在保证基本的浮点数精度的同时, 有效地节省浮点数的存储空间. 当然, 这种编码方法是一种有损的
编码方法, 它会在舍弃的过程中不可逆地丢失掉一部分数据信息, 因此在实际应用时, 也需要将损失的程度
纳入考虑的范围, 使其处于可接受的区间内.
具体来说, 针对浮点数的基于精度的幂变换编码的具体步骤是: 首先, 通过一个手动指定或默认的参数
base 和 n, 分别作为底数和精度的大小; 然后, 对所有的浮点数乘以以 base 为底数、以 n 为指数的幂, 将浮点
数变换成整数; 最后计算整数所需的最大编码位宽, 对变换后得到的整数序列进行编码, 同时保存幂变换的
底数和指数信息. 一般底数的取值为 2 或 10, 通俗地说, 如果底数设置为 10, 精度为 n 就表示需要保留小数
点后第 n 位. 由此, 在指定了合适的参数后, 数据在基于精度的幂变换后将会降低到合适的精度, 同时,可以
有效地节约存储的空间. 基于精度的幂变换编码位宽的具体计算公式如下.
Pi=Xibasen
EncodeWidth( X )  max Width( Pi )
1≤i≤n

(6)
(7)

如图 7 的例子所示, 对于一组原始浮点数序列而言, 如果直接使用普通的浮点数直接编码, 那么每个浮点
数会编码为符号区、指数区、尾数区这 3 个部分. 这种方法虽然最大限度地保留了浮点数的精度, 但是会占
用很大的空间. 而通过基于精度的幂变换编码, 首先将浮点数转换为整数的形式, 然后对得到的整数序列再
进行编码, 所需的编码空间将会大大减少.

图7

基于精度的幂变换编码示意图

在这样的编码方式下, 由于浮点数依据精度乘以幂次后被转换为整数的形式, 因此浮点数的编码方式又
有了更多的选择和组合形式. 很多原本只适用于整数数据的编码方式, 就同样也可以适用于浮点数数据的编
码, 包括游程编码 RLE、基于差分的编码 TS2DIFF 等, 这大大地丰富了浮点数编码的可行方案, 为提升浮点
数存储的空间性能提供了更多的可能.
对于浮点数形式的游程编码: 首先, 根据定义的小数精度 n, 将所有的浮点数统一乘以 10 的 n 次幂, 转换
为整数类型; 然后, 对得到的整数序列, 统计连续出现的整数的数值和频次; 最后, 对数值列和频次列分别进
行编码压缩. 相应地, 在解码时, 通过游程编码的解码方式, 根据数值列和频次列还原回整数列, 最后对所有
的整数统一除以 10 的 n 次幂, 解码出原始的浮点数.
对于浮点数形式的差分编码: 首先, 根据定义的小数精度 n, 同样转换为整数类型, 将所有的浮点数统一
乘以 10 的 n 次幂; 然后, 对得到的整数序列做一阶差分, 再对得到的序列统一减去最小值; 最后, 对差分列进
行编码压缩, 并保存起始值和最小值等信息. 相应地, 在解码时, 通过差分编码的解码方式, 根据差分列、起

1184

软件学报 2024 年第 35 卷第 3 期

始值和最小值, 还原回原来的整数列, 最后对所有的整数统一除以 10 的 n 次幂, 解码出原始的浮点数.
在 IoTDB 数据库的系统实现中, 如果针对浮点数类型的序列采用游程编码或差分编码的方式进行存储,
那么用户可以指定最大精度参数, 作为浮点数小数点后位数, 若用户未指定该参数, 则系统会根据配置文件
中缺省的浮点数精度项进行配置. 但是需要强调的是: 如果采取了基于精度的幂变换的编码方法, 浮点数会
因此损失一些精度, 这可能会对之后的数据分析或其他任务场景带来一些问题. 于是, 在实际应用中需要进
行一个取舍, 在节省空间占用和保留适当的浮点数精度之间选择一个恰当的平衡点.
3.3 布尔值数据模态
布尔值数据是一类特殊的数值, 它们只有 0 和 1 两种取值. 因此, 与其他数值模态的数值不同, 布尔值模
态无须像普通的整数一样存储, 而是可以只需要更少的编码位数来表示. 不过, 在如一般的 Java 布尔型数据
存储模式中, 因为最小单位的原因, 每个布尔型数据仍然需要 1 字节的空间来保存. 但事实上, 1 字节所具有
的 8 个比特位中, 只有 1 个比特位发挥了真正的作用, 另外 7 个比特位的空间未被有效地利用. 因此, 这也为
布尔值的编码留下了减小存储占用的改进空间.
针对布尔型数据的特点, 可以使用位图合并存储的编码方式. 位图是由一系列二进制位组成的数据结构,
其中每一位的值只有 0 或 1. 于是, 位图可以用来紧凑地表示一组布尔类型的值, 每个二进制位代表一个布尔
值. 如图 8 的例子所示, 这种数据结构在存储布尔类型的数据时非常有效, 因为它可以大大减少存储空间的使
用量. 通过这样的编码方式, 每个布尔值数据的编码位宽只需要通过 1 个比特位来保存, 大大地节省了空间.
不过更进一步地, 布尔型数据还可以应用一些其他编码方式.

图8

布尔值编码示意图

对于布尔值的数值, 还可以在此基础上进而使用游程编码的方式进行编码. 这是因为在某些应用场景中,
布尔值模态的数据列不太会出现数值反复波动变换的情况. 并且因为布尔值只有 0 或 1 两种取值, 也即对于
很多情况来说, 布尔值数据量经常会出现连续多个数的值相同的情况. 而游程编码方式恰好擅长处理具有连
续多个相同的值的数据, 因此对于布尔值模态的数据, 将前述布尔值编码的方法进一步结合游程编码方式,
有机会带来空间性能的进一步改善.
3.4 频域数据模态
频域数据和时域数据, 可以通过时频变换算法实现相互转换. 通过对时频变换后的结果取模, 可以得到
频域分量的幅值. 由此就可以得到一组非负数值数列, 数列中的每一项表示所在频域分量的幅值. 王浩宇等
研究者在 VLDB2022 的研究工作[26]中, 对基于时频转换的编码进行了详细阐述.
基于时频转换的编码, 将时域和频域的变换运用到了数据编码之中. 其具体计算步骤是: 首先, 通过
DFT 等时频变换方法将时域数据转换为频域数据. 注意到, 变换后的频域数据具有非常高的精度, 但实际上
某些成分是可以被省略的, 只需要保留部分成分即可. 因此, 可以根据频域分量的绝对值大小, 舍去那些分量
值过低的部分. 如图 9 的例子所示, 在频域图中可以看到: 可以通过设定一个恰当的阈值, 舍去低于此阈值的
分量, 再对保留下来的主要成分进行编码.

贺文迪 等: Apache IoTDB 中的多模态数据编码压缩

图9

1185

频域编码示意图

此外, 通过观察发现: 由于数值的分布是有偏斜的, 这意味着某些数值会明显大于其他数值. 因此, 如果
直接在存储中为所有的数值设定一个相同的位宽将造成空间性能的浪费. 于是, 通过把这些数值从大到小降
序排列, 这样做的目的是使后一个数值所需的位宽一定不会超过前一个数, 进而可以通过统计上一个数值的
前置 0 的方法来判断当前数值的位宽, 由此即可得到编码后的二进制序列, 同时也需记录排序时的索引等信
息. 需要注意的是: 当前得到的时频变换的编码是一种有损的基于时频转换编码算法, 因为在时频变换之后
的分量选取中会造成一定的精度损失, 但好处是带来了空间性能的改善.
在此基础上, 可以进一步得到无损的基于时频转换编码算法. 在有损的基于时频转换编码算法基础上,
计算编码值与原始值的偏差, 额外保存一个偏差值列, 那么在解码时就可以得到原本的数值序列. 同时, 需要
事先指定一个精度参数, 比如小数点后指定位数, 以此来决定需要保留的偏差值列的精度. 在合适的应用场
景下, 这样的偏差往往是有限的, 因此记录偏差值列所需的额外编码空间也将是有限的. 通过这样的方法, 在
时频转换编码的基础上, 以更大的空间为代价, 换来了更高的编码精确性. 通常来说, 这种无损的基于时频转
换编码算法对周期性越强的数据越能发挥出更好的效果.
3.5 文本数据模态
文本模态在存储时一般以字符的形式直接进行存储, 每个字符占用固定的字节位数. 在某些数据中, 比
如姓名、地址、城市等数据的应用场景中, 会大量地使用文本模态的数据形式. 同时, 在系统的日志文件等信
息中, 也会涉及文本模态的存储. 因此, 除了直接使用一般的字符形式进行存储外, 还可以更有针对性地采取
其他针对文本模态的编码方法.
3.5.1

字符型编码
对于文本模态的数据, 可以使用字典编码算法 [27], 其核心思想是, 对文本中出现的重复部分进行替代.

通过这样的替代, 将原本需要重复占用较大空间的字符串, 使用简单的符号和索引去替代, 由此达到节省空
间的效果. 其具体步骤是: 对于出现的重复多次的文本内容, 通过记录一次文本内容, 和同一文本多次出现
的位置, 来表示该文本内容. 这样节省了同一个字符串因为反复出现而需要被反复存储的成本, 特别是当某
些重复字符串的单个存储空间就十分大的情况下. 这种编码方式擅长处理文本中出现一些字符串多次重复的
情况; 而假如每个字符串出现次数都不多, 甚至比如只出现了 1 次, 那么字典编码就无法起到节省空间的效
果, 甚至还会因为额外的索引开销造成空间性能的下降.
3.5.2

日志文本编码
日志文本是一种特殊的文本模态数据. 日志文件通过文本的形式, 记录了系统在运行过程中复杂的状态

信息. 通过观察发现, 日志文件中往往包含了大量的时间戳信息和版本号、IP 地址、状态码等数值信息. 如图
10 的日志文本模态例子所示, 这是一个 IoTDB 运行日志文件, 观察文件可以初步分析出, 该文件的每一条日
志的起始是一个时间戳信息, 用来表示该条数据发生的时间; 在文本中, 存在很多数值信息, 并且观察到其
中有些数值较小, 而有些数值相对较大; 存在部分版本号、地址等其他具有一定范式规律的数值信息.

1186

软件学报 2024 年第 35 卷第 3 期

图 10

日志文本模态数据示意图

作为文本模态的数据, 它们往往都被按照普通意义上的字符来保存. 但事实上, 这些信息使用字符来存
储并不划算. 因为对于一个有 n 位数的数字信息来说, 如果用字符表示, 就需要通过 n 个字符来表示; 但是如
果直接用数值来表示, 就只需要一个数值型数据来保存. 因此, 针对日志型文本数据, Skibiński 等研究者在日
志文本编码的研究工作 [28] 中提出了一种日志文件的编码方法. 这种方法的主要思路是, 提取出其中的数值
信息.
在日志文本模态中出现的不同类型的数值信息中, 对于时间戳信息来说, 日志文件中的时间戳信息构成
一组时间戳序列. 并且因为日志文件中的时间戳信息通常来说是随时间递增的, 因此可以再针对得到的序列
进一步编码, 由此得到文本模态的时间戳信息的编码结果. 对于 IP 地址信息, 考虑到 IP 地址信息的数据特点,
通常由若干个数值和点组成. 于是, 可以首先对 IP 地址按照点分开, 得到若干个数值, 再对数值进行编码. 对
于其他数值信息, 可以按照一般的数值编码方式来存储, 由此得到文本模态的其他数值信息的编码结果.
此外, 考虑到将日志文件中的部分数值信息从原来的文本形式转变为数值形式, 其产生的空间性能的提
升, 对相对较大的数和相对较小的数来说可能是不同的. 一般而言, 相对较大的数由此带来的提升会更大, 而
相对较小的数由此带来的提升会较小甚至是会有负面的效果. 因此, 在做文本模特的数值信息的编码时, 可
以首先对数值大小做一个判断, 对于超过某一阈值的数值来说, 应采用此编码方法; 否则就不采用此编码方
法, 而是继续按照原有的字符串形式进行存储.

4

系统实现
在本节中, 我们将详细介绍 Apache IoTDB 的系统实现. IoTDB 引擎是系统的核心部分, 包含一个存储引

擎, 可以管理数以百万计的时间序列, 支持每秒写入数以百万计的数据点; 包含一个经过优化的查询引擎,
可以在短时间内得到数万亿数据点的查询结果; 以及包含一个适应物联网应用场景特点的 LSM 日志结构化
合并树, 负责在写密集型的工作负载中处理延迟到达的数据, 其中, 对于延迟时间较短的点, 数据将首先被缓
存在 MemTable 中, 然后按时间排序刷新到磁盘持久化为 TsFile 文件.
MemTable 的持久化过程, 即是数据从内存刷写入磁盘的流程. 每个 TsFile 处理器负责维护一个处于工作
状态的 MemTable, 并维护一个刷写 MemTable 队列来处理处于待持久化状态的 MemTable. 最终, MemTable
会被逐一地刷写到 TsFile 中, 以实现数据的持久化. 当我们在磁盘上刷写一个 MemTable 时, 首先, MemTable
会被附加到一个处于开放状态的 TsFile 中, 当这个处于开放状态的 TsFile 的大小超过某一预设的阈值或者达
到一定时间期限时, 该 Tsfile 文件将被关闭, 它的数据将会被刷写到文件的末尾. 接着, 一个新的处于开放状
态的 TsFile 将会被创建, 用来继续刷写后续的 MemTable.
持久化的过程包括如下几个步骤: 首先, 在每次写数据后, 若满足以下 3 个条件中的 1 个, 将会触发持久

贺文迪 等: Apache IoTDB 中的多模态数据编码压缩

1187

化, 然后通过调用 TsFile 刷写方案来执行持久化操作: 工作 MemTable 的刷写标志字段被设置为真; 或者工作
MemTable 占用的内存总量超过某一预设阈值; 或者工作 MemTable 中每条序列的平均数据点数量超过某一预
设阈值, 该条件是为了防止内存中数据块数据点过多造成查询时间过长. 此外, 也可使用定时持久化功能, 对
数据定时的执行持久化.
整个流程通过刷写管理器进行管理. 一个 TsFile 处理器可能对应多个需要持久化的 MemTable, 但同一时
刻, 每个 TsFile 处理器最多只能执行 1 个持久化任务, 以防止对同一个 TsFile 进行并发写入. 通过注册 TsFile
处理器方法, 来注册需要持久化的 MemTable 所对应的 TsFile 处理器, 包括两个注册源: 一个是 TsFile 处理器,
在需要刷写或关闭时注册自己; 另一个是持久化子线程刷写线程, 在一个 MemTable 的持久化任务结束后, 再
次注册其对应的 TsFile 处理器, 来检查是否仍存在其他需要持久化的 MemTable.
如图 11 所示, 持久化过程采用流水线的方式, 一共分为 3 个阶段、两个任务队列. 流水线的 3 个阶段主
要包括: (1) 排序阶段, 负责给每个物理量对应的数据块排序, 将数据按照时间戳升序排列; (2) 编码阶段, 负
责给每个数据块进行编码, 将数据编码成字节流, 这正是我们所述的编码方法在系统中出现的位置; (3) I/O 阶
段, 负责将完成编码的数据块, 持久化到磁盘的 TsFile 文件上.
此外, 通过两个任务队列负责进行线程间交互.


第 1 个任务队列是编码任务队列, 负责从排序线程到编码线程, 包括如下子任务: 刷写存储组 IO 任
务启动模块, 负责开始一个数据块存储组的持久化; 数据块存储组 IO 任务结束模块, 负责结束一个
数据块存储组的持久化.



第 2 个任务队列是 IO 任务队列, 负责从编码线程到 IO 线程, 包括如下子任务: 数据块存储组 IO 任
务启动模块, 负责开始一个数据块存储组的持久化; 数据块写入器, 负责将一个数据块持久化到磁盘
上; 数据块存储组 IO 任务结束模块, 负责结束一个数据块存储组的持久化.

图 11

5

系统实现示意图

实验分析
在本节中, 我们将着重针对编码压缩后的空间性能: 一方面是在算法层面, 验证和分析针对不同模态数

据的编码算法的效果; 另一方面是在系统层面, 对 Apache IoTDB 系统与其他时序数据库系统的整体空间性能
进行对比. 实验的环境配置是: 一台 CPU 为 Intel(R) Core(TM) i7-8565U CPU@1.80 GHz, 内存 8.0 GB 的电脑.
5.1 实验设计
在实验数据集方面, 我们选取若干个时间序列数据集进行实验, 主要包括如表 1 所示的不同类型的混合
数据集, 这些数据集的规模各不相同, 部分来自真实的应用场景, 部分来自人工组成, 涵盖如交通、能源、地
理等应用领域. 其中, 某些时间戳数据来自数据采集时的时间信息, 某些数值数据和频域数据主要来自采集
的数据, 某些布尔型数据主要来自数据中的状态信息, 某些日志文本数据来自数据库的运行日志..

1188

软件学报 2024 年第 35 卷第 3 期
表1
数据集名称
Inst
Est
Gol
BD
Text1
Text2
Text3
Text4
Earthquake
Fuel
Metro
Transport
DW
GS
HX
LT

数据行数
1 048 576
488 641
377 487
5 380
7 218
5 428
6 214
1 694
75 809
131 747
7 281
2 049
10 108
7 385
8 354
12 673

实验数据集

时间戳
√
√
√
√
√
√
√
√
√
√
√
√
√
√
√
√

数值








√
√
√
√
√
√
√
√

布尔值
√
√
√
√




√
√
√
√





文本




√
√
√
√









频域












√
√
√
√

5.2 实验结果
首先在编码算法层面, 对于不同数据模态的数据, 围绕编码算法进行实验, 以验证和分析算法在空间性
能上的效果. 同时, 调整采用的实验数据规模, 以验证算法在各个数据规模下性能的稳定性.
混合模态 1 实验使用以时间戳模态数据和布尔型模态数据组成的数据集进行编码算法空间性能实验, 记
录待编码的原始数据大小和经过编码后的数据大小, 比较经过编码后空间性能是否得到提升; 同时, 通过改
变数据集 20%100%不等的使用比例来调整数据的规模, 以验证编码算法在不同数据规模上是否均可取得良
好的效果. 如图 12 实验结果显示: 在不同数据规模下的各个数据集上, 混合模态 1 实验在空间性能均取得了
大幅提升.

(a) Inst 数据集

(b) Est 数据集

(c) Gol 数据集

(d) BDt 数据集

图 12

混合模态 1 编码算法实验结果图

贺文迪 等: Apache IoTDB 中的多模态数据编码压缩

1189

混合模态 2 实验使用以时间戳模态数据和日志文本模态数据组成的数据集进行编码算法空间性能实验,
记录待编码的原始日志文本数据的大小和经过编码后的数据大小, 比较经过编码后空间性能是否得到提升;
同时, 通过改变数据集 20%100%不等的使用比例来调整数据的规模, 以验证编码算法在不同数据规模上是
否均可取得良好的效果. 如图 13 实验结果显示, 在不同数据规模下的各个数据集上, 混合模态 2 实验在空间
性能均取得了小幅提升.

(a) Text1 数据集

(b) Text2 数据集

(c) Text3 数据集

(d) Text4 数据集

图 13

混合模态 2 编码算法实验结果图

混合模态 3 实验使用以时间戳模态数据、数值模态数据和布尔值模态数据组成的数据集进行编码算法空
间性能实验, 记录待编码的原始时间戳列数据的大小和经过编码后的数据大小, 比较经过编码后空间性能是
否得到提升; 同时, 通过改变数据集 20%100%不等的使用比例来改调整数据的规模, 以验证编码算法在不
同数据规模上是否均可取得良好的效果. 如图 14 实验结果显示, 在不同数据规模下的各个数据集上, 混合模
态 3 实验的空间性能均取得了明显提升.
混合模态 4 实验使用以时间戳模态数据、数值模态数据和频域模态数据组成的数据集进行编码算法空间
性能实验. 对于频域模态数据, 使用无损的基于时频变换的编码算法. 记录待编码的原始数值列数据的大小
和经过编码后的数据大小, 比较经过编码后空间性能是否得到提升. 同时, 通过改变数据集 20%100%不等
的使用比例来改调整数据的规模, 以验证编码算法在不同数据规模上是否均可取得良好的效果. 如图 15 实验
结果显示: 在不同数据规模下的各个数据集上, 混合模态 4 实验在空间性能均取得了明显提升.
更进一步地, 在编码算法层面之外, 我们还进行了系统层面的实验对比. 比较数据集文件经过 IoTDB 系
统和其他不同系统的存储方案后, 最终实际文件存储的空间大小的对比. 这有助于在系统层面, 比较和评估
数据库系统在数据存储时的整体性能, 综合了数据库系统的编码压缩算法、数据存储方案设计、具体系统实
现等多方面的因素.

1190

软件学报 2024 年第 35 卷第 3 期

(a) Earthquake 数据集

(b) Fuel 数据集

(c) Metro 数据集

(d) Transport 数据集

图 14

混合模态 3 编码算法实验结果图

(a) DW 数据集

(b) GS 数据集

(c) HX 数据集

(d) LT 数据集

图 15

混合模态 4 编码算法实验结果图

贺文迪 等: Apache IoTDB 中的多模态数据编码压缩

1191

在数据库系统实验中, 选取包括 InfluxDB、TDEngine、DolphinDB 在内的数据库系统, 进行系统层面的
空间性能对比实验. 记录写入前的原始数据集文件大小和写入数据库系统后得到的最终文件大小, 比较经过
不同数据库系统的存储方案后, 空间性能的差异. 如图 16 数据库系统实验结果图显示: Apache IoTDB 数据库
系统的空间压缩比结果在各个数据集上均取得了领先, 优于 InfluxDB、TDEngine、DolphinDB 数据库系统. 这
说明 Apache IoTDB 的编码算法及其存储系统实现, 在空间性能上更加具有优势. 不过也注意到: 在某些数据
集上, 部分数据出现了压缩比大于 1 的情况, 这可能与该数据集的数据特点和该系统采用的编码方法有关.

图 16

6

数据库系统实验结果图

总结和展望
本文全面阐述了 Apache IoTDB 时序数据库系统针对多模态数据的编码方法. 围绕时间序列数据编码问

题, 特别是针对工业物联网等应用场景, 对时间戳数据、数值数据、布尔值数据、频域数据、文本数据等多
种不同的数据模态, 通过对其进行深入的观察和分析, 挖掘不同模态数据的规律和特点.
我们的编码方法可以充分地利用不同模态数据的特点, 特别是包括时间戳数据的特点等, 进行有针对性
的编码设计, 以实现更好的空间性能. 此外, 针对时序数据中存在的数据质量问题, 特别是实际应用场景中可
能出现的数据缺失、数据延迟、数据重复等情况, 将其纳入编码的考虑中. 我们的系统比较全面地将针对不
同模态数据的编码算法纳入系统存储方案的实现中, 在若干个不同模态的数据集上, 通过在编码算法层面的
实验以及数据库系统层面的实验, 验证了其空间性能上的效果.
在本文工作的基础上, 也存在一些未来值得进一步研究和完善的工作. 未来将进一步扩展数据模态的类
别, 针对不同模态设计和完善有针对性的编码压缩算法. 此外, 进一步利用数据模态之间的关联, 包括利用某
一模态的数据对另一模态的数据进行预测等, 充分地挖掘和利用模态之间的关系, 并将其进一步运用于
Apache IoTDB 时序数据库的多模态数据编码压缩中.
References:
[1]

Wang HT, Wang ZC, Chen F, et al. Research on industrial big data application based on time series database. Heavy Machinery,
2020(4): 1721 (in Chinese with English abstract).

[2]

Wang MQ, Wei K, Jiang CY. New challenges in time series data processing in industrial Internet of Things. Information and
Communications Technology and Policy, 2019(5): 49 (in Chinese with English abstract).

1192

[3]

软件学报 2024 年第 35 卷第 3 期

Wang C, Qiao JL, Huang XD, et al. Apache IoTDB: A time series database for IoT applications. Proc. of the ACM on Management
of Data, 2023, 1(2): Article No. 195.

[4]

Wang C, Huang XD, Qiao JL, et al. Apache IoTDB: Time-series database for Internet of Things. Proc. of the VLDB Endowment,
2020, 13(12): 29012904.

[5]

Zhang C, Tang Z, Li KL, et al. A polishing robot force control system based on time series data in industrial Internet of Things.
ACM Trans. on Internet Technology, 2021, 21(2): 122.

[6]

Blalock DW, Madden S, Guttag JV. Sprintz: Time series compression for the Internet of Things. Proc. of the ACM on Interactive
Mobile Wearable and Ubiquitous Technologies, 2018, 2(3): Article 93.

[7]

Campobello G, Segreto A, Zanafi S, et al. RAKE: A simple and efficient lossless compression algorithm for the Internet of Things.
In: Proc. of the European Signal Processing Conf. 2017.

[8]

Huffman D. A method for the construction of minimum-redundancy codes. Proc. of the IRE, 1952, 40(9): 10981101.

[9]

Vo NA, Alistair M. Index compression using 64-bit words. Software Practice and Experience, 2010, 40(2): 131147.

[10]

Chen HM, Li J, Mohapatra P. RACE: Time series compression with rate adaptivity and error bound for sensor networks. In: Proc.
of the IEEE Int’l Conf. on Mobile Ad-Hoc & Sensor Systems. IEEE, 2004.

[11]

Deepu CJ, Heng CH, Lian Y. A hybrid data compression scheme for power reduction in wireless sensors for IoT. IEEE Trans. on
Biomedical Circuits and Systems, 2017, 11(2): 245254.

[12]

Spiegel J, Wira P, Hermann G. A comparative experimental study of lossless compression algorithms for enhancing energy
efficiency in smart meters. In: Proc. of the 16th IEEE Int’l Conf. on Industrial Informatics (INDIN 2018). Porto: IEEE, 2018.
447452.

[13]

Azar J, Makhoul A, Barhamgi M, et al. An energy efficient IoT data compression approach for edge machine learning. Future
Generation Computer Systems, 2019, 96: 168175.

[14]

Yu XY, Peng YQ, Li FF, et al. Two-level data compression using machine learning in time series database. In: Proc. of the 36th
IEEE Int’l Conf. on Data Engineering (ICDE). IEEE, 2020.

[15]

2023. https://docs.influxdata.com/influxdb/clustered/

[16]

2023. https://gitee.com/dolphindb/Tutorials_CN/tree/master

[17]

2023. https://docs.taosdata.com/

[18]

2023. https://iotdb.apache.org/

[19]

Fang CG, Song SX, Mei YN. On repairing timestamps for regular interval time series. Proc. of the VLDB Endowment, 2022, 15(9):

[20]

Nandivada VK, Barik R. Improved bitwidth-aware variable packing. ACM Trans. on Architecture & Code Optimization, 2013,

18481860.
10(3): 122.
[21]

Xiao JZ, Huang YX, Hu CY, et al. Time series data encoding for efficient storage: A comparative analysis in apache IoTDB. Proc.
of the VLDB Endowment, 2022, 15(10): 21482160.

[22]
[23]

Golomb SW. Run-length encodings (Corresp.). IEEE Trans. on Information Theory, 1966, 12(3): 399401.
Song B, Xiao LM, Qin GJ, et al. A deduplication algorithm based on data similarity and delta encoding. In: Proc. of the GeoSpatial
Knowledge and Intelligence 4th Int’l Conf. on GeoInformatics in Resource Management and Sustainable Ecosystem (GRMSE
2016). 2016. 245253.

[24]

Pelkonen T, Franklin S, Cavallaro P, et al. Gorilla: A fast, scalable, in-memory time series database. Proc. of the VLDB
Endowment, 2015, 8(12): 18161827.

[25]

Liu CW, Jiang H, Paparrizos J, et al. Decomposed bounded floats for fast compression and queries. Proc. of the VLDB Endowment,
2021, 14(11): 25862598.

[26]

Wang HY, Song SX. Frequency domain data encoding in apache IoTDB. Proc. of the VLDB Endowment, 2022, 16(2): 282290.

[27]

Welch TA. A technique for high-performance data compression. Computer, 1984, 17(6): 819.

[28]

Skininski P, Swacha J. Fast and efficient log file compression. In: Proc. of the Communications of the 11th East European Conf. on
Advances in Databases and Information Systems. 2007.

贺文迪 等: Apache IoTDB 中的多模态数据编码压缩

1193

附中文参考文献:
[1]

王红涛, 王志超, 陈峰, 等. 基于时序数据库的工业大数据应用研究. 重型机械, 2020(4): 1721.

[2]

王妙琼, 魏凯, 姜春宇. 工业互联网中时序数据处理面临的新挑战. 信息通信技术与政策, 2019(5): 49.

贺文迪(1999－), 男, 硕士生, CCF 学生

黄向东(1989－), 男, 博士, 助理研究员,

会员, 主要研究领域为时序数据库.

CCF 专业会员, 主要研究领域为工业数
据管理, 分布式存储系统.

夏天睿(2002－), 男, 本科生, 主要研究

王 建 民 (1968－), 男 , 博 士 , 教 授 , 博 士

领域为时序数据库.

生导师, CCF 高级会员, 主要研究领域为
数据库, 工作流, 大数据与知识工程.

宋 韶 旭 (1981－ ), 男 , 博 士 , 副 教 授 , 博
士生导师, CCF 专业会员, 主要研究领域
为数据库, 数据质量, 时序数据清理, 大
数据集成.


软件学报 ISSN 1000-9825, CODEN RUXUEW
Journal of Software [doi: 10.13328/j.cnki.jos.007102]
©中国科学院软件研究所版权所有.

E-mail: jos@iscas.ac.cn
http://www.jos.org.cn
Tel: +86-10-62562563

*

AutoConfig: 面向深度学习编译优化的自动配置机制
张洪滨 1,2, 周旭林 1,2, 邢明杰 2, 武延军 2, 赵 琛 2
1

(中国科学院大学, 北京 100049)

2

(中国科学院 软件研究所, 北京 100190)

通信作者: 武延军, E-mail: yanjun@iscas.ac.cn

摘

要: 随着深度学习模型和硬件架构的快速发展, 深度学习编译器已经被广泛应用. 目前, 深度学习模型的编译

优化和调优的方法主要依赖基于高性能算子库的手动调优和基于搜索的自动调优策略. 然而, 面对多变的目标算
子和多种硬件平台的适配需求, 高性能算子库往往需要为各种架构进行多次重复实现. 此外, 现有的自动调优方案
也面临着搜索开销大和缺乏可解释性的挑战. 为了解决上述问题, 提出 AutoConfig, 一种面向深度学习编译优化的
自动配置机制. 针对不同的深度学习计算负载和特定的硬件平台, AutoConfig 可以构建具备可解释性的优化算法
分析模型, 采用静态信息提取和动态开销测量的方法进行综合分析, 并基于分析结果利用可配置的代码生成技术
自动完成算法选择和调优. AutoConfig 创新性地将优化分析模型与可配置的代码生成策略相结合, 不仅能保证性
能加速效果, 还能减少重复开发的开销, 同时可以简化调优过程. 在此基础上, 进一步将 AutoConfig 集成到深度学
习编译器 Buddy Compiler 中, 对矩阵乘法和卷积的多种优化算法建立分析模型, 并将自动配置的代码生成策略应
用在多种 SIMD 硬件平台上进行评估. 实验结果可验证 AutoConfig 在代码生成策略中完成参数配置和算法选择的
有效性. 与经过手动或自动优化的代码相比, 由 AutoConfig 生成的代码可达到相似的执行性能, 并且无需承担手动
调优的重复实现开销和自动调优的搜索开销.
关键词: 深度学习编译器; 编译优化; 代码生成; 自动配置机制
中图法分类号: TP314
中文引用格式: 张洪滨, 周旭林, 邢明杰, 武延军, 赵琛. AutoConfig: 面向深度学习编译优化的自动配置机制. 软件学报. http://
www.jos.org.cn/1000-9825/7102.htm
英文引用格式: Zhang HB, Zhou XL, Xing MJ, Wu YJ, Zhao C. AutoConfig: Automatic Configuration Mechanism for Deep Learning
Compilation Optimization. Ruan Jian Xue Bao/Journal of Software (in Chinese). http://www.jos.org.cn/1000-9825/7102.htm

AutoConfig: Automatic Configuration Mechanism for Deep Learning Compilation Optimization
ZHANG Hong-Bin1,2, ZHOU Xu-Lin1,2, XING Ming-Jie2, WU Yan-Jun2, ZHAO Chen2
1

(University of Chinese Academy of Sciences, Beijing 100049, China)

2

(Institute of Software, Chinese Academy of Sciences, Beijing 100190, China)

Abstract: Deep learning compilers have been widely employed with the rapid development of deep learning models and hardware
architectures. At present, the compilation optimization and tuning methods of deep learning models mainly rely on high-performance
operator libraries and automatic compiler tuning. However, facing various target operators and adaptation requirements of several hardware
platforms, high-performance operator libraries should conduct multiple implementations for different architectures. Additionally, existing
auto-tuning schemes face challenges in substantial search overheads and interpretability. To this end, this study proposes AutoConfig, an
automatic configuration mechanism for deep learning compilation optimization. Targeting different deep learning workloads and multiple

*

基金项目: 国家重点研发计划 (2022YFB4401402)
本文由“编译技术与编译器设计”专题特约编辑冯晓兵研究员、郝丹教授、高耀清博士、左志强副教授推荐.
收稿时间: 2023-09-11; 修改时间: 2023-10-30; 采用时间: 2023-12-14; jos 在线出版时间: 2024-01-05

软件学报 ****年第**卷第**期

2

hardware platforms, AutoConfig builds interpretable performance analysis models, conducts a thorough assessment via static information
extraction and dynamic overhead measurement, and automates algorithm selection and configuration tuning for code generation. The key
innovation of this study is combining the optimization analysis model and a configurable code generation strategy, which ensures a
performance acceleration effect and reduces repeated development overheads with the simplified tuning process. Furthermore, this study
integrates AutoConfig into a deep learning compiler Buddy Compiler, builds analysis models for convolution and matrix multiplication
optimization, and evaluates the optimization on multiple SIMD hardware platforms. Experimental results indicate that AutoConfig
effectively completes parameter configuration and algorithm selection in the code generation strategy. Additionally, compared with the
codes by manual or automatic optimization, the codes generated by AutoConfig can yield comparable performance without both the
repeated manual tuning implementation overheads and auto-tuning search overheads.
Key words: deep learning compiler; compilation optimization; code generation; automatic configuration mechanism

深度学习在处理图像 [1]、语音 [2]、文本 [3]等各个场景中得到了越来越广泛的应用. 为了取得更优异的表现性
能, 深度学习模型的设计日益精妙和复杂. 举例而言, 早期经典的图像分类模型 AlexNet[1]有 600 万参数, 如今流行
的大语言模型 ChatGPT[4]的参数量已经达到 1 750 亿, 参数量的飞跃式增长为深度学习模型在硬件设备中高效落
地带来重要挑战.
目前支持深度学习模型落地主要有两类技术, 一类是通过开发高性能算子库来加速模型 [5−7]. 这些算子库通常
使用编译器内建函数或汇编级别指令来手动实现算子的核心逻辑, 可以在特定的计算场景中提供充分的算子优化
机会. 然而要在大部分计算场景中都获得较好的性能, 需要在算子库中为不同规模的算子输入重复编写程序逻辑,
这个开发流程费时费力. 而且程序逻辑中对优化算法和优化参数的选择也与硬件平台强相关, 这使得一套程序逻
辑无法被不同的硬件平台直接复用, 因此这类技术依赖于手动调优, 通常不具有跨平台的普适性.
另一类技术是构建支持深度学习模型的编译优化框架. 这些框架的优化流程与处理高级语言程序的传统编译
器 (如 LLVM[8]) 有共通之处, 它们将深度学习模型的落地视作一个编译优化的过程: 首先采用某种高层抽象的语
言来表示模型, 然后将高层抽象逐步下降到硬件级别中间表示, 同时在下降的不同阶段结合多种策略对模型进行
优化. 与调用高性能算子库相比, 深度学习编译优化技术能够从全局的角度发掘模型的优化空间, 发挥跨硬件平台
做自动调优的潜力. 然而现有深度学习编译优化框架的自动调优方式通常基于搜索策略. 当面对程序逻辑复杂且
具有大量调优参数的算法时, 庞大的搜索空间会导致调优过程的开销变得难以承受, 而且参数选取的过程也不具
备可解释性.
针对上述问题, 本文提出了 AutoConfig, 一种面向深度学习编译优化的自动配置机制. AutoConfig 由重写模
式 (rewrite pattern) 和优化分析模型两个部分组成. 重写模式提供可配置的代码生成策略, 以驱动优化分析模型进
行参数选取和算法选择. 优化分析模型可以根据硬件平台信息确定调优参数范围, 并使用基准程序从计算、访存
和特殊指令开销等多个方面量化模型中的权重系数, 以指导优化算法的选择. 通过综合考虑计算负载特点和硬件
特征等要素, AutoConfig 能够完成调优参数选取、优化算法选择和自动代码生成, 只需一次优化实现就能适配多
种硬件平台. 本文将 AutoConfig 集成在深度学习编译器 Buddy Compiler[9]中, 并针对实际的深度学习计算负载场
景, 在多种 SIMD 平台上对 AutoConfig 的参数配置及优化算法的选择进行探究, 并将基于优化分析模型的代码生
成策略与 TVM[10]的自动调优策略进行跨平台的性能比较. 实验结果验证了 AutoConfig 作为全新的深度学习编译
优化开发范式的实用性和有效性.
本文第 1 节介绍相关工作. 第 2 节介绍基础知识, 包括深度学习计算负载及其优化方法、深度学习编译优化
的基本流程和 AutoConfig 的实现所依托的编译基础设施. 第 3 节介绍 AutoConfig 的设计思路、模块组成、使用
方式和生态集成. 第 4 节介绍 AutoConfig 中的优化分析模型, 并以矩阵乘法和卷积为例, 针对不同的优化算法进
行建模和分析. 第 5 节介绍 AutoConfig 中的静态信息提取和动态开销测量策略. 第 6 节呈现实验设计、结果和分
析. 第 7 节给出结论.

1 相关工作
在深度学习模型诞生的早期, 人们通过开发高性能算子库来加速模型的推理过程. cuDNN[5]是一个由 NVIDIA

张洪滨 等: AutoConfig: 面向深度学习编译优化的自动配置机制

3

开发的深度学习加速库, 它提供了一系列深度学习计算负载实现, 采用启发式方法为负载实现选择最佳的算法, 并
使用 CUDA 技术来加速, 可以在 NVIDIA GPU 上实现高效的深度学习推理. oneDNN[6]是由 Intel 开发的用于深度
神经网络的数学内核库, 它可以发挥 Intel 处理器体系结构和多核处理器的优势, 实现深度学习计算负载的高性能
计算. MIOpen[7]是由 AMD 开发的数学和科学计算库, 专为深度学习在 AMD 设备上的并行计算而设计. 高性能算
子库的开发通常针对某类特殊硬件, 实现上一般依赖手工调优. 这种调优方式尽管在某些具体场景能取得良好的
效果, 但是不具备跨平台迁移的能力.
近年来, 随着摩尔定律趋于终结, 为了应对深度学习领域对高算力的需求, 越来越多计算场景引入了 FPGA、
CGRA、TPU[11]等深度神经网络加速器进行加速. 为了将深度学习模型的计算模式高效映射到多样的硬件设备上,
基于深度学习编译优化的框架设计成为了研究重点. TVM 是目前工业界中较为常用的深度学习编译框架, 它将模
型优化的过程视为对张量程序的变换, 可借助 autoTVM[12]、Ansor[13]等基于代价模型和启发式算法的搜索策略优
化算子执行逻辑. 但是在面对程序逻辑复杂、调优参数众多的算法时, 庞大的搜索空间会使调优开销变得不可承
受, 且调优方式也不具备理论上的可解释性. 除了以搜索策略为主的编译框架, 还有其他的工作如 ROLLER[14]和
SparTA[15]. ROLLER 可基于硬件平台完成张量程序的分块调优, 并采用递归算法实现代码生成, SparTA 则提出了
一种充分利用稀疏性的端到端模型优化通路. 这两种框架通过针对某种特定优化方式或利用数据特性来从新的视
角发掘优化的机会, 但对于其他通常情况则存在应用的局限.

2 基础知识
本节首先以卷积和矩阵乘法为例介绍深度学习模型计算负载及其优化方法, 然后概述深度学习编译优化的基
本流程, 最后介绍 AutoConfig 的实现所依托的编译基础设施 MLIR[16].
2.1 深度学习计算负载及其优化方法
矩阵乘法及其优化方法

2.1.1

近年来, 随着大语言模型的迅速发展, Transformer[17]已成为自然语言处理领域中极具影响力的经典模型. 注意
力机制是 Transformer 中的关键模块, 它能够有效处理长序列和目标序列中的依赖关系. 该模块通过计算序列中不
同位置之间的相似度, 并为每个位置分配一个权重系数, 从而实现对不同位置的加权关注. 在具体实现中, 需要首
先计算查询向量和键向量之间的相似度, 然后将相似度与值向量相乘并加权求和, 最终得到加权后的值向量. 因为
此类计算模式在注意力机制中广泛存在, 所以优化矩阵乘法是高效执行 Transformer 模型的关键 [18].
矩阵乘法的一种在算法级别的优化方法是 Strassen 方法 [19] , 它通过减少乘法操作的次数来降低计算的理论
时间复杂度. 在工业界中, 矩阵乘法的优化策略主要集中在提高计算效率和减少资源消耗上, 包括利用多线程和多
核处理器做并行计算、将大矩阵划分成小块做分块计算、优化矩阵在内存中的排布来减少缓存未命中次数等高
性能优化方法.
卷积层及其优化方法

2.1.2

卷积神经网络是在计算机视觉领域中广泛使用的模型. 它通过卷积模块来提取图像的局部特征. 卷积层是卷
积模块的核心, 它通过在输入数据上滑动一个固定大小的卷积核来提取图像的局部信息. 卷积层中可以包含多个
卷积核, 每个卷积核可以提取不同的特征. 在深度学习模型中, 卷积还有空洞卷积、深度可分离卷积等不同的变
种, 它们根据输入的张量表示也可分为“NHWC”和“NCHW”等不同的数据排列方式. 不同的卷积类型、输入尺寸
和卷积核大小、数据排列方式会影响计算负载优化的策略.
卷积的优化方法有很多种, 包括从矩阵乘法的视角看待卷积 [20] 并应用 Strassen 方法做优化、基于 FFT 和
Winagrad 的变换方法 [21]和 Im2Col 算法 [22]等. 其中 Im2Col 算法是目前最通用的优化算法之一, 其主要思想是先将
卷积核重叠部分展开和变换输入矩阵, 然后利用高性能的矩阵乘法实现来加速卷积运算, 如图 1(a) 所示. 需要注意
的是变换的开销由于涉及到访存, 因此不可忽略.

软件学报 ****年第**卷第**期

4

矩

⑤ 将计算结果存储到
输出矩阵对应元素
① 将卷积核元素广播为向量
阵
矩
出
核
输
积
卷

阵

出

输

阵
换
矩
变
入
入
输
插
后
量
换
向
变
列
将 矩阵
② 入
输
后

算

计

乘

相

阵

矩
行
执
核
④
积
卷

核
积 阵
卷
个 或矩
多
或 向量
个
一 积核 ① 将卷积核重叠部分
③ 卷
成
展开为列向量
组

阵
矩

入

④

融

合

乘

加

计

算

③ 将输出矩阵中
对应元素加载为向量

输

② 将输入矩阵中
对应元素加载为向量

阵

矩

入

输

(b) 基于广播操作的优化算法

(a) lm2Col+GEMM优化算法

图1

卷积优化算法图示

Broadcast 向量化算法

2.1.3

在执行卷积和矩阵乘法等计算负载时, 可以通过改变原始算法的操作顺序来换取更好的访存模式, 以为向
量化提供充分的空间. 本文采用了一种基于广播操作的优化算法 (Broadcast 算法), 它的计算逻辑是从一个矩阵
中顺序取出向量, 从另一个矩阵中顺序取出标量并使用广播操作构造出向量, 从而达到向量化计算的效果, 如
图 1(b) 所示. 在具体实现时, Broadcast 算法会将需要跨行的矩阵计算转换为访存模式更友好的多次迭代单行
计算, 并专注于对内层循环做向量化. 本节的后续内容将基于该算法和其他算法采用优化分析模型进行建模、
比较和验证.
2.2 深度学习编译优化的基本流程
深度学习编译优化的基本流程如图 2(a) 所示. 首先, 来自深度学习模型中的计算负载会通过框架前端引入成
为各种中间表示. 接着, 在中间表示的各个层级会进行编译优化, 优化后的结果随后被送到编译器和工具链中完成
硬件代码生成. 在这个过程中, 中间表示层级是优化和调优的关键部分 [23].
现有的优化和调优策略一般分为编译器的自动调优和调用手动优化高性能库两个大类, 二者的特性决定了它
们各自适合不同的场景. 自动调优在循环优化上有出色的效果, 在同类硬件上可以复用基础设施避免重复开发, 但
是由于缺乏可解释性, 这种优化和调优方法在新型硬件架构上需要准确的代价模型和耗时的搜索调优过程才能发
挥作用. 手动优化能够最大化地利用硬件特性, 可以达到极致性能表现, 但往往需要使用平台特定的编程模型或接
口, 即使同类硬件也需要重复开发优化, 造成大量的工程和维护开销.
2.3 编译基础设施 MLIR
MLIR[16]是一种现代编译基础设施, 它致力于提供模块化、可复用、可扩展的多层中间表示. 目前, MLIR 已
经逐渐形成一个编译生态, 尤其是在深度学习编译优化领域, 许多前端框架和后端硬件提供了对 MLIR 的编译支
持 [24−26]. MLIR 的特点在于其所抽象出的多层中间表示. 每一层中间表示称作方言 (Dialect), 这是 MLIR 扩展机制
的核心概念. MLIR 提供一系列内置的核心方言, 包括高层级面向领域计算的方言 (TOSA Dialect, Linalg Dialect).
循环变换级别的方言 (Transform Dialect, Affine Dialect, SCF Dialect). 硬件抽象级别的方言 (MemRef Dialect,
Vector Dialect). 特定后端级别的方言 (LLVM Dialect, GPU Dialect). 在每一种方言内可以定义操作 (operation)、类
型 (type)、属性 (attribute), 来提供特定抽象表示的语义.
MLIR 提供了强大的扩展机制和自动化能力, 用户可以使用声明式的方法灵活地扩展出自定义方言. 所有的
内置方言均可与自定义方言兼容, 并且可以复用所有的内置编译下降通路与优化. 这种扩展能力还使得 MLIR 支
持与其他已有的编译器对接, 便于支持更多的后端和硬件平台.

张洪滨 等: AutoConfig: 面向深度学习编译优化的自动配置机制

5

AutoConfig 编译优化自动配置机制

深度学习框架

承接深度学习模型的 MLIR 方言

深度学习计算负载中间表示

Linalg Dialect/TOSA Dialect

AutoConfig 目标
- 一次优化实现适配多种硬件平台
- 配置优化算法的最佳代码生成策略
- 选择最佳优化算法进行代码生成

TVM Relay IR 中间表示, TorchDynamo 图表示, MLIR Linalg/TOSA 方言等

自动调优

手动优化

自动配置

•

代价模型

•

高性能算子库

•

可配置的代码生成

•

搜索空间

•

平台特定 Intrinsic

•

优化分析模型

•

搜索策略

•

高性能编程接口

•

硬件信息收集策略

优化算法 + 核心 MLIR 方言
- Broadcast 算法
- Im2Col 算法
- Winograd 算法

- MemRef 方言
- Affine 方言
- Vector 方言

可配置的代码生成重写模式
- 粗颗粒度重写高层算子
- 优化算法的可配置代码生成策略
- 根据优化分析模型自动配置代码生成

... ...

... ...

优化分析模型
- 总开销建模为计算 + 访存 + 特定开销
- 硬件信息提取结果指导参数配置范围
- 根据算法特性构建动态测量程序集合

LLVM IR+汇编代码生成

LLVM IR ｜ 特定硬件工具链 ｜ 模拟器 ｜ 运行时环境
目标硬件平台

硬件平台：CPU 高性能扩展 ｜ GPGPU ｜ 领域特定加速器

(a) AutoConfig 机制的概述

深度学习模型

多级中间表示编译工具

M LIR- LLVM 翻译器

LLVM 系统编译器

目标文件

forward.mlir

buddy-opt

buddy-translate

buddy-llc

forward.o

Populate Pattern
Fetch Operation

Register

Register

{Analysis Model->Rewrite Pattern}

M

ƛ(vs)specInsi Analyze

Model=ƛ(vs)FPO+ƛ(vs)DM+

可配置的代码生成重写模式

维护分析模型和重写模式之间的映射关系

Implement

基于静态信息提取和动态开销测量的优化分析模型
Algorithm Analysis：算法特性分析
Dynamic Cost：动态测量开销
Configure Range：参数可配置范围

i+1

基于 Broadcast 的卷积优化算法
Broadcast 指令｜FMA 计算｜访存开销
硬件信息的静态提取

程序开销的动态测量

Registers,Cache…

Ld/St,FMA,Broadcast…

基于 Img2Col 变换的卷积优化算法
Img2Col 变换｜矩阵乘法算子｜访存开销

void BroadcastPattern (Operation *op,
ConversionPatternRewriter &rw,int64_t vs) {
... ...
// Configure Vector Type.
auto vectorTy = mlir :: VectorType::get ({vs}, f32);
rw.create<vector::BroadcastOp>(..., vectorTy, ...);
... ...
}
void Img2ColPattern (Operation *op,
ConversionPatternRewriter &rw, int64_t vs) {
... ...
// Configure Vector Type.
auto vectorTy = mlir :: VectorType :: get ({vs}, f32);
rw. create<affine :: AffineVectorLoadOp>> (..., vectorTy, ...);
... ...
rw. create<affine :: AffineVectorStoreOp> (..., vectorTy, ...);
... ...
}

(b) AutoConfig 机制的设计、实现与集成

图2

AutoConfig 的概述、设计、实现与集成

3 AutoConfig 概述
本节概述 AutoConfig 机制的设计思路、模块组成、使用方式和生态集成.
3.1 AutoConfig 的设计思路
深度学习编译器间的差异主要体现在优化方式和调优策略的设计方面. 如图 2(a) 所示, 目前领域内广泛使用
如下两种优化和调优策略: (1) 计算与调度分离的编译优化方式, 结合编译器的自动调优机制; (2) 图级别编译器生
成特定优化算子, 结合手动调优的高性能算子库. 前者通过提供优化与调优编程接口, 将调优的复杂度交给编译器
的使用者; 后者通过使用硬件平台特定的编程模型来编写高性能库, 将调优的复杂度交给高性能库的编写者.
本文提出的 AutoConfig 是一种新的面向深度学习计算负载的编译优化和调优方法, 其核心设计思路是编写
可配置的代码生成重写模式, 结合优化分析模型自动配置调优过程, 将调优的复杂度交给编译器的设计者和开发
者. 这种设计思路可以利用编译器桥接软硬件的天然优势, 获取计算负载的语义信息和计算平台硬件信息, 实现可
配置的编译优化和调优机制. 如图 2(a) 所示, AutoConfig 采用优化分析模型自动配置调优策略, 并选择合适的优化
算法进行代码生成, 有效地权衡了多种优化算法和调优策略. 相比自动调优和手动优化, AutoConfig 的设计能够达
成一次优化实现适配多种硬件平台的目标, 并且具有性能可解释、可迁移的特性.
可配置的代码生成重写模式是 AutoConfig 优化方法的基础. 通过采用粗颗粒度重写的优化策略, AutoConfig

软件学报 ****年第**卷第**期

6

可以在保障优化通用性和高性能的同时, 将优化算法实现为可配置的编译优化. 这种策略集成到编译工具链中, 使
得不同目标硬件平台的代码生成可以进行灵活的配置. 此外, 重写模式的配置过程依赖于优化分析模型, 它可以在
计算负载的编译时评估计算开销, 并根据此评估结果自动选择并配置最优的代码生成策略.
优化分析模型是 AutoConfig 调优方法的基础. 该模型对优化算法的总体计算开销进行建模, 将其分解为计算、
访存和特殊指令的开销总和. 编译优化的开发者在实现代码生成重写模式时, 也需配套构建参数化的优化分析模
型, 并根据硬件信息确定可配置参数取值范围. 在编译过程中, 优化分析模型通过具体的参数配置和动态测量的程
序特性来确定总开销. 各优化算法对应的预测总开销是选择代码生成策略的关键依据, 以确保最终的编译优化结
果能够最小化计算开销.
值得强调的是, AutoConfig 中的各模块构成了可以服务于任意编译优化和调优的基础设施, 并具有良好的可
扩展性. 在当前领域特定计算和体系结构发展的黄金时代, 各领域计算负载到多种硬件平台的编译和优化需求日
益增长. AutoConfig 中的硬件信息收集策略可以针对特定硬件进行定制, 用户可根据需要扩展默认提供的动态测
量程序集合, 以及划定给定程序的子集. 无论是优化分析模型的定义还是配置规则的注册, AutoConfig 都提供了通
用的基础设施, 以满足灵活多变的调优需求. AutoConfig 的设计能够帮助开发者解耦编译优化的开发、分析、调
优的过程, 并为更精细的分工提供了可能性. 这种设计使得开发者可以专注于特定子模块进行深入开发, 并通过复
用现有组件降低开发成本.
3.2 AutoConfig 的模块组成
AutoConfig 的架构由驱动器、可配置的代码生成重写模式和优化分析模型这 3 大核心组件构成, 它们共同协
作为编译优化算法服务. 其中, 优化分析模型抽象出优化算法的特征, 而代码生成重写模式负责表达算法的优化逻
辑. 如图 2(b) 所示, 分析模型与重写模式之间存在直接的映射关系, 由 AutoConfig 驱动器维护这一映射, 并确保其
无缝集成到深度学习编译工具链中.
AutoConfig 驱动器是核心控制模块, 负责调用各模块接口进行协同工作, 并且对接深度学习编译工具链. 当深
度学习编译工具执行搭载 AutoConfig 的编译优化 Pass 时, AutoConfig 驱动器在所有已注册的分析模型中筛选出
总开销最低的一个. 随后, 它利用选定的分析模型和相对应的重写模式来生成优化代码.
优化分析模型融合了静态信息提取与动态开销测量这两大环节, 并结合编译优化开发者对优化算法的特征分
析, 最终形成一个综合的评估函数. 模型的分析是一个动静结合的过程: 静态信息主要来源于对优化算法及硬件平
台的特性提取; 动态信息来自于算法特征在目标硬件平台上的执行开销. 如图 2(b) 所示, 对优化算法的静态分析用
来确定计算次数、访存频率和特殊指令的发生次数, 而硬件信息的静态提取用于明确配置项取值范围. 动态开销测
量模块根据这一配置范围获取具体测量结果作为各部分开销的权重. 硬件信息的精确提取和分析对于明确可配置
的范围至关重要, 这不仅可以减少动态测量的开销, 也能够高效地选择配置项. 总的来说, 优化分析模型的设计与评
估是 AutoConfig 准确性的关键, 本文第 4 节将详尽阐述以向量计算尺寸为调优点的优化分析模型构建过程.
代码生成重写模式是对编译优化算法逻辑的直接实现. 在代码生成方面, AutoConfig 采用 MLIR 作为中间表
示, 并利用 MLIR 基础设施提供的各层级操作接口实现重写模式. AutoConfig 要求在这些重写模式中嵌入可配置
的参数, 它们的设置依据来自前述优化分析模型的结果. AutoConfig 的设计思路强调将这些可配置参数与特定的
重写模式相绑定, 而非与整体的编译优化 Pass 绑定, 这样的设计提升了调优的精细度和灵活性, 使得每一次优化
都可以根据特定的需求和条件进行个性化调整与配置.
3.3 AutoConfig 的使用方式
AutoConfig 旨在服务于编译优化的开发者, 提供了一系列接口以辅助优化和调优的开发. 区别于使用固定策
略开发编译优化, AutoConfig 要求开发者在创建重写模式时明确定义可配置项及其可能的取值范围. 此外, 用户需
要针对优化算法构造分析规则和相应的动态测量程序集合. 这些分析规则、动态测量程序集合和可配置项取值范
围共同构成优化分析模型. 针对每种计算负载的不同优化方式, 需要独立定义相应的分析模型和重写模式,
AutoConfig 驱动器负责将它们配对并完成注册. 集成到 MLIR 优化 Pass 中的 AutoConfig 驱动器, 在触发时根据计

张洪滨 等: AutoConfig: 面向深度学习编译优化的自动配置机制

7

算负载和硬件平台信息评估每一组分析模型的开销. 随后 AutoConfig 开始执行自动配置, 它通过整合计算负载的
尺寸信息、分析模型和硬件信息, 全面评估所有可能的算法与配置选择, 从而选择出最佳策略进行优化代码生成.
如算法 1 所示, 本文以可配置参数 vs 为例展示针对计算负载 op 的两种编译优化算法的自动配置和代码生成
过程. 首先, 为可配置参数 vs 设置可选的配置数值. 这里可以采用统一的配置集合以适应所有硬件后端, 或者根据
各种硬件特性定制专门的配置集合. 例如, 对于 x86 的 AVX512 硬件平台, 可选配置集合可以是{64, 128, 256}, 而
对于 Arm Neon 硬件平台, 则可以是{16, 32, 64}. 在选择了特定的配置集合后, 可以配置动态测量程序子集以适应
所需的开销函数, 以此来降低配置成本.
进一步地, 开发者需针对不同优化算法定义相应的重写模式和分析模型. 这些重写模式以 MLIR 为中间表示,
利用其多层级的 MLIR 方言来丰富语义表达. 优化分析模型结合分析规则、动态测量程序集合以及可配置参数的
取值范围, 共同定义了编译优化的配置空间以供编译优化过程中的搜索和决策使用. 在用户接口方面, 本文提出的
方法使用 rewritePattern 函数实现可配置的重写模式, 并使用 lambda 表达式来约定优化分析模型. 其中, rewritePattern
函数接受目标操作以及可配置参数, 并在函数体内使用各个操作的构造函数实现编译优化的代码生成逻辑. 同时,
为了构建分析模型, lambda 表达式通过其捕获列表传递动态测量程序集合, 通过参数列表传递可选配置数值集合.
该模型在 lambda 函数体内部完成构建, 详细的构建过程将在本文的第 4 节中展开讨论.
在定义了多种优化算法的重写模式和分析模型之后, 开发者需要初始化一个 AutoConfig 对象, 这一对象作为
驱动器, 负责将每对重写模式和分析模型进行注册. AutoConfig 对象的 populate 函数负责为每个重写模式选择最
优的配置项, 然后对不同的重写模式进行比较和评估, 最终选择出执行开销最小的重写模式和最佳配置项, 将其整
合进编译优化 Pass 及其工具链.
算法 1. AutoConfig 机制的使用.
( )
1. function MatchAndRewrite op

2.

// 以可配置参数 vs 为例, 设置可选配置数值

3.

configRange ← setRange (16, 32, 64, 128, 256)

4.

// 设置开销函数子集, 默认使用开销函数全集

5.

costSet ← setCostSet(BroadcastCost, FMACost, MemoryCost, . . .)

6.

// 定义优化算法重写模式 1

7.

pat1 ← rewritePattern1(op, . . .)

8.

// 分析优化算法构造分析规则 1

9.

rule1 ← func(op, . . .)

10.

// 初始化分析模型 1

11.

model1 ← initAnalyModel(rule1, costSet, configRange)

12.

// 定义优化算法重写模式 2

13.

pat2 ← rewritePattern2(op, . . .)

14.

// 分析优化算法构造分析规则 2

15.

rule2 ← func(op, ...)

16.

// 初始化分析模型 2

17.

model2 ← initAnalyModel(rule2, costSet, configRange)

18.

// 初始化 AutoConfig 对象

19.

autoConfig ← initAutoConfig ()

20.

// 将分析模型 1 和重写模式 1 注册到 AutoConfig 对象中

21.

autoConfig.register(model1, pat1)

22.

// 将分析模型 2 和重写模式 2 注册到 AutoConfig 对象中

软件学报 ****年第**卷第**期

8

23.

autoConfig.register(model2, pat2)

24.

// AutoConfig 对象选择合适的算法和配置进行代码生成

25.

autoConfig.populate ()

26. end function
3.4 AutoConfig 的生态集成
本文将 AutoConfig 集成到了 Buddy Compiler 中, 展示了 AutoConfig 对深度学习编译器的适配能力及其在优
化深度学习计算任务方面的调优能力. Buddy Compiler 是一个基于 MLIR 的特定领域编译器, 它致力于打造面向
深度学习领域的软硬件协同编译生态. 如图 2(b) 所示, Buddy Compiler 中的多级中间表示编译工具 buddy-opt 会
调用 AutoConfig 的各个模块对关键的深度学习操作进行分析和配置, 随后选择适当的代码生成策略进行编译优
化, 并将代码编译下降后翻译至 LLVM IR, 最终生成针对目标硬件的汇编代码. Buddy Compiler 支持面向 CPU SIMD/
Vector、GPU、加速器的代码生成, 通过集成 AutoConfig 的优化与调优方法, 它能够实现面向多种硬件后端的性
能迁移和优化复用, 从而打造出高效通用的编译通路.
除了在 Buddy Compiler 中的集成之外, AutoConfig 更重要的贡献是为 MLIR 生态提供了新的调优方法和思
路. MLIR 是深度学习编译器的主流生态之一, MLIR 上游提供 PDL 和 Transform 方言作为基础对高层中间表示进
行调度和优化, 例如循环分块、顺序调整、向量化等. 基于 MLIR 的深度学习编译器 IREE 除了采用上游提供的
基础方言之外还通过自定义方言进行调度, 并且使用强大的运行时环境实现多种平台的部署. 不同于使用特定方
言进行调优和调度, AutoConfig 的创新点在于使用多层级的方言 (例如 Vector 方言, Affine 方言等) 编写可配置的
优化算法重写模式, 并为优化算法建立分析模型, 搭配硬件信息收集策略提供的开销函数, 可以在编译时确定优化
重写模式的配置参数, 从而实现面向特定计算负载和硬件的代码生成. 同时, AutoConfig 与 MLIR 生态中的其他调
优方式并不冲突, 使用上游调度方言或者自定义方言的方式也可以集成 AutoConfig 使得其调优过程具备可解释
性和可预测性.

4 优化分析模型
AutoConfig 的可解释性源于其优化分析模型. 该分析模型可以对优化算法的计算特征进行建模, 从而近似预
测其执行开销. 本节提出一种向量尺寸可配置的优化分析模型建模方法, 可以在编译时根据高层中间表示中计算
负载的输入尺寸和 SIMD 硬件平台特征来预测计算负载优化算法的期望开销, 从而有效指导 AutoConfig 进行代
码生成. 值得注意的是, AutoConfig 并不耦合与某个特定的优化分析模型, 而是开放出可注册的接口, 用户可以按
需注册自己的优化分析模型.
4.1 模型总体描述
在深度学习编译优化中, 对优化算法的选择以及算法内的参数配置都会对性能产生影响. 这些影响主要体现
在计算开销、访存开销和特殊指令执行开销 3 个方面. 一般来说, 优化算法与朴素算法相比具有更低的计算开销,
但是由于对算法的优化通常会引入额外的内存排布操作和特殊指令, 这会带来额外的访存开销和特殊指令执行开
销. 设 CostAll 为一个优化算法的理论总开销, 则有:
CostAll = CostArith + CostMemory + CostSpecIns

(1)

其中, CostArith 表示计算开销, 在 SIMD 浮点计算场景中主要指融合乘加操作 (FMA) 的执行开销. CostMemory 表示
访存开销, 主要指访问内存、缓存和寄存器组等存储模块所需的开销. CostSpecIns 则涵盖了除了计算指令和访存指
令以外其他特殊指令的执行开销. 不同的优化算法通常会引入不同的特殊指令, 比如 Broadcast 向量化算法会引入
广播操作指令, 针对卷积优化的 Im2Col 算法会引入维度变换指令等. 朴素算法一般不引入特殊指令, 它的理论总
开销只由计算开销和访存开销构成.
为了衡量这些开销, 设优化算法中包含的浮点运算指令个数 (floating point operations) 为 FPO , 内存访问指令

张洪滨 等: AutoConfig: 面向深度学习编译优化的自动配置机制

9

个数 (data movement) 为 DM , 所包含的特殊指令种数为 M , 且第 i 条特殊指令的个数为 NumSpecInsi , 则上述公式可
进一步描述为:
CostAll = λArith FPO + λMemory DM +

M
∑

λSpecInsi NumSpecInsi

(2)

i=1

其中, λArith 、 λMemory 和 λSpecIns 分别表示计算开销、访存开销和特殊指令开销的重要性系数. 以计算开销为例, 由于
它的大小与浮点运算指令个数成正比, 因此 λArith 的含义就是单位运算指令开销. 重要性系数与优化算法的参数配
置和执行平台紧密相关, 优化算法的参数调优越有效, 越能充分发挥执行平台的硬件特点, 则重要性系数越小. 在
实际优化中, 该系数将通过第 5.2 节的动态测量流程确定.
基于上述对一个优化算法理论总开销的建模方式, 进一步提出优化加速比的概念, 以综合比较不同优化算法
所带来的性能差异, 并作为 AutoConfig 选择最优算法的依据. 优化加速比的定义是: 相对于某个基准算法, 在相同
的编译优化流程和硬件执行环境下, 优化算法的理论总开销与基准算法的比值. 具体而言, 对于优化算法 A 和优化
算法 B 而言, A 相较于 B 的优化加速比可表示为:
SpeedUpAll =

A
CostAll
B
CostAll

(3)

在优化分析时, 还可以在计算、访存、特殊指令等不同方面定义局部优化加速比 SpeedUpArith 、 SpeedUpMemory
和 SpeedUpSpecIns , 它们能够帮助分析不同算法在不同方面的优劣情况. 以计算方面为例, 当 A 和 B 两种算法的计算
开销重要性系数保持一致时, 其局部优化加速比可表示为:
A
CostArith
λArith FPOA FPOA
SpeedUpArith =
=
=
B
CostArith λArith FPOB FPOB

(4)

此时通过分别统计两种算法的浮点运算指令个数, 就可以求得两者在计算方面的局部优化加速比. 第 4.2 节
会借助该指标来探究不同算法在计算、访存方面的优化表现.
4.2 算法特性分析
标量算法与向量算法的特性分析

4.2.1

矩阵乘法是一种典型的深度学习计算负载. 设两个输入矩阵分别为 M × K 的矩阵 A 和 K × N 的矩阵 B, 输出
矩阵为 M × N 的矩阵 C, 下面基于优化分析模型, 对朴素的标量算法和向量化的 Broadcast 算法做建模分析.
标量算法采用 3 层嵌套循环完成计算逻辑, 总的循环次数为 MNK . 在每一次循环中, 会进行一次 FMA 操作,
因此其浮点运算指令个数为 MNK . 此外在每一次循环中, 默认会涉及到对矩阵 A、B 和 C 各一次的读操作和对
矩阵 C 的一次写操作, 但是在实际实现时可以把对矩阵 C 的读和写操作提取到长度为 K 的内层循环外面, 因此内
存访问指令个数为 2MN + 2MNK . 标量算法没有特殊指令执行开销.
基于 Broadcast 算法的向量化实现可通过参数 vs 来调节向量化长度, 以对长度为 N 的最内层循环做向量化.
1
1
在 Broadcast 算法中, 总的循环次数为 MNK , 因此浮点运算指令个数为 MNK , 每个 FMA 操作同时对 vs 个
vs
vs
元素进行乘加运算. 在进入最内层循环之前, 需要使用一种标量广播的特殊指令将矩阵 A 的单个元素广播成长度
为 vs 的向量数组, 该指令共需调用 MK 次. 在每次运算的过程中, 分别需要对矩阵 B 和 C 进行一次长度为 vs 的向
量读操作, 将其与广播后的向量数组进行乘加运算, 并在最内层循环结束后将结果向量写回矩阵 C. 因此该算法的
1
内存访问指令个数为 MK + 3MNK .
vs
设将向量化长度设置为 vs 时, 执行一条向量 FMA 指令的计算开销为 CostFMA [vs] , 以向量数组的形式访问内
存的访存开销为 CostMemory [vs] , 将标量操作视为 vs = 1 时的向量操作, 则对标量算法与向量算法的特性分析结果
如表 1 所示. 从中可以看出, 向量算法通过引入了特殊的广播指令并改变了原先的访存模式来为向量化提供优化
空间. 一般来说 vs 越大, 则向量化越充分. 但是当 vs 过大时, 一方面负载的输入尺寸规模可能较小, 可向量化的空
间不足. 另一方面向量化所带来的计算便利可能难以弥补寄存器溢出的开销. 因此硬件平台信息会对向量化长度
的配置形成约束, 而基于优化分析模型也容易探究不同向量化长度对算法在计算、访存等方面上的影响.

软件学报 ****年第**卷第**期

10

表1
分类

标量算法与向量算法的特性分析

标量算法

向量算法

浮点运算指令个数

MNK

1
MNK
vs

内存访问指令个数

2MN + 2MNK

特殊指令个数

0

MK + 3MNK

局部优化加速比
1 CostFMA [vs]
×
vs CostFMA [1]

1
vs × CostMemory [vs]
2N + 2NK
CostMemory [1]

K + 3NK

1
vs

∞

MK

Im2Col 与 Broadcast 算法的特性分析

4.2.2

深度学习模型里的卷积层通常处理的是四维张量, 这些张量会带有 batch、channel 和 feature 等维度. 设 N 表
示 batch, C 表示 channel, F 表示 feature, Hi × Wi 、 Hk × Wk 和 Ho × Wo 分别表示张量二维切片中的输入、卷积核
和输出所对应的尺寸, 下面以数据排布形式为 NCHW_FCHW 的卷积层为例 (即输入张量的数据排布为 NCHW, 卷
积核张量的数据排布为 FCHW, 输出张量的数据排布为 NFHW), 对 Im2Col 与 Broadcast 两种算法做优化分析.
Im2Col 算法首先通过 Im2Col 策略, 对卷积层的输入进行数据重排, 并对卷积核进行维度变换, 然后通过矩阵
乘法操作得到结果矩阵张量, 最后将该张量再进行维度变换恢复成卷积层的输出. 由于维度变换直接对张量进行
原地操作, 不涉及浮点计算和内存访问, 因此主要关注该算法的数据重排开销和矩阵乘法操作开销. 在数据重排的
过程中, 需要进行 NCHk Wk Ho Wo 次迭代, 每次迭代会读取输入中的一个元素, 再存储到矩阵 B 中, 该部分的访存开
销为 2NCHk Wk Ho Wo , 不涉及计算开销. 在矩阵乘法操作中, 开销等价于 N 次固定规模矩阵乘法的开销. 设每次矩
阵乘法的两个输入矩阵和一个输出矩阵分别为 A、B、C, 则 A 矩阵的行数 Arow = F , B 矩阵的行数 Bcol = Ho Wo , A
矩阵的列数 Acol = CHk Wk . 若采用针对矩阵乘法的 Broadcast 向量算法进行计算, 则一次向量化矩阵乘法的迭代次
1
1
1
数为 FCHo Wo Hk Wk , 浮点运算指令个数为 FCHo Wo Hk Wk , 内存访问指令个数为 FCHk Wk + 3FCHo Wo Hk Wk ,
vs
vs
vs
共需要用到 FCHk Wk 条标量广播的特殊指令. 综合 Im2Col 策略和所有矩阵乘法操作, Im2Col 算法的总浮点运算
1
1
指令个数为 NFHo WoCHk Wk , 总内存访问指令个数为 2NCHk Wk Ho Wo + NFCHk Wk + 3NFHo WoCHk Wk , 总共需
vs
vs
要用到 NFCHk Wk 条标量广播的特殊指令.
1
若直接采用 Broadcast 算法, 在朴素算法的基础上对内层循环 Wo 进行向量化, 则迭代次数为 NFCHo Hk Wk Wo .
vs
1
总浮点运算指令个数等于迭代次数, 内存访问指令个数为 NFCHo Hk Wk + 3NFCHo Hk Wk Wo , 标量广播的特殊指
vs
令使用次数为 NFCHo Hk Wk .
对 Im2Col 算法和 Broadcast 算法的特性分析如表 2 所示. 可以看出, 当输入宽度 Wo 较大时, Broadcast 算法在
访存上更占优势. 而当输入高度 Ho 较大时, Im2Col 算法的优化性能更好. 这是因为 Broadcast 算法直接对 Wo 所对
应的内层循环进行向量化, 而 Im2Col 算法会通过将卷积层转换成矩阵乘法操作来对 CHk Wk 所对应的内层循环进
行向量化. 所以在同样的向量化长度下, 虽然两种优化算法所需的浮点运算指令个数相同, 但是 Im2Col 算法需要
花费额外的访存开销进行数据重排, 与此同时 Broadcast 算法需要花费较多的特殊指令执行开销. 在这种情况下便
需要优化分析模型基于输入尺寸对算法的选择做出合理的判断.
表2
分类
浮点运算指令个数

内存访问指令个数
特殊指令个数

Im2Col 与 Broadcast 算法的特性分析

Im2Col算法

NFHo Wo CHk Wk

1
vs

2NCHk Wk Ho Wo + NFCHk Wk +
1
3NFHo Wo CHk Wk
vs
NFCHk Wk

Im2Col相对Broadcast的局部优化
加速比

Broadcast算法

NFCHo Hk Wk Wo

1
vs

NFCHo Hk Wk +
3NFCHo Hk Wk Wo
NFCHo Hk Wk

1
F + 3FWo

1
vs

1
vs

2Wo + F + 3FWo
Ho

1
vs

张洪滨 等: AutoConfig: 面向深度学习编译优化的自动配置机制

11

5 静态信息提取与动态开销测量
优化分析模型是 AutoConfig 可解释性的来源, 其准确性由静态信息的提取和动态开销的测量共同决定. 静态
信息提取阶段收集计算负载的尺寸信息和硬件平台数据, 旨在确定可配置参数的范围, 可以看作是对配置空间的
精化和剪枝, 从而降低调优开销. 动态开销测量阶段通过在指定优化平台上测量示例程序, 以最终确定对应计算、
访存和特殊指令的权重系数, 为优化分析模型提供量化数据来选择合适的优化算法, 进而完成代码生成. 为了聚焦
于 AutoConfig 的设计思想, 本文只使用单指令多数据流 SIMD 作为示例进行论述.
5.1 硬件信息的静态提取
静态信息提取根据计算负载的优化分析模型的需求进行采集, 结合这些硬件信息以及低层编译器的代码生成
策略可以分析出加速指令的使用和额外的开销. 面向 CPU 的优化需要利用 SIMD 的加速能力并且使用 Cache 降
低访存开销, 因此本文展示的静态信息提取主要包括收集 SIMD 寄存器堆的尺寸和 Cache 的信息. 配合生成的
LLVM 代码和汇编代码进行静态分析.
SIMD 寄存器关键信息提取与分析

5.1.1

SIMD 计算单元的关键信息包括 SIMD 寄存器长度和 SIMD 寄存器堆的容量. SIMD 寄存器的长度标志着一
条 SIMD 指令能够操作的元素个数. SIMD 寄存器堆的容量标志着一个 SIMD 中间表示语句能够使用的最大非溢
出尺寸. 使用这两个硬件信息能够结合编译器使用的 SIMD 抽象尺寸分析出计算的内存溢出次数.
编译策略选择的主要的挑战在于向量中间表示参数和实际机器关键信息之间存在差距. MLIR 和 LLVM IR
的向量抽象提供了灵活的语义, 并且根据后端硬件平台生成 SIMD 代码, 这意味着中间表示层面有更强的向量语
义的表达能力来承载计算负载的编译优化, 同时能够避免在多个后端重复实现优化算法. 然而, 物理机器上的
SIMD 寄存器的长度和数量是固定的, 需要生成多条 SIMD 指令来弥合向量中间表示到 SIMD 器件的鸿沟. 一旦
中间表示层面使用的向量抽象尺寸超过了 SIMD 寄存器堆的容量, SIMD 寄存器数量不足以完成所有的计算, 此
时汇编代码生成器就会将生成访存指令, 将寄存器中的元素溢出到内存, 腾挪出空闲寄存器完成计算, 当溢出的元
素被后续计算使用时, 再将这部分元素从内存中取回到 SIMD 寄存器, 这就造成了访存操作的额外开销.
在向量中间表示的转换过程中, 为了兼容多种后端硬件平台, 向量抽象的长度和类型不会发生改变. 图 3 展示
了使用融合乘加操作 (FMA) 给出的具体分析示例. 示例函数由 MLIR 编写, 接受 3 个内存抽象的中间表示
(memref) 作为参数, 从这 3 个内存抽象中加在向量中间表示, 对 3 个向量进行融合乘加运算, 并将输出的向量存储
到目标内存中间表示中. 示例使用两个不同长度的向量表示, 长度分别为 128 和 1 024, 向量中的每个元素为 32 位
浮点数. 在从 MLIR 编译至 LLVM IR 之前, 在 MLIR 层级还需要将不同方言的操作改写为低层级方言中的等价操
作这个过程向量表示的尺寸不会改变. 最终, 所有低层级方言中的操作都被翻译为 LLVM IR. 图 3(a2) 和图 3(b2)
描述了融合乘加函数的 LLVM IR 代码. 图 3 高亮了 MLIR 和 LLVM IR 中的向量类型. 在这个编译过程中, 中间
表示的转换和翻译会重写各层级的方言和操作, 但这些向量抽象的长度和类型从未改变.
但是, 与 MLIR 和 LLVM IR 所抽象的向量中间表示参数不同, 汇编代码级别的参数与实际机器关键信息强相
关. 此处选择 AVX512 扩展来展示编译结果. AVX512 的寄存器堆包括 32 个 zmm SIMD 寄存器, 每个寄存器的宽
度为 512 位. 图 3(a3) 展示了向量抽象长度为 128 的融合乘加函数的汇编代码. 汇编代码清楚地表现出了 SIMD 指
令的加载、计算和存储的过程. 加载和存储操作使用 vmovups 指令, 融合乘加操作使用 vfmadd213ps 指令. 在汇编
代码中, 每个 vfmadd213ps 指令需要两个 zmm 寄存器. 所以向量抽象类型为 vector<128xf32> 的融合乘加操作需
要 16 个 zmm 寄存器 (4096/512×2) 就可以完成计算. 当向量类型为 vector<1024xf32> 时, 理论上需要 128 个 zmm
寄存器, SIMD 寄存器堆但只有 32 个可用. 图 3(b3) 的汇编代码展示了寄存器首先用于进行加载操作, 直至 32 个
zmm 寄存器用尽. 此后指令 vfmadd213ps 进行融合乘加的计算. 由于没有空闲寄存器可用, 需要将计算结果溢出
到内存中, 将寄存器腾挪出来负责后续计算. 最后进行存储操作时, 溢出的计算结果被重新加载回寄存器然后存储
到目标缓冲区. 因此, 由于汇编代码和硬件平台指令集绑定的, 不合理的向量化配置会降低执行性能.

软件学报 ****年第**卷第**期

12

(a1) 向量抽象长度为 128 的 MLIR 的融合乘加示例程序

(b1) 向量抽象长度为 1 024 的 MLIR 的融合乘加示例程序

(a2) 向量抽象长度为 128 的融合乘加 MLIR 示例程序翻译至 LLVM IR

(b2) 向量抽象长度为 1 024 的融合乘加 MLIR 示例程序翻译至 LLVM IR

(a3) 向量抽象长度为 128 的融合乘加 MLIR 对应的汇编代码

(b3) 向量抽象长度为 1 024 的融合乘加 MLIR 对应的汇编代码

(a) 向量抽象长度为 128 的 MLIR 的融合乘加编译通路

(b) 向量抽象长度为 1 024 的 MLIR 的融合乘加编译通路

图3

不同向量配置下的融合乘加计算负载与 SIMD 寄存器的使用情况

综上所述, 选择向量配置实际是在权衡迭代开销和溢出开销. 如果使用保守的向量配置策略, 寄存器堆可以承
载所有的计算负载需求, 但是需要多次迭代才能完成所有的计算, 即增加了迭代的开销. 如果使用激进的向量配置
策略, 寄存器堆需要溢出到内存才可以承载所有的计算负载, 这增加了访存开销, 但是可以用更少次数的迭代完成
全部计算, 节省了循环迭代的开销. 这种权衡关系可以指导参数配置范围的选择.
Cache 信息提取与分析

5.1.2

优化算法的访存模型对实际性能有重要的影响, 因此编译优化生成代码对 Cache 的利用十分关键. 本节对 L1
数据 Cache 进行信息收集和分析, 主要关注 Cache 容量和 Cache 块尺寸. 这些信息可以辅助编译器制定循环分块
或者循环顺序的策略, 尽可能利用 Cache 降低访存开销.
以实验平台 Intel(R) Xeon(R) Gold 5218R CPU 为例, L1 数据 Cache 的容量为 32 KB, 而每个 Cache 块的大小
为 64 字节. 这个 Cache 块的大小可以看作与 vector<16xf32> 向量抽象尺寸相当, 这与 AVX512 的寄存器长度恰好
一致. 当使用 vector<16xf32> 的整数倍作为向量抽象时, 数据之间并不会存在 Cache 命中或未命中的相互影响. 正
如 第 5.1.1 节所提及的, 本文所选择的配置项均为 16 的整数倍. 因此, 对于本文这种以向量化为核心的优化策略,
Cache 的状态不会对优化分析模型的准确性产生显著的影响. 对于访存密集型的优化任务, Cache 相关的可配置项
是非常重要的, 并应当纳入优化分析模型. 针对这种优化任务, Cache 容量的抽象表示和 Cache 的结构都是必须进
行详细提取和分析的要素. 以该实验平台为例, 访存操作一共可以在 L1 数据 Cache 中存放 vector<8192xf32>, 以

张洪滨 等: AutoConfig: 面向深度学习编译优化的自动配置机制

13

及对应的二维向量表示, 例如 vector<8x1024xf32>、vector<4x512xf32> 等. L1 数据 Cache 采用 8 路组相连的设计,
每一路 64 个 Cache 块. 因此, 在优化过程中还需要关注访存操作的抽象长度是否会超出每一路的 Cache 块数量.
为了达到高效的访存模式, 高层代码优化策略可以根据以上分析采用特定的循环分块和循环顺序策略.
5.2 程序开销的动态测量
动态开销测量针对优化所在平台确定计算、访存和特殊指令开销的权重系数, 并传入优化分析模型来得到最
终的执行时间表达式. 为此, AutoConfig 维护了一个动态测量的程序集合, 其中的每个实例程序都体现了某一种程
序特性. 该集合允许用户进行删改和添加, 使得用户可以根据其优化需求来调整程序集合. AutoConfig 提供的集合
能够覆盖大部分与性能有关的 MLIR 操作, 并且这些动态测量的示例程序都源自实际的测试和优化片段. 这些示
例程序的种类和迭代次数都是可配置的, 用户可据此定制合适的示例, 确保得到的测量值既准确又可靠, 从而避免
随机误差的影响.
本节以 Broadcast 向量化算法的开销表示为例阐述动态测量流程, 涉及的测量开销如表 3 所示. 其中包括访存开
销 CostMemory [vs] 、SIMD 计算开销 CostFMA [vs] 和表示广播指令的特定操作开销 CostBroadcast [vs] , 它们分别对应了优
化分析模型的 3 项权重系数 λArith 、 λMemory 和 λSpecIns . 具体来说, 在 SIMD 浮点计算场景中, λArith 是单条 FMA 指令
的执行时间, λMemory 则是单条访存指令的执行时间. 而 λSpecIns , 在对 Broadcast 向量化算法的分析中, 等价于单条广播
操作指令的执行时间, 因为该优化算法涉及的特殊指令只有广播指令. 其他优化算法中特殊指令也同理.
表3

Broadcast 向量化算法的开销表示

开销表示
CostMemory [vs]

开销描述

示例程序描述

将 vs 个元素进行访存操作

MemCopy示例程序的执行时间

CostFMA [vs]

vs 个数据进行融合乘加操作的开销

FMA示例程序与MemCopy示例程序执行时间的差分

CostBroadcast [vs]

vs 个数据进行广播操作的开销

BroadcastFMA示例程序与FMA示例程序执行时间的差分

为了量化这些开销, 本节使用动态测量程序集合中的特定子集在多个目标硬件平台上进行了实验. 其中访存
开销的测量选择内存拷贝示例程序, 其中包括使用向量方言的内存加载和存储指令. FMA 开销的测量选择了 FMA
示例程序, 该程序结合了访存指令和融合乘加指令. 表示广播指令的特殊指令开销的测量选择了 BroadcastFMA
的示例程序, 其中包括访存指令、广播指令和融合乘加指令. 以上示例程序均使用 2 的 20 次方作为计算负载尺
寸, 计算负载迭代 10 000 次, 通过测量总的执行时间来确定 CostMemory [vs] 、 CostFMA [vs] 和 CostBroadcast [vs] 的量化数值.
表 4 是在 AVX512 平台下的实验测量结果, 从中可以看出, 访存指令开销 CostMemory [vs] 平均为 FMA 指令开
销 CostFMA [vs] 和表示广播指令的特定操作开销 CostBroadcast [vs] 的 20 倍左右. 在同样的硬件平台下, 这里得出的开
销比值与权重系数 λArith 、 λMemory 和 λSpecIns 之间的比值是一致的, 因此这些数据可以为优化分析模型提供权重数
值, 进而分析出整体算法的性能表现, 并选择合适的算法和配置进行代码生成.
表4

开销表示的测量结果 (ms/每 10 000 次迭代)

测量开销
CostMemory [vs]

vs =1

vs =16

vs =32

vs =64

vs =128

vs =256

12 028.5

1 648

1 751.5

1 779.5

1 989.5

2 345.5

CostFMA [vs]
CostBroadcast [vs]

644
738

70
175

142
209

83
204

102
303

52
154

6 实验设计、结果和分析
本节面向实际的深度学习计算负载场景, 在 AVX512 和 ARM Neon 两种 SIMD 平台上, 对 AutoConfig 的参数
配置及优化算法的选择进行探究, 并将基于分析模型的代码生成策略与 TVM 的自动调优策略进行跨平台的性能
比较. 其中, 在第 6.2 节会探究向量化长度参数的选取对优化性能的影响, 指出基于不同硬件平台的特性来确定参
数配置的意义. 在第 6.3 节会利用硬件信息和动态开销数据来计算不同算法的预测加速比, 以验证 AutoConfig 的

软件学报 ****年第**卷第**期

14

算法选择是否准确. 在第 6.4 节将 AutoConfig 与基于自动调优策略的 TVM 进行跨平台的代码生成性能比较.
AutoConfig 基于 Buddy Compiler 实现, 在代码生成阶段使用了开源编译器框架 LLVM, 版本为 17.0.0; 在对比实验
中采用的 TVM 版本为 v0.14.0.
6.1 优化场景的选择
本文选取了一个经典的卷积神经网络 EfficientNet (模型文件来自 https://coral.ai/models/image-classification/),
并针对其中的关键深度学习计算负载即不同输入尺寸的卷积层进行优化. 卷积神经网络是以卷积层作为主要模块
的深度学习模型. 对于卷积神经网络而言, 靠近输入部分的卷积层其功能主要在于从输入中提取初步特征, 因此卷
积的输入尺寸较大而通道数较少. 处于模型中间部分或靠近输出部分的卷积层其功能主要在于从上一层输出的特
征中进一步提取深层信息, 因此卷积的输入尺寸较小而通道数较多.
EfficientNet 是经典的卷积神经网络模型, 本节基于 EfficientNet 完成实验探究. 表 5 对该模型文件中与计算和
访存相关的粗粒度计算负载进行了统计, 可以看出卷积 (conv_2d_nhwc_hwcf) 的计算占所有计算负载的大多数
(38/64), 可见通过从模型中的不同部分选取了尺寸多样的卷积作为优化对象具有一定的代表性, 能够反映优化算
法的选择对整体性能的影响.
表5

EfficientNet 高层计算负载统计

高层计算负载
batch_matmul
depthwise_conv_2d_nhwc_hwcm
conv_2d_nhwc_hwcf
collapse_shape
expand_shape

出现次数
1
11
38
12
2

6.2 基于硬件信息的参数配置
为了验证硬件信息对优化算法参数选取的影响, 在 AVX512 和 ARM Neon 平台中分别以 {16, 32, 64, 128, 256}
的数值设置向量化长度, 并在不同的卷积优化场景中计算优化加速比, 随后对于这两个平台分别选取平均实际加
速比最大的向量化长度设置, 将相关数据记录在表 6 和表 7 中. 为了方便对比, 也将适合 AVX512 的向量化长度
配置应用于 ARM Neon 平台中进行测试, 将相关数据记录在表 8 中. 图 4 是面向 ARM Neon 平台时, 选取不同向
量化参数 vs 的 Broadcast 算法优化卷积的执行时间比较.
表6

基于 AVX512 平台, 向量化长度为 64 时的最佳算法选择探究

卷积尺寸
(1×1536×7×7, 192×1536×1×1)
(1×192×7×7, 1536×192×1×1)
(1×48×30×30, 384×48×3×3)
(1×32×114×114, 96×32×3×3)
(1×64×58×58, 64×64×3×3)

标量执行时间
(ms)
75.3
76.9
647
1 726
581

Im2Col执行
时间 (ms)
2.2
2.1
16.9
76.9
19.9

Broadcast执行
实际加速比 预测加速比
时间 (ms)
14.5
6.74
2.51
14.7
7.14
2.83
32.5
1.91
1.58
46.6
0.66
0.83
18.0
9.91
1.84

算法选择结果
是否正确
√
√
√
√
√

一方面, 从图 4 中可以看出, 在 ARM Neon 平台上选择 16 作为向量化长度能够获得更小的执行时间和更准确
的预测结果, 这说明向量化长度参数的选取会对优化性能产生关键影响. 另一方面, 根据表 6, 在 AVX512 平台上选
择 64 作为向量化长度是更优的参数配置, 但是根据表 7 和表 8, 该参数对 ARM Neon 平台来说并不是最优的. 这说
明采用较大的向量化长度不一定在每个平台上都能得到较好的执行性能, 还需要考虑硬件平台里 SIMD 寄存器个数
和 Cache 容量的限制. 实践中发现, 在面向 AVX512 的硬件后端时, {32, 64, 128} 是合适的参数项配置范围. 而在面
向 ARM Neon 的硬件后端时, 参数选择 {16, 32, 64} 可以得到理想的调优效果, 这反映了参数的可取范围受硬件信息
的约束 (举例而言, 不同平台有不同的指令向量长度. AVX512 的指令向量长度为 512 位, 而 ARM Neon 的指令向量
长度只有 128 位). 由此可见, 合理的参数配置能够提高算法选择的精确度, 同时提升代码的执行性能.

张洪滨 等: AutoConfig: 面向深度学习编译优化的自动配置机制

表7

基于 ARM Neon 平台, 向量化长度为 16 时的最佳算法选择探究

卷积尺寸
(1×1536×7×7, 192×1536×1×1)
(1×192×7×7, 1536×192×1×1)
(1×48×30×30, 384×48×3×3)
(1×32×114×114, 96×32×3×3)
(1×64×58×58, 64×64×3×3)

表8

15

标量执行时间
(ms)
53
49.2
461
1 229
1 211

Im2Col执行
时间 (ms)
2.9
4.5
15.4
49.7
50.1

Broadcast执行
实际加速比 预测加速比
时间 (ms)
28.0
10.00
1.51
26.7
5.88
1.57
63.9
4.17
1.15
47.8
0.97
0.94
47.7
0.95
0.94

算法选择结果
是否正确
√
√
√
√
√

基于 ARM Neon 平台, 向量化长度为 64 时的最佳算法选择探究

卷积尺寸
(1×1536×7×7, 192×1536×1×1)
(1×192×7×7, 1536×192×1×1)
(1×48×30×30, 384×48×3×3)
(1×32×114×114, 96×32×3×3)
(1×64×58×58, 64×64×3×3)

标量执行时间
(ms)
53
49.2
461
1 229
1 211

Im2Col执行
时间 (ms)
11.5
11.2
19.9
45.9
46.9

Broadcast执行
实际加速比 预测加速比
时间 (ms)
89.5
7.69
1.86
88.5
7.69
2.02
192
10.00
1.28
144
3.12
0.88
147
3.12
0.87

算法选择结果
是否正确
√
√
√
×
×

卷积尺寸

Conv(1×64×58×58, 64×64×3×3)
Conv(1×32×114×114, 96×32×3×3)
Conv(1×48×30×30, 384×48×3×3)
Conv(1×192×7×7, 1536×192×1×1)
Conv(1×1536×7×7, 192×1536×1×1)
0

200

400
vs=64

图4

600
800
1 000
执行时间 (ms)
vs=16
vs=1

1 200

1 400

ARM Neon 平台下不同向量化参数 vs 的 Broadcast 算法优化卷积的执行时间 (ms)

6.3 优化分析模型的最佳算法选择
为了验证 AutoConfig 进行算法选择的有效性, 首先基于硬件信息和动态开销求出 Im2Col 相对于 Broadcast
的预测加速比. 预测加速比大于 1 表示在优化分析模型中 Im2Col 的执行性能优于 Broadcast, 此时 AutoConfig 会
选择 Im2Col 算法实施代码生成策略. 反之选择 Broadcast 算法实施代码生成策略. 然后在不同的硬件平台上基于
不同的配置参数, 分别显性指定 Im2Col 和 Broadcast 算法进行代码生成, 测量两种算法所得到优化代码各自的实
际执行时间, 计算出 Im2Col 相对于 Broadcast 的实际加速比, 最后在不同的卷积优化场景中 AutoConfig 的预测加
速比进行验证, 实验结果如表 6、表 7 和表 8 所示. 图 5 是在 AVX512 平台下, 确定最佳参数配置 vs=64 时的最佳
算法选择结果. 图 5 中计算得出了 Im2Col 和 Broadcast 算法的相对执行时间占比, 一个算法对应的柱形部分越短,

卷积尺寸

则该算法的性能与另一个算法相比越好, 此时选择该算法是最佳的策略.
Conv(1×64×58×58,64×64×3×3)
Conv(1×32×114×114,96×32×3×3)
Conv(1×48×30×30,384×48×3×3)
Conv(1×192×7×7, 1536×192×1×1)
Conv(1×1536×7×7,192×1536×1×1)
0

20
30
40
50
60
70
80
90
100
不同算法优化卷积的相对执行时间占比 (%)
Im2Col 算法执行时间占比
Broadcast 算法执行时间占比
AutoConfig 的算法选择

图5

10

不同算法优化卷积的相对执行时间占比

软件学报 ****年第**卷第**期

16

根据图 5, 结合表 6–表 8 对算法选择结果的正确性验证, 可以看出 AutoConfig 在多种硬件平台和不同向量化
长度配置下, 能够有效地完成代码生成策略中的算法选择, 这体现在对于大部分卷积优化场景, AutoConfig
能够预测得到实际性能更好的优化算法. 尽管如此, AutoConfig 的分析模型也有其局限性, 这会导致它在某些情况
下 (如处理表 8 中后两个卷积尺寸时) 的算法选择结果存在偏差. 具体而言, 在第 4.1 节的建模中笼统地将各种访
存操作带来的开销视为同一类开销 CostMemory , 但实际上, CostMemory 还应包括 Cache 未命中的代价、从内存中预取
数据的代价、数据从寄存器中溢出的代价等. 在实际的访存中预取数据的操作是穿插在运算操作之间来隐藏延迟
的, 它与读取 Cache 操作的频率共同反映了算法对内存和缓存的利用程度, 而寄存器溢出的代价则反映了算法对
寄存器组的使用情况. 由此可见, 提高优化分析模型对硬件行为的抽象程度和描述精度是实现更加实用的自动配
置机制的关键.
6.4 AutoConfig 的代码生成性能验证
TVM 采用搜索的方式进行自动调优, 搜索的过程中需要通过枚举优化参数并实际执行程序来确定最佳的优
化策略. 与 TVM 相比, AutoConfig 进行配置的所需开销主要体现在动态开销的测量上. 基于静态硬件信息和动态
开销测量的结果, AutoConfig 能够直接借助代价模型完成参数配置和算法选择, 实现代码生成的过程. 针对深度学
习计算负载场景, 本节将 AutoConfig 与基于自动调优策略的 TVM 进行跨平台的性能比较.
在 AVX512 平台和 ARM Neon 平台上, 对不同尺寸的矩阵乘法 (Matmul) 和卷积 (Conv) 实施调优的实验结果
如表 9 和表 10 所示. 可以看出 AutoConfig 和 TVM 的优化相比于原始程序取得了较明显的性能提升, 且 AutoConfig
的优化加速比和所生成优化代码的绝对执行时间与 TVM 的优化在一个数量级内, 具有可比性.
表9

AVX512 平台下 AutoConfig 与其他调优机制的性能对比

负载尺寸
Matmul(64×65536, 65536×256)
Matmul(1024×1024, 1024×1024)
Conv(1×1536×7×7, 192×1536×1×1)
Conv(1×192×7×7, 1536×192×1×1)
Conv(1×48×30×30, 384×48×3×3)
Conv(1×32×114×114, 96×32×3×3)
Conv(1×64×58×58, 64×64×3×3)

TVM基准性能 TVM调优性能
(ms)
(ms)
2 518
189
3 483
95
160
2.8
159.5
0.9
184
2.4
445.1
10.3
153
1.4

TVM调优
加速比
13.3
36.7
57.1
177.2
76.7
43.2
109.3

AutoConfig基 AutoConfig调 AutoConfig调
准性能 (ms) 优性能 (ms)
优加速比
4 248
242
17.6
3 801
218
17.4
75.3
2.2
34.2
76.9
2.1
36.6
647
16.9
38.3
1 726
46.6
37.0
581
18
32.3

进一步的, 为了评估 AutoConfig 相比 TVM 在调优时的性能优势, 本文定义单位加速比所需调优开销, 表示优
化算法在与标量算法相比实现性能提升时, 平均每单位倍数的性能提升所需的调优开销, 该指标直观反映了调优
行为对生成代码加速的贡献力度. 基于表 10 使用单位加速比所需调优开销, 对 AutoConfig 和 TVM 在卷积优化场
景进行描述, 实验结果如图 6 所示. 结果表明, 与 TVM 的自动调优方式相比, AutoConfig 无需通过耗时较长的搜
索方式确定最优参数, 只需要完成硬件信息提取和动态开销测量, 即可依托代价模型实现可自动配置的代码生成,
且由生成的代码可达到与自动调优相似的执行性能.
表 10

ARM Neon 平台下 AutoConfig 与其他调优机制的性能对比

负载尺寸
Matmul(64×65536, 65536×256)
Matmul(1024×1024, 1024×1024)
Conv(1×1536×7×7, 192×1536×1×1)
Conv(1×192×7×7, 1536×192×1×1)
Conv(1×48×30×30, 384×48×3×3)
Conv(1×32×114×114, 96×32×3×3)
Conv(1×64×58×58, 64×64×3×3)

TVM基准性能 TVM调优性能 TVM调优加 AutoConfig基 AutoConfig调 AutoConfig调
(ms)
(ms)
准性能 (ms) 优性能 (ms)
速比
优加速比
6 194
134
6 455
140
46.2
46.1
1 300
76
3 889
127
17.1
30.6
234.1
8.2
53
11.5
28.5
4.6
228.6
7.4
49.2
11.2
30.9
4.4
299.5
3.1
461
19.9
96.6
23.2
349.4
9.4
1 229
45.9
37.2
26.8
125.6
4.6
412
16.8
27.3
24.5

卷积尺寸

张洪滨 等: AutoConfig: 面向深度学习编译优化的自动配置机制

Conv(1×64×58×58,64×64×3×3)

0.20

Conv(1×32×114×114,96×32×3×3)

0.18

Conv(1×48×30×30,384×48×3×3)

0.17

Conv(1×192×7×7,1536×192×1×1)

0.18

Conv(1×1536×7×7,192×1536×1×1)

0.19

0.66
3.12
1.12
0.47
1.37

1.0
1.5
2.0
2.5
3.0
单位加速比所需调优开销 (ms)
AutoConfig 单位加速比所需调优开销
TVM 单位加速比所需调优开销
(a) AVX512 平台单位加速比所需调优开销的比较

卷积尺寸

0

17

0.5

Conv(1×64×58×58,64×64×3×3)

0.09

Conv(1×32×114×114,96×32×3×3)

0.09

Conv(1×48×30×30,384×48×3×3)

0.10

2.64
3.63
0.89

Conv(1×192×7×7,1536×192×1×1)

0.52

Conv(1×1536×7×7,192×1536×1×1)

0.50
0

2.72
2.73

1.0
1.5
2.0
2.5
3.0
3.5
单位加速比所需调优开销 (ms)
AutoConfig 单位加速比所需调优开销
TVM 单位加速比所需调优开销
(b) Arm Neon平台单位加速比所需调优开销的比较

图6

7 总

3.5

0.5

4.0

单位平台加速比所需调优开销的跨平台比较

结

本文提出了 AutoConfig, 一种面向深度学习编译优化的自动配置机制. 针对不同的深度学习计算负载和硬件
平台, 该机制构建了具备可配置的代码生成重写模式和可解释性的优化分析模型, 通过分析静态提取的信息和动
态测量的开销来确定最佳的参数配置与优化算法, 从而进行代码生成. 本文还将 AutoConfig 集成到深度学习编译
器 Buddy Compiler 中, 旨在达成一次优化实现适配多种硬件平台的目标. 通过对深度学习模型中的卷积和矩阵乘
法的优化实验, 本文验证了 AutoConfig 自动配置优化和调优方法能够解决当前深度学习编译器调优开销大, 优化
效果可解释性差的问题. AutoConfig 生成的优化代码可以达到与编译器自动调优相似的性能表现, 同时避免了重
复实现和反复调优.
本文期望 AutoConfig 成为自动配置编译优化的基础设施, 并拥有开放的使用模式. 本文阐述的优化分析模型
和动静融合的调优策略并非与 AutoConfig 的基础设施紧密耦合. 用户可以提供不同的优化分析模型和调优机制
来驱动 AutoConfig, 从而实现更加高效的编译优化. 例如, 用户可以针对其他深度学习场景进行代码生成, 可以采
用更多样的优化算法进行比较和调优, 也可以从硬件平台收集更细颗粒度的信息并建立理论模型来提高性能预测
的准确性, 还可以集成 Amdahl 模型、Roofline 模型等不同的分析模型进行针对性的调优等. 因此, 本文更长远的
意义在于提供了全新的编译优化开发范式, 将编译优化解耦为开发、分析和调优的过程, 为编译优化提供了新的
研究方向.
References:
[1]

Krizhevsky A, Sutskever I, Hinton GE. ImageNet classification with deep convolutional neural networks. In: Proc. of the 25th Int’l Conf.
on Neural Information Processing Systems. Lake Tahoe: ACM, 2012. 1097–1105. [doi: 10.5555/2999134.2999257]

[2]

Radford A, Kim JW, Xu T, Brockman G, McLeavey C, Sutskever I. Robust speech recognition via large-scale weak supervision. In: Proc.

软件学报 ****年第**卷第**期

18

of the 40th Int’l Conf. on Machine Learning. Honolulu: ACM, 2023. 1182. [doi: 10.5555/3618408.3619590]

[3]

Ouyang L, Wu J, Jiang X, Almeida D, Wainwright CL, Mishkin P, Zhang C, Agarwal S, Slama K, Ray A, Schulman J, Hilton J, Kelton
F, Miller L, Simens M, Askell A, Welinder P, Christiano PF, Leike J, Lowe R. Training language models to follow instructions with
human feedback. In: Proc. of the 36th Int’l Conf. on Neural Information Processing Systems. New Orleans: NeurIPS, 2022.
27730–27744.

[4]

Brown TB, Mann B, Ryder N, et al. Language models are few-shot learners. In: Proc. of the 34th Int’l Conf. on Neural Information
Processing Systems. Vancouver: ACM, 2020. 159. [doi: 10.5555/3495724.3495883]

[5]

Chetlur S, Woolley C, Vandermersch P, Cohen J, Tran J, Catanzaro B, Shelhamer E. CuDNN: Efficient primitives for deep learning.
arXiv:1410.0759, 2014.

[6]

Li JH, Qin ZN, Mei YJ, Cui JZ, Song YF, Chen CY, Zhang YF, Du LS, Cheng XH, Jin BH, Ye J, Lin E, Lavery D. OneDNN graph
compiler: A hybrid approach for high-performance deep learning compilation. arXiv:2301.01333, 2023.

[7]

Khan J, Fultz P, Tamazov A, Lowell D, Liu C, Melesse M, Nandhimandalam M, Nasyrov K, Perminov I, Shah T, Filippov V, Zhang J,
Zhou J, Natarajan B, Daga M. MIOpen: An open source library for deep learning primitives. arXiv:1910.00078, 2019.

[8]

Lattner C, Adve V. LLVM: A compilation framework for lifelong program analysis & transformation. In: Proc. of the 2004 Int’l Symp.
on Code Generation and Optimization. San Jose: IEEE, 2004. 75–86. [doi: 10.1109/CGO.2004.1281665]

[9]

Zhang HB, Xing MJ, Wu YJ, Zhao C. Compiler technologies in deep learning co-design: A survey. Intelligent Computing, 2023, 2: 0040.
[doi: 10.34133/icomputing.0040]

[10]

Chen TQ, Moreau T, Jiang ZH, Zheng LM, Yan E, Cowan M, Shen HC, Wang LY, Hu YW, Ceze L, Guestrin C, Krishnamurthy A.
TVM: An automated end-to-end optimizing compiler for deep learning. In: Proc. of the 13th USENIX Conf. on Operating Systems
Design and Implementation. Carlsbad: ACM, 2018. 579–594. [doi: 10.5555/3291168.3291211]

[11]

Jouppi NP, Young C, Patil N, et al. In-datacenter performance analysis of a tensor processing unit. In: Proc. of the 44th ACM/IEEE
Annual Int’l Symp. on Computer Architecture. Toronto: Association for Computing Machinery, 2017. 1–12. [doi: 10.1145/3079856.
3080246]

[12]

Chen TQ, Zheng LM, Yan E, Jiang ZH, Moreau T, Ceze L, Guestrin C, Krishnamurthy A. Learning to optimize tensor programs. In:
Proc. of the 32nd Int’l Conf. on Neural Information Processing Systems. Montreal: ACM, 2018. 3393–3404. [doi: 10.5555/3327144.
3327258]

[13]

Zheng LM, Jia CF, Sun MM, Wu Z, Yu CH, Haj-Ali A, Wang YD, Yang J, Zhuo DY, Sen K, Gonzalez JE, Stoica I. Ansor: Generating
high-performance tensor programs for deep learning. In: Proc. of the 14th USENIX Conf. on Operating Systems Design and
Implementation. ACM, 2020. 49. [doi: 10.5555/3488766.3488815]

[14]

Zhu H, Wu R, Diao Y, Ke S, Li H, Zhang C, Xue J, Ma L, Xia Y, Cui W, Yang F, Yang M, Zhou L, Cidon A, Pekhimenko G. ROLLER:
Fast and efficient tensor compilation for deep learning. In: Proc. of the 16th USENIX Symp. on Operating Systems Design and
Implementation. Carlsbad: USENIX, 2022. 233–248.

[15]

Zheng NX, Lin B, Zhang QL, Ma LX, Yang YQ, Yang F, Wang Y, Yang M, Zhou LD. SparTA: Deep-learning model sparsity via tensorwith-sparsity-attribute. In: Proc. of the 16th USENIX Symp. on Operating Systems Design and Implementation. Carlsbad: USENIX,
2022. 213–232.

[16]

Lattner C, Amini M, Bondhugula U, Cohen A, Davis A, Pienaar J, Riddle R, Shpeisman T, Vasilache N, Zinenko O. MLIR: Scaling
compiler infrastructure for domain specific computation. In: Proc. of the 2021 IEEE/ACM Int’l Symp. on Code Generation and
Optimization (CGO). Seoul: IEEE, 2021. 2–14. [doi: 10.1109/CGO51591.2021.9370308]

[17]

Vaswani A, Shazeer N, Parmar N, Uszkoreit J, Jones L, Gomez AN, Kaiser Ł, Polosukhin I. Attention is all you need. In: Proc. of the
31st Int’l Conf. on Neural Information Processing Systems. Long Beach: ACM, 2017. 6000–6010. [doi: 10.5555/3295222.3295349]

[18]

Kim S, Hooper C, Wattanawong T, Kang M, Yan RH, Genc H, Dinh G, Huang QJ, Keutzer K, Mahoney MW, Shao YS, Gholami A. Full
stack optimization of transformer inference: A survey. arXiv:2302.14017, 2023.

[19]

Stothers AJ. On the complexity of matrix multiplication [Ph.D. Thesis]. Edinburgh: The University of Edinburgh, 2010.

[20]

Cong J, Xiao BJ. Minimizing computation in convolutional neural networks. In: Proc. of the 24th Int’l Conf. on Artificial Neural
Networks and Machine Learning. Hamburg: Springer, 2014. 281–290. [doi: 10.1007/978-3-319-11179-7_36]

[21]

Lavin A, Gray S. Fast algorithms for convolutional neural networks. In: Proc. of the 2016 IEEE Conf. on Computer Vision and Pattern
Recognition (CVPR). Las Vegas: IEEE, 2016. 4013–4021. [doi: 10.1109/CVPR.2016.435]

[22]

Chellapilla K, Puri S, Simard P. High performance convolutional neural networks for document processing. In: Proc. of the 10th Int’l
Workshop on Frontiers in Handwriting Recognition. La Baule: University of Rennes, 2006.

[23]

Li MZ, Liu Y, Liu XY, Sun QX, You X, Yang HL, Luan ZZ, Gan L, Yang GW, Qian DP. The deep learning compiler: A comprehensive

张洪滨 等: AutoConfig: 面向深度学习编译优化的自动配置机制

19

survey. IEEE Trans. on Parallel and Distributed Systems, 2021, 32(3): 708–727. [doi: 10.1109/TPDS.2020.3030548]

[24]

Katel N, Khandelwal V, Bondhugula U. MLIR-based code generation for GPU tensor cores. In: Proc. of the 31st ACM SIGPLAN Int’l
Conf. on Compiler Construction. Seoul: Association for Computing Machinery, 2022. 117–128. [doi: 10.1145/3497776.3517770]

[25]

Vasilache N, Zinenko O, Bik AJC, Ravishankar M, Raoux T, Belyaev A, Springer M, Gysi T, Caballero D, Herhut S, Laurenzo S, Cohen A.
Composable and modular code generation in MLIR: A structured and retargetable approach to tensor compiler construction.
arXiv:2202.03293, 2022.

[26]

Hu PC, Lu M, Wang L, Jiang GY. TPU-MLIR: A compiler for TPU using MLIR. arXiv:2210.15016, 2023.

张洪滨(1997－), 男, 博士生, CCF 学生会员, 主

武延军(1979－), 男, 博士, 博士生导师, CCF 杰

要研究领域为编译技术.

出会员, 主要研究领域为操作系统, 系统安全.

周旭林(2001－), 男, 硕士生, CCF 学生会员, 主

赵琛(1967－), 男, 博士, 博士生导师, CCF 高级

要研究领域为编译技术.

会员, 主要研究领域为编译技术, 操作系统, 网络
软件.

邢明杰(1980－), 男, 高级工程师, CCF 专业会
员, 主要研究领域为编译技术.


软件学报 ISSN 1000-9825, CODEN RUXUEW
Journal of Software, [doi: 10.13328/j.cnki.jos.007139]
©中国科学院软件研究所版权所有.

Büchi 自动机确定化分析工具

E-mail: jos@iscas.ac.cn
http://www.jos.org.cn
Tel: +86-10-62562563

∗

马润哲, 田聪, 王文胜, 段振华
(西安电子科技大学 计算机科学与技术学院,陕西 西安

710071)

通讯作者: 田聪, 王文胜 E-mail: ctian@mail.xidian.edu.cn

摘 要:

wswang@xidian.edu.cn

无限字自动机的确定化是理论计算机研究重要的一部分,在形式化验证,时序逻辑,模型检测等方面有重

要应用.自 Büchi 自动机提出半个世纪以来,其自动机的确定化算法始终是其中的基础.有别于当初只是在理论上
对其大小上下界的探索,利用日新月异的高性能计算机技术不失为一种有效的辅助手段.为了深入研究非确定性 B
üchi 自动机确定化算法的实现过程,我们希望重点研究确定化过程中的索引能否继续被优化的问题,实现了确定
化研究工具 NB2DR.可以对非确定性 Büchi 自动机进行高效的确定化,并通过工具提供的分析其确定化过程来达到
对其确定化算法改进的目的.通过对生成的确定性无限字自动机的索引的深入分析来探索相关索引的理论.该工具
还实现了可以根据需要的 Büchi 自动机的大小与字母表参数,生成确定化的 Rabin 自动机族,亦可以反向根据需要的
指定索引的大小来生成全部 Büchi 自动机族,测试生成无限字自动机的等价性等功能.
关键词: Büchi 自动机;Rabin 自动机;无限字自动机确定化
中图法分类号: TP311
中 文 引 用 格 式 : 马 润 哲 , 田 聪 , 王 文 胜 , 段 振 华 . Büchi 自 动 机 确 定 化 分 析 工 具 . 软 件 学 报 . http://www.jos.org.cn/
1000-9825/7139.htm
英文引用格式: Ma RZ, Tian C, Wang WS, Duan ZH. Tool for Determinization of BüchiAutomata. Ruan Jian Xue Bao/Journal of
Software (in Chinese). http://www.jos.org.cn/1000-9825/7139.htm

Tool for Determinization of Büchi Automata
MA Run-Zhe, TIAN Cong, WANG Wen-Sheng, DUAN Zhen-Hua
(School of Computer Science and Technology, Xidian University, Xi’an 710071, China)
Abstract:

The determinization of ω automata is an important part of theoretical computer research, and has important applications in

formal verification, sequential logic, and model checking. Since the Büchiautomata was proposed for half a century, the deterministic
algorithm of its automata has always been the basis of it. Different from the exploration of the upper and lower bounds of its size in theory
at the beginning, the use of high-performance computer technology that is changing with each passing day is an effective auxiliary means.
In order to deeply study the implementation process of the deterministic algorithm of non-deterministic Büchiautomata, we hope to focus
on the question of whether the index can continue to be optimized, and realize the deterministic research tool NB2DR. The
non-deterministic Büchiautomata can be determinized efficiently, and the determinization algorithm can be improved by analyzing the
determinization process provided by the tool. A theoretical upper bound on correlation indexing is explored through an in-depth analysis of
the indexing of generated deterministic ω automata. The tool also realizes that the deterministic Rabin automaton family can be generated
according to the size of the required Büchiautomaton and the alphabet parameters, and it can also be reversed to generate all the
Büchiautomata families according to the size of the specified index required, and the test generates ω Functions such as equivalence of
automata.
Key words:

∗

Büchiautomata; rabin automaton; ω automata determinization

基金项目:国家自然科学基金(62192734); 国家重点研发计划(2018AAA0103202);
收稿时间: 2023-09-18; 修改时间: 2023-10-30; 采用时间: 2023-12-13; jos 在线出版时间: 2024-01-05

马润哲 等: Büchi 自动机确定化分析工具

2205

自动机的确定化是理论计算机领域的基础问题之一,在形式化验证与模型检测中具有重要的应用[1-5] .自动
机的确定化与求补密切相关,求补是指对一个接受语言为𝐿𝐿的自动机𝐴𝐴, 其求补是指找到一个自动机𝐴𝐴̃,满足𝐴𝐴̃接

受的语言为𝐿𝐿的补集.为了验证系统是否满足期望的性质,首先将系统表达为自动机𝐴𝐴,性质表达为自动机𝐵𝐵,然后

对𝐵𝐵进行求补,再与𝐴𝐴取交,得到的自动机𝐶𝐶,若𝐶𝐶接受的语言为空集,则可以判定系统满足该性质.这个问题和自动
机的确定化是强烈相关的.假如一个自动机是确定化的,那么我们只需要对这个自动机对偶化,即可得到其求补
的自动机.并且,对于 Büchi 自动机,尽管在理论复杂度方面,不需要确定化的求补方法优于基于确定化的求补方
法,但文献[6] 中的实验结果表明基于确定化的求补方法在实际中具有更好的效果. 无限字自动机的确定化是计
算复杂度理论中的重要问题,是逻辑判定过程的基础 [7],也有助于解决无限博弈求解问题;目前实际效果最好的
公式博弈求解工具[8]便是基于确定性 parity 自动机实现的. 无限字自动机的确定化还应用于马尔科夫决策过
程的推理、基于监控的运行时验证等.
对于定义在有限字上的有限状态自动机来说,其确定化是很明确的.通常对于一个状态为𝑛𝑛的非确定的自
动机来说,可以采用子集构造法来得到状态为2𝑛𝑛 的确定性自动机.其核心思想就是通过幂集枚举出自动机运行

中所有可达的状态合集,只需要将这些所有可能都看作新的自动机的状态,那么这个新的自动机将是确定化的.

但是对于定义在无限字上的有限状态自动机,同样的问题将变得更为复杂,因而无限字自动机的确定化研
究具有重要意义.Büchi 自动机是无限字自动机的一种,在基于线性时序逻辑(LTL) 的模型检测过程中,LTL 等
可以等价的转换成 Büchi 自动机,对该逻辑公式的可满足性检查相应的会变为对 Büchi 自动机的确定化过程,
Büchi 自动机的确定化在模型检测领域有重要的作用.
Büchi

[1]

在他的论文中证明一元二阶逻辑是可判定性时,首先介绍了 Büchi 自动机.这是有限自动机在无限

字上的推广.与普通有限字上的自动机相比,不同点在于其接受条件,很自然的,区别于普通自动机的接受条件,
即“状态序列访问到接受节点”,Büchi 自动机的接受条件为“状态序列访问无限多次接受节点”.此外,有限字
的确定性自动机和非确定性自动机的表达能力是一致的,而区别于此,确定性的 Büchi 自动机的表达能力严格
低于非确定性的 Büchi 自动机.换言之,非确定性的 Büchi 自动机不存在一种算法来得到确定性的 Büchi 自动机.
为了研究 Büchi 自动机的确定化问题,许多表达能力更强的无限字上的自动机被提出,包括 Rabin 自动
机,Streett 自动机,parity 自动机,Emerson-Lei 自动机[2,9,10,11,12] 等.这些自动机主要通过添加复杂的接受条件限制,
来提高其表达能力,以接受更广泛的无限字,拥有更广阔的语言集.经过半个世纪左右的研究,如今 Büchi 自动机
确定化理论体系基本完备[3,4,13,14],相关自动机的理论状态上下界很多都得到证明[11],但仍有不少问题是开放的,
而随着计算机技术性能的提升与算法的进步,很多不好得到的大型自动机状态也可以通过工具进行仿真模拟
运行.
然而目前这方面的工具并不多,包括 SPOT,GOAL[16,17,18] 等.本工具参考了相关工具的优点,具备自动机的
相关建模能力,可以选择相关确定化算法对输入的自动机进行建模,同时根据实际需要可以完成对自动机族的
筛选,自动机确定化过程中对接受条件的分析,以及比较,还可以进行高效的等价性测试,重点支持历史树算法
的分析推演,探究历史树的节点规律,为进一步的确定化研究提供帮助.在对非确定自动机的确定化研究中,非
确定性自动机的确定化会碰到各种问题,即使有很多理论上下界的数值支撑,但是实际应用中的自动机远远达
不到这些理论边界,我们推出的工具可以在已知的理论算法基础上进行高效的确定化操作.同时可以观测确定
化中的自动机的状态,对进一步研究做出实验贡献.
本文共分五节,第一节介绍基础自动机理论的相关知识,第二节介绍工具的整体框架与功能,第三节展示工
具的效果,第四节介绍相关工作,第五节总结全文并展望未来发展方向.

1 基础知识
本节将介绍基础自动机理论相关知识.

2206

Journal of Software 软件学报 Vol.32, No.7, July 2021

1.1 无限字与无限字自动机
自动机是一个五元组𝐴𝐴 = {Σ, 𝑄𝑄, 𝑄𝑄0 , 𝛿𝛿, 𝜆𝜆},其中Σ是字母表,表示有限字母的集合;𝑄𝑄是状态集,表示有限状态的

集合;𝑄𝑄0 是初始状态集,表示初始状态的集合;𝛿𝛿是转移函数,表示一个状态读入一个字母可达状态集合的映射
𝛿𝛿 ⊆ 𝑄𝑄 × Σ × 𝑄𝑄; 𝜆𝜆是接受条件.

有限字𝑤𝑤是由一串字母构成的有序序列𝑤𝑤 = 𝑎𝑎1 𝑎𝑎2 𝑎𝑎3 … 𝑎𝑎𝑛𝑛 .其中𝑎𝑎𝑖𝑖 ∈ Σ(1 ≤ 𝑖𝑖 ≤ 𝑛𝑛).无限字为无限长度的字母

序列𝛼𝛼 = 𝑎𝑎1 𝑎𝑎2 𝑎𝑎3 …..其中,常见的无限字为循环无限字,类似于无限循环小数,表示为𝛼𝛼 = 𝛼𝛼1 (𝛼𝛼2 )𝜔𝜔 ,其中𝛼𝛼1 为循环

前缀,𝛼𝛼2 为循环主体.

给定无限字𝛼𝛼,以及自动机𝐴𝐴,那么𝛼𝛼在𝐴𝐴上的运行序列为𝜌𝜌 = 𝜌𝜌0 𝜌𝜌1 . . ∈ 𝑄𝑄 𝜔𝜔 ,满足𝜌𝜌0 ∈ 𝑄𝑄0 并且< 𝜌𝜌𝑖𝑖 , 𝑤𝑤𝑖𝑖 , 𝜌𝜌𝑖𝑖+1 > ∈

𝛿𝛿对所有𝑖𝑖 ≥ 0成立.若𝜌𝜌满足接受条件𝜆𝜆,则称𝛼𝛼是可以被自动机𝐴𝐴接受的,所有被接受的字组成的集合则是自动机
𝐴𝐴的语言,记作𝐿𝐿(𝐴𝐴).

对 自 动 机 𝐴𝐴 和 长 度 为 𝑙𝑙 的 字 𝑤𝑤 , 转 移 图 ( 𝛿𝛿~𝑔𝑔𝑔𝑔𝑔𝑔𝑔𝑔ℎ ) 记 为

𝐺𝐺𝑤𝑤𝐴𝐴 = (𝑉𝑉𝑤𝑤𝐴𝐴 , 𝐸𝐸𝑤𝑤𝐴𝐴 ) . 其 中 顶 点 集

𝑉𝑉𝑤𝑤𝐴𝐴 = {〈𝑝𝑝, 𝑖𝑖〉|𝑝𝑝 ∈ 𝑄𝑄, 0 ≤ 𝑖𝑖 ≤ 𝑙𝑙},边集𝐸𝐸𝑤𝑤𝐴𝐴 = {〈〈𝑝𝑝, 𝑖𝑖〉, 〈𝑞𝑞, 𝑖𝑖 + 1〉〉 | 〈𝑝𝑝, 𝑤𝑤(𝑖𝑖), 𝑞𝑞〉 ∈ 𝛿𝛿}.该图的意义在于可以说明对于状态𝑝𝑝

经由字𝑤𝑤到达𝑞𝑞,等价于在𝐺𝐺𝑤𝑤𝐴𝐴 图中存在一条路径从⟨p, 0⟩到达⟨𝑞𝑞, 𝑙𝑙⟩.

𝜔𝜔自动机是自动机的一种,区别在于接受条件不同.一个无限字能否被𝜔𝜔自动机接受取决于这个无限字在自

动机上的运行𝜌𝜌与该自动机接受条件的关系.我们用𝐼𝐼𝐼𝐼𝐼𝐼(𝜌𝜌)指该运行中出现无限次的状态集合,这里主要介绍本
文涉及的𝜔𝜔自动机:

Büchi 自动机,𝜆𝜆 = 𝐹𝐹 ⊆ 𝑄𝑄,𝜌𝜌是可接受的当且仅当𝐼𝐼𝐼𝐼𝐼𝐼(𝜌𝜌)⋂𝐹𝐹 ≠ ∅.

generalized Büchi 自 动 机 , 𝜆𝜆 = {𝐹𝐹1 , 𝐹𝐹2 , 𝐹𝐹3 , … , 𝐹𝐹𝑘𝑘 } , 其 中 𝐹𝐹𝑖𝑖 ⊆ 𝑄𝑄(1 ≤ 𝑖𝑖 ≤ 𝑘𝑘) . 𝜌𝜌 是 可 接 受 的 当 且 仅 当 对 于

∀ 𝑖𝑖 (1 ≤ 𝑖𝑖 ≤ 𝑘𝑘),都有𝐼𝐼𝐼𝐼𝐼𝐼(𝜌𝜌)⋂𝐹𝐹𝑖𝑖 ≠ ∅.

Rabin 自动机,𝜆𝜆 = {< 𝐺𝐺1 , 𝐵𝐵1 >, … . , < 𝐺𝐺𝑘𝑘 , 𝐵𝐵𝑘𝑘 >},其中,𝐺𝐺𝑖𝑖 , 𝐵𝐵𝑖𝑖 ⊆ 𝑄𝑄 (1 ≤ 𝑖𝑖 ≤ 𝑘𝑘),称〈𝐺𝐺𝑖𝑖 , 𝐵𝐵𝑖𝑖 〉为 Rabin pair. 𝜌𝜌是可接

受的当且仅当∃ 𝑖𝑖 (1 ≤ 𝑖𝑖 ≤ 𝑘𝑘),使得𝐼𝐼𝐼𝐼𝐼𝐼(𝜌𝜌)⋂𝐺𝐺𝑖𝑖 ≠ ∅,并且𝐼𝐼𝐼𝐼𝐼𝐼(𝜌𝜌)⋂𝐵𝐵𝑖𝑖 = ∅.
1.2 自动机的确定化与Büchi自动机的确定化

一 个自 动 机 𝐴𝐴 = {Σ, 𝑄𝑄, 𝑄𝑄0 , 𝛿𝛿, 𝜆𝜆}被 称作 确 定 的,当 且 仅当 满 足 条 件:|𝑄𝑄0 | = 1并 且对 所 有 𝑝𝑝 ∈ 𝑄𝑄 , 𝑎𝑎 ∈ Σ,都 有

{|{𝑞𝑞 ∈ 𝑄𝑄| < 𝑝𝑝, 𝑎𝑎, 𝑞𝑞 >∈ 𝛿𝛿}|} ≤ 1.将一个自动机由非确定性转变为确定性自动机被称为自动机的确定化.

对于 Büchi 自动机来说,Büchi 自动机确定化和 Büchi 自动机的求补,是成对出现的,如果一个 Büchi 自动

机是确定化的,那么其补自动机就可以通过对其接受条件的对偶化直接得出,而自动机求补在模型检测领域有

广泛重要的应用,这反应出了 Büchi 自动机的确定化在基础理论方面具有非常重要的意义.
1.3 基于状态的自动机和基于转移的自动机
通常我们的自动机都是基于状态的,也就是接受条件的集合元素是状态.但是随着确定化理论的研究,往往
基于转移的自动机是比较容易获得的.另外,线性时序逻辑 LTL 公式也可以直接转化成基于转移的 generalized
Büchi 自动机.这在后面的确定化算法中有详细的说明,随着确定化的进行,每一个转移(即状态到字母到状态的
有序三元组)所拥有的信息量更加的丰富,具有更强的表达能力,因而将一个转移本身作为新的自动机的接受条
件就变得更加自然与便捷.这两者一定条件下可以互相等价转化,但是不同自动机的状态数等规模可能会有指
数级别的差距.
1.4 有序树
在有限字上的有限状态机的子集构造法,通过状态的幂集即可表达有限状态机的所有可能状态.这在扩展
到无限字的情况时无法直接应用,因为仅仅知道状态的幂集,并不能反应出在无限序列中的状态,比如对于状态
的一个幂集,有限状态机中表示运行序列到达过这些状态,但是在无限字和无限序列中,仅仅知道“到达过”这

马润哲 等: Büchi 自动机确定化分析工具

2207

些状态是不够的,还需要知道是否无限次的访问.因为对有限的状态和无限的字来说,依据抽屉原理,一定是有
部分状态是无限多次访问的.如何记录这些被无限多次访问的状态,将其和被有限次访问的状态区别开来,这是
对无限字自动机确定化研究所需要面对的首要问题.通过对子集构造法思想的扩展,将不同状态集合依照一定
规则组合成树形数据结构成为一种有效的手段.
首先比较重要的研究成果是 Muller[4]的 Muller-Schupp 树,核心是通过着色(三种颜色)的方式记录不同的节
点,优点是非常直观,但是有大量冗余信息. Safra[13]为了缩减冗余节点,引入了有序树的概念.有序树是一种全新
的数据结构,基本思想是根节点的状态集为该树包含状态的超集,每一个节点的所有子节点的状态集合的并集
为该节点状态集合的真子集.同时每一棵树的所有节点依照一定的命名规则对应有自己唯一的识别标识. 有
序树简化了 Muller-Schupp 树中右边子冗余的部分,只保留了(会包含全部到达最终状态的)左边部分,因此可以
大幅降低存储数据的空间开销.Safra 在有序树的基础上建立了 Safra-树,其核心命名规则是依照有序树中的每
个节点从根节点出发的生成顺序依次来命名.基于 Safra 树的 Büchi 自动机确定化算法依然是当今主流的
Büchi 自动机确定化算法之一.Schewe 在 Safra 树的基础上对命名规则进行了改进,建立了历史树.历史树是一
种数据结构,反映了无限自动机在运行中的当前状态相关的性质,主要体现在可以即使反应自动机所经历的循
环中,可能无限多次访问的状态节点和一次未曾访问的节点的性质.历史树的结构为节点的集合,分为根节点和
子节点,每个节点包含三种主要属性,分别为标签 lable,命名 name,标记 flag.其中标签存储对应的 Büchi 自动机
的状态集合,反映了该节点所经历的所有状态.命名为该节点依据历史树的唯一对应位置坐标,根节点命名为
root,其他节点命名依据有序树的规则,例如 1.3.2 为根节点的第一子节点的第三个子节点的第二个子节点.所有
树的每一个点都满足该要求则说明这棵树是紧凑的,例如假如一个节点为根节点的第二个子节点,但是根节点
的第一个子节点被删除了,那么这个节点将“向左”移动成为根节点的第一个子节点,其命名将从 2 变为 1.该
操作叫做节点的重命名,对历史树所有节点进行重命名被称作历史树的序列化.标记指当前节点的状态,共有三
种可能,分别为无状态,接受状态,不稳定状态.新生成的节点默认都是无状态,后面两种状态在确定化算法中会
随着树之间的转化与节点的增删而改变.
我们在 Schewe 的历史树基础上通过对节点分类,将每个节点指向的索引进行冗余分析,依据是否可以合并
提出了新的命名规则,建立了 c-历史树 [19].具体的说,通过原本历史树上的每一个节点都具有绝对的路径名,例
如上面的 1.3.2,在一定条件下,这个节点所决定的索引对的拒绝集,是可以与另一个节点比如 1.4.1 所决定的索
引对的拒绝集相交为空.那么这两个索引就可以完成合并,因为合并之后的新的接受集和拒绝集将于原来的两
个索引对保持语义的一致.这样就可以使这两个节点享用同一个名字,这就是新的命名规则.依据此命名规则得
到的历史树就是 c-历史树,依据 c-历史树将大幅降低 Büchi 自动机确定化为 Rabin 自动机的索引数.
1.5 基于有序树的Büchi自动机的确定化算法
对于 Büchi 自动机的确定化算法,目前主流是基于有序树的确定化算法.这可以近似看作是子集构造法的
扩展,将状态子集扩展为有序树.本质上通过对有序树节点的命名来达到记忆是否无穷多次出现的目的.其算法
思想为:
1.通过 Büchi 自动机的初始状态和命名规则构造初始有序树.初始有序树即为确定化后的 Rabin 自动机
初始状态.
2.依据 Büchi 自动机的转移关系从初始状态出发,通过初始有序树繁衍生成全部有序树.其中有序树的生
成规则不依赖于命名规则的不同而不同,生成的全部有序树将作为确定化后的 Rabin 自动机的全部状态集.
3.依据有序树之间的转移关系得到确定化后的 Rabin 自动机的转移关系,同时按相应命名规则标记树上
的节点为接受/拒绝节点,并依此得到确定化后的基于转移的 Rabin 自动机的接受条件.
4.(可选)将基于转移的确定性 Rabin 自动机等价转换为基于状态的确定性 Rabin 自动机.

2208

Journal of Software 软件学报 Vol.32, No.7, July 2021

目前存在的 3 种命名规则:Safra-树命名规则,历史树命名规则和 c-历史树命名规则分别对应了 Safra 算
法,Schewe 算法和我们的算法.
1.6 完全自动机
一个 完全 自动 机 𝐹𝐹𝐹𝐹 = {Σ, 𝑄𝑄, 𝑄𝑄0 , 𝛿𝛿, 𝜆𝜆},其字母 表定 义为 Σ = 𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃𝑃(𝑄𝑄 × 𝑄𝑄),转移关 系 𝛿𝛿 定义 为:对 所 有

𝑝𝑝, 𝑞𝑞 ∈ 𝑄𝑄 , 𝑎𝑎 ∈ Σ, < 𝑝𝑝, 𝑎𝑎, 𝑞𝑞 >∈ 𝛿𝛿 iff < 𝑝𝑝, 𝑞𝑞 >∈ 𝑎𝑎.

完全自动机是一种特殊的自动机 [20],其思想是将转移关系表扩充到最大化,使得任意一个相同状态数,相同

字母表的自动机都能“嵌入”完全自动机其中,使得同状态的所有自动机都将是他的子集.为此,其字母表的大
2

小达到了2(|𝑆𝑆|) , 由于其庞大的字母表,在自动机理论研究证明中,对完全自动机构造合适的字将简化证明复杂

度.

完全自动机技术是一种用于证明自动机理论上下界的处理思路.其核心思想将普通自动机的字母表扩充
到最大的可能性上,得到最大的完全自动机,在证明应用中,我们只需要把精力放在构造巧妙的字,来代替之前
证明中需要构造的自动机家族,这可以极大的简化有关的上下界证明,甚至得出新的更紧上下界.
这里引用一个例子来说明完全自动机在相关理论证明的优越性.
定理 1.1 对任意状态为 n,字母表大小为 2 的自动机,其补自动机的状态数下界为 2 n.
这一定理虽然是直观的,但是其证明并不简单,传统方法历经等多人的研究[21],才通过构建无限自动机家族
的形式得到证明.然而使用完全自动机的辅助,构造合适的“桥梁”字,可以直接得到上面定理的证明.证明过程
简要如下.
证明:事实上,我们只需要能构造出一个满足要求的 n 状态的自动机,证明其补自动机的状态数至少为2𝑛𝑛 ,即

可完成证明,而完全自动机就刚好可以满足要求.记𝐹𝐹𝐴𝐴𝑛𝑛 = (Σ𝑛𝑛 , 𝑄𝑄𝑛𝑛 , 𝐼𝐼𝑛𝑛 , 𝛿𝛿𝑛𝑛 , 𝜆𝜆𝑛𝑛 ) 为状态为 n 的完全自动机,其中满

足𝑄𝑄𝑛𝑛 = 𝐼𝐼𝑛𝑛 = 𝜆𝜆𝑛𝑛 = {𝑠𝑠0 , 𝑠𝑠1 , … , 𝑠𝑠𝑛𝑛−1 }.对每个状态子集𝑇𝑇 ⊆ 𝑄𝑄𝑛𝑛 ,设一个函数𝐼𝐼𝐼𝐼(𝑇𝑇)用于表示该子集中所有状态“自己
到自己”的字的集合:𝐼𝐼𝐼𝐼(𝑇𝑇) = {〈𝑞𝑞, 𝑞𝑞〉|𝑞𝑞 ∈ 𝑇𝑇}.取任意两个字𝑢𝑢 𝑇𝑇 , 𝑣𝑣𝑇𝑇 ,满足𝑢𝑢 𝑇𝑇 ∈ 𝐼𝐼𝐼𝐼(𝑇𝑇), 𝑣𝑣𝑇𝑇 ∈ 𝐼𝐼𝑑𝑑(𝑄𝑄𝑛𝑛 \𝑇𝑇).则 uT vT 不是

FAn 的 语 言 . 记 FAn 的 一 个 补 集 自 动 机 为 CA, 那 么 uT vT 则 是 CA 的 语 言 , 也 就 是 说 , 存 在 一 个 𝑞𝑞�𝑇𝑇 使 得
𝑢𝑢𝑇𝑇

𝑣𝑣𝑇𝑇

𝑞𝑞�𝐼𝐼 �⎯⎯⎯⎯⎯� 𝑞𝑞�𝑇𝑇 �⎯⎯⎯⎯⎯� 𝑞𝑞�𝐹𝐹 .考虑到 S 的子集 T 相应的个数为 2n,若能证明每一个 T 对应至少一个不同的𝑞𝑞� 𝑇𝑇 ,那么即
𝑢𝑢𝑇𝑇1

可证明.考察 T1 ≠ T2 ,假设对应的 𝑞𝑞�𝑇𝑇1 = 𝑞𝑞� 𝑇𝑇2 .一定存在至少一个状 s 满足 s 属于{T1 \ T2 },因此 s�⎯⎯⎯⎯⎯�
𝑣𝑣𝑇𝑇2

𝑢𝑢𝑇𝑇1

𝑣𝑣𝑇𝑇2

𝑠𝑠 �⎯⎯⎯⎯⎯� s ,则𝑢𝑢 𝑇𝑇1 𝑣𝑣𝑇𝑇2 是原自动机 FAn 的语言.另一方面,又由于𝑞𝑞�𝐼𝐼 �⎯⎯⎯⎯⎯� 𝑞𝑞�𝑇𝑇1 = 𝑞𝑞�𝑇𝑇2 �⎯⎯⎯⎯⎯� 𝑞𝑞�𝐹𝐹

原自动机 FAn 的补集 CA 的语言,得到矛盾.所以可以得出 FAn 的补集至少要 2 n 个状态数.

,则𝑢𝑢 𝑇𝑇1 𝑣𝑣𝑇𝑇2 是

完全自动机理论的证明关键是构造合适的完全自动机,将验证的问题延迟到了转移分析的问题上,从而由
传统的构造合适的自动机族优化为构造合适的字,这往往更加直观和方便.

2 工具实现
2.1 工具架构
工具分为数据处理,确定化算法,分析验证三个模块.具体架构如图 1 所示

马润哲 等: Büchi 自动机确定化分析工具

2209

cc
图 1 NB2DR 工具框架

使用 java 编写（jdk 版本为 18.0.1.1）,代码总计非空行约 3 万行.数据处理模块将自动机格式标准化,兼容
了其他工具如 GOAL 的自动机格式,同时包含了无限字的创建,历史树的建模,以及各种自动机的接受条件判定
通式和完全自动机生成.确定化算法模块整合了索引数最优的 Safra 算法,状态数最优的 Schewe 算法[14],同时还
实现了基于新的命名规则,剪掉了冗余的部分历史树分叉,在状态数与 Schewe 持平条件下达到更小索引的我们
的算法 [19].分析验证模块面向确定化研究的实际问题,针对性的实现研究需要的常用功能,包括等价性测试,索
引包含分析,自动机族生成等部分.等价性测试采用近似的穷举无限字的算法,可以对输入的任意两个自动机进
行等价性测试.自动机生成与分析模块可以按照输入要求生成指定类型的自动机家族,相当于新建满足一定条
件的自动机数据集,同时对其的确定化可以分析其运行状态和需要的诸如状态数,索引数,pair 集合判断合并情
况等运行细节.
2.2 数据处理
该模块主要定义了工具运行所需要的数据结构,包括各种无限字自动机结构.输入部分可以选择自行构建
任意结构的自动机,也支持以 GOAL 程序的配置文件标准导入相应的自动机[8].对于完全自动机可以支持对任
意状态数,任意接受条件的生成.同时输出部分引入 Graphviz 插件[22],将生成的自动机转移关系输出成该插件支
持的 DOT 语言,方便将简单的自动机生成可视化图像.该插件是一款常见的开源可视化工具,基于多种布局决
策算法,可以根据 DOT 文件生成较为直观的有向图.重要的数据结构如下.
𝝎𝝎 自动机结构.𝜔𝜔自动机数据结构主要分为状态节点集合,字母表集合,转移关系集合,接受条件类型部分.

对于 n 状态的自动机,节点标签默认为 0 到 n-1.对于字母表大小为 m 的自动机,字母默认顺序为从 0 到 m-1.转
移关系使用继承自二维数组的类来存放,第一维度为起始状态,第二维度为字母,值为该起始状态经由该字母所
到达的终末状态集合.如果为空则设为空集,如果每一个终末集合均为单元素集合,那么该自动机就是确定的.
完全自动机结构.完全自动机技术是一种用于证明自动机理论上下界的处理思路.其核心思想将普通自动
机的字母表扩充到最大的可能性上,得到最大的完全自动机,使得同状态的所有自动机都将是他的子集.由于其
庞大的字母表,在证明应用中,我们只需要把精力放在构造巧妙的字,来代替之前证明中需要构造的自动机家
族,这可以极大的简化有关的上下界证明,甚至得出新的更紧上下界.现有工具并没有相关的完全自动机实现,
本工具可以实现任意指定状态数 n 的完全自动机生成,并对较小的 n 进行可视化展示.
2.3 确定化算法
该模块为工具核心部分, 整合了索引数最优的 Safra 算法,状态数最优的 Schewe 算法,以及在状态数与

2210

Journal of Software 软件学报 Vol.32, No.7, July 2021

Schewe 持平条件下达到很小索引的我们的算法.这三者算法均是基于有序树的 Büchi 自动机确定化算法,区别
主要在命名规则的不同上.
有序树作为确定化算法的核心,负责存储节点数据信息.本工具采用离散的哈希表存储有序树,其键为树中
节点的绝对坐标,依照 Schewe 算法的历史树坐标命名记录.值为树中的节点.其信息包含节点标记,节点标签,节
点属性,节点状态.节点标记为标记其对应命名规则的参数,Safra 为 1,Schewe 为 2,我们的算法为 3.节点标签记
录该节点在对应命名规则下的命名.节点属性记录该节点代表的自动机状态集合,由于状态都是由数字从 1 到 n
依次表示,该属性由 vector 类存储.节点状态用于标记节点当前是否处于接受/不稳定状态.
对应命名规则主要有三种.Safra 的命名规则核心是依据有序树上节点的生成顺序,确定化后其规模上界为
2

𝑂𝑂(𝑛𝑛𝑙𝑙𝑙𝑙𝑙𝑙 𝑛𝑛)

,得到的 Rabin pair 索引数目上界为 n.Schewe 命名规则依据有序树的节点绝对坐标,确定化后其规模上

界为(1.65n)n,得到的 Rabin pair 索引数目上界为 2 n.我们的命名规则依据剪掉冗余历史树分叉节点,依据可达性
缩减历史树节点的名称数目,得到新的 c-历史树, 确定化后其规模上界为(1.65n)n,得到的 Rabin pair 索引数目上
𝑛𝑛

界为2 2 .

这 里 首 先 以 Schewe 算 法 为 例 , 演 示 确 定 化 算 法 . 输 入 为 标 准 的 非 确 定 性 Büchi 状 态 自 动 机

𝐵𝐵 = {Σ, 𝑄𝑄, 𝑄𝑄0 , 𝛿𝛿, 𝐹𝐹}(如图 2),输出为确定性 Rabin 转移自动机. 𝑅𝑅 = {Σ, 𝑄𝑄𝑟𝑟 , 𝑄𝑄, 𝛿𝛿𝑟𝑟 , 𝜆𝜆𝑟𝑟 }.

图 2 Büchi 自动机 B 实例
(1) 首先根据自动机的初始状态生成初始历史树ℎ0 .具体为,取其根节点的标签为𝐵𝐵的初始状态.将该历史

树ℎ0 加入输出的 Rabin 自动机的状态集,并将其设置为该状态机的初始状态.

(2) 由ℎ0 出发开始进行历史树遍历.这一步将完成生成全部可能历史树的目的.由于历史树的性质,其根节

点标签最大为𝐵𝐵的状态数 n,而节点具有“任意节点 t 的全部子节点的标签并集为 t 的标签真子集”这一特点,
每一层都会减少,因此历史树最多有 n 层,(不会是无限树),因此历史树的总量是一定的.也就是说,可以通过遍历
搜索的思路得到全部的历史树可能.换言之,这些历史树的集合将作为确定性 Rabin 自动机的所有状态集,而这
些历史树的最大值即为相应确定性 Rabin 自动机的状态数上界.下面是具体的遍历算法:
a. 数据预处理.建立历 Rabin 自动机状态集𝑄𝑄𝑟𝑟 ,亦即历史树集合,将ℎ0 添加进入该集合, 𝑄𝑄𝑟𝑟 ={ℎ0 }.建立 Rabin

自动机初始状态集合𝑄𝑄𝑟𝑟0 = {ℎ0 },建立 Rabin 自动机的转移关系集合𝛿𝛿,建立 Rabin 自动机的接受条件集合
𝜆𝜆 = {< 𝐺𝐺1 , 𝐵𝐵1 >, < 𝐺𝐺2 , 𝐵𝐵2 >, … < 𝐺𝐺𝑘𝑘 , 𝐵𝐵𝑘𝑘 > }

建立当前待处理历史树队列 qh 并将 h0 压入该队列,即 qh={ℎ0 },队列满足先进先出的原则.

b. 弹出 qh 中的元素 h,即得到当前待处理历史树 h.建立当前待处理字母队列 qa 并将字母表中的字母全部

压入该队列.
c. 弹出 qa 中的元素 a,即得到当前待处理字母 a,依照历史树转移算法得到 h 经由 a 的后继历史树ℎ′,即
(ℎ, 𝑎𝑎, ℎ′).具体历史树转移算法如下:

i 对历史树 h 进行全节点扩充.首先预处理需要将 h 上的全部节点的标记改为无状态.对于 h 树上的每一个

节点𝑡𝑡,添加一个该节点的最右子节点𝑡𝑡′.并且𝑡𝑡′标签为𝑡𝑡节点与接受状态𝜆𝜆的交际,𝑡𝑡′的命名依照之前历史树的节点
命名规则不变,𝑡𝑡′的标记统一为无状态.

马润哲 等: Büchi 自动机确定化分析工具

2211

ii 对 i 中得到树从上到下每一层进行相同性检验.对于每一层的节点来说,建立该层的已有状态队列 qse.从
最左节点开始往右逐个扫描其标签,判断其标签中的每一个状态是否在 qse 中.如果不在,则将该状态加入 qse
中;如果在则删除该标签中的该状态.重复此过程直至该层从左到右所有节点均被进行相同性检验.
iii 对 ii 中得到的树的每一个节点进行包含性检验.建立这一步的包容性检验队列 qc,首先将根节点 root 压
入该队列 qc.从 qc 读取当前节点𝑡𝑡,判断该节点𝑡𝑡的标签状态集是否与𝑡𝑡的所有子节点的标签状态集的并集相同,
如果是,则删除𝑡𝑡的全部子节点,然后将 t 的标签改为接受状态;如果否,则将𝑡𝑡的全部子节点均压入 qc 中.重复此操
作直至 qc 中没有节点为止.
iv 对 iii 中得到的树进行删除空节点.这一步就是遍历全部树中的节点,将所有空节点直接删除.
v 对 iv 中得到的树进行序列化.建立这一步的序列化队列 qo,首先将根节点 root 压入该队列 qo.从 qo 读取
当前节点 t,首先判断该节点是否标记为不稳定节点.如果是,则将其子节点全部修改标签为不稳定节点,并压入
qo 中.如果否,则继续判断该节点的命名是否正确(例如 3.3.2 节点实际位置应该是 2.3.2),如果正确,则将该节点
的所有子节点压入 qo 中;如果不正确,则将其重命名为正确的名字,并将节点的标签改为不稳定节点.从 qo 中弹
出该节点并反复操作知道 qo 中再无节点.至此得到的树即是ℎ经字 a 得到的后继历史树ℎ′.
将得到的转移(ℎ, 𝑎𝑎, ℎ′)添加进入 Rabin 自动机转移关系中.如果ℎ′中存在不稳定节点,则将该转移添加进入

该节点名字 i 对应的𝜆𝜆𝑟𝑟 <Gi,Bi>的 Bi 中.如果ℎ′中存在接受节点,则将该转移添加进入该节点名字 i 对应的𝜆𝜆𝑟𝑟
<Gi,Bi>的 Gi 中.

d 判断ℎ′是否存在于𝑄𝑄𝑟𝑟 中,如果不存在,则将ℎ′添加进入𝑄𝑄𝑟𝑟 中,并将ℎ′压入 qh 队列.重复 c 步骤直到 qa 为空,

即全部字母表中的字母均经历过历史树ℎ′的遍历.

e 重复 b 步骤直到 qh 队列为空,此时完成全部历史树生成,同时也相应完成了全部转移关系和接受条件的
生成.如图 3 所示.

图 3 确定化为 Rabin 自动机算法中产生得到全部历史树

2212

件.

Journal of Software 软件学报 Vol.32, No.7, July 2021

(3) 整合上一步得到的确定性 Rabin 转移自动机,检查其接受条件𝜆𝜆𝑟𝑟 ,将其中的空 pair 删除,得到最终接受条
接受条件为𝜆𝜆 = {({ℎ0 [1]ℎ2 , ℎ4 [0]ℎ6 , ℎ9 [0]ℎ10 , ℎ10 [0]ℎ6 }, {∅}), ({ℎ5 [1]ℎ3 , ℎ9 [1]ℎ12 , ℎ11 [1]ℎ13 }, {∅}),
({ℎ7 [1]ℎ10 }, {∅}), ({ℎ10 [1]ℎ7 , ℎ13 [1]ℎ11 }, {ℎ11 [0]ℎ4 }), ({ℎ12 [1]ℎ9 }, {ℎ11 [0]ℎ4 })}

如图 4 所示为最终确定化得到的 Rabin 自动机.

图 4 生成的 Rabin 自动机 R 实例
(4) 之前得到的是基于转移的确定性 Rabin 自动机,状态数大小为𝑜𝑜((1.65𝑛𝑛)𝑛𝑛 ).可以继续利用相应算法将其

转化为基于状态的确定性 Rabin 自动机.目前主要有两种方法,分别适合于不同大小字母表时的情况.

第一种较简单的思路是根据每一个转移和字将状态数扩充.由于转移是(ℎ, 𝑎𝑎, ℎ′)对,将每一个后继的 h’和其

经历的字 a 可以看作一个静态的整体进行绑定当作新的确定的基于状态的 Rabin 自动机的状态,这样得到的
Rabin 自动机状态数为𝑙𝑙 ⋅ 𝑜𝑜((1.65𝑛𝑛)𝑛𝑛 ),其中 n 为基于转移的 Rabin 自动机状态数,𝑙𝑙为基于转移的 Rabin 自动机

的字母表大小.这样做虽然增大了确定化的自动机的状态数,不过可以得到更加直观的基于状态的 Rabin 自动
2

机.该思路在字母表比较小的时候很容易达成,但是由于 n 个状态的自动机的字母表上限高达2𝑛𝑛 个,所以在具有

较大字母表时,会对最终确定化的状态数有指数级的影响.

这时,可以采用第二种策略,也就是从历史树本身入手对转移关系进行改进.转移关系的本质是在(Rabin)接
受条件中,pair 的选择决定于两棵树之间转移关系中出现的接受节点和不稳定节点.那么很自然的思路就是可
以对历史树依据接受节点和不稳定节点进行扩展.首先对于每一个历史树𝑇𝑇,记其节点数为 m,那么每一个节点

马润哲 等: Büchi 自动机确定化分析工具

2213

有接受节点、不稳定节点和其他节点三种可能,也就是3𝑛𝑛 个不同的历史树,接下来再根据历史树的性质将其中

大多数不合理的树删除.首先接受节点和不稳定节点只能是叶子节点,同时若一个节点为接受节点,那么其左侧
兄弟节点和父节点均不可能是不稳定节点.其次对于不稳定节点来说,若一个节点为不稳定节点,那么其右侧的

兄弟节点和全部子节点一定是不稳定节点.基于此得到的历史树族就是该历史树𝑇𝑇 经由添加节点接受信息后
的扩展历史树(enriched history tree),将全部历史树的扩展历史树算出即可得到新的基于状态的确定性 Rabin
自动机的状态节点,同时接受条件变为由对应树节点的该节点为接受节点的扩展历史树集合和该节点为不稳
定节点的扩展历史树集合构成的对的集合.该全部扩展历史树的数目可以通过递归估算为𝑜𝑜((2.66𝑛𝑛)𝑛𝑛 )个,即相
应确定性 Rabin 自动机状态复杂度为𝑜𝑜((2.66𝑛𝑛)𝑛𝑛 ).

2.4 分析验证

这一模块可以辅助自动机领域的部分研究.包括等价性测试,索引包含分析,自动机家族生成,完全自动机
生成等部分.
等价性测试.对于任意无限字的有限状态自动机来说,想直接得到其接受语言集是比较困难的,甚至是不可
能在有限空间内表示清楚的.因此,想直接验证两个自动机的语言是否等价往往需要依赖于自动机本身的特点.
通常,如果能直观看出自动机的语言集,尤其是在自动机本身状态数较小时还是相对容易的.部分工具的等价性
验证需要将自动机重新转回时序逻辑语言,利用求解器进行辅助.但随着自动机的状态数增多,接受条件的索引
也增多,尤其是两个自动机种类不同时,接受条件也更加的复杂,这时候等价性验证将变得非常困难.本工具在
这个问题上进行了一定尝试,具体的思路就是生成一系列的无限字,然后观察其在待验证自动机的运行状态,是
否满足相对应的接受条件.只要找到一个反例即可判定两个自动机是不等价的,如果在足够多的无限字上都能
满足接受,那么就可以判断在一定的置信区间上这两个自动机是等价的.
具体来说,首先是生成一个无限字.这个无限字默认采用循环无限字的结构,根据字母表随机产生其前缀字
和循环节字,然后对于𝜔𝜔来说,实际采用𝑤𝑤次,通常设置𝑤𝑤为对应自动机的状态数的𝑚𝑚𝑚𝑚倍.接下来让两个自动机分
别运行这个字,判断其接受条件,对于需要无限次访问的参数判断是否达到𝑚𝑚次,若达到则判定其结论为接受.如
果两个自动机的接受结论不一致,则可以判定两个自动机的语言不一致.如果结论一致,则继续重复这个过程,
重复𝑚𝑚次,如果𝑚𝑚次两个自动机的语言都一致,那么得到结论两个自动机的语言是一致的.
自动机族生成.同类自动机家族生成模块可以方便我们生成指定条件的自动机族.对于给定不大的状态数
为𝑛𝑛且字母表大小为𝑘𝑘的 Büchi 自动机,本工具支持生成符合条件的随机自动机(单个)以及在内存设置允许的情
况下进行枚举所有自动机(全部).依照的算法是根据状态数𝑛𝑛和字母表大小𝑘𝑘确定可能的转移关系规模,得到𝑘𝑘个
由𝑛𝑛到2𝑛𝑛 的映射作为 Büchi 自动机的转移关系,且对应的全部𝑛𝑛𝑛𝑛个幂集按照空集,单状态集,双状态集…直到含

所有状态的集其全部子集的顺序生成,即共有(2𝑛𝑛 )𝑛𝑛𝑛𝑛 种可能.该枚举操作做了多线程优化,可以一边生成自动机
一边对其进行其他如确定化的操作.同时进行了内存管理,若该次操作生成的自动机大小规模超过了指定内存

大小,则会终止运行.

生成的自动机族可以实时进行按需要筛选保存.例如可以生成全部状态数为 3,字母表数为 3 的 Büchi 自动
机家族,满足其确定化之后得到的 Rabin 自动机索引数为 4.这为分析 Büchi 自动机等价转化为确定性 Rabin 自
动机时的运行流程和其确定化规律提供了直观的大数据用例.同时还可以筛选出包含有任意可指定的相同子
转移关系的 Büchi 自动机,换言之这些自动机会包含同样的转移关系部分.这些自动机相当于是在同样的原初
自动机基础上添加了额外的若干转移关系得到的,在对这些 Büchi 自动机族做确定化之后,就可以看出对于确
定的部分转移关系保持不变的情况下,新添加的这些转移关系会对确定化后的 Rabin 自动机产生怎样的影响.
索引分析.该分析可以对于 Rabin 自动机的接受条件进行直观的展示分析,往往我们通过 Schewe 算法得到
的 Rabin 转移自动机拥有大量的空索引和重复索引,甚至有时会出现矛盾索引,该模块可以直观指出不同索引
之间的相互依赖关系.遗憾的是目前还无法直接优化确定化后的 Rabin 转移自动机使其达到最精简的地步,只

2214

Journal of Software 软件学报 Vol.32, No.7, July 2021

能起到相应的辅助的作用.要想得到最优的确定性 Rabin 自动机还需要人工根据该模块进行识别其接受状态,
再做手动筛选除去空索引和重复索引,依照矛盾索引对无冲突的索引进行并集操作合并简化优化.通过对索引
的包含分析,我们可以观察确定性的自动机的索引分布,探究其不同合并可能的内在规律.例如上一章中的例子
中,可以看到接受条件为
𝜆𝜆 = {({ℎ0 [1]ℎ2 , ℎ4 [0]ℎ6 , ℎ9 [0]ℎ10 , ℎ10 [0]ℎ6 }, {∅}), ({ℎ5 [1]ℎ3 , ℎ9 [1]ℎ12 , ℎ11 [1]ℎ13 }, {∅}),
({ℎ7 [1]ℎ10 }, {∅}), ({ℎ10 [1]ℎ7 , ℎ13 [1]ℎ11 }, {ℎ11 [0]ℎ4 }), ({ℎ12 [1]ℎ9 }, {ℎ11 [0]ℎ4 })}

这里前三个 Rabin pair 由于拒绝集均为空集,因此可以将这三者合并到一个 pair 中,起到减少索引数的作
用.田等人在 Büchi 自动机的相关削减索引理论方面取得了部分成果,通过对冗余历史树的节点合并,重新为每
个节点分配新的名字,得到新的命名方式,并基于此得到新的 Büchi 自动机确定化算法[19],我们工具也实现了这
一算法,方便验证得到更小的确定性 Rabin 自动机.

3 工具展示
3.1 确定化算法展示
本节通过两个 Büchi 自动机实例的确定化来展示工具的确定化效果,这两个自动机分别是任意自动机和完
全自动机.
Büchi 自动机 B1 如图 5 所示,将其按照状态数,字母表数,以及状态转移关系和接受状态集合作为参数输入.
运行 Büchi 自动机确定化(历史树算法),可以直接得到确定化的基于转移的 Rabin 自动机.生成的 Büchi 自动机
如图 6 所示.

图 5 输入的 Büchi 自动机 B1 实例

马润哲 等: Büchi 自动机确定化分析工具

2215

图 6 生成的 Rabin 自动机实例

完全自动机因为比较字母表非常大,这里只取 n=2 时的完全自动机采用工具生成,如图 7 所示,其中初始状
态设置为 s0,接收状态设置为 s1.运行 Büchi 自动机确定化(历史树算法),可以直接得到确定化的基于转移的
Rabin 自动机.生成的 Büchi 自动机如图 8 所示.

图 7 输入的完全 Büchi 自动机实例

2216

Journal of Software 软件学报 Vol.32, No.7, July 2021

图 8 输出的完全 Rabin 自动机实例
3.2 自动机族生成
对于指定输入 Rabin pair 数目为 2,目标 Büchi 自动机的状态数为 3 时,可以输出部分满足该条件的 Büchi
自动机,如图 9 所示.

马润哲 等: Büchi 自动机确定化分析工具

图9

2217

n=3 时产生的 Büchi 自动机族

3.3 实验与分析
这部分我们考虑得到的确定性 Rabin 自动机的索引数目的优化问题.我们采用之前工具的自动机族生成
功能产生需要的数据集.考察不同状态数和字母表大小对索引的影响.其中数据集选择为:指定状态数由 3 到 8,
字母表大小分别设置为 2,5,10,随机生成 200 个符合条件的 Büchi 自动机.首先并使用工具对其确定化,并进行
等价性测试.再依算法[19]对其索引数进行优化.全部每组的初始 Büchi 自动机,确定化后的 Rabin 自动机,索引
优化后的 Rabin 自动机三者均通过了等价性测试.其中索引优化的具体结果如表 1 所示
NBW 和 DRW 分别表示非确定 Büchi 自动机和确定化后得到的确定性 Rabin 自动机,#状态数和#字母表
分别表示 Büchi 自动机的状态数目和字母表大小,每组数据均产生了 200 个相应条件的自动机.#索引数为确定
化后得到的全部 Rabin 自动机索引数目的平均值.可以从表中看到,字母表和状态数对索引数目都有影响,其增
大都会导致索引数目的增大.我们的优化算法确实有一定效果,尤其在字母表增大的情况下比较明显.
本节展示不同的三种 Büchi 自动机确定化算法的效果与工具的可用性.这里数据集全部由之前工具的自
动机族生成功能产生我们需要的数据集,非确定性 Büchi 自动机族.这里数据集选择为状态数指定从 3 到 8,每个
状态对三种字母表大小,分别为 2,5,10,每一种参数随机生成 200 个符合条件的非确定性 Büchi 自动机.对每个
自动机分别采用三种确定化算法得到确定 Rabin 自动机.实验结果见表 1.
表 1 三种 Büchi 自动机算法确定化效果比较
NBW

DRW #Safra

DRW #Schewe

DRW #our

#eq

#states

#alpha

#states

#Rabin
pairs

#states

#Rabin pairs

#states

#Rabin
pairs

3

2

18

1.150

9.1

1.240

9.1

1.240

T

3

5

20

1.730

10.2

1.900

10.2

1.900

T

3

10

21

2.530

12.2

2.470

12.2

2.350

T

4

2

75

2.160

24.3

1.600

24.3

1.600

T

4

5

80

2.735

25.1

2.960

25.1

2.955

T

4

10

123

3.450

25.9

3.880

T

2

320

2.365

110

4.030
2.020

25.9

5

110

2.010

T

5

5

439

3.670

120

4,020

120

3.895

T

5

10

460

3.970

138

6.180

108

5.445

T

6

2

1513

3.530

552

1.995

552

1.995

T

6

5

1783

4.150

593

4.445

593

4.435

T

6

10

2319

4.815

609

9.100

609

7.650

T

7

2

6739

4.174

1208

2.140

1208

2.140

T

7

5

7809

4.530

1243

4.920

1243

4.915

T

7

10

8630

5.130

1381

9.030

1381

8.650

T

8

2

13748

4.375

1893

1.990

1893

1.990

T

8

5

15416

4.920

2173

4.520

2173

4.510

T

2453

9.700

2453

8.670

T

8

10

17312

5.740

2218

Journal of Software 软件学报 Vol.32, No.7, July 2021

NBW 和 DRW 分别表示非确定的 Büchi 自动机和确定的 Rabin 自动机.#Safra, #Schewe, #our 分别表示
Safra 的基于 Safra 树的确定化算法,Schewe 的基于历史树的确定化算法和我们的基于 c-历史树的确定化算
法.#state #alpha #Rabin pairs 分别表示状态数,字母表大小和 Rabin 自动机接受条件的索引数.#eq 表示等价性
验证.从表中数据可以看出,Safra 的算法相比于 Schewe 和我们的算法,虽然索引较小,但是状态数很大.我们的算
法和 Schewe 相比在相同状态数下有较小的索引数.不过这在 Büchi 自动机状态数较小和字母表较少时并不明
显,这是因为我们的算法的命名规则需要在状态数很大,迁移数较多时才能有明显的缩减效果.
为此,我们针对性的设计数据集为较大迁移数的 Büchi 自动机族(接近完全自动机).选择状态数从 3 到 7 的
非确定 Büchi 自动机,每一个状态值对应迁移数分别为 11,24,40,60,84,字母表大小分别为 4,6,8,10,12.其确定化
结果见表 2.
表 2 复杂情况下我们与 Schewe 算法对比

NBW
#states

#trans

#

DRW
#alpha

#states

#trans

#Schewe

tr

#our

#Rabin
pair

#time

an

#Rabin
pair

#time

s
为

3

11

4

9

36

4

0.01

3

0.01

4

24

6

30

180

7

0.05

4

0.05

5

40

8

228

1824

15

0.26

6

0.29

6

60

10

2316

23160

31

34.65

11

35.25

自
动
机
的
转
移
数

7

84

12

-

-

-

T.O,

-

T.O.

目
,#

time 为运行时间(s), T.O.为运行超时(超过 1h).可以看到在状态数为 7 时对应 Büchi 自动机的确定化已经超时,
在状态数为 5 和 6 时,我们的算法得到的索引数为 6 和 11,大幅小于 Schewe 的算法对应的 15 和 31.

4 相关工作
Althoff 等人[16] 开发的 OmegaDet 工具对 Büchi 自动机的确定化算法进行了研究,他们主要对比采用了 Safra
和 Muller-Schupp 的确定化算法,该工具虽然局限于发表时间较早只有两种并不最优的确定化算法进行实验,但
是 OmegaDet 通过直观对比 MullerSchupp 树和 Safra 树的异同,展示了无限字自动机确定化领域的发展思想,
即通过删除冗余节点达到简化系统的目的.这对我们的工具的研究是一种很重要的启发.
Duret-Lutz 等人[17]开发的 SPOT 工具主要面向与模型检测实际需求中的 LTL 到基于转移的 Generalized
Büchi 自动机的转换,它更偏向于作为模型检测器与数据类型算法结构等的桥梁,而不是在于自动机确定化方
面的研究.虽然 SPOT 提供了一定的相关自动机模型接口,但是其服务在 19 年已经不再提供支持.
GOAL[18] 是一款比较成熟的无限字自动机与逻辑的研究工具,支持多种自动机的生成与确定化算法,以及
求补等操作.正因为 GOAL 设计的比较全面而且代码比较早,其自动机的底层数据结构比较复杂,一定程度上增

马润哲 等: Büchi 自动机确定化分析工具

2219

大了运行大规模自动机,尤其是运行完全自动机时的时间与空间开销,更适用于面向教学演示等规模较小的自
动机应用场景.相比之下我们的工具可以精准的解决面临的问题,更好的发挥服务器潜能,铺垫后续的研究,这
也是我们团队实现本工具的初衷.

5 总结与展望
无限字自动机的确定化一直是理论计算机领域,特别是形式化验证与时序逻辑领域重要的组成部分,随着
如今计算机性能的高速发展,使用适当工具对复杂自动机进行确定化是研究其内在确定化逻辑的重要方法.我
们实现了确定化研究工具 NB2DR.可以对非确定性 Büchi 自动机进行高效准确的确定化,并通过工具提供的分
析其确定化过程来达到对其确定化算法改进的目的.
未来将进一步提高工具的普适性,加入对于 Emerson-lei 等自动机的支持,并对无限字自动机的索引理论上
界进行深入研究.
References:
[1]

Büchi J. On a decision method in restricted second order arithmetic. In: Proc. of the 1960 International

Congress on Logic, Methodology and Philosophy of Science. Stanford University Press, 1962. 1–12.
[2]

Rabin M. Decidability of second-order theories and automata on infinite trees. Transactions of the

American Mathematical Society, 1969, 141: 1–35. [doi: 10.2307/1995086]
[3]

Boker U, Kupferman O. Translating to co-Büchi made tight, unified, and useful. ACM Trans. on

Computational Logic, 2012, 13(4): 29:1-29:26. [doi: 10.1145/2362355.2362357]
[4]

Muller. Infinite sequences and finite machines. In: Proceedings of the 4th Annual Symposium on

Switching Circuit Theory and Logical Design FOCS 1963: pp. 3-16. IEEE Computer Society Press, 1963, 10(4:13):
1-27. [doi: 10.2168/LMCS-10(4:13)2014]
[5]

Vardi, M.Y.: The Büchi complementation saga. In: Thomas, W., Weil, P. (eds.) STACS 2007. LNCS, vol.

4393, pp. 12–22. Springer, Heidelberg (2007)
[6]

Tsai MH, Fogarty S, Vardi M, Tsay YK. State of Büchi complementation. Logical Methods in Computer

Science. 2014, 10(4:13): 1-27. [doi: 10.2168/LMCS-10(4:13)2014]
[7]

Safra S. Exponential Determinization for omega-Automata with Strong-Fairness Acceptance Condition

(Extended Abstract). In: Proc. of the 24th Annual ACM Symp. on Theory of Computing (STOC’92). 1992: 275–282.
[doi: 10.1145/129712.129739]
[8]

Meyer P, Sickert S. Strix: explicit reactive synthesis strikes back! In: Proc. of the 30th International

Conference on Computer Aided Verification (CAV’18). 2018: 578-586. [doi: 10.1007/978-3-319-96145-3_31]
[9]

Streett R. Propositional dynamic logic of looping and converse. In: Proc. of the 13th Annual ACM Symp.

on Theory of Computing (STOC’81). 1981: 375-383. [doi: 10.1145/800076.802492]
[10] Mostowski A. Regular expressions for infinite trees and a standard form of automata. In: Proc. of the
Computation Theory. LNCS 208, Springer, Berlin, Heidelberg, 1984. 157–168. [doi: 10.1007/3-540-16066-3_15]
[11] Piterman N. From nondeterministic Büchi and Streett automata to deterministic parity automata. Logical
Methods in Computer Science, 2007, 3(3:5): 1-21. [doi: 10.2168/LMCS-3(3:5)2007]
[12] Safra S. Exponential Determinization for omega-Automata with Strong-Fairness Acceptance Condition
(Extended Abstract). In: Proc. of the 24th Annual ACM Symp. on Theory of Computing (STOC’92). 1992: 275–
282. [doi: 10.1145/129712.129739]

2220

Journal of Software 软件学报 Vol.32, No.7, July 2021

[13] Safra S. On the complexity of 𝜔𝜔-automata. In: Proc. of the 29th Annual Symp. on Foundations of

Computer Science (FOCS’88). IEEE Computer Society, 1988: 319-327. [doi: 10.1109/SFCS.1988.21948]

[14] S. Schewe. Tighter Bounds for the Determinisation of Büchi Automata. FOSSACS 2009: 167-181. 2009:

578-586. [doi: 10.1007/978-3-319-96145-3_31]
[15] Tian C, Wang WS, Duan ZH. Making Streett Determinization Tight. In: Proc. of the 35th Annual
ACM/IEEE Symp. on Logic in Computer Science (LICS’20). 2020: 859-872. [doi: 10.1145/3373718.3394757]
[16] Althoff C, Thomas W, Wallmeier N. Observations on determinization of Büchi automata. Theoretical
Computer Science, 2006, 363: 224-233. [doi:10.1016/j.tcs.2006.07.026]
[17] Duret-Lutz A, Poitrenaud D. SPOT: an extensible model checking library using transition-based
generalized Büchi automata. In: Proc. of the IEEE Computer Society’s 12th Annual Internation Symp. on Modeling,
Analysis, and Simulation of Computer and Telecommunications Systems (MASCOTS’04). 2004: 76-83. [doi:
10.1109/MASCOT.2004.1348184]
[18] Tsay YK, Chen YF, Tsai MH, Wu KN, Chan WC. GOAL: A Graphical Tool for Manipulating Büchi
Automata and Temporal Formulae. In: Proc. of the Tools and Algorithms for the Construction and Analysis of
Systems. LNCS 4424, Springer, Berlin, Heidelberg, 2007. 466–471. [doi: 10.1007/978-3-540-71209-1_35]
[19] Tian C, Duan ZH. Büchi Determinization Made Tighter. https://doi.org/10.48550/arXiv.1404.1436
[20] Yan, Q.. Lower Bounds for Complementation of ω-Automata Via the Full Automata Technique. In:
Bugliesi, M., Preneel, B., Sassone, V., Wegener, I. (eds) Automata, Languages and Programming. ICALP 2006.
Lecture Notes in Computer Science, vol 4052. Springer, Berlin, Heidelberg. https://doi.org/10.1007/11787006_50
[21] Galina Jirásková,State complexity of some operations on binary regular languages,Theoretical Computer
Science,Volume 330, Issue 2,2005,Pages 287-298,ISSN 0304-3975,https://doi.org/10.1016/j.tcs.2004.04.011.
[22] Ellson, J., Gansner, E., Koutsofios, L., North, S.C., Woodhull, G. (2002). Graphviz— Open Source
Graph Drawing Tools. In: Mutzel, P., Jünger, M., Leipert, S. (eds) Graph Drawing. GD 2001. Lecture Notes in
Computer Science, vol 2265. Springer, Berlin, Heidelberg. https://doi.org/10.1007/3-540-45848-4_57


软件学报 ISSN 1000-9825, CODEN RUXUEW
Journal of Software [doi: 10.13328/j.cnki.jos.007096]
©中国科学院软件研究所版权所有.

E-mail: jos@iscas.ac.cn
http://www.jos.org.cn
Tel: +86-10-62562563

*

DBI-Go: 动态插桩定位 Go 二进制的非法内存引用
陈金宝, 张 昱, 李清伟, 丁伯尧
(中国科学技术大学 计算机科学与技术学院, 安徽 合肥 230026)
通信作者: 张昱, E-mail: yuzhang@ustc.edu.cn

摘

要: Go 语言, 也称 Golang, 由于其语法简单、原生支持并发、自动内存管理等特性, 近年受到很多开发者的欢

迎. Go 语言期望开发者不必了解变量或对象是分配在栈上还是在堆中, 而由 Go 编译器的逃逸分析来决定分配位
置, 再由 Go 垃圾收集器自动回收无用的堆对象. Go 的逃逸分析必须正确决定对象的分配位置以保证内存状态的
正确性. 然而, 目前 Go 社 区中逃逸相关问题频发, 潜在导致程序崩溃等致命问题, 而目前对该方面的研究缺失. 为
有效检测编译器生成的代码是否存在可能引起运行时崩溃的非法内存引用, 填补研究空白, 对 Go 程序执行进行抽
象建模, 并提出两条判定写入违例的规则. 基于这两条规则, 克服 Go 二进制中高层语义缺失、运行时信息不便获
取等挑战, 设计一个轻量化的分析工具 DBI-Go. DBI-Go 采用静态分析加动态二进制插桩的分析方式, 基于动态二
进制分析框架 Pin 来实现, 可以识别 Go 二进制中违例的 store 指令. 实验结果表明, DBI-Go 可以检测出 Go 社区中
所有已知的逃逸相关 Issues; DBI-Go 还发现一个目前 Go 社区未知的问题, 该问题已经得到确认. 在实际项目上的
应用则表明 DBI-Go 可以帮助开发人员找出逃逸算法的错误. 测试结果还表明 DBI-Go 采取的措施可以有效降低
误报率且在 93.3% 的情况下带来的额外运行时开销小于原先的 2 倍. 同时, DBI-Go 无需修改 Go 的编译运行时, 可
以适配不同版本的 Go, 有较高的适用性.
关键词: 二进制分析; 动态二进制插桩; 静态分析; Go; 编译器测试; 逃逸分析
中图法分类号: TP314
中文引用格式: 陈金宝, 张昱, 李清伟, 丁伯尧. DBI-Go: 动态插桩定位 Go 二进制的非法内存引用. 软件学报. http://www.jos.org.
cn/1000-9825/7096.htm
英文引用格式: Chen JB, Zhang Y, Li QW, Ding BY. DBI-Go: Dynamic Binary Instrumentation for Pinpointing Illegal Memory
References in Go Binaries. Ruan Jian Xue Bao/Journal of Software (in Chinese). http://www.jos.org.cn/1000-9825/7096.htm

DBI-Go: Dynamic Binary Instrumentation for Pinpointing Illegal Memory References in Go
Binaries
CHEN Jin-Bao, ZHANG Yu, LI Qing-Wei, DING Bo-Yao
(School of Computer Science and Technology, University of Science and Technology of China, Hefei 230026, China)
Abstract: The Go programming language, also known as Golang, has become popular with developers in recent years due to its simple
syntax, native support for concurrency, and automatic memory management. This language expects that developers do not need to know
whether variables or objects are allocated on the stack or in the heap. The escape analysis of the Go compiler determines the allocation
location, and then the garbage collector automatically recycles unreachable heap objects. Go’s escape analysis must correctly determine the
allocation location of the object to ensure the memory state correctness. However, escape analysis related problems frequently occur in the
Go community at present, potentially causing fatal problems such as program crashes, and there is currently a lack of research on this
aspect. To effectively detect whether the code generated by the compiler has illegal memory references that may cause runtime crashes and

*

基金项目: 国家自然科学基金 (62272434)
本文由“编译技术与编译器设计”专题特约编辑冯晓兵研究员、郝丹教授、高耀清博士、左志强副教授推荐.
收稿时间: 2023-09-10; 修改时间: 2023-10-30; 采用时间: 2023-12-14; jos 在线出版时间: 2024-01-05

软件学报 ****年第**卷第**期

2

fill the research gap, this study conducts abstract modeling on the Go program and proposes two rules for verifying the validity of store
instructions. Based on these two rules, it overcomes the challenges of lacking high-level semantics in Go binaries and inconvenient access
to runtime information and designs a lightweight analysis tool DBI-Go. DBI-Go adopts static analysis plus dynamic binary instrumentation
and is implemented based on Pin, a dynamic binary analysis framework. Meanwhile, DBI-Go can identify illegal store instructions in Go
binaries. Evaluation results show that DBI-Go can detect all known escape-related issues in the Go community, and also discover an issue
that is previously unknown to the Go community. Finally, this issue has been confirmed. The applications in actual projects show that DBIGo can assist developers in finding bugs in escape analysis algorithms. Evaluation results also show that the measures adopted by DBI-Go
can reduce the false positive rate, and the extra runtime overhead brought by DBI-Go in 93.3% of the cases is less than twice the original.
Additionally, DBI-Go can be adapted to different versions of Go without modifying Go’s compilation and runtime, therefore yielding wide
applicability.
Key words: binary analysis; dynamic binary instrumentation; static analysis; Go; compiler testing; escape analysis

Golang (简称 Go)[1]是由 Google 提出并于 2009 年开源的新兴编程语言. Go 语言因其语法简单、静态强类型、
自动内存管理、原生支持高并发、编译型等特性, 得到许多开发者的认可. 2009 年和 2016 年两度成为 Tiobe 编程
语言排行榜的年度明星 [2]. 根据 2022 年 Go 开发者雇佣报告 [3], 全球 10.5% 的开发人员将 Go 作为主力编程语言,
亚洲占比最高, 达 57 万人, 而中国更是有超过 16% 的开发者使用 Go 语言.
随着软件无处不在, 软件的安全及效能变得越来越重要, 而其中内存管理方式及其实现机制对软件开发产能、
安全和效能的影响尤为重要. 传统的 C/C++ 语言需要开发者显式决定变量是分配在栈、静态数据区、还是堆中,
显式管理对象的分配与回收, 这使得程序极易出错, 给开发和维护带来极大的负担. 为提升内存安全性和软件开发
产能, 越来越多的现代编程语言, 如 Java、Python、Go 等, 提供垃圾回收 (garbage collection, GC) 等自动内存管理
机制, 使得开发者无需管理对象的回收. Python 采用以引用计数为主、以分代解决循环引用为辅的 GC[4], 其采用
全局解释器锁 (GIL) 使多线程在竞争时串行执行. Java 虚拟机如 OpenJDK 则提供多种丰富的 GC 算法来专注于
吞吐量 (throughput)、延迟 (latency)、或内存占用 (memory footprint) 等不同性能指标, 它们组合运用多线程 stopthe-world (STW, 多线程回收时让用户代码停止执行)、分代、紧致 (compaction, 回收期间通过移动对象来紧致内
存以解决碎片问题) 或不同的并发 (concurrent, 回收与用户代码并发执行) 等技术 [5]. 而 Go 的 GC 建立在基本没有
碎片问题的分配器 TCMalloc[6]之上, 使用的是无分代的、无紧致、并发的三色标记清除算法, GC 算法轻量. 这些
特性使得 Go 在特定场景下拥有 Python 般流畅的开发体验的同时, 又能达到接近 C++的运行效率 [7].
Go 使用逃逸分析 (escape analysis) 在编译阶段来决定对象是否堆分配, 使得开发者无需手动指定对象的分配
位置. 现有逃逸分析有关的研究主要集中在 Java 语言上 [8−13], 而非 Go. 对于栈分配, 只需通过修改栈帧指针即可几
乎零开销地完成分配和回收; 而堆分配则需要 GC 承担相当厚重的分配和回收代价. Java 语言呈现给开发者的观
点是对象在堆中分配, 而具有局部作用域的基本类型的值和对象类型的引用才会在栈上分配. 为降低 GC 的负担,
现代 Java 虚拟机在即时编译器中利用逃逸分析结果来开展内存使用优化, 如将未逃逸出方法的对象进行标量替
换 (即展开为一系列基本类型的值), 进而可以自动在栈上分配 [10]. Go 语言则期望让开发者专注于程序功能逻辑本
身, 不必指定变量或对象是分配在栈还是堆中, 而由编译器的逃逸分析来判定对象是否逃逸到堆上, 进而决定分配
的位置. Go 语言的逃逸分析是编译流水线中的必要流程, 其还承担了维护 Go 内存安全以及 GC 正常运行的责任.
Go 语言编译器的逃逸分析需要正确判断一个对象是否逃逸并分配在堆上, 否则极有可能产生若干内存漏洞, 如悬
垂指针等, 导致 Go 程序运行时异常行为甚至崩溃 (panic), 在实际应用环境中造成较大损失.
一个正确的 Go 逃逸分析需要找到所有需要堆分配的解, 使得程序可以正常运行, 不会因为内存漏洞而崩溃.
然而, 目前 Go 社区中的逃逸问题频发, 据不完全统计, 从 2017 年至今已有 17 个和逃逸分析相关的 issues (如图 1
所示), 并且无减少收敛趋势 (如图 2 所示). 比如 2022 年的 issue#54247 [14], 逃逸分析和后续编译优化的配合出错,
导致本该逃逸的对象栈分配, 造成堆对象引用了悬挂的栈指针; 2021 年的 issue#44614 [15], 由于逃逸分析的分析出
错, 导致全局对象引用了悬挂的栈指针. Go 的 GC 在运行时发现不正确的指针后会直接崩溃 [16], 因此和逃逸相关
的问题一旦出现, 通常都是致命的问题. 另外, 由于 GC 不是这种错误发生的第一现场, 因此通常难以定位这种运

陈金宝 等: DBI-Go: 动态插桩定位 Go 二进制的非法内存引用

3

行时崩溃产生的原因, 从而 Go 的 GC 的崩溃输出几乎不能提供有效的信息帮助开发者定位问题. 像前述的
issue#44614 从 Go1.14 便开始出现, 直至 Go1.17 发布前才被修复, 影响了 Go1.14.x、Go1.15.x、Go1.16.x 这 3 个
大版本.
issue 号
54247
53289
51481
52008
47415
49755
47276
44614
43818
41635
42944
31573
32959
29000
29353
26987
23045

版本
go1.17
go1.18
go1.18
go1.15
go1.16
go1.16
go1.16
go1.14 至1.16
go1.17
go1.16
go1.16
go1.13
go1.13
go1.11 至1.12
go1.13
go1.12
go1.11

备注
堆指向栈的问题
堆指向栈的问题
堆指向栈的问题
堆指向栈的问题
CGO 相关逃逸问题
堆指向栈的问题
堆指向栈的问题
堆指向栈的问题
寄存器传参问题
逃逸调试信息错误
堆指向栈的问题
逃逸带来的 runtime 问题
逃逸带来的编译问题
逃逸带来的 runtime 问题
逃逸带来的 runtime 问题
逃逸带来的 runtime 问题
逃逸带来的 uintptr 问题

5
issue 数量

年份
2022
2022
2022
2022
2021
2021
2021
2021
2021
2020
2020
2019
2019
2018
2018
2018
2017

社区中 Go 逃逸错误相关 issues (不完全统计)

3
2
1
2017

issue 链接: https://github.com/golang/go/issues/

图1

4

图2

2018

2019 2020
年份

2021

2022

社区中 Go 逃逸错误相关 issues 数量变化趋势

针对 Go 程序和/或 Go 语言编译器的漏洞检测, 目前多集中在对 Go 程序的并发、竞争检测方面 [17−19], 与 Go
语言编译器的逃逸分析相关的研究屈指可数, 仅有 Wang 等人的工作 [20]使得一些对象可以绕过 Go 语言编译器的
逃逸分析, 从而节省堆内存的使用. 目前尚无相关工作来寻找经 Go 语言编译器编译出的代码中的错误内存引用,
学术界对此的研究缺失. 同时, 目前 Go 官方对逃逸分析正确性的测试手段也较为有限, 难以辅助开发人员对逃逸
分析算法进行进一步的优化或重构.
为了有效检测编译器生成的代码是否存在可能引起运行时崩溃的非法内存引用, 在产品上线前就能及时发现
问题, 避免产品在实际生产环境中出现崩溃, 造成损失, 也为了辅助开发人员对内存优化相关算法, 如逃逸分析进
行优化和重构, 验证算法的正确性, 本文提出一种结合静态分析和动态插桩检测 Go 二进制中可能导致非法内存
引用的 store 指令的方法, 并基于 Pin[21]实现了工具原型 DBI-Go, 它可以在不修改 Go 编译运行时系统的情况下对
Go 二进制程序进行检测, 具有较好的适用性. 该工具权衡静态分析和动态插桩, 并结合 Go 的语言特性, 大大减少
了误报率和插桩带来的额外开销.
本文的主要贡献点包括以下内容.
• 对 Go 二进制程序进行抽象, 并基于此抽象分析提出两条判定非法内存引用的判定规则. 非法内存引用由二
进制程序中将不当的地址通过 store 指令写入: (1) 将栈地址存入栈外; (2) 将较浅栈帧中的对象的地址存入较深栈帧.
• 提出了 DBI-Go——第 1 个使用动态二进制插桩方式分析 Go 二进制中潜在非法内存引用的工具. DBI- Go
可以将 Go 的二进制可执行文件与 Go 语义以及 Go 的运行时管理系统关联起来, 有着较低的误报率, 且在大多数
情况 (93.3%) 有不超过 2 倍的额外运行时开销. DBI-Go 还可以精准给出违例的发生位置, 帮助快速定位问题.
• 实验结果表明, DBI-Go 可以检测出 Go 社区中所有已知的逃逸相关 issue, 有着较高的漏洞覆盖率. 同时 DBIGo 还发现了一个目前 Go 社区未知的问题. 该问题已经在 golang-nuts 中得到 Go 官方维护人员的确认 (https://
groups.google.com/g/golang-nuts/c/YZVFzwnPixM), 并已向社区提交 issue 有待 Go 官方的进一步修复 (https://github.
com/golang/go/issues/61730).
• 对 DBI-Go 的实际应用还表明, 其可以辅助开发人员对 Go 逃逸分析算法进行优化和重构, 帮助找到新逃逸

软件学报 ****年第**卷第**期

4

分析算法的错误.
本文第 1 节介绍 Go 逃逸分析及现状, 引出研究动机. 第 2 节主要对 Go 二进制进行抽象并提出了两条判定规
则. 第 3 节则介绍 DBI-Go 的设计挑战以及其最终的设计与实现, 包括其是如何恢复 Go 二进制上的运行时信息和
语义信息及如何减少误报和降低开销. 第 4 节则对 DBI-Go 的漏洞覆盖率、误报率以及带来的额外开销进行实验
评估. 第 5 节对 DBI-Go 的局限性进行讨论. 第 6 节介绍相关工作. 第 7 节进行总结与展望.

1 相关基础与研究动机
本节简要介绍 Go 语言的运行时系统和逃逸分析, 并结合社区 issue 概述了目前 Go 逃逸分析的现状, 说明了
目前对逃逸分析正确性验证的不足, 引出本文的研究动机.
1.1 Go 的运行时系统与逃逸不变式
Goroutine 及其栈管理

1.1.1

Go 语言使用协程 Goroutine[22] 作为 Go 程序的执行上下文. Goroutine 是轻量级的用户态线程, 与由操作系统
直接调度的操作系统级线程 Thread 不同, Goroutine 的调度是由 Go 的运行时系统进行管理的. 每个 Goroutine 都
有自己独有的栈, 但它的额外开销和默认栈大小都比线程小很多. 与操作系统线程的栈不同, Goroutine 的栈是 Go
运行时系统使用堆内存来模拟的. Go 运行时系统从操作系统申请堆内存后会长期持有, 再通过其内部的内存分配
器按照一定的策略和时机从中划分出部分内存用于模拟 Goroutine 的栈.
图 3 示意了 Go 运行时系统对每个 Goroutine 的栈管理结构, 其中 stack 结构包含两个字段: lo 和 hi, 分别表示
栈的低地址和高地址边界, 它们描述一个栈的内存地址范围位于 [lo, hi) 之间. 每个 Goroutine 在 Go 运行时系统中
用一个 g 类型的对象表示, g 对象的前几个字段描述它的执行栈, 包括一个类型为 stack 的字段, 用于描述该
Goroutine 的栈的地址范围. Go 的运行时系统会在 Goroutine 的栈空间不足时进行栈扩展. 当发生栈扩展时, Go 运
行时会进行栈拷贝, 将旧栈的内容复制到新分配的栈, lo 和 hi 也会进行相应的更改, 因此 Go 程序执行期间, 每个
Goroutine 的栈并非固定在内存中的同一段连续空间保持不变.
stack.lo

stack.hi
高地址

Goroutine
栈

其余函数栈帧

栈增长方向

栈结构

低地址

顶层函数
栈帧

Goroutine结构

rsp

图3

Goroutine 执行栈管理示意

Go 的垃圾回收与逃逸不变式

1.1.2

Go 语言内存管理主要依赖于其运行时系统, Go 从操作系统申请内存后会长期持有, 将其分为 Goroutine 栈
(如前文所述) 和 Go 堆进行管理. Go 堆使用 TCMalloc[6]进行快速的并发分配, 并通过 Go 的并发垃圾收集 (GC)[23]
实现堆空间的回收. Go 中的堆对象会使用诸如 runtime.newobject 等运行时函数在 Go 堆上自动分配.
这种内存管理机制使得 Go 程序员无需了解一个变量是分配在栈上还是堆中 [24]. Go 向程序员保证, 程序执行
的任何时刻中, 任意由垃圾回收标记算法标记可达的对象都处于生命周期内, 即, 不可能出现悬挂指针. 作为 Go
编译流水线上的一个重要优化遍, Go 的逃逸分析用于决定一个对象是堆分配还是栈分配. 为了内存安全以及 GC
的正常运行, Go 的设计者提出了以下两条逃逸不变式 [25].
• 逃逸不变式 1: 指向栈对象的指针不可存储在堆中.
• 逃逸不变式 2: 指向栈对象的指针生命期不可超出该栈对象.

陈金宝 等: DBI-Go: 动态插桩定位 Go 二进制的非法内存引用

5

Go 的逃逸分析在决定对象是堆分配还是栈分配时必须遵循上述不变式. 逃逸分析若发现有对象违反上述不
变式 (即违例), 则该对象会被堆分配. 如代码 1 所示, 其中 heapObj 是堆对象, 其在第 5 行引用了 i 的地址. 因此根
据逃逸不变式 1: “指向栈对象的指针不可存储在堆中”, i 需要堆分配. 在代码 2 中, ptr 在第 6 行引用了 x 的地址.
ptr 在外层循环声明, x 在内层循环隐式声明, ptr 的生命期长于 x. 因此根据逃逸不变式 2: “指向栈对象的指针生命
期不可超出该栈对象”, x 需要堆分配.
代码 1. 逃逸不变式 1 示例.
1. //堆对象获取了 i 的指针导致 i 被堆分配
2. func heapAssignment() {
3.

heapObj := newobj()

4.

i := 1

5.

heapObj.a = & i

6.

use(heapObj)

7. }
代码 2. 逃逸不变式 2 示例.
1. //ptr 引用 x, ptr 循环深度小于 x, 生命期长于 x, x 堆分配
2. func loop() {
3.

var ptr *int

4.

for i := 0; i < 5; i++ {

5.

x := i

6.

ptr = & x

7.

}

8.

use(ptr)

9. }
逃逸分析必须正确地决定对象的分配位置, 若有对象的分配位置违反了上述两个逃逸不变式之一, 即存在非
法内存引用, 则会导致 Go 程序运行时的异常行为甚至导致运行时崩溃 (panic), 在实际应用环境中造成较大损失.
1.2 Go 逃逸问题现状
Go 社区中逃逸类相关问题频发

1.2.1

在社区 issue 中, Go 的逃逸问题频发, 且每次出现的问题都是致命问题. 近几年出现的相关 issue 的不完全统
计可见图 1. 这些 issues 有的是因为逃逸分析算法的错误导致, 有的是因为逃逸分析和其他编译优化的错误配合
导致. 下面将通过这些 issue 中的两个典型例子来进行说明.
• 逃逸分析算法的错误. 逃逸分析算法的错误会直接导致一些对象的分配位置出错. 以 issue#44614 [15]为例, 其
一个简化版的实例如代码 3 所示. 在该例子中, 对象 r 由于被全局变量 sink 引用而被堆分配. 函数 global2stack 中
的参数 p, 在 return p 语句中被堆对象 r 获取, 因此所有传给 global2stack 的指针理应被标记为逃逸, 但由于逃逸分
析的漏洞, 逃逸分析在分析 global2stack 函数时认为参数 p 没有逃逸. 这就导致了在代码 3 中, 地址被传给
global2stack 的变量 i 理应被堆分配, 但却被逃逸分析错误地认为应该栈分配.
代码 3. issue44614 简化版示例.
1. var sink interface{}
2. func global2stack(p *int) (r *int) {
3. sink = & r

软件学报 ****年第**卷第**期

6

4.

return p

5. }
6. func g2s() {
7.

i := 1 //i 应该堆分配, 却被错误地栈分配

8.

j := global2stack(& i)

9.

_=j

10. }
• 逃逸分析和后续编译优化的错误配合. 有时虽然逃逸分析没有出错, 但后续的一系列的编译优化却可能造成
错误. 一个例子是社区 issue#54247 [14]. 其一个简化版的实例如代码 4 所示. 在该例中, obj1 是栈对象, 但逃逸分析
后续编译流程中对 defer 的处理导致函数 Recover 的参数 objs 逃逸到堆上, 这就导致在第 4 行, 栈对象 obj1 的地
址被堆对象 objs 获取, 违反逃逸不变式, 导致错误.
代码 4. issue54247 简化版示例.
1. func Escape(task func()) {
2.

var obj1 obj

3.

defer Recover(

4.

& obj1,

5.

) //obj1 应该堆分配, 却被错误地栈分配

6.

task()

7. }
8. func Recover(objs ...*obj) {
9.

use(objs)

10. }
由于这些对象分配位置出错, 因此引用这些对象的堆和全局对象极易出现悬挂指针. Go 向程序员保证, 程序
执行的任何时刻中, 任意可达的对象都处于生命周期内, 即, 不可能出现悬挂指针. 当 Go 的 GC 遇到悬挂指针时会
直接在运行时 panic, 使得整个 Go 程序异常终止. 若在实际生产环境中出现该问题, 则很有可能带来不可挽回的损
失. 同时, GC 不是发生这种错误的第一现场, 以代码 3 为例, 其发生错误的第一现场应是第 4 行的 return p, 在该处
栈对象的地址被传递给堆对象. 因此, GC 的 panic 具有延后性和不可预知性. 由于 GC 不是错误的第一现场, 其崩
溃输出多和 GC 相关而和事发的第一现场无关. 后文图 4 所示为 issue#44614 和 issue#54247 中的崩溃输出. 其多
为显示 GC 的工作流程和状态信息, 不能准确地显示栈对象是在什么位置被传递给了全局或堆对象. 不完整的信
息也给排查问题带来了困难, issue#44614 中逃逸分析的漏洞从 Go1.14 开始出现, 直至 Go1.17 发布前才被修复,
影响了 Go1.14 至 Go1.16 这 3 个大版本.
逃逸分析正确性验证的现状

1.2.2

Go 语言官方对逃逸分析正确性的验证手段也较为有限, 目前在 Go 语言官方给出的逃逸分析的测试中对逃
逸分析正确性的检验存在比较函数返回值和比较 DeBug 信息这两种途径.
途径一 [26]是通过检查两次调用同一个函数的返回值是否相同来判断逃逸分析正确性的. 在这个函数内部会
为一个对象分配一块空间, 这部分空间理应堆分配, 这样在两次函数调用中将该空间的地址返回时为不同的地址,
这就可以通过测试. 但是如果该空间由于错误的逃逸分析导致被栈分配, 因为栈帧的布局在编译完成后已经确定,
故将导致在同一个栈帧下两次调用该函数返回的该变量的地址将为相同的地址, 这样就不能通过测试. 这种情况
只能适用于小规模测例, 而且要求在同一个栈帧下调用相同函数进行测试, 较难推广.

陈金宝 等: DBI-Go: 动态插桩定位 Go 二进制的非法内存引用

(a) issue44614 中的崩溃输出

7

(b) issue54247 中的崩溃输出

图4

崩溃输出示例

另外一种途径 [27]是通过标注的方式来进行检查的. 通过将预先人工标注的信息以及逃逸分析在 debug 模式下
打印输出的信息进行比对来验证逃逸分析算法的正确性.
这两种途径从本质上来讲都是通过标注测试的方式来实现的, 需要事先人工分析 Go 程序并进行标注, 其无
法用于验证实际应用程序中事先未知的错误.
此外, Go 的运行时系统在 GC 时会验证对象的指向是否有错, 但该方法的范围有限. 首先, 并不是所有错误的
指向都能被 GC 发现. 其次, 该验证机制将错误的指向延迟到 GC 时才进行检测, 不能及时发现问题, 且 GC 一旦
发现问题会直接 panic, 造成整个程序的崩溃, 可能在实际生产环境中造成不可弥补的损失. 最后, GC 的崩溃输出
较难理解, 难以帮助开发人员定位问题的发生位置, 为后续排查工作带来不便.
总之, Go 目前测试和验证方法十分有限, 在实际应用程序中发生错误时, 现有方法无法用于定位发生错误的
位置以及发生错误的原因. 图 1 中的各类 issue 也印证了 Go 现有方法的局限性.
1.3 研究动机
传统的内存相关漏洞的寻找工作多集中于 C/C++程序的二进制 [28−30]. 但由于 Go 程序有独特的运行时管理机
制以及独特的语义, 传统基于 C/C++的工作并不能直接应用在 Go 二进制上. 目前学术界在 Go 程序的漏洞寻找上
有不少工作, 但这些工作多集中在并发、数据竞争相关的漏洞寻找上 [17,18,31]. 目前缺少验证 Go 编译器生成的代码
是否满足 Go 逃逸不变式的研究.
如何保证 Go 编译器中逃逸分析的决策结果以及经过其他编译器优化遍之后生成的代码仍然符合逃逸不变
式是非常重要的. 然而, 正如第 1.2 节所述, Go 目前逃逸相关 issue 频发且无较为有效的测试手段, 且现有相关研
究缺失. 为了有效解决这类问题, 确保经过逃逸分析和一系列优化遍后生成的二进制可执行文件中没有违反 Go
逃逸不变式的情况, 研制一个可以有效检测实际应用中违反 Go 逃逸不变式情况的工具是很有必要的.

2 Go 程序逃逸不变式违例的判定规则
本节对 Go 程序进行抽象, 随后结合 Go 的逃逸不变式抽象出 Go 程序中违反逃逸不变式的判定规则.
2.1 Go 程序抽象
根据 Go 的文档 [23,32], Go 应用程序的内存由全局数据区、受 GC 管理的 Go 堆区、每个 Goroutine 的栈区组
成. 基于此, 我们将一个 Go 程序的内存分为 3 部分: Goroutine 栈、Go 堆区以及 Go 全局数据区, 并提出如公式 (1)
所示的抽象. 整个世界 W 由一个含有 n 个 Goroutine 的集合 GS 、一个 Go 堆区 GH 、一个 Go 全局数据区 GG 组
成. 每个 Goroutine G 由运行在其上的用户代码 C 以及对应的 Goroutine 栈 S 组成. GH 、 GG 、 S 均是内存地址
的集合, 其中: GH 和 GG 分别记录 Go 程序在用的有效堆地址和全局数据区地址; S 是 Goroutine 的栈地址, 由从
addrlo 到 addrhi 的连续内存地址组成. C 是一个指令列表, 由于这里只关心潜在引起 Go 逃逸不变式违例相关的指

软件学报 ****年第**卷第**期

8

令, 因此指令列表中只包含涉及存储指针的 store 指令和可能引起控制流变化的 cmp 和 br 指令. 其中指令 store
addrdst, addr 代表将 addr 存入 addrdst 所指向的内存区域.
(World)
W
(Goroutine set)
GS
(Go heap)
GH
(Go global)
GG
(Goroutine)
G
(Code)
C
(Goroutine stack) S
(Memory address) addr
(Instruction)
instr
code( instr )
codeG (C0 )
codeS (C)

def

= C
= G
def
= S0
def

::=
::=
::=
::=
::=
::=
::=
::=
::=

(GS, GH, GG)
(G1 , G2 , . . . , Gn )
{addr}
{addr}
(C, S)
[instr]
[addrlo , addrhi )
n (unsigned nums)
store addrdst , addr
| br addr | cmp

instr ∈ C
G · C == C0
S0 == codeG(C).S

(1)

(2)

2.2 违反 Go 逃逸不变式的判定规则
为便于描述, 首先引入一些辅助函数, 定义见公式 (2). 其中 code(instr) 用于获取指令 instr 所在的指令序列 C ,
codeG( C ) 用于获取执行指令序列 C 的 Goroutine, codeS( C ) 用于获取执行指令序列 C 的 Goroutine 栈.
为了后面讨论违反逃逸不变式的判定规则以及栈对象的生命期, 接下来定义两个概念.
定义 1 (不合法的 store 指令). 若一条 store 指令 sl: store addrdst, addr 的内存访问违反了 Go 逃逸不变式, 则将
其记为 illegal (sl ) .
定义 2 (栈对象所在的栈帧深度). 若 addr 为某栈对象地址, 则 f d (addr) 代表 addr 指向的栈对象所在的栈帧深
度. 处于栈顶的函数栈帧深度为 1, 其余函数的栈帧深度沿着栈上的调用链依次加 1. 越靠近栈顶的栈帧, 其栈帧深
度越小.
根据第 1.1 节所述的两条 Go 逃逸不变式, 可得违反 Go 逃逸不变式的情况为:
• 违反逃逸不变式 1: 栈对象指针被堆对象获取.
• 违反逃逸不变式 2: 栈对象指针被生命期更长的对象获取.
针对违反逃逸不变式 1 的情况, 其表现为堆对象指向了栈对象, 栈对象地址在某处被存入堆对象中. 因此, 针
对指令 sl: store addrdst, addr , 可用公式 (3) 来判断是否违反了逃逸不变式 1, 即 addrdst 是堆地址且 addr 是当前执行
指令 sl 所在的 Goroutine 栈中的地址时, 指令 sl 会因引起逃逸不变式的违例而视为不合法.
S = codeS (code (sl ))
addr ∈ S ∧ addrdst ∈ GH
illegal (sl )

(3)

针对违反逃逸不变式 2 的情况, 比当前栈对象生命期更长的对象可能有多种情况, 包括堆对象、全局对象、
其他 Goroutine 的栈对象, 以及当前 Goroutine 的栈对象. 前 3 种情况都认为是当前 Goroutine 栈之外的对象. 下面
分别进行讨论.
• 将栈对象地址写入当前 Goroutine 栈之外的违例情况. 除了公式 (3) 讨论过的堆对象外, 所有全局对象都可
认为比当前栈对象生命期更长. 此时表现为全局对象指向了栈对象, 栈对象地址在某处被存入全局变量中. 因此,
针对指令 sl: store addrdst, addr, 可用公式 (4) 来判断是否将栈地址存入全局变量中.
S = codeS (code (sl ))
addr ∈ S ∧ addrdst ∈ GG
illegal (sl )

(4)

由于不同 Goroutine 的执行受 Go 运行调度的影响可能相互交叠, 分处在不同 Goroutine 栈中的栈对象生命期
可能存在交叠, 也可能存在不确定的一先一后, 因此将当前 Goroutine 中的一个栈对象的地址存到另一个 Goroutine
栈中也是不安全的. 为此, 针对指令 sl: store addrdst, addr, 可用公式 (5) 来判断是否将当前 Goroutine 中的栈地址存
入了另一个 Goroutine 栈中.

陈金宝 等: DBI-Go: 动态插桩定位 Go 二进制的非法内存引用

9

S = codeS (code (sl )) , G1 = codeG (code (sl ))
∃G2 , addr ∈ S ∧ addrdst ∈ G2 .S ∧ G1 , G2
illegal (sl )

(5)

由于 Go 的地址空间由 Go 堆、全局数据区, 以及所有 Goroutine 的栈组成, 因此, 一个对象若不在当前
Goroutine 栈中, 则要么在 Go 堆中, 要么在全局数据区, 要么在其他 Goroutine 栈中. 即:



 S = codeS (code (sl )) , G1 = codeG (code (sl ))


 addrdst < S ⇐⇒ addrdst ∈ GH ∨ addrdst ∈ GG ∨ (∃G2 , addrdst ∈ G2 .S ∧ G1 , G2 )

(6)

结合公式 (6), 可将公式 (3)、公式 (4)、公式 (5) 合为一个式子, 此时便得到判定违反逃逸不变式的第 1 条规
则, 该规则揭示了栈对象地址被写入当前 Goroutine 栈之外的内存使用违例情况.
规则 1. 栈对象地址被写入当前 Goroutine 栈之外的违例判别.
sl : store addrdst , addr
S = codeS (code (sl ))
addr ∈ S ∧ addrdst < S
.
illegal (sl )

该规则表示将当前 Goroutine 栈的对象地址存入当前 Goroutine 栈外, 导致栈外对象引用栈内对象是不符合
Go 的逃逸不变式要求的, 如图 5(a) 所示.
stack.hi

高地址

stack.hi

高地址
…
深栈帧

栈帧

栈外对象

栈对象

栈
增
长
方
向

栈对象

…
浅栈帧
栈对象
rsp

rsp
低地址

stack.lo

…

栈
增
长
方
向
低地址

stack.lo
Goroutine
栈

Goroutine
栈
(a) 栈外对象指向栈对象

图5

(b) 较深栈帧栈对象指向较浅栈帧栈对象

规则 1 及规则 2 示意

• 将栈对象地址写入当前 Goroutine 栈内的违例情况. 在同一个 Goroutine 栈中, 不同栈对象生命期亦有差距, 比
如作用域较浅的栈对象比作用域深的栈对象生命期长, 较深函数栈帧中的栈对象比较浅函数栈帧中的栈对象生命期
长. 由于在二进制中已经难以看到源代码层级的作用域, 此处只能通过栈对象所在函数栈帧的深浅来判断其生命期,
认为较深栈帧栈对象生命期更长, 如图 5(b) 所示. 由此可以引出判定违反逃逸不变式的第 2 条判定规则, 即规则 2.
规则 2. 栈对象地址被写入当前 Goroutine 栈内的违例判别.
S = codeS (code (sl ))

sl : store addrdst , addr
addr ∈ S ∧ addrdst ∈ S ∧ f d (addr) < f d (addrdst )
.
illegal (sl )

3 DBI-Go 的设计与实现
本节将介绍 DBI-Go, 一款用于识别 Go 二进制中写入指针的 store 指令并在运行时验证其是否违反 Go 逃逸
不变式的工具的具体设计与实现.
3.1 设计目标和设计思路
DBI-Go 的设计目标主要包括以下几点.

软件学报 ****年第**卷第**期

10

• 轻量快速. 该工具应该尽可能轻量化, 可以以较快的速度分析出结果, 提升效率.
• 适用性强. 该工具应该尽可能独立于 Go 的编译工具链, 可以支持不同 Go 版本编译器生成的二进制, 支持在
未修改编译运行时的原生 Go 二进制上进行检测.
• 低误漏报. 有效降低误报率可以大大减少人工筛选漏洞的时间. 同时, 减少漏报率可以确保该工具可以有效
发现潜在的漏洞.
为达到上述设计目标, 对 DBI-Go 的设计的主要思路是结合 Go 语言的特性来快速原型化. 整个设计中的关键
主要聚集于两点: (1) 程序分析方式的选择; (2) 如何结合并利用第 2 节抽象并总结出的违例判定规则.
• 分析方式的选择. 目前学术界已经开发了多种工具 [17,18,33,34] 来识别 Go 应用程序中的各类漏洞, 主要使用两
种分析技术——静态分析和动态分析. 静态分析无需执行即可分析 Go 源代码. 然而, 静态分析在识别方面的覆盖
范围有限. 此外, 由于不精确的指针分析, 静态分析可能会引入许多误报或漏报. 当在实际的 Go 应用程序中进行
大范围分析时, 这种不精确性会迅速累积. 因此, 单纯的静态分析难以满足 DBI-Go 的低误漏报的设计目标.
现有的动态分析通过额外的运行时信息监视 Go 程序的执行, 可以减少误报和漏报. 但是这些动态方法需要
修改 Go 编译器或运行时来收集必要的数据, 与 Go 的编译运行时强耦合. 当 Go 的编译运行时发生改变时, 这些
动态方法很容易失效. 这种要求很大程度上阻碍了这些工具在实际生产环境中的 Go 应用上的使用, 也不符合 DBI-Go
的适用性强的设计目标.
在 Go 应用程序的二进制代码上进行动态分析可以摆脱这些问题, 既可以在运行时监视 Go 程序的执行, 又无
需修改 Go 的编译运行时. 因此, DBI-Go 以动态二进制插桩作为主要的分析方式.
• 如何利用规则 1 和规则 2. 确定了程序分析方式之后, 就可以着手设计 DBI-Go. 第 2 节抽象总结的规则 1 和
规则 2 仅提供了判定的理论. 若要使用规则 1 和规则 2, 通过观察其形式, 不难发现, 在二进制代码上使用该规则
必须考虑以下两个关键问题.
(1) 如何识别存储指针的指令 store addrdst, addr: 这类指令改变内存之间的引用关系, 潜在可能违背逃逸不变式.
(2) 如何获取 Go 运行时 Goroutine 的栈信息: 由第 1.1.1 节可知, Go 程序执行期间 Goroutine 的栈地址区间不
是一成不变的, 因此需要有途径准确获取当前时刻的栈信息.
这两个关键点也是程序分析的难点. 第 3.2 节将介绍使用静态分析识别写入指针的 store 指令的挑战以及我
们的解决方案. 第 3.3 节将介绍如何从较为封闭的 Go 运行时中获取 Go 程序运行时 Goroutine 的栈信息.
DBI-Go 的整体架构如图 6 所示. DBI-Go 主要由两个组件组成.
规则1: 栈->栈外
规则2: 浅栈->深栈

Go ABI规范
先验知识
StoreFinder
Store指针的
控制流识别

bb1 cmp1
bb2
bb3
store
ca11

Go运行时栈
信息识别

先验知识
Go运行时
栈信息

运行时
验证器

bb4
回调函数的插桩

函数、指令流
反汇编信息
及符号信息

插桩信息

插桩信息

输入
Go可执
行程序

反汇编API

Pin

插桩API

图6

程序运行信息

运行时
信息
程序
开始执行

验证信息

整理
输出

Log
文件

StoreValidator

程序运行时信息API Pin

DBI-Go 整体架构

(1) StoreFinder: 它使用静态分析提取 Go 二进制程序中存入指针的 store 指令, 并在二进制代码上插桩 (第
3.2 节).
(2) StoreValidator: 它以运行时回调函数的形式识别违反 Go 逃逸不变式的内存引用漏洞, 并输出错误日志信

陈金宝 等: DBI-Go: 动态插桩定位 Go 二进制的非法内存引用

11

息, 便于开发者定位错误 (第 3.3 节).
DBI-Go 基于 Pin[21] 设计实现了上述两个组件, 包括约 1 000 行 C++代码. Pin 是由 Intel 开发的支持 IA-32、
X86-64 和 MIC 指令集架构的动态二进制插桩框架, 可用于实现动态程序分析工具.
3.2 使用静态分析从 Go 二进制中恢复 Go 的 store 指针语义
Go 是编译型语言, Go 程序一旦被编译链接后其二进制代码中所有指令都会固定下来. 因此, 可以通过静态分
析的方式识别 Go 程序二进制代码中的 store 指令. 然而, 若要判定一条 store 指令是否在存储指针 (即规则 1 和规
则 2 中的 store addrdst, addr 形式), 就还需要获取 Go 程序的一些高层语义信息, 如类型等. 下面先分析难点, 然后给
出解决思路及其关键点的实现方法.
难点分析及解决思路

3.2.1

根据 Zeng[35] 的工作, 在 C 语言的二进制中, 所有高级类型信息, 如整型、浮点型和指针类型, 在编译后都会
丢失, 二进制代码中仅有的两种类型是寄存器和内存位置. 在 Go 语言中也类似, Go 二进制程序已经难以区分指
针类型和非指针类型.
• Go 二进制中指针的识别难点. Go 语言中, 指针类型用于传递对象地址, 不能进行指针运算. Go 的 GC 会扫
描指针, 堆指针指向的对象会在合适的时机被 GC 回收. 然而, Go 语言中有一种类型 uintptr[36], 其大小和普通指针
相同, 可以容纳任意的指针类型的值, 可以用于进行指针运算等操作. 但 GC 并不把 uintptr 当作指针, 因此也不会
基于该指针值进行对象标记. 若把某栈对象的指针转为 uintptr 后存入堆对象, 并在之后不通过它访问对象, 那么
这种存入操作并不违反 Go 的逃逸不变式, 因 Go 并不将 uintptr 视作指针. 然而, Go 二进制中仅有的两种类型是寄
存器和内存位置, 难以判断某个寄存器或者内存位置中的值是指针还是 uintptr. 若对其不做区分, 都视为指针, 则
势必会带来很多误报, 影响精度和效率.
• Go 二进制中地址存入操作的识别难点. 除了类型信息的缺失, Go 二进制上的地址存入操作与用户代码中的
地址存入操作也有较大差别. Go 编译器在编译 Go 程序时会执行若干程序变换, 在用户代码中生成诸多与 Go 运
行时管理相关的代码, 以便 Go 运行时系统对 Go 程序的管理. 在这些代码中会产生若干违反 Go 逃逸不变式 store
操作, 但 Go 的运行时保证了这些操作的安全性. 若不将这些操作排除, 也会带来较多的误报.
• 解决思路. Zhong 等人 [17] 通过 Go 运行时系统中管理并发的相关函数恢复了 Go 二进制上的并发语义. 这启
发我们可以通过静态分析的方式, 识别 Go 二进制用户代码中和内存管理、垃圾回收相关的 Go 运行时函数来恢
复相关的 store 指针的语义. 经过对 Go 运行时相关函数的分析可发现, Go 运行时和写屏障相关的运行时函数可以
用来恢复相应的 store 指令. 第 3.2.2 节中介绍该机制, 并在第 3.2.3 节中介绍如何使用该机制来恢复满足要求的
store 指令并使用 Pin API 为其注册运行时的回调函数.
Go 的写屏障

3.2.2

Go 运行时的不可或缺的部分为垃圾回收 (GC) 系统. 尽管 GC 在幕后运作, 却有数个运行时函数与其息息相
关. 这些运行时函数分为两类: 一部分可供用户主动调用, 用于配置 GC 参数或强制启动新的 GC 周期; 另一部分
由编译器在编译期间自动插入, 在运行时辅助 GC 的运行, 确保 GC 相关内存状态的准确性. 在这个体系中, 编译
器自动插入的 runtime.gcWriteBarrier 函数在维护 GC 相关内存状态的正确性方面扮演着关键角色.
Go 的 GC 系统在堆内存使用达到特定阈值时会中断用户程序的运行, 对那些由根集中的指针直接或间接可
达的对象进行扫描和标记, 标记出仍在生命周期中的对象, 随后释放已经不再使用的对象. 在这个过程中, 用户程
序完全暂停, 因此在逻辑上对 GC 的发生没有感知, 这确保了内存读写不会对 GC 状态造成任何干扰.
Go 的并发 GC 在特定阶段允许用户程序与其并行, 以减少等待时间. 然而, 在 GC 对对象进行标记的过程中,
用户程序的内存读写可能会修改对象的引用关系, 这可能导致 GC 的标记与实际情况不一致, 从而错误地清理正
在使用的对象. 例如, 如果在 GC 完成对栈指针的扫描后, Go 应用程序将某个堆对象的地址存入栈对象中, 而这个
新引用关系的创建没有被 GC 感知到, 即该堆对象没有被 GC 标记, 那么在这一轮 GC 结束后, 该栈对象持有了一
个已经被释放的堆对象地址, 从而导致内存错误. 因此, 在 GC 运行期间, GC 需要获知所有指针类型的内存写入,

软件学报 ****年第**卷第**期

12

以检查这些在运行过程中发生改变的引用关系. 在 Go 程序中, 如果一个 store 操作可能存入指针类型, 则 Go 编译
器会在编译期间在该 store 操作周围生成特定的控制流, 插入 runtime.gcWriteBarrier 函数, 该函数在 GC 时会接管
相应的 store 操作, 并负责帮助 GC 维护正确的内存引用关系.
Go 编译器只会对用户代码中包含指针的 store 操作插入 runtime.gcWriteBarrier, 且不会对一些可能带来误报
的运行时函数中的 store 操作插入该函数. 若能够在二进制中识别相应的结构, 就能恢复 Go 的 store 语义, 大大减
轻由于 Go 运行时、非指针 store 等带来的误报问题, 同时还能够减少很多对不必要的 store 插桩, 减轻动态二进制
插桩带来的额外开销.
利用 Go 的写屏障机制恢复 store 指针的语义

3.2.3

编译生成的与 runtime.gcWriteBarrier 相关的汇编级控制流模式如图 7 所示. 由于编译器在编译时在 store 周
围插入的特定控制流有固定的结构, 所以其最后生成的二进制中围绕 runtime.gcWriteBarrier 也有特定的结构. 将
这些特征总结抽象, 可以形成图 7 中的特定控制流模式, 以便后续精准识别.

源代码

编译器转换后代码模式

图7

二进制控制流模式

编译生成的 gcWriteBarrier 相关控制流

我们提出算法 1 来识别 Go 二进制中该特定的控制流模式. 其主要思路在于通过寻找特定的 cmpl 指令来确
定满足要求的基本块 bb1, 再通过 bb1 中的控制流跳转语句 (JNE) 来确定满足要求的两个后继基本块 bb2 和 bb3,
要求 bb2 和 bb3 有且仅有一个相同的后继.
算法 1. 识别图 7 中的控制流并为其中满足 store 指针语义的指令设置回调函数.
输入: Go 二进制文件 Bin;
输入: Bin 中全局变量 runtime.writeBarrier 的地址 wb.
运行结果: 识别出 Bin 中满足 Go 的 store 语义的指令, 并为其在 Pin 中注册运行时的回调函数
1. FOR ALL instr in Bin DO
2.
3.

IF instr.opcode==cmpl THEN
IF instr.operands[0]==0 且 instr.operands[1] 的有效地址 == wb THEN

4.

j=instr 之后最近的 jne 指令, 且该 jne 之前没有其他跳转指令

5.

bb3=BB(j.target)

j 的目的地址处的基本块

6.

bb2= BB(j.next)

j 的下一条指令处的基本块

7.

IF len(successors(bb2))==1 且 successors(bb2)==successors(bb3) THEN

8.
9.

FOR ALL s:store in bb2 DO
在 store 指令 s 前注册运行时回调函数

10.

END FOR

11.

FOR ALL c:call in bb3 DO

12.

IF c.targetFunc 以 runtime.gcWriteBarrier 为前缀 THEN

陈金宝 等: DBI-Go: 动态插桩定位 Go 二进制的非法内存引用

13.

识别 call 指令 c 的参数

14.

在 call 指令 c 前注册运行时回调函数

15.

END IF

16.

END FOR

17.

13

END IF

18.
19.

END IF
END IF

20. ENDFOR
在算法 1 中, 基本块 bb2 中的所有 store 指令被认为都可能存储指针, 并将这些 store 转为规则 1 和规则 2 接
受的形式: store addrdst, addr. 之后, 通过使用 Pin 的 API 为这些 store 指令注册运行时回调函数. 在程序恰好运行到
这些 store 指令之前时, 注册的回调函数会被执行用来检测其是否违反了 Go 逃逸不变式.
对于基本块 bb3, 它对应在 GC 期间调用 runtime.gcWriteBarrier 函数来接管 store. 因此, 可以在 bb3 中识别
runtime.gcWriteBarrier 所需的参数 (即图 7 中所示的 ptr 和 val). 在实现中发现, Go 编译器为了优化调用 runtime.
gcWriteBarrier 的流程, 减少准备参数的开销, 为 runtime.gcWriteBarrier 函数生成了不同版本, 这些不同的版本只
有传参的寄存器有区别. 比如 runtime.gcWriteBarrierR9 函数, 相比于原版的 runtime.gcWriteBarrier, 参数 val 使用
寄存器 R9 来进行传递, 其余流程均与 runtime.gcW-riteBarrier 相同. 为此, 可以识别 runtime.gcWriteBarrierRXX
的后缀来判断其传递 val 参数的寄存器. 当识别出 runtime.gcWriteBarrier 的参数后, 就可将其转为 store addrdst,
addr 的形式 (ptr 对应 addrdst , val 对应 addr), 并在 call 指令前注册运行时的回调函数用于在运行时检测该 call 指
令所代表的 store 是否违反了 Go 逃逸不变式.
3.3 在运行时回调函数中恢复 Go 运行时栈信息
Go 运行时函数库以静态链接的方式与 Go 应用代码链接起来形成可执行的 Go 程序. Go 运行时函数负责在运
行时管理 Go 程序运行所需的堆、Goroutine 的调度以及 Goroutine 的栈等. 用户编写代码时无需了解运行时的实
现细节, 比如对象如何分配, Goroutine 如何调度, Goroutine 栈如何管理等. 但是, 如若要在二进制层面分析内存的
引用关系, 分析栈对象地址是否被存储到栈外、是否违反逃逸不变式, 这就要求必须能够获得受 Go 运行时管理的
一些信息, 比如当前 Goroutine 的栈信息等. 然而, Go 的运行时管理系统并不像操作系统一样提供了若干 API 用于
在外部获取系统运行时信息. Go 的运行时系统相对封闭, 没有完备的 API 供外部获得当前 Go 程序的运行时信息.
幸运的是, 我们注意到 Go 语言的 ABI 规范 [32]中定义了一些运行时信息的存储位置, 比如当前的 Goroutine,
来供运行时函数使用. 这意味着可以通过在二进制中添加回调函数的方式获得这些运行时信息.
在第 3.2.3 节中, 已为满足要求的 store 指令注册了运行时的回调函数. 该回调函数需要结合运行时信息来使
用规则 1 和规则 2 检测这些 store 是否违反了 Go 的逃逸不变式. 第 3.3.1 节将介绍该回调函数在运行时如何利用
Go 的 ABI 从 Go 二进制中获得当前执行指令的 Goroutine 及其栈的相关信息.
在运行时回调函数中获得 Goroutine 栈信息

3.3.1

由第 1.1 节可知, Goroutine 栈是在操作系统进程的堆内存中模拟, 那么要获知 Goroutine 栈的范围, 判断某个
指针是否是栈指针就不能简单地用操作系统的系统栈去判定. 为了得到 Goroutine 的栈信息, 需要获得运行时中用
于管理 Goroutine 的 g 对象. 之后通过解析 g 对象的前几个字段的内存布局即可获得相应 Goroutine 的栈信息.
虽然 Go 运行时中的 g 对象不对用户暴露, 但幸运的是, 根据 Go 的 ABI-Internal[32]的约定可知, 在 AMD64 架
构中, R14 寄存器会保存当前执行的代码所在的 Goroutine, 也就是 g 对象的地址. 再结合 ABI-Internal 中有关基本
类型大小和对齐的约定以及前文所述的 g 和 stack 的结构, 可以通过公式 (7) 获得当前栈的 lo、hi:



 lo = ∗REGR14


 hi = ∗ (REGR14 + 8)

(7)

软件学报 ****年第**卷第**期

14

在利用 Go 的 ABI-Internal 获得 Go 的运行时栈信息时, 我们发现旧版本的 Go (Go1.16.15 及以下) 的 ABI 与
较新版本 Go (Go1.16.15 以上) 现行的 ABI-Internal 不同. 旧版本 Go 中, g 对象的地址不在寄存器 R14 中, 且旧版
本的 Go 并没有相应的 ABI 文档. 为了了解如何获得旧版本 Go 中的运行时信息, 我们对旧版本的 Go 的编译运行
时系统进行了人工分析. 最终发现在旧版本 Go 中, g 对象的地址存放在 TLS (thread local storage) 中的固定位置,
作为线程本地存储的一部分. 为了区分新版本和老版本的 Go, DBI-Go 在加载二进制时, 会首先获得系统 Go 的版
本, 根据 Go 的版本采取不同的策略. 针对 Go1.16.15 以上的 Go, 会使用 Go 现行的 ABI-Internal 从寄存器 R14 中
获得 g 对象的地址. 在 Go1.16.15 及以下的 Go 中, 则会从 TLS 中的固定位置获得 g 对象的地址, 并随后获得运行
时栈信息.
获得相应的栈信息后, 即可检测某地址是否在当前 Goroutine 栈中, 随后可结合规则 1 和规则 2 判断该 store
是否违反 Go 的逃逸不变式. 若该 store 违反了逃逸不变式, 就会结合该指令的地址获得其所在的函数, 并向 log 文
件中输出相应的出错信息, 包括该指令的地址、所在的函数、违反不变式的原因以及当前的运行时栈信息. 这些
信息可以在之后帮助开发者更快地找到问题.
3.4 采用多种措施减少误报
为了减少误报, DBI-Go 主要采取了以下措施.
(1) 措施 1: 过滤掉非 Go 函数. Go 的运行时最终以静态链接库的形式和用户代码链接成可执行文件, 其中除
了 Go 函数外还包括许多汇编和 C 函数. 汇编和 C 函数不遵守 Go 的 ABI 约定, 对这些函数进行分析会得到错误
的结果. 同时, 这些非 Go 函数也不遵守 Go 的逃逸不变式, 因此也无需对其进行分析. Go 的代码以包 (package) 的
形式进行管理, 每个函数都有其所在的包. 基于此观察, DBI-Go 采用基于模式匹配的方法, 通过函数名判断每个函
数是否在某个包内, 并据此过滤掉所有非 Go 函数.
(2) 措施 2: 过滤掉 Go 运行时函数. Go 除了会在用户代码中生成若干与运行时管理相关的代码外, 还会使用
runtime 包中的运行时函数来进行运行时管理. 这些运行时函数会产生若干违反 Go 逃逸不变式的 store 操作, 但这
些操作由 Go 的运行时保证了其安全性. 因此在 DBI-Go 的实现中会过滤掉 runtime 包中的函数以避免误报.
(3) 措施 3: 过滤掉非指针 store. 利用第 3.2.3 节中的方法, DBI-Go 可以恢复 Go 二进制中 store 指针的语义, 过
滤掉不包含指针的对象的 store. 使用该措施可以大大降低将非指针类型诸如 int、uintptr 等当作指针从而带来的
误报, 提升 DBI-Go 的分析精度.
以上措施不仅可以降低误报, 还降低了 DBI-Go 的整体开销. 通过上述措施, 我们提升了 DBI-Go 的精度和分
析效率 (详见第 4.3 节).

4 实验评估
对 DBI-Go 的实验评估在 x86-64 的机器上进行. 实验环境如下所示.
• 操作系统: Ubuntu 22.04.3 LTS (GNU/Linux 5.15.0-48-generic x86_64).
• CPU: 2 × AMD EPYC 7 763 64-Core Processor.
• 内存: 1.0 TiB.
• 涉及的 Go 版本: Go1.11 至 Go1.20.5.
实验试图回答以下研究问题.
(1) DBI-Go 对已知漏洞的覆盖情况如何, 能否发现新的漏洞?
(2) DBI-Go 插桩的回调函数带来的额外开销有多少, 是否满足了轻量快速的目标?
(3) DBI-Go 的适用性如何, 能否在不同的 Go 版本上正常使用?
4.1 有效性测试
为了对 DBI-Go 的漏洞覆盖率进行测试, DBI-Go 测试了图 1 中目前所有已知的社区例子. 针对图 1 中目前可
以复现且有着复现例子的 issue, 我们使用对应的 Go 版本将这些例子编译, 并之后使用 DBI-Go 插桩. 图 1 中提供

陈金宝 等: DBI-Go: 动态插桩定位 Go 二进制的非法内存引用

15

复现代码并可复现的 issue 共有 5 个, 分别为 issue#29000[37], issue#31573[38], issue#44614[15], issue#47276 [39]和
issue#54247[14]. 其涉及的 Go 版本为 Go1.11 至 Go1.17, 年份跨度为 2018–2022 年. 最终的结果显示, DBI-Go 的漏
洞覆盖率较高, 其可以检测出图 1 中所有可复现的例子中的违反 Go 逃逸不变式的 store 指令. 输出的 log 文件相
比于 Go 原始 GC panic 时产生的信息可以更清晰地展示出产生错误的指令及其所在的函数, 可以帮助更快定位原
因. 以 issue#44614 [15]为例, 其简化版代码可见代码 3. 图 8(a) 为社区 issue 中 Go GC 的 log, 图 8(b) 为 DBI-Go 的
log. 相比于 GC 的 log, 其可以更精确的显示问题的产生地, 比如二进制指令地址以及所在的函数. 若能结合
DWARF 信息, 还可以找到对应的源代码的位置, 更便于问题定位.

(a) issue 中 GC log

(b) DBI-Go log

图8

GC log 与 DBI-Go log

Go 的标准库 (std) 和编译工具链 (cmd) 提供了大量测试用例和 Benchmark, 覆盖了其中的众多常用 API. Go
的标准库和编译工具链中共有 277 个包提供了测试用例. 我们使用最新版本的 Go (Go1.20.5), 将这些测试用例和
Benchmark 编译成可执行文件, 并随后使用 DBI-Go 进行检测. 结果表明, 在这 277 个包中的 276 个没有发现问题,
然而, 在 syscall 包中, DBI-Go 发现 Go 在处理切片的字面量时将栈上的数组地址存到了全局变量中. 它的简化版
示例如图 9 所示. LEAQ 0x8(SP), AX 指令将栈对象的地址 0x8(SP) 存入了寄存器 AX 中, 接下来的 MOVQ 指令
将该栈地址 store 到了某全局变量处. 该 store 违反了规则 1 并被 DBI-Go 所捕获.

经Go编译器
编译

图9

syscall 包中发现的违反 Go 逃逸不变式的 store

目前该问题已经在 golang-nuts 中得到 Go 官方维护人员的确认 (https://groups.google.com/g/golang-nuts/
c/YZVFzwnPixM), 并已向社区提交 issue, 有待 Go 官方的进一步修复 (https://github.com/golang/go/issues/61730).
除了上述对使用 Go 原生逃逸分析算法的编译器生成的二进制的测试, 我们还将 DBI-Go 用于评测一个在
Gollvm (一个基于 LLVM 的 Go 编译器) 上重构的 Go 逃逸分析算法. 通过用 DBI-Go 检测经重构逃逸分析算法后
的 Gollvm 生成的二进制中是否有非法的内存引用, 来判断重构后的新逃逸分析算法的正确性, 并辅助开发人员进
行 Debug. 在实际评测中, 用 DBI-Go 可以发现新逃逸分析算法引起的内存分配问题. 以代码 5 和代码 6 为例, 代
码 5 中的 New 函数将字面量 prefixError{}的地址返回出函数; 代码 6 中, 对象 b 的地址被全局对象 gm 获取, 根据
逃逸不变式 2 它们理应堆分配. DBI-Go 发现重构后的逃逸分析算法在特定场景下会将代码 5 中本应在堆中分配
的 prefixError{}以及代码 6 中本应在堆中分配的 b 均分配在栈上. 通过这些例子, DBI-Go 帮助新逃逸算法的开发

软件学报 ****年第**卷第**期

16

者修复了重构后的逃逸算法上的 Bug.
代码 5. DBI-Go 发现的新逃逸算法出错的例子 1.
1. package nomain
2. type prefixError struct{s string}
3. func New(f string, x ...interface{}) error {
4.

// 误将字面量 prefixError{}分配到栈上

5.

return & prefixError{}

6. }
7. func (e *prefixError) Error() string {
8.

return “1” + e.s

9. }
代码 6. DBI-Go 发现的新逃逸算法出错的例子 2.
1. var gm map[string]interface{}
2.
3. func Test() {
4.

// 误将 b 分配到栈上

5.

var b int

6.

gm[“1”] = & b

7. }
4.2 额外开销
额外运行时开销

4.2.1

相比于只需在运行前执行一次的静态分析和初始化, 插桩的回调函数带来的额外运行时开销在反复执行时会
占据主要比例, 这些额外的运行时开销主要包括以下几部分: (1) 调用回调函数的开销; (2) 获取 Go 的运行时信息
的开销; (3) 利用规则 1 和规则 2 进行运行时验证的开销. 为了了解这些额外运行时开销的影响, 我们使用第 4.1
节中所述的 Go 标准库和编译工具链中的 277 个包中的测例进行了测试. 最终, 记录了插桩的回调函数带来的额
外运行时开销相比于直接执行时所花费的开销的比值. 为了表述方便, 在下文使用 Rc/o 表示额外的运行时开销相
比于直接执行时所花费的开销的比值.
为了了解 Rc/o 的分布, 对得到的额外开销数据使用了 KDE (kernel density estimation, 核密度估计, 一种用于估
计随机变量的概率密度函数的非参数方法)[40, 41]估计了 Rc/o 在这 277 个包中的分布密度, 如后文图 10 所示. 该曲
线的波峰在 Rc/o 约为 0.25 处达到, 且绝大多数的额外开销相比于原生开销的比值均小于 2 (93.3%) 只有在极少数
store 密集型的程序中该比值才会大于 4 (2.8%). 产生额外的 2 倍运行时开销是可以承受的.
额外初始化开销

4.2.2

在前文中提到, 由于初始化部分只需在运行前执行一次, 因此其相比于可以反复执行的运行时开销可以忽略.
但用于初始化的该部分开销在使用时也会对总时间造成影响, 因此对该部分开销的测试也是必要的. 额外的初始
化开销包括以下几部分时间: (1) Pin 加载用户二进制的开销; (2) 反汇编的开销; (3) 使用静态分析, 利用 Go 的写屏
障机制恢复 Go store 语义的开销. 使用和第 4.2.1 节相同的测试集和测试方式, 通过记录额外的初始化开销与原生
开销的比值, 并使用 KDE 估计分布密度, 可以得到图 11. 为了表述方便, 在下文使用 Ri/o 来表示额外的初始化开
销相比于原生开销的比值.
图 11 有两波波峰, 第 1 波大约在 Ri/o 为 4 处, 另一波对应的 Ri/o 则超过了 100. 相比于原生开销 100 倍的额外

陈金宝 等: DBI-Go: 动态插桩定位 Go 二进制的非法内存引用

17

初始化开销是惊人的. 为了了解该部分比值为何如此之高, 我们将 Ri/o 大于 100 的部分单独进行分析. 通过分析发
现, 这部分例子原生开销很小, 均不超过 10 ms, 与其相对应的 DBI-Go 的初始化开销均在 500 ms 左右, 如图 12 所
示, 仅在极个别例子上初始化开销超过了 1 500 ms. 这表明 DBI-Go 的初始化开销的下限在 500 ms 左右, 因此在遇
到原生开销很小, 只有几毫秒的测例时显得 Ri/o 很大. 但实际上, 500 ms 的初始化开销是完全可以接受的.
0.8
callback overhead

init overhead

0.008

0.7
0.007
0.6
0.006
0.005
密度

密度

0.5
0.4

0.004

0.3

0.003

0.2

0.002

0.1

0.001

0

0
10
2
4
6
8
额外开销相比与原生开销比值

0

图 10

12

0

Rc/o 的核密度分布曲线

图 11

100
200
300
400
500
额外开销相比与原生开销比值

Ri/o 的核密度分布曲线

DBI-Go 初始化开销 (μs)

3 000 000
2 500 000
2 000 000
1 500 000
1 000 000
500 000
0

22

28

24

86

26

17

38

27

08

28

82

28

图 12

05

29

78

29

04

30

71 20 00 79 58 62 03 96 26 64 38 32 29
30 32 33 34 35 36 38 38 39 39 43 50 64
原生开销 (μs)

比值大于 100 的额外初始化开销与原生开销

4.3 误报率测试
为了验证 DBI-Go 所利用的 Go 写屏障机制以及第 3.4 节中的两个措施对误报率的影响, 对这些措施进行了
单独或组合的测试. 在下文中, 使用“措施 1” 来代表第 3.4 节中的“过滤掉非 Go 函数” 措施; 使用“措施 2” 来代表
第 3.4 节中的“过滤掉 Go 运行时函数”措施; 使用“措施 3”来代表使用第 3.4 节中的“过滤掉非指针 store” 措施; 使
用“无”代表不使用任何措施, 直接插桩 Go 二进制中的所有 store. 我们使用 Go 的标准库和编译工具链提供的 277
个包, 测试方法与第 4.1 节相同.
最终的测试结果如表 1 所示. 从中看出, 单独使用措施 1 和 2 都没有效果. 这是因为目前 G 的二进制中都包
含大量的运行时管理函数以及汇编函数等非 Go 函数, 单独过滤运行时函数或者非 Go 函数无法消除在单一二进
制 (包) 上的误报. 从表 1 中可以看出, 同时使用措施 1 和 2 相比单独的措施 1 或 2 可以大大降低误报的包的数量,
但此时误报率仍然较高, 这是因为此时还没有恢复 Go 二进制中 store 指针的语义, 仍对所有 Go 用户代码中的
store 进行检查. 单独使用措施 3 的误报率也较高, 原因在于 Go 运行时中有诸多违反 Go 逃逸不变式的 store, 但运
行时保证了其安全性. 同时使用措施 2 和措施 3 可以带来最低的误报率, 在测试的 277 个包中误报率为 0. 在测试
中, 措施 1 无法在措施 3 的基础上进一步降低误报, 这是因为 Go 编译器只会对 Go 函数插入写屏障, 因此使用措
施 3 就潜在的消除了所有非 Go 函数带来的影响. 虽然措施 1 无法在措施 3 的基础上进一步降低误报率, 但并不

软件学报 ****年第**卷第**期

18

意味着措施 1 是无用的. 措施 1 可以让措施 3 的静态分析阶段跳过诸多无意义的非 Go 函数, 降低初始化阶段所
带来额外开销, 因此措施 1 也是必不可少的. 为了揭示措施 1 对减少初始化开销的作用, 我们只采用措施 2 和措
施 3, 不采用措施 1 去重复第 4.2.2 节中的测试, 结果显示在这 277 个包中, 总初始化时间增加了约 20.14%.
表1

各个措施误报率测试结果

减少误报的措施

总包数

报错的包数

误报的包数

误报率 (%)

无
措施1

277

277

276

99.64

277

277

276

99.64

措施2

277

277

276

99.64

措施1+措施2

277

24

23

95.83

措施3

277

274

273

99.63

措施3+措施1

277

274

273

99.63

措施3+措施2

277

1

0

0.00

措施3+措施1+措施2

277

1

0

0.00

4.4 开源项目测试
为了了解真实世界 Go 项目中是否有违反 Go 逃逸不变式引发的内存安全问题, 我们选取了 18 个在各个领域
具有代表性的 Go 开源项目来进行测试, 这些开源项目涉及 Web、数据库、分布式系统、边缘计算、云存储等多
个领域. 选取的仓库如表 2 所示. 测试时使用和第 4.1 节相似的测试方法: 以包为单位, 将这些项目提供的测试用
例和 Benchmark 编译成可执行文件, 并随后使用 DBI-Go 进行检测. 在这 18 个开源仓库中共有 1 730 个包提供了
测试用例或者 Benchmark. 最后的测试结果显示, DBI-Go 在这些仓库中并未检测到问题.
表2
类型

Web

测试的代表性 Go 开源项目

仓库名

简介

可测试的包数

hugo

轻量 Web 框架

123

beego

Go 语言 Web 框架

54

go-restful

用 Go 构建的 REST 风格的 Web 服务包

1
1

websocket

WebSocket 协议的 Go 实现

代理

v2ray-core

网络代理工具

64

数据库

dgraph
etcd

分布式的、可扩展的图数据库管理系统
分布式键值数据库

84
93

cli

GitHub 官方命令行工具

170

gogs

自托管 Git 服务

24

边缘计算

kubeedge

Kubernetes 原生边缘计算框架

66

存储管理

open-local
rclone

云原生本地存储管理系统
云存储管理

9
133

git服务

依赖注入框架
序列化

IOC-golang

依赖注入框架, 便于搭建任何 Go 应用

55

protobuf-go

Google 数据交换格式

43

json-iterator/go

encoding/json 包的高性能替代

10
108
680

分布式

grpc-go

gRPC 的 Go 语言实现

容器管理

kubernetes

生产级容器调度和管理

日志管理

zap

快速、结构化、分级的日志
合计

12
1 730

4.5 适用性测试
上述测试中针对额外运行时开销、Go 标准库以及编译工具链的测试中所使用的 Go 编译器为当前的最新版

陈金宝 等: DBI-Go: 动态插桩定位 Go 二进制的非法内存引用

19

本 (Go1.20.5). 在对漏洞覆盖率测试中, 我们会根据图 1 中每个 issue 所描述的 Go 版本来选取相应版本的 Go 编译
器. 最终结果显示 DBI-Go 在这些 Go 版本中均可正常运行 (Go1.11 至 Go1.20.5).

5 DBI-Go 的局限性讨论
综合来看, DBI-Go 仍有进一步优化的优化空间. 对此, 本节列出来了目前 DBI-Go 的一些局限性及一些可能
的解决思路.
• 动态分析工具的代码覆盖率问题. DBI-Go 是基于二进制插桩的动态检测器, 因此它无法检测 Go 程序中未
被执行的路径中的 store 指令. 这是所有动态分析工具不得不面对的问题. 为了改善这种情况, 可以使用基于代码
覆盖率的模糊测试等技术来提升代码覆盖率.
• 现有规则可进一步扩充. DBI-Go 的运行时验证部分基于规则 1 和规则 2. 其中规则 2 基于 Go 的逃逸不变式
2 “指向栈对象的指针生命期不可超出该栈对象”总结得到. 目前规则 2 所考虑的“生命期超出该栈对象”的情况分
为 5 种: 全局对象、堆对象、更深的栈对象、栈帧更深的栈对象以及其他 Goroutine 栈对象. 但实际上“生命期超
出该栈对象”还有其他情况, 比如更浅的作用域中的栈对象等. 由于这些情况在二进制中不好识别, 目前规则 2 未
考虑. 因此规则 2 目前仍有进一步扩充的空间.
• 误报无法完全消除. 目前基于 gcWriteBarrier 的 store 语义恢复机制只适用于栈到堆或全局的 store 指令. 违
反逃逸不变式的栈到栈的 store 指令的检测目前仍需要插桩二进制中的所有 store 指令. 尽管目前已做了一些筛选,
比如不插桩运行时函数中的 store, 跳过汇编函数等. 但由于缺乏 Go 的高层语义信息, 比如类型描述符, 此时仍有
可能带来误报. 同时, gcWriteBarrier 是 Go 编译器在编译期间在源代码层级上增加的调用, 其以对象为粒度, 而非
二进制中的指令. 若某对象中既包含指针类型也包含非指针类型 (如 uintptr), 当该非指针类型的值等于某个栈指
针时, 由于 DBI-Go 假定其为指针, 此时仍有可能产生误报. 若要杜绝此类问题, 一种可能的方法是利用 Go 运行时
中的 bitmap. 该 bitmap 可以指示内存中何处是指针, 何处不是指针, Go 的 GC 会利用该 bitmap 进行标记和清扫.
但引入 bitmap 会导致 DBI-Go 和 Go 的不同版本运行时强绑定, 降低其可扩展性, 同时还会进一步增大 DBI-Go 的
额外开销. 因此, 综合考虑, 由于目前 DBI-Go 的误报率在可接受的范围中, 故没有引入 bitmap 机制.
• ABI 需要根据不同 Go 版本进行适配. DBI-Gos 实现的其中关键一点在于利用 Go 的 ABI 获得 Goroutine 运
行时栈信息. 目前 Go1.17 及以上版本采用现行的 ABI-Internal, 而 Go1.16.15 及以下则是另一套 ABI. 为了保证适
用性, 目前 DBI-Go 针对不同版本做了适配. 若未来 Go 的 ABI 发生进一步变化, 则 DBI-Go 也需要进行相应的适
配. 不过需要注意的是, Go 运行时中 Goroutine 的栈结构比较稳定, 其结构从 2014 年的 Go1.4 到 2023 年最新的
Go1.20 版本均未发生变化. 这意味着未来只需对 ABI 进行适配, 而使用 Goroutine 栈信息的运行时验证部分则无
需更改.
• 依赖 Go 的写屏障机制的正确性. DBI-Go 的分析目前依赖于 Go 编译器在编译期间插入的 gcWriteBarrier 函
数, 若 Go 编译器由于误判等原因没有对某 store 指针到堆的操作插入 gcWriteBarrier 函数, 则 DBI-Go 无法判断是
否有违反逃逸不变式的情况.
• 需要二进制上的符号信息. DBI-Go 需要二进制中的符号信息才能正常工作, 包括函数名、全局变量名等. 目
前 DBI-Go 无法在完全剥离符号信息的二进制上进行分析.
• 额外开销可进一步降低. 在额外开销方面, 目前回调函数的额外开销仍有继续优化的空间. Pin 作为一个通
用的动态二进制分析框架, 没有针对 Go 的特定优化. DBI-Go 在设计实现时本着快速原型化的理念, 也未针对 Go
做大量优化. 目前 DBI-Go 所做的优化有使用基于匹配的方式跳过 Go 的运行时函数和一些非 Go 函数 (如一些汇
编函数、C 函数) 以及基于 gcWriteBarrier 机制只插桩可能 store 指针的指令. 未来基于 Go 语言的特性可以探索
更多的优化方式, 减少 Pin 插桩带来的额外开销.
• 无法自动修复漏洞. DBI-Go 只是一个 Go 二进制中的漏洞检测器, 它并不能自动修复检测到的漏洞. 漏洞产
生的原因有很多种, 比如逃逸分析的错误、编译优化的错误等. 通过 DBI-Go 检测出的漏洞需要 Go 编译器开发人
员的进一步分析和修复. 但 DBI-Go 的 log 可以帮助开发人员更快的确认漏洞出现的位置及原因.

软件学报 ****年第**卷第**期

20

• 有待进一步大规模测试. 目前 DBI-Go 在真实世界开源项目上的测试尚未发现问题, 只在 18 个仓库上进行
了测试, 覆盖面不足. 未来期望能够进行更大规模的测试, 以期找出开源仓库中的问题, 帮助改善 Go 语言软件的
内存安全性, 提升其可靠性.

6 相关工作
6.1 动态二进制分析
动态二进制分析框架, 如 Pin[21]、Valgrind[42]、DynamoRIO[43], 可以用于插桩任意指令来执行动态分析. 这些
工具为研究者们的分析带来了很大的便利. Amitabha 等人 [44]研究了如何有效地插桩 x86 机器码中的内存访问以
支持软件事务内存和分析. Patil 等人 [45]基于 Pin 设计了 Pinplay, 一个基于执行捕获和确定性重放的并行程序分析
框架. Zhong 等人 [17]基于 DynamoRIO 设计了一个动态二进制工具, 用于检测 Go 中的并发问题.
基于动态二进制分析的漏洞检测包括前面提到的并发漏洞分析、污点分析 [46] 、逆向工程 [47] 和执行重
放 [48]等.
6.2 内存漏洞检测
目前已有的内存相关的漏洞检测工作主要是针对 C/C++这种具有弱静态类型系统的编程语言来进行的, 因为
强制类型转换、任意指针的存在使得悬空引用、边界溢出等内存漏洞更容易发生, Song 等人于 2013 年 [30]通过系
统性地建立内存损坏的一般模型, 揭示了 C/C++容易遭受内存漏洞的主要原因. 现有的内存漏洞检测工作依赖于
静态程序分析或动态程序分析来进行.
静态检测: 分析程序 (源) 代码, 并生成对于所有可能的代码执行都是保守正确的结果. 静态检测的一部分工
作基于形式化验证: Clarke 等人提出了一种使用有界模型检查 (BMC) 对 ANSI-C 程序进行形式化验证的工具 [49]
来检测内存问题. 更多的静态检测工作则是基于符号执行: CUTE[50]通过结合符号执行和具体执行, 将内存图作为
输入来执行自动化测试; EXE [51] 是利用符号执行来自动生成导致实际代码崩溃的输入, 进行快速的错误定位;
Klee[52]则是通过符号执行来自动生成测试.
动态检测: 通过分析单个程序的执行, 并输出仅对单个运行有效的精确分析结果. 消毒器 (sanitizer)——静态
插入运行时监视器, 并在运行时进行检测——是动态检测工具的典型代表. Serebryany 等人在 2012 年提出了
AddressSanitizer (ASAN)[28], 它通过插桩应用程序中的内存访问操作, 在运行时建模影子内存, 从而能识别缓冲区
溢出、悬垂指针、内存泄漏等内存漏洞. 由于它能够在不牺牲完备性的情况下实现了检测效率, AddressSanitizer
已经被集成到许多常用的编译工具链中, 包括针对 C/C++的编译器 GCC、LLVM, 以及 Go 语言编译器. 但 ASAN
目前在 Go 编译器中的使用场景受限, 仅能检测 Go 语言中和 C 语言进行交互的相关代码上的内存错误 [53], 对于
纯 Go 语言代码尚不支持. ASAN 与 DBI-Go 的相同点在于二者都是基于插桩, ASAN 是在编译时插桩, DBI-Go 是
基于动态二进制插桩. 区别在于 ASAN 的设计目的是检测诸如缓冲区溢出之类的通用的内存漏洞, 不能检测 Go
中违反 Go 逃逸不变式的 store; 且其所能检测的内存漏洞只有在被触发时才能发现 (如内存被释放、指针被解引
用时), 此时 Go 程序可能已经崩溃. 而 DBI-Go 则是针对 Go 语言专门设计, 利用 Go 的逃逸不变式这一独特特性,
可以在内存隐患发生的第一现场就报错 (比如将栈地址存入堆时). 因此即使 ASAN 可以支持纯 Go 的代码, 其也
无法代替 DBI-Go. 类似地, 还有许多使用类似方式进行针对其他问题检测的漏洞检测工具, 如用于检测未初始化
内存的使用情况的 MemorySanitizer[54], 用于检测数据竞争的 ThreadSanitizer[55], 利用动态内存检查来检测对象有
效性的 EffectiveSan[56] 等. Song 等人 [29] 则是在 2019 年对 Sanitizing 这种技术进行了对比总结, 描述了不同的
Sanitizer 工具的性能和可扩展性. 此外, 还有部分工作通过修改运行时系统来实现运行时检测, 如 DieHard [57] 、
SoftBound[58].
6.3 Go 的漏洞检测
目前针对 Go 的漏洞检测已有许多工作. Lange 等人 [19]为 Go 中的消息传递机制进行建模, 为 Go 的消息传递
机制提出了一个验证框架. Lauinger 等人 [33]提出了 go-safer, 一种全新的静态分析工具, 用来识别 Go 源代码中对

陈金宝 等: DBI-Go: 动态插桩定位 Go 二进制的非法内存引用

21

unsafe 包的不安全使用. Wang 等人 [59] 设计了 HERO, 用于检测 Go 中依赖管理导致的问题. Liu 等人 [18] 提出了
GCatch, 用于自动检测和修复 Go 中的并发问题. Chabbi 等人 [31] 使用现有的数据竞争检测器在 Uber 项目中发现
了超过 2000 个数据竞争. Li 等人 [60]设计了 CryptoGo, 用于检测 Go 中和加解密相关 API 的误用. Zhong 等人 [17]
首次提出了使用动态二进制插桩的方式检测 Go 中的并发问题. 目前与 Go 漏洞检测相关的工作多数集中在对用
户代码的漏洞的检测, 且多与并发有关. 本文提出的 DBI-Go 是首个验证 Go 编译器生成的代码的是否满足 Go 逃
逸不变式的工具.
6.4 Go 的逃逸分析
目前 Go 中和逃逸分析的相关工作较少. Google 曾在 2015 年总结了当时 Go 逃逸分析的缺陷, 指出了其分析
的保守之处 [61]. Wang 等人 [20]则注意到了 Go 逃逸分析的一些保守之处, 其工作使得一些对象可以绕过 Go 的逃逸
分析, 从而节省堆内存的使用.

7 总结与展望
本文主要提出了 DBI-Go, 一个用于 Go 应用程序的新型漏洞检测工具. DBI-Go 使用静态分析辅助动态二进
制插桩的分析方法, 以 Go 二进制文件为输入, 检测 Go 编译器生成的代码中是否有违反 Go 逃逸不变式的 store.
DBI-Go 使用静态分析的方法, 结合 Go 的 gcWriteBarrier 机制恢复 Go 的 store 语义. DBI-Go 的运行时回调函数在
运行时结合 Go 的 ABI 约定获得 Go 的运行时栈信息来辅助分析. DBI-Go 使用约 1 000 行 C++代码实现, 为比较
轻量的检测工具.
实验表明, DBI-Go 可以检测出目前 Go 社区已经确认的问题, 呈现了较高的漏洞覆盖率, 同时 DBI-Go 还成功
检测出一个之前未知的问题, 目前该问题已经得到 Go 官方的确认并在等待进一步修复. 在实际项目上的应用还
表明 DBI-Go 可以辅助开发人员对内存优化相关算法, 如逃逸分析算法, 进行优化和重构, 验证算法的正确性. 对
误报率的测试则表明 DBI-Go 所采取的措施可以有效地降低误报. 实验结果还表明, DBI-Go 在不同版本的 Go 编
译器编译出的二进制上都能正常工作 (Go1.11 至 Go1.20.5), 体现了较高的可扩展性. 额外开销的测试结果则表明
DBI-Go 会产生在可接受范围内的约常数倍的开销.
本文还分析了 DBI-Go 的不足之处. 未来将继续改进 DBI-Go, 以期实现更高的代码覆盖率、更高的精度以及更
小的额外开销, 并将进行更大规模, 更大范围的测试, 以期找到更多漏洞, 帮助改善 Go 语言软件的可靠性和安全性.
References:
[1]

Go. The Go programming language. 2023. https://go.dev/

[2]

TIOBE. Programming language hall of fame. 2023. https://www.tiobe.com/tiobe-index

[3]

Taylor N. 2022 Hiring report: Golang developers. 2022. https://sgp.technology/2022-hiring-report-golang-developers/

[4]

Salgado PG. Garbage collector design. 2023. https://devguide.python.org/internals/garbage-collector/

[5]

Schatzl T. Java garbage collection: The 10-release evolution from JDK 8 to JDK 18. 2022. https://blogs.oracle.com/javamagazine/post/
java-garbage-collectors-evolution?source=:em:nw:mt::::RC_WWMK200429P00043C0061:NSL400242337

[6]

Ghemawat S, Menage P. TCMalloc: Thread-caching Malloc, 2009. http://goog-perftools.sourceforge.net/doc/tcmalloc.html

[7]

Fua P, Lis K. Comparing Python, Go, and C++ on the N-queens problem. arXiv:2001.02491, 2020.

[8]

Blanchet B. Escape analysis for object-oriented languages: Application to Java. ACM SIGPLAN Notices, 1999, 34(10): 20–34. [doi: 10.
1145/320385.320387]

[9]

Whaley J, Rinard M. Compositional pointer and escape analysis for Java programs. In: Proc. of the 14th ACM SIGPLAN Conf. on Objectoriented Programming, Systems, Languages, and Applications. Denver: ACM, 1999. 187–206. [doi: 10.1145/320384.320400]

[10]

Choi JD, Gupta M, Serrano MJ, Sreedhar VC, Midkiff SP. Stack allocation and synchronization optimizations for java using escape
analysis. ACM Trans. on Programming Languages and Systems, 2003, 25(6): 876–910. [doi: 10.1145/945885.945892]

[11]

Kotzmann T, Mössenböck H. Escape analysis in the context of dynamic compilation and deoptimization. In: Proc. of the 1st
ACM/USENIX Int’l Conf. on Virtual Execution Environments. Chicago: ACM, 2005. 111–120. [doi: 10.1145/1064979.1064996]

[12]

Kotzmann T, Wimmer C, Mössenböck H, Rodriguez T, Russell K, Cox D. Design of the Java HotSpot™ client compiler for Java 6. ACM

软件学报 ****年第**卷第**期

22

Trans. on Architecture and Code Optimization, 2008, 5(1): 7. [doi: 10.1145/1369396.1370017]

[13]

Stadler L, Würthinger T, Mössenböck H. Partial escape analysis and scalar replacement for Java. In: Proc. of the 2014 Annual
IEEE/ACM Int’l Symp. on Code Generation and Optimization. Orlando: ACM, 2014. 165–174. [doi: 10.1145/2581122.2544157]

[14]

Go. Go/issues/54247. 2022. https://github.com/golang/go/issues/54247

[15]

Go. Go/issues/44614. 2021. https://github.com/golang/go/issues/44614

[16]

Go. Bad pointer. 2023. https://github.com/golang/go/blob/2fcfdb96860855be0c88e10e3fd5bb858420cfe2/src/runtime/mbitmap.go#L321

[17]

Zhong CX, Zhao QD, Liu X. BinGo: Pinpointing concurrency bugs in Go via binary analysis. arXiv:2201.06753, 2022.

[18]

Liu ZH, Zhu SF, Qin BQ, Chen H, Song LH. Automatically detecting and fixing concurrency bugs in Go software systems. In: Proc. of
the 26th Int’l Conf. on Architectural Support for Programming Languages and Operating Systems. ACM, 2021. 616–629. [doi: 10.1145/
3445814.3446756]

[19]

Lange J, Ng N, Toninho B, Yoshida N. A static verification framework for message passing in go using behavioural types. In: Proc. of the
2018 IEEE/ACM 40th Int’l Conf. on Software Engineering. Gothenburg: IEEE, 2018. 1137–1148. [doi: 10.1145/3180155.3180157]

[20]

Wang C, Zhang MR, Jiang Y, Zhang HF, Xing ZC, Gu M. Escape from escape analysis of Golang. In: Proc. of the 42nd IEEE/ACM Int’l
Conf. on Software Engineering: Software Engineering in Practice. Seoul: IEEE, 2020. 142–151.

[21]

Luk CK, Cohn R, Muth R, Patil H, Klauser A, Lowney G, Wallace S, Reddi VJ, Hazelwood K. Pin: Building customized program
analysis tools with dynamic instrumentation. ACM Sigplan Notices, 2005, 40(6): 190–200. [doi: 10.1145/1064978.1065034]

[22]

Go. Goroutines. 2023. https://go.dev/doc/effective_go#goroutines

[23]

Go. A guide to the Go garbage collector. 2023. https://go.dev/doc/gc-guide

[24]

Go. Frequently asked questions (FAQ). 2023. https://go.dev/doc/faq#stack_or_heap

[25]

Go. Go escape analysis invariant, 2022. https://github.com/golang/go/blob/master/src/cmd/compile/internal/escape/escape.go#L22

[26]

Go. Go/test/escape.go. 2022. https://github.com/golang/go/blob/master/test/escape.go#L10

[27]

Go. Go/test/escape_level.go. 2022. https://github.com/golang/go/blob/master/test/escape_level.go#L14

[28]

Serebryany K, Bruening D, Potapenko A, Vyukov D. AddressSanitizer: A fast address sanity checker. In: Proc. of the 2012 USENIX
Annual Technical Conf. Boston: USENIX Association, 2012. 309–318.

[29]

Song D, Lettner J, Rajasekaran P, Na Y, Volckaert S, Larsen P, Franz M. SoK: Sanitizing for security. In: Proc. of the 2019 IEEE Symp.
on Security and Privacy. San Francisco: IEEE, 2019. 1275–1295. [doi: 10.1109/SP.2019.00010]

[30]

Szekeres L, Payer M, Wei T, Song D. SoK: Eternal war in memory. In: Proc. of the 2013 IEEE Symp. on Security and Privacy. Berkeley:
IEEE, 2013. 48–62. [doi: 10.1109/SP.2013.13]

[31]

Chabbi M, Ramanathan MK. A study of real-world data races in Golang. In: Proc. of the 43rd ACM SIGPLAN Int’l Conf. on
Programming Language Design and Implementation. San Diego: ACM, 2022. 474–489. [doi: 10.1145/3519939.3523720]

[32]

Go. Go internal ABI specification, 2022. https://github.com/golang/go/blob/master/src/cmd/compile/abi-internal.md

[33]

Lauinger J, Baumgärtner L, Wickert AK, Mezini M. Uncovering the hidden dangers: Finding unsafe Go code in the wild. In: Proc. of the
19th IEEE Int’l Conf. on Trust, Security and Privacy in Computing and Communications. Guangzhou: IEEE, 2020. 410–417. [doi: 10.
1109/TrustCom50675.2020.00063]

[34]

Uber-Go. Goleak. 2023. https://github.com/uber-go/goleak

[35]

Zeng B. Static analysis on binary code. 2012. https://engineering.lehigh.edu/sites/engineering.lehigh.edu/files/_DEPARTMENTS/cse/
research/tech-reports/2012/lu-cse-12-001.pdf

[36]

Go. The Go programming language specification. 2023. https://go.dev/ref/spec

[37]

Go. Go/issues/29000. 2018. https://github.com/golang/go/issues/29000

[38]

Go. Go/issues/31573. 2019. https://github.com/golang/go/issues/31573

[39]

Go. Go/issues/47276. 2021. https://github.com/golang/go/issues/47276

[40]

Rosenblatt M. Remarks on some nonparametric estimates of a density function. The Annals of Mathematical Statistics, 1956, 27(3):
832–837. [doi: 10.1214/aoms/1177728190]

[41]

Parzen E. On estimation of a probability density function and mode. The Annals of Mathematical Statistics, 1962, 33(3): 1065–1076.
[doi: 10.1214/aoms/1177704472]

[42]

Nethercote N, Seward J. Valgrind: A framework for heavyweight dynamic binary instrumentation. ACM SIGPLAN Notices, 2007, 42(6):
89–100. [doi: 10.1145/1273442.1250746]

[43]

DynamoRIO. 2002. https://dynamorio.org/

[44]

Roy A, Hand S, Harris T. Hybrid binary rewriting for memory access instrumentation. In: Proc. of the 7th ACM SIGPLAN/SIGOPS Int’l
Conf. on Virtual Execution Environments. Newport Beach: ACM, 2011. 227–238. [doi: 10.1145/1952682.1952711]

陈金宝 等: DBI-Go: 动态插桩定位 Go 二进制的非法内存引用

[45]

23

Patil H, Pereira C, Stallcup M, Lueck G, Cownie J. PinPlay: A framework for deterministic replay and reproducible analysis of parallel
programs. In: Proc. of the 8th Annual IEEE/ACM Int’l Symp. on Code Generation and Optimization. Toronto: ACM, 2010. 2–11. [doi:
10.1145/1772954.1772958]

[46]

Brumley D, Newsome J, Song D, Wang H, Jha S. Towards automatic generation of vulnerability-based signatures. In: Proc. of the 2006
IEEE Symp. on Security and Privacy. Berkeley: IEEE, 2006. 15–16. [doi: 10.1109/SP.2006.41]

[47]

Lin ZQ, Jiang XX, Xu DY, Zhang XY. Automatic protocol format reverse engineering through context-aware monitored execution. In:
Proc. of the 15th Symp. on Network and Distributed System Security. San Diego: NDSS, 2008. 1–15.

[48]

Narayanasamy S, Pokam G, Calder B. Bugnet: Continuously recording program execution for deterministic replay debugging. In: Proc. of
the 32nd Int’l Symp. on Computer Architecture. Madison: IEEE, 2005. 284–295. [doi: 10.1109/ISCA.2005.16]

[49]

Clarke E, Kroening D, Lerda F. A tool for checking ANSI-C programs. In: Proc. of the 10th Int’l Conf. on Tools and Algorithms for the
Construction and Analysis of Systems. Barcelona: Springer, 2004. 168–176. [doi: 10.1007/978-3-540-24730-2_15]

[50]

Sen K, Marinov D, Agha G. CUTE: A concolic unit testing engine for C. ACM SIGSOFT Software Engineering Notes, 2005, 30(5):
263–272. [doi: 10.1145/1095430.1081750]

[51]

Cadar C, Ganesh V, Pawlowski PM, Dill DL, Engler DR. EXE: Automatically generating inputs of death. ACM Trans. on Information
and System Security, 2008, 12(2): 10. [doi: 10.1145/1455518.1455522]

[52]

Cadar C, Dunbar D, Engler DR. KLEE: Unassisted and automatic generation of high-coverage tests for complex systems programs. In:
Proc. of the 8th USENIX Conf. on Operating Systems Design and Implementation. San Diego: USENIX Association, 2008. 209–224.
[doi: 10.5555/1855741.1855756]

[53]
[54]

Go. Go 1.18 release notes. 2022. https://tip.golang.org/doc/go1.18#go-command
Stepanov E, Serebryany K. MemorySanitizer: Fast detector of uninitialized memory use in C++. In: Proc. of the 2015 IEEE/ACM Int’l
Symp. on Code Generation and Optimization. San Francisco: IEEE, 2015. 46–55. [doi: 10.1109/CGO.2015.7054186]

[55]

Serebryany K, Iskhodzhanov T. ThreadSanitizer: Data race detection in practice. In: Proc. of the 2009 Workshop on Binary Instrumentation
and Applications. New York: ACM, 2009. 62–71. [doi: 10.1145/1791194.1791203]

[56]

Duck GJ, Yap RHC. EffectiveSan: Type and memory error detection using dynamically typed C/C++. ACM SIGPLAN Notices, 2018,
53(4): 181–195. [doi: 10.1145/3296979.3192388]

[57]

Berger ED, Zorn BG. DieHard: Probabilistic memory safety for unsafe languages. ACM SIGPLAN Notices, 2006, 41(6): 158–168. [doi:
10.1145/1133255.1134000]

[58]

Nagarakatte S, Zhao JZ, Martin MMK, Zdancewic S. SoftBound: Highly compatible and complete spatial memory safety for C. ACM
SIGPLAN Notices, 2009, 44(6): 245–258. [doi: 10.1145/1543135.1542504]

[59]

Wang Y, Qiao L, Xu C, Liu YP, Cheung SC, Meng N, Yu H, Zhu ZL. Hero: On the chaos when PATH meets modules. In: Proc. of the
43rd IEEE/ACM Int’l Conf. on Software Engineering. Madrid: IEEE, 2021. 99–111. [doi: 10.1109/ICSE43902.2021.00022]

[60]

Li WQ, Jia SJ, Liu LM, Zheng FY, Ma Y, Lin JQ. CryptoGo: Automatic detection of Go cryptographic API misuses. In: Proc. of the 38th
Annual Computer Security Applications Conf. Austin: ACM, 2022. 318–331. [doi: 10.1145/3564625.3567989]

[61]

Vyukov D. Go escape analysis flaws, 2015. https://docs.google.com/document/d/1CxgUBPlx9iJzkz9JWkb6tIpTe5q32QDmz8l0BouG0Cw

陈金宝(1999－), 男, 硕士生, 主要研究领域为现

李清伟(2001－), 男, 硕士生, 主要研究领域为程

代语言编译和运行时系统, 软件安全.

序语言运行时, 程序分析.

张昱(1972－), 女, 博士, 教授, CCF 杰出会员, 主

丁伯尧(1999－), 男, 博士生, CCF 学生会员, 主

要研究领域为面向新兴计算的编程系统, 软件分

要研究领域为面向内存安全的程序分析, 多语言

析与系统优化, 智能计算, 数据计算, 量子计算.

程序交互与适配, 现代语言编译和运行时系统.


软件学报 ISSN 1000-9825, CODEN RUXUEW
Journal of Software [doi: 10.13328/j.cnki.jos.007100]
©中国科学院软件研究所版权所有.

E-mail: jos@iscas.ac.cn
http://www.jos.org.cn
Tel: +86-10-62562563

*

DDoop: 基于差分式 Datalog 求解的增量指针分析框架
沈天琪 1,2, 王熙灶 1,2, 宾向荣 1,2, 卜 磊 1,3
1

(计算机软件新技术全国重点实验室 (南京大学), 江苏 南京 210023)

2

(南京大学 计算机科学与技术系, 江苏 南京 210023)

3

(南京大学 软件学院, 江苏 南京 210023)

通信作者: 卜磊, E-mail: bulei@nju.edu.cn

摘

要: 指针分析是对软件进行编译优化、错误检测的核心基础技术之一. 现有经典指针分析框架, 如 Doop, 会将

待分析程序和分析算法转化成 Datalog 评估问题并进行求解, 如程序规模较大, 单次求解分析时间开销较大. 在程
序频繁变更发布的情况下, 相关程序分析的开销更是难以负担. 近年来, 增量分析作为一种在代码频繁变更场景下
有效复用已有分析结果提升分析效率的技术受到了越来越多的关注. 然而, 目前的增量指针分析技术通常针对特
定算法设计, 支持的指针分析选项有限, 其可用性也受到较大限制. 针对上述问题, 设计并实现一种基于差分式
Datalog 求解的增量指针分析框架 DDoop (Differential Doop). DDoop 实现增量输入事实生成技术与增量分析规则
自动化重写技术, 将多版本程序增量分析问题表达为差分 Datalog 评估问题, 从而可以充分利用成熟的差分式
Datalog 求解引擎, 如 DDlog, 来实现端到端的增量指针分析, 并最大化兼容复用 Doop 中已有的指针分析实现, 提
供透明的增量化支持. 在广泛应用的真实世界程序上对 DDoop 进行实验评估, 实验结果显示 DDoop 相较于非增
量的 Doop 框架具有显著的性能优势, 同时高度兼容 Doop 中已有的各种指针分析规则.
关键词: 指针分析; 增量分析; Datalog 引擎; 增量计算; 差分式 Datalog
中图法分类号: TP314
中文引用格式: 沈天琪, 王熙灶, 宾向荣, 卜磊. DDoop: 基于差分式Datalog求解的增量指针分析框架. 软件学报. http://www.jos.org.
cn/1000-9825/7100.htm
英文引用格式: Shen TQ, Wang XZ, Bin XR, Bu L. DDoop: Incremental Pointer Analysis Framework Based on Differential Datalog
Evaluation. Ruan Jian Xue Bao/Journal of Software (in Chinese). http://www.jos.org.cn/1000-9825/7100.htm

DDoop: Incremental Pointer Analysis Framework Based on Differential Datalog Evaluation
SHEN Tian-Qi1,2, WANG Xi-Zao1,2, BIN Xiang-Rong1,2, BU Lei1,3
1

(State Key Laboratory for Novel Software Technology (Nanjing University), Nanjing 210023, China)

2

(Department of Computer Science and Technology, Nanjing University, Nanjing 210023, China)

3

(Software Institute, Nanjing University, Nanjing 210023, China)

Abstract: Pointer analysis is a core and fundamental technology for software compiler optimization and bug detection. Existing classic
pointer analysis frameworks such as Doop will transform the programs to be analyzed and analysis algorithms into Datalog evaluation
problems like too large program size and solve them. As a result, the analysis time overhead of a single solution can be high, and the
program analysis overhead can hardly be afforded especially in situations where programs are frequently changed and released. In recent
years, as a technology that effectively reemploys existing analysis results and improves analysis efficiency under frequent code changes,
incremental analysis has caught increasing attention. However, since current incremental pointer analysis techniques are often designed for

*

基金项目: 国家自然科学基金 (62232008, 62172200); 江苏省前沿引领技术基础研究专项 (BK20202001); 中央高校基本科研业务费专项
资金 (020214380101)
本文由“编译技术与编译器设计”专题特约编辑冯晓兵研究员、郝丹教授、高耀清博士、左志强副教授推荐.
收稿时间: 2023-09-11; 修改时间: 2023-10-30; 采用时间: 2023-12-14; jos 在线出版时间: 2024-01-05

软件学报 ****年第**卷第**期

2

specific algorithms, the supported pointer analysis options are limited and their usability is significantly restricted. To this end, this study
designs and implements Differential Doop (DDoop), an incremental pointer analysis framework based on Differential Datalog evaluation.
DDoop implements incremental input fact generation and automatic rewriting for incremental analysis rules, expressing incremental
analysis problems of multi-version programs as Differential Datalog evaluation problems. Finally, a mature Differential Datalog solution
engine like DDlog can be fully utilized to achieve end-to-end incremental pointer analysis, maximizing compatibility and reuse of existing
pointer analysis implementations in Doop and providing transparent support for incrementalization. Additionally, experimental evaluation of
DDoop is conducted on widely adopted real-world programs. The results show that compared to the non-incremental Doop framework,
DDoop has a significant performance advantage while highly compatible with a variety of pointer analysis rules existing in Doop.
Key words: pointer analysis; incremental analysis; Datalog engine; incremental computation; Differential Datalog

随着信息技术的发展, 我们已经处于一个软件定义一切的时代. 大到航空、电力、铁路系统, 小到智能家居、
移动支付, 软件在生活中已经几乎是无处不在. 而随着软件的普及, 软件故障可能会严重影响人们的财产或生命安
全, 甚至带来灾难性的后果, 因此人们对软件质量的要求越来越高, 而对软件质量的保障就显得愈发重要. 静态程
序分析是一类重要的软件质量保障技术, 其能够对程序代码进行自动化扫描, 而无需实际运行程序.
指针分析是公认最基础的静态程序分析技术之一, 其用于在编译期静态计算程序中的指针变量在运行时可能
指向的内存位置 (或对象). 指针分析的结果通常可表示为待分析程序中每个指针的指向集 (points-to set). 这些信
息对于推理面向对象程序中的别名关系和过程间控制流至关重要, 被广泛地应用于错误检测 [1]、安全分析 [2]、程
序验证 [3]、编译优化 [4]等一系列技术中. 这些技术可以视为指针分析的客户端分析, 也即它们需要将指针分析产
生的结果作为输入, 因而指针分析的效率和精度直接影响了后续的客户端分析. 也正因如此, 虽然已经有超过 40
年的研究历史, 指针分析至今仍是静态程序分析学术研究中的重点领域 [5].
然而, 指针分析目前的实用性仍有所不足. 一方面, 尽管研究人员已经在指针分析的算法和实现上投入了大量
人力, 但是即使是目前最先进的全量指针分析算法或框架, 其可扩展性仍然有所欠缺 [6]: 将高精度的指针分析应用
到数百万行的大型程序上可能需要花费超过数小时 [7]. 对于大多数生产应用来说, 这样的时间消耗通常是不可接
受的. 此问题严重削弱了高精度指针分析在现实世界大规模程序中的应用能力.
另一方面, 在如今提倡的 DevOps 实践 [8]中, 代码的变更和发布与以往相比更加频繁. CI/CD 是 DevOps 实践
中的重要流程, 静态代码扫描是 CI/CD 中的一个关键环节, 指针分析又是静态代码扫描的基础. 频繁的变更意味
着更频繁地对代码进行扫描, 这对指针分析的可扩展性提出了更高要求. 另外, 频繁的变更也意味着连续两个版本
的代码差异往往很小: 通常仅限于程序的某个模块. 换言之, 对于整个程序而言, 在一次变更后, 程序的变化主要是
局部的, 相应地, 指针分析结果的变化通常也仅占很小比例.
在处理代码变更频繁且单次变更变化相对局部的场景时, 传统的全程序程序分析方法效率较低: 如果我们每
次都对整个程序进行完整的全程序指针分析, 会发现在绝大多数情况下, 可推断的指针指向关系实际上在新旧两
次分析中是完全不变的, 尤其是对于程序中未发生代码变更或不受代码变更影响的部分 [9]. 这意味着我们在这些
部分进行了完全重复的冗余计算, 这无疑是对计算资源的巨大浪费, 同时也会导致开发者等待 CI/CD 流程的时间
过长, 严重影响开发者的工作效率.
有鉴于此, 已经有研究人员提出了增量指针分析技术 [9,10]. 增量指针分析是一种能够有效利用先前分析结果的
方法, 它通过保留分析中的中间结果, 仅针对两次输入程序中不同的部分计算指针指向集的变化, 从而达到减少重
复计算, 节省运行时间的效果. 近年来 Liu 等人提出的 IPA[9]和 SHARP[10]分别是上下文不敏感指针分析和上下文
敏感指针分析的最新增量化工作. 它们通过在 Andersen 风格的指针分析算法中识别指向集传播的特殊性质, 从而
减少增量分析过程中的冗余计算. 然而, 这两个工作对增量指针分析的支持仅限于少数几种指针分析, 并且都是基
于命令式语言 (Java) 开发的, 这在一定程度上限制了它们的可选择性和易用性, 特别是在需要进行面向特定场景
定制的开发时.
另一方面, Doop[11]是一个基于 Datalog 的声明式指针分析框架, 其被认为是在过去 10 年内最主流的指针分析框
架, 被用于实现和比较新提出的指针分析算法 [12,13]. Doop 会先将待分析程序转换为输入事实, 然后将这些输入事实和
相应的分析规则传递给 Datalog 引擎进行计算, 最后将 Datalog 引擎的输出作为分析结果. Doop 提供了一组优雅的规

沈天琪 等: DDoop: 基于差分式 Datalog 求解的增量指针分析框架

3

则来处理不同的分析设置, 如上下文敏感性、反射处理等, 目前在 Doop 中已经实现了约 50 种不同的指针分析算法.
然而, 目前的 Doop 框架中仅提供了全程序指针分析算法, 并不支持增量指针分析算法. 此外, 从分析算法的角度来提
供增量支持的方式需要对现有的指针分析算法及实现进行大规模重构, 增加了分析算法的开发和维护成本.
本文设计并实现了一种新的增量指针分析框架 DDoop (Differential Doop), 旨在为 CI/CD 流程提供高效的增
量指针分析, 并充分兼容和复用 Doop 中提供的大量指针分析实现. 我们的核心洞察是将增量指针分析委托给支
持增量评估机制的 Datalog 引擎, 而无需修改现有的指针分析算法和实现. 在 Datalog 中, 增量评估是一种优化技
术, 用于在现有计算结果的基础上, 仅对新添加或修改的数据进行重新计算, 以减少重复计算和提高查询效率. 我
们观察到, 代码的变更在 Doop 中实际上可以映射到输入事实的变更, 其可作为增量 Datalog 引擎的输入, 经增量
评估后即可得到增量输出结果 (即增量分析结果), 而无需对分析规则进行任何修改.
我们的 DDoop 框架采用了基于差分数据流 (differential dataflow, DDF) 的 DDlog 引擎 [14], 这是一种能够支持
高效的增量评估的 Datalog 引擎. Doop 框架目前所使用的 Datalog 引擎 Soufflé [15]并不支持增量评估机制. Zhao 等
人 [16]尝试为 Soufflé 提供增量评估的支持, 但由于 Soufflé 设计与实现机制的限制, 他们的工作只能为 Soufflé 中的
部分语法结构提供增量特性, 这种弱增量能力不足以支持 Doop 框架中的复杂指针分析规则. Ritsogianni[17]曾试图
在 Doop 框架中整合 DDlog 引擎以提供增量分析的能力, 然而其方法设计与实现过于粗糙, 仅能支持 Doop 中最
简单的分析. 此外, 根据其实验评估, 它只是在每一轮分析的时候将 Soufflé 引擎换成 DDlog 引擎, 实际运行的还
是完整评估, 并未有效的利用 DDlog 引擎的增量能力.
因此, 在我们的 DDoop 增量指针分析框架中, 主要需要解决以下两个挑战: 1) 如何高效地将代码变更转换为
输入事实的变更; 2) 如何将 Doop 中基于 Soufflé 的指针分析实现移植到支持增量评估机制的 DDlog 引擎. 为了解
决这些问题, 本文将 DDoop 增量指针分析框架设计为前后端分离的架构: 前端实现增量输入事实生成, 以获取对
应代码变更的输入事实变更; 后端通过自动化规则重写器, 将 Doop 中现有的 Soufflé 版本规则重写转换为 DDlog
版本规则. 通过这种方式, 我们的框架能够透明地兼容复用 Doop 框架中现有的指针分析规则, 而无需在算法实现
层面进行修改即可实现增量效果. 具体而言, DDoop 框架做到了如下两点: 1) 兼容 Doop 框架本身已经拥有的丰富
的、各种精度的指针分析规则; 2) 利用增量特性, 尽量减少重复计算, 以在连续的分析场景下比非增量框架的批量
模式更快地得到运行结果. 实验评估结果显示, DDoop 增量框架在我们的实验基准集上相比原始 Doop 框架可实
现平均约 5×, 最高约 36×的加速.
总而言之, 本文作出了以下主要贡献.
(1) 设计并实现了一种基于差分式 Datalog 的增量指针分析框架 DDoop, DDoop 框架主要面向代码变更频繁
的 CI/CD 场景设计. 值得注意的是, 尽管我们并未引入新的增量指针分析算法, 但我们成功地在 DDoop 框架中实
现了对 Doop 中现有的大量指针分析算法的透明增量化支持.
(2) 详细阐述了 DDoop 框架的前端和后端设计, 包括如何有效地识别和处理代码变更, 以及如何将这些变更
映射到指针集的变化, 从而减少不必要的重复计算.
(3) 通过实验评估验证了 DDoop 框架的分析效率和兼容性. 实验评估结果表明 DDoop 框架对 Doop 框架中现
有指针分析规则实现了兼容性和可复用性的最大化, 且在处理各种大小和复杂度的代码变更时, 都能实现显著的
分析加速.
本文第 1 节介绍本工作的相关背景知识. 第 2 节介绍所提出的增量指针分析框架 DDoop 的整体架构设计与
实现. 第 3 节通过实验展示 DDoop 框架的性能和兼容性. 第 4 节讨论 DDoop 框架设计与实现中的一些局限性.
第 5 节回顾相关工作. 第 6 节对本工作进行总结, 并提出未来工作展望.

1 背景知识
1.1 指针分析
在目前的大多数主流编程语言中 (如 C、C++、Java 等), 都存在指针或引用类型, 指针或引用类型变量的值

软件学报 ****年第**卷第**期

4

为运行时程序地址空间中的内存地址, 即它“指向”了内存中的某个位置 (或对象). 在 Java 这样的面向对象语言中,
指针可以是局部变量或实例字段, 而内存位置通常对应堆对象. 从指针到对象的这种指向关系被称为指针的指向
信息, 指向信息是编译优化和静态程序分析的基础. 指针分析是一种用来静态计算程序中指针 (或引用) 在运行时
的指向信息的上近似的静态程序分析技术. 经过 40 余年的发展, 指针分析已经是一个积累了大量文献 [5,18]的研究
领域, 但鉴于其基础地位和重要性, 指针分析仍然是静态程序分析学术研究的重点.
在本节我们主要介绍针对 Java 语言的指针分析. 形式化地说, 我们可以将 Java 指针分析视为一个映射集为
pt : (V ∪ H × F) → P(H) , 其中 V 代表待分析程序中的局部变量集合, F 代表待分析程序中的实例字段集合, H 代表

待分析程序中的抽象堆对象集合. 在 Java 程序中, 由于存在循环和递归, 因此在执行过程中可以无限地创建堆对
象. 为了可判定性和可扩展性, 指针分析必须使用堆抽象, 将无限大小的堆划分为有限数量的抽象堆对象. 指针分
析 pt 将一个局部变量 x ∈ V 或一个实例字段 (oi , f ) ∈ H × F 映射到一个抽象堆对象集合 s ∈ P(H) , 这个抽象堆对象
集合即指针分析结果, 被称为指针的指向集.
上述形式化定义的 Java 指针分析是上下文不敏感指针分析. 上下文不敏感指针分析的定义和实现相对简单,
并且针对大规模程序的可扩展性更好. 然而上下文不敏感指针分析未对程序中的方法调用上下文进行建模, 在分
析过程中会合并不同过程间动态执行路径上的行为, 导致较大的分析精度损失. 实践证明, 上下文敏感性对于提
高 Java 程序指针分析精度很有用. 上下文敏感指针分析对不同调用上下文下的相同方法和堆对象分别进行分析,
从而区分不同过程间动态执行路径上的行为. 理论上, 调用上下文可以概括为与调用点的控制流相关的程序状态
的某种抽象. 因此, 可以通过使用的不同上下文元素来区分上下文, 得到上下文敏感性的不同变体. 对于 Java 等面
向对象语言, 通常使用 3 种上下文敏感性, 即调用点敏感性、对象敏感性以及类型敏感性, 它们的上下文元素分别
是调用点、抽象堆对象 (分配点) 和类型.
上下文敏感指针分析同样可以形式化定义为一个从指针到指向集的映射. 其与上下文不敏感指针分析的定义
的不同之处在与其中的指针和抽象堆对象都是带上下文限定的. 具体来说, 上下文敏感指针分析是一个映射
cpt : (C × V ∪ HC × H × F) → P(HC × H) , 其中, C 代表方法上下文, 而 HC 代表堆上下文. 上下文敏感指针分析 cpt

会将一个带上下文限定的局部变量 (c, x) ∈ C × V 或实例字段 (hc, oi , f ) ∈ HC × H × F 映射到一个带上下文限定的抽
象堆对象集合 cs ∈ P(HC × H) , 即上下文敏感指向集.
除上下文敏感性外, 指针分析中还有其他几个影响精度的维度: 流敏感性、字段敏感性、堆模型等. 一般来
说, 某个维度不敏感必然牺牲算法的一部分精度, 而某个维度敏感必然增加算法复杂度, 降低算法实现的效率. 指
针分析当前研究工作面临的主要问题是, 在保证算法精度的同时平衡时间/空间上的消耗, 设计并实现可扩展的、
高精度的指针分析算法.
1.2 Datalog
Datalog 是一种基于逻辑编程的声明式查询语言, 其在语法上是 Prolog 的一个子集. Datalog 被广泛用于处理
结构化数据, 并在数据库管理、人工智能、声明式网络和程序分析等领域中发挥着重要作用. 这些领域使用
Datalog 语言作为查询图和关系结构以及实现迭代和递归的声明式抽象, Datalog 提供了一个声明式接口, 允许程
序员专注于任务 (what to do) 而非低级细节 (how to do).
Datalog 语言的语法非常简洁. 一个 Datalog 程序由一组 Datalog 规则组成, 规则包含规则头和规则体两部分.
规则格式如公式 (1) 所示.
A :- B1 , B2 , . . . , Bn

(1)

其中, A 和 Bi 表示 Datalog 中的谓词, A 是这条规则的规则头, 而 B1, B2,…, Bn 的合取构成规则体. Datalog 的逻辑
基础源自一阶谓词逻辑, 每条规则都是一个蕴含式, 例如公式 (1) 对应的蕴含式为 B1 ∧ B2 ∧ . . . ∧ Bn → A . 一个谓词
可以带有参数, 如 P(e1 , e2 , . . . , ek ) 表示一个元谓词. 谓词的实例元组被称为事实.
reachable(X, Y) :- edge(X, Y)
reachable(X, Y) :- edge(X, Z), reachable(Z, Y)

(2)

沈天琪 等: DDoop: 基于差分式 Datalog 求解的增量指针分析框架

5

公式 (2) 中展示了一个实例 Datalog 程序, 其中包含 2 条用于计算图中的可达性信息的规则. 谓词 edge(X, Y)
表示图中从节点 X 到节点 Y 存在一条边, 谓词 reachable(X, Y) 表示图中从节点 X 到节点 Y 存在一条可达路径. 第
1 条规则表示若 X 和 Y 间存在一条边, 则 Y 从 X 可达; 第 2 条规则表示若从 Z 到 X 可达且 Z 和 Y 间存在一条边,
则 Y 从 X 可达.
给定一个 Datalog 程序以及一组输入事实, 经 Datalog 引擎评估后得到一组输出事实. Datalog 引擎通常使用
自底向上的评估模型, 它重复应用规则, 直到不再有新的事实产生或达到终止条件. 为了在计算的时候尽量避免重
复之前迭代中已经完成的结果, 提出了一种名为半朴素法 (semi-naive) 的更优的自底向上评估方法. 此外, 在实际
应用中, Datalog 常常需要处理大规模的数据集和复杂的查询, 涉及到输入事实的变化. 为了提高 Datalog 的性能,
引入了增量评估技术. 基于差分数据流的增量评估是一种优化技术, 它通过跟踪输入数据的变化, 并更新相应的输
出结果, 从而避免重复计算和减少计算开销. 这种增量评估技术在处理大规模数据和持续查询时非常有效, 显著提
高了 Datalog 的查询性能和效率.
如今, Datalog 已经发展成为一类语言规范: 由于 Datalog 在不同应用场景中的发展和拓展, 出现了一些不同
的 Datalog 方言. 不同的 Datalog 方言加入了为了增强语言表现力或方便程序员开发而设计的拓展语言特性. 例如
bddbddb [19] 、Soufflé [15] 、DDlog [14] 等. 尽管这些方言间并不能直接互通, 它们都可以被归类为“Datalog 语言”.
Datalog 方言的实现被称为 Datalog 引擎.
最后介绍 Datalog 语言在程序分析领域中的应用. 最初, 静态分析研究人员因为使用传统的命令式语言实现
具体的程序分析算法而感到困扰, 因为这需要开发人员具备强大的工程能力. 大量的开发时间并未用于实现算法,
而是用于处理各种细节性的工程问题, 例如极其复杂的数据结构设计、内存管理、程序并行化过程中的信号量同
步、死锁问题的解决、异常处理、边界条件检查甚至性能调优等.
声明式的 Datalog 语言在这方面有其突出的优势: 由于声明式语言的抽象特性, 这类工程问题可以委托给
Datalog 引擎处理, 让开发人员可以专注于算法逻辑, 无需过多关心底层实现细节. Datalog 语言在指针分析领域的
应用可以追溯到 2006 年的 bddbddb[19]. Doop 是当前最先进的 Java 指针分析框架, 其同样采用了基于 Datalog 的
声明式分析范式. Doop 将指针分析算法表述为 Datalog 规则, 并实现了一个从 Java 程序中抽取语义信息并转换
为 Datalog 引擎可接受的输入事实格式的前端. 通过这种方式, Doop 框架成功地将指针分析从指针赋值图上的一
个图计算问题转换为了 Datalog 引擎上的一个数据查询问题.

2 DDoop 框架
本节我们主要介绍针对 Java 语言的增量指针分析框架 DDoop 的架构设计, 并介绍实现中的相关细节. 在现
有非增量的 Doop 指针分析框架的基础上, 我们在 DDoop 框架中提出了增量输入事实生成技术和 DDlog 增量分
析程序自动生成技术, 以兼容复用 Doop 框架中已有的大量指针分析实现, 并提供高效的透明增量化支持. 图 1 展
示了 DDoop 增量指针分析框架的整体架构.
前端
待分析程序

增量事实生成器

增量事实

增量规则重写器

增量分析程序

上一轮
分析结果

后端
分析选项

Soufflé 规则

DDlog 规则 DDlog 编译器

图1

框架整体架构

分析结果

软件学报 ****年第**卷第**期

6

从整体架构上, DDoop 框架主要可分为前端和后端两个部分, 这两个部分都拥有增量特性.
(1) 增量生成输入事实的前端. 其自动化处理待分析程序, 从中抽取变更信息并转化为后端增量评估所需的增
量输入.
(2) 自动化生成 DDlog 增量分析程序的后端. 根据输入的分析选项, 后端会将原始 Doop 框架的 Soufflé 规则
自动化重写为支持增量评估的 DDlog 规则, 并生成相应的增量分析程序.
首先, 我们对图 1 所示的 DDoop 框架的整体运行流程概述如下: 根据分析输入, 我们可以确定分析使用的
Soufflé 指针分析规则, DDoop 后端会将这份分析规则自动化重写为 DDlog 版本的增量分析程序. 随后, DDoop 前
端会收到待分析程序对应的一系列代码提交, 其将会按照顺序, 将代码变更转化为 DDlog 增量分析程序可接受的
增量输入事实形式. 增量分析程序接收到代码变更的增量输入事实后, 增量评估产生分析结果的增量输出.
接下来, 我们将详细介绍 DDoop 框架前端和后端这两个部分.
2.1 前端: 增量输入事实构建
DDoop 框架和 Doop 框架都将指针分析问题归约为 Datalog 程序评估问题. Datalog 引擎接受一组事实和规则
作为输入. 在 Doop 框架中, 规则对应指针分析算法, 而输入事实则对应待分析程序的一组语义性质. 为此, 我们需
要一个前端来对待分析程序进行预处理, 从中抽取与指针分析相关的程序语义性质作为 Datalog 引擎的输入事实.
前端的存在是为了实现分析框架的端到端过程: 将不能直接被 Datalog 引擎处理的 jar 包转换为其可接受的输入
事实形式.
Doop 框架中已经提供了一个前端, 能够将一个完整的 Java 程序 (jar 包) 转换为 Soufflé 引擎的输入事实. 其
前端的输入生成器抽取语义信息时不涉及类之间的相互依赖信息. 它使用 Soot 框架加载 jar 包中所有的类字节码
文件, 然后对已加载的每个类并行化的抽取其中的语义信息. 在具体处理时, 每个类中的方法在逻辑上都是独立
的. 但是 Doop 中的前端在我们的增量场景下存在一些问题.
● 第 1 个问题是, Soufflé 引擎和 DDlog 引擎的输入在形式上存在细微差异: DDlog 引擎是为增量评估设计的,
输入时需要标记每条输入事实是插入还是删除 (特别地, 某一条事实在增量过程中的更新会被处理为一条删除旧
值和一条插入新值的组合); 而 Soufflé 引擎将所有输入事实均视为插入.
● 第 2 个问题是, Doop 框架的前端并未考虑到增量设计: 对于待分析程序的每一个版本, 其都会从头开始处
理整个待分析程序. 但在连续输入的增量场景中, 绝大多数的 Java 类文件都是不变的, 对应的语义信息是完全相
同的. 这就导致了重复计算问题.
解决第 1 个问题相对容易, 这只是格式上的区别. 我们只需要: 1) 标记每条输入事实是插入还是删除; 2) 格式
转换. 然而, 对第 2 个问题, 我们需要一个支持增量事实生成的全新前端. 这也是我们在此处的最主要挑战.
框架前端架构

2.1.1

在我们的增量指针分析框架中, 我们希望避免重复计算: 即对于那些在代码变更中未发生变化的类和方法, 我
们可以复用此前生成的输入事实. 为此, 对于给定两个版本的 jar 包, 我们需要一种方案来识别出在代码变更中未
发生变化的类文件, 并在处理时跳过它们. 为此, 我们需要解决以下 3 个主要问题.
(1) 划分合适的基本“增量单元”, 其粒度必须适当. 在实现增量分析时, 选择适当的“增量单元”是至关重要的.
该单元应能在保证分析精度的同时, 最大程度地降低实现和计算的复杂度. 我们需要确定, 是将方法、类还是编译
模块视为“增量单元”. 增量单元越大, 每一次出现变更时需要重做的范围就会越大, 但是增量单元越小, 用来判断
增量的范围就相对更加复杂.
(2) 找出能够标识增量单元未发生变化的元素, 或者说一种“摘要”. 为了实现快速的增量变更检测, 我们需要
一种能够快速且准确地反映“增量单元”变化的“摘要”机制. 这种机制可以采用“增量单元”哈希值, 或者是结合“增
量单元”的某些性质, 如代码行数, 方法数量等. 选择哪种“摘要”机制取决于它们能否提供足够的信息来判断代码
的变化, 同时又能在短时间内完成计算.
(3) 维护我们在增量过程中的必要信息. 在增量分析过程中, 有效地管理和存储分析信息是一个挑战. 我们需

沈天琪 等: DDoop: 基于差分式 Datalog 求解的增量指针分析框架

7

要跟踪哪些“增量单元”已经被处理, 哪些还未被处理. 此外, 我们可能需要保存一些中间结果, 以便在后续过程中
使用. 因此, 我们需要一个高效、可靠且易于操作的信息管理的方式, 以支持复杂的查询和更新操作.
针对上述问题, 我们设计了一个高效且精确的、支持增量输入事实生成的前端. DDoop 的前端架构如图 2 所
示. 框架前端中分别给出了如下解决方案: 基于实证研究的增量单元粒度选择与确定, 基于程序性质与哈希的增量
单元摘要设计, 基于内存数据库的增量信息高效存储与管理. 在第 2.1.2–2.1.4 节, 我们将对各部分进行详细介绍,
而在第 2.1.5 节, 我们将对一个实现中的优化进行介绍.

待分析程序

类文件

类过滤器

变更类

增量事实

方法过滤器

变更方法

事实生成器

查询类摘要

查询方法摘要
Redis

查询需删除事实、维护增量信息

图2

框架前端架构

增量单元粒度选择

2.1.2

确定合适的增量单元粒度是解决增量问题的关键之一. 在 Java 增量分析中, 增量粒度可以有多种选择, 例如
编译单元级、类级、方法级、语句级甚至更加精细的表达式级. 在 Doop 框架中, 我们注意到, 它天生就在类级并
行, 因此可以直接排除粒度更粗的编译单元级增量粒度. 此外, 尽管 Doop 框架的并行处理粒度在类级别, 但其处
理逻辑中方法与方法之间仍然是独立的, 所以从实现的角度来说, 比方法级别更加精细的增量单元可能会引入相
当规模的强耦合关系, 不能做到增量单元间的高效并行.
因此, 在 Doop 框架设置下, 我们主要关注类级和方法级两种增量单元. 为了确定哪一种增量单元更加适合我
们的场景, 我们在一组基准项目上开展了关于增量单元粒度选择的实证研究, 通过简单的快速实验, 对比不同粒度
的增量单元对于增量输入事实生成的效率的影响.
我们在 DaCapo 基准 (如 xalan、Jedis、PMD 等) 和一些真实世界大型项目 (如 ErrorProne、ZooKeeper 等) 上
开展了实证研究. 我们发现, 对运行时 Soot 对象进行简单的哈希在多次运行中并不能保证幂等性, 因此我们选择
将其中的 SootMethod 对象转化为中间表示 (intermediate representation, IR). 我们认为, 如果 IR 表示不同, 则此方
法发生了变化 (关于增量单元摘要的设计细节, 见第 2.1.3 节). 在我们的两级摘要设计过程中, 我们首先会过滤出
那些内容存在变化的类, 之后才会对这些类中的每一个方法计算摘要以进一步缩小增量计算范围. 那么, 这一策略
要相较于基于类层级的增量方案节省时间, 就要求在变化过程中平均每一个类变化的方法数量相对有限. 不妨假
设我们发生变化的类中平均一共有 n 个方法, 其中平均有 x 个方法发生变化, 记单个类事实生成时间为 a, 生成摘
要信息为 b, 要让我们的二级摘要方案时间消耗更少, 即需要满足如下约束: a · n ⩾ b · n + a · x . 实证研究结果显示,
对每个方法做从 SootMethod 对象到摘要的转化操作平均需要 0.09 ms, 而为每个方法生成相应的事实平均需要
0.25 ms. 通过前述不等式约束可以得出结论: 类中少于 64% 的方法变化时我们的二级摘要可以节约时间.
根据实证研究的实验结果以及上述结论, 我们发现方法级别的增量单元能够更有效地处理这个问题, 因此我
们选择以方法作为 DDoop 前端的基本增量单元粒度.
增量单元摘要设计

2.1.3

为了标识一个增量单元在代码变更中是否发生变化, 我们需要为增量单元设计一种摘要机制以表征这种变
化. 我们设计了一种高效的两级摘要方案, 旨在平衡性能和增量粒度. 具体而言, 我们首先使用类的字节码文件对

软件学报 ****年第**卷第**期

8

应的字符数组的哈希值加长度作为一个类的标识. 一旦这个标识发生变化, 就说明类中存在改变, 需要进入下一级
摘要. 在第 2 级摘要中, 我们会对这个类在方法级别进行匹配. 我们首先将 SootMethod 对象转化为方法的 IR 表
示, 并计算哈希作为摘要. 在这样的摘要设计下, 如果一个方法的源代码发生了任何有意义的变化, 这种变化都会
反映到字节码层面的摘要上.
我们以图 3 中的一次代码变更为例, 对我们的两级摘要方案的设计进行说明. 以上的代码中, 旧版本包含了
A1, B, C1 这 3 个类, 新版本中则包含了 A1, B, C2 这 3 个类, 同时 A1 类中的方法在此次变更中也产生了变化. 为
了方便叙述, 我们在此处忽略它们依赖的库以及其中没有显式定义的任何其他方法. 在第 1 级摘要中, 我们会使
用 A1.class, B.class, C2.class 这 3 个字节码文件对应的字节码数组的摘要 (hash+长度) 对类进行标识, 于是就可以
识别出不变的类 B, 变化的类 A1, 删除的类 C1, 新增的类 C2. 针对 A1, 它会进入第 2 级摘要, 我们会对其中的方
法 f(), f2(), method() 生成对应的 IR 摘要, 并且和之前旧 A1 类对应摘要进行对比. 我们就可以确定 A1 中新增了
f(), 删除了 f1(), f2() 发生变化以及 method() 保持不变. 对于类的字段信息、修饰符信息, 当类发生变化时, 我们会
默认这些信息发生了改变, 进行增量处理.
1

public class B {}

2

- private class C1 {}

3

+ private class C2 {}

4

- private class A1 {

5

-

B b;

6

-

void f1(){}

7

-

void f2(){}

8

+

void f(){}

9

+

void f2(){this.b =this.b2;}

10

-

void method() {}

11

- }

图3

Java 代码变更示例

增量信息存储与管理

2.1.4

在增量输入事实生成的过程中, 需要记录一些必要的增量信息. 其中最主要的内容包括前文提及的增量单元
的信息以及对应生成的 facts 组. 我们的框架首先基于上一轮增量生成中维护的当前程序“活跃”增量单元对应的
增量信息确定本轮增量生成中哪些增量单元需要重新进行处理. 对于那些本轮不再出现的增量单元, 我们的框架
将会将它们对应的 facts 从缓存中取出标记为删除; 对于本轮新出现或者发生变更的增量单元, 则将维护它们的相
关信息, 并将新的 facts 写入缓存. 通过这样的机制, 缓存系统独立记录了增量前端当前的状态, 提供了下一轮增量
分析所需的必要信息.
为此, 我们需要设计一种缓存方案来维护这些增量信息. 我们考虑了几种常见的缓存方案: 基于文件系统的缓
存、基于磁盘数据库的缓存、基于内存数据库的缓存以及前端程序在内存内直接管理缓存的方案.
从实现方面来看, 数据库方案相对简单, 开发者只需定义数据结构, 而无须关心具体如何安排这些数据, 如何
做增删改查等这些细节问题. 同时内存数据库在此场景下的性能远高于磁盘数据库. 综合考虑效率和开发效率, 我
们最终决定选择使用成熟的内存数据库 Redis 作为我们存储和管理增量信息的方式.
从软件工程方面来看, 在现代软件架构中, 应用通常会在分布式节点上部署, Redis 正适合这样的场景. 在这样
的架构中, 节点间的信息交换需要通过网络栈进行, 频繁的 I/O 操作可能会成为系统中最耗时的部分. 我们通过按
需读写的策略优化了 I/O 粒度和读写频率: 我们的系统会首先基于一份相对较小的摘要信息确定哪些类真正发生
了变化. 对于那些未发生变化的增量单元对应的 facts, 我们完全不会在程序运行过程中读写它们. 通过这种优化,
我们的设计能够在高度分布式的环境中同样有效地降低 I/O 开销, 提高系统性能.

沈天琪 等: DDoop: 基于差分式 Datalog 求解的增量指针分析框架

9

增量前端剪枝优化

2.1.5

通过增量单元的设计, 我们已经大幅减少了需要重新抽取输入事实的类和方法数量. 然而在实现过程中, 由于
框架使用的 Soot 框架的类加载机制, 为了获取一个类的所有依赖, 需要加载大量的类. 不仅包括输入的 jar 包中的
所有类, 还包括这些类可能引用的系统类库中的类. 这个过程是迭代的, 直到无法继续找到传递依赖的类为止. 这
样的类加载策略是为了尝试对输入的待分析程序进行一个尽可能精确的建模. 在 DDoop 框架的早期实现中, 我们
采用了这种类加载策略, 以保证与原始 Doop 框架有等价的指针分析精度. 然而, 这种实现相对低效.
在我们关注的变更频繁的增量场景下, 一次代码提交引入的变化通常是局部的, 不会导致整个程序引用的基
础类库发生巨大的变化. 因此, 全面类加载中也包含重复计算. 因此, 我们提出了对类加载过程的剪枝优化策略, 通
过减少需要加载的类数量来提升前端增量输入事实生成的速度.
图 4 展示了我们所采用的剪枝策略. 在增量输入事实生成的类加载过程中, 我们不再加载间接引用的类库中
的类. 也即在加载类时, 我们只会包括应用类和被应用类直接引用的类库中的类. 然而, 我们的剪枝策略可能会对
精度产生影响. 我们将在第 3.4 节和第 4.2 节分别从实验和理论层面就剪枝策略对精度的影响进行讨论.
lib classes
.......
class

class

class

class

剪枝

class

APP classes
class

class

图4

class

class

类加载过程剪枝策略示意图

2.2 后端: 增量分析程序生成
Datalog 是 Prolog 的一个子集, 它是一种更为限制和精简的逻辑编程语言. 现代 Datalog 引擎都遵循了 Datalog
语言的理论规范, 但为了增强表现能力都对语言进行了拓展. 然而不同引擎之间的拓展并不完全相同, 并且目前也
不存在通用的标准语法. 因此, 不同的 Datalog 引擎之间一般并不能直接互通.
Soufflé 和 DDlog 是当前流行的两种 Datalog 引擎. Soufflé 是 Doop 框架目前所采用的 Datalog 引擎, 而 DDlog
支持高效的增量评估. 如果将 Doop 中面向 Soufflé 引擎实现的分析规则直接提交给 DDlog 编译器, 因为语法细节
层面的细微变化, 是无法通过编译的.
这意味着, 切换具体的 Datalog 引擎需要对所有已经有的 Datalog 规则进行语义等价的重写, 这无疑是一项艰
巨的任务. Doop 框架在发展过程中就有过一次切换 Datalog 引擎的经历, 它从对学术使用有相当限制的 LogicBlox
引擎切换到了当时新发布的、专为程序分析开发的 Soufflé 引擎. 开发者为此耗费了大量精力 (大约 10 人月)[20]完
全重写了指针分析的规则库, 将 LogicBlox 引擎所用的 LogiQL 语言切换到了 Soufflé 语言.
在我们框架的设计目标中, 我们需要兼容 Doop 框架已有的丰富的 Soufflé 指针分析规则. 这就意味着我们需
要对 Doop 框架中已有的 Soufflé 指针分析规则进行完全的重写. 然而, 这其中存在两项主要的挑战, 使得像 Doop
开发者曾经做过的那样进行手动转换在软件工程领域变得不可接受.
(1) Doop 框架利用 Soufflé 语言的模块化特性动态地根据具体的分析选项情况从规则库中“装配”生成新的规
则, 而 DDlog 语言中没有显式的模块化语法. 这就意味着, 对于 Doop 中的数十种精度的分析及其对应的大量的分

软件学报 ****年第**卷第**期

10

析选项, 我们可能需要准备数千个对应不同情况的 DDlog 规则文件. 即使这种手动转换可以实现, 但是它会带来
巨大的工作量, 并导致规则库代码的严重膨胀. 更糟糕的是, 这种设计将会给后期对这些规则的维护带来巨大的
困难.
(2) 随着 Doop 框架的流行, 有相当一部分场景性的程序分析算法和新的指针分析算法实际上就是在 Doop 框
架所提供的指针分析规则基础之上开发的. 从兼容性出发, 我们需要尽力支持这种类型的潜在代码. 然而, 如果我
们使用的是手动转换, 我们将无法做到这一点. 因 Datalog 引擎切换导致的兼容性破坏的一个例子是 Zipper[12],
Zipper 是由 Li 等人提出的一种新的选择性上下文敏感指针分析算法, 在基于 LogicBlox 引擎的旧版本 Doop 中实
现, 目前由于当前版本的 Doop 手动切换到了 Soufflé 引擎, 导致 Zipper 在当前版本的 Doop 中不可用 [21].
面向这两项挑战, 我们设计了一个自动化的 Datalog 规则重写器. 这个自动化规则重写器是我们框架后端的
核心, 其设计目标是能够处理 Doop 框架内任何分析选项的组合产生的 Soufflé 指针分析规则. 通过规则重写器,
我们可以全自动化地将这些分析规则重写为 DDlog 引擎的格式, 而无需进行繁琐而容易出错的手工转换. 同时,
这个自动化重写器也使我们可以对基于 Doop 框架开发的其他分析进行兼容.
框架后端架构

2.2.1

我们框架的后端架构如图 5 所示. 其流程基于 Doop 框架的规则生成部分. 首先, 我们将分析选项输入 Doop
框架, 以获得对应的 Soufflé 规则文件. 然后, 这个规则文件会被输入到我们的自动化规则重写器中, 转换为语义等
价的 DDlog 规则. 最后, DDlog 工具链会将这些规则编译成支持增量评估的 DDlog 程序, 以响应前端生成的增量
输入事实.
模块系统

规则拓展

语法结构

分析选项

增量分析程序
增量规则重写器

规则库

Soufflé 规则

DDlog 规则

图5

DDlog 编译器

框架后端架构

如前文所述, Soufflé 和 DDlog 是两种不同的 Datalog 方言, 它们在语法和拓展上都存在一系列的差异. 这些差
异主要可以分为 3 类.
(1) 第 1 类是语法结构差异. 在 Soufflé 和 DDlog 中很多语言结构的语义是相同的, 但是在具体的语法定义上
存在细微差异, 例如类型系统的命名、变量命名规范等. 另外, 在 Soufflé 还存在一些仅用于辅助编译的注解等.
(2) 第 2 类是规则拓展差异. Soufflé 对 Datalog 规则的拓展比 DDlog 更为激进, 例如 Soufflé 规则中引入了析
取元素, 并允许输入关系的可变性.
(3) 第 3 类是模块系统差异. Soufflé 拥有组件结构, 这是一个相对完善的模块系统, 而 DDlog 中则不存在对应
的结构. 正因为有了这个组件系统, Doop 框架可以根据分析选项通过条件编译的方式组装出我们需要的规则.
因此, 我们的重写过程需要在这两种方言之间建立一个映射, 以克服两者之间的语法和拓展差异. 接下来, 我
们将分别针对这 3 类差异进行详细的介绍, 并提出相应的解决方案.
语法结构重写

2.2.2

根据语法结构的语义性质, 我们可以将语法结构的处理方式进一步细分为两种策略: “重写”和“忽略”.
“重写”策略主要针对那些在规则层面有实际意义的语法结构, 如类型系统的差异、文法格式的区别, 以及变
量命名规范的不同等. 处理这类差异的基本策略是对抽象语法树 (AST) 的子树进行模式转换. 例如, 参考图 6 中
Soufflé 参考代码的第 1 行和 DDlog 参考代码中的第 5 行中的类型定义, 可以发现这两种语言在类型定义上的差

沈天琪 等: DDoop: 基于差分式 Datalog 求解的增量指针分析框架

11

异: 首先是类型定义语法上的轻微差异, 例如 Soufflé 中的“.type”与 DDlog 中的“typedef”; 其次则是类型系统上的
差异, 例如 Soufflé 中的“symbol”表示所有的字符串类型而 DDlog 中对应的类型名为“Tsymbol” (“IString”的别名).
这些差异都可以通过在 AST 的结构上进行对应的转换来简单地解决. 同样的处理方案也适用于变量命名风格、
内置函数命名、输入规则是否不变等方面的差异.
1

.type node = symbol

1

import fp

2

.decl node_ord(a:node,b:number)

2

import intern

3

.output node_ord

3

import souffle_lib

4

.comp Graph{

4

import souffle_types

5

.decl edge(a:node,b:node)

5

typedef node = Tsymbol

6

.input edge

6

relation graph_edge(a:node, b:node)

7

}

7

input relation

8

.init

8

graph_edge(a, b) : - graph_edge_shadow(a, b).

9

output relation

graph = Graph

9

graph_edge_shadow(a:node, b:node)

node_ord(a:node, b:Tnumber)

10

node_ord(?a, ?a_ord) : -

10

node_ord(a, a_ord) : -

11

(?a_ord = ord(?a), graph.edge(?a, _));

11

var a_ord = ord(a), graph_edge(_, a).

12

(?a_ord = ord(?a), graph.edge(_, ?a)).

12

node_ord(a, a_ord) : -

13

.plan 1:(2,1), 2:(1,2)

13

var a_ord = ord(a), graph_edge(a, _).

(a) Soufflé 参考代码

图6

(b) DDlog 参考代码

Soufflé 和 DDlog 语义等价的 Datalog 代码示例

“忽略”策略主要用于处理作为编译辅助内容的语法结构, 这些元素在 DDlog 中并没有对应的表示. 例如, 图 6
中 Soufflé 参考代码第 13 行的. plan 语句就是一种指导规则子句重排策略的语法结构. 在“忽略”策略下, 这类与
Soufflé 特定的语句将被我们的重写器直接忽略.
规则拓展重写

2.2.3

相比 DDlog 引擎, Soufflé 引擎在规则的拓展层面拥有更加强大的支持. 例如, 它可以支持 Datalog 语言不支持
的析取操作; 它也可以支持否定规则子句的无条件前置; 此外, 它还拥有一个优秀的规则子句重排器, 不仅可以推
断出对应规则可能的最优半朴素评估顺序, 还可以自动消除评估中的不确定规则.
对于这些与语义密切相关的差异, 不能通过单纯的 AST 结构重写来解决. 不过幸运的是, Datalog 规则的表现
力保证了这些 Soufflé 引擎的拓展在 DDlog 中存在一种语义等价的语法形式.
第一, 针对析取. 虽然 DDlog 中没有直接表示析取的语法结构, 但是通过并列两条规则的方式, DDlog 可以表
示这个语义. 以下是一个 Soufflé 中简单的例子.
node_ord(?a, ?a_ord):(?a_ord = ord(a), graph.edge(?a, _));
(?a_ord = ord(a), graph.edge(_, ?a)).
这个规则表示 node_ord 关系中的内容需要增加来自于两个推导的结果的并集, 中间通过表示析取的“;”连接.
遵循 Datalog 规则, 我们也可以将它重写为不包含“;”的等价形式如下.
node_ord(?a, ?a_ord) :- ?a_ord = ord(?a), graph.edge(?a, _).
node_ord(?a, ?a_ord) :- ?a_ord = ord(?a), graph.edge(_, ?a).
在图 5 的例子中, 这条规则出现在 Soufflé 参考代码中第 10–12 行, 和转换后 DDlog 参考代码中第 10–13 行
的两条规则相比, 我们还可以看到规则子句的重排, 有关于这部分的重写, 我们稍后就会谈到它.
第二, 针对函数类或带否定规则子句的重排. 在处理此类规则子句时, Soufflé 和 DDlog 对子句顺序的限制有

软件学报 ****年第**卷第**期

12

所不同, DDlog 中要求规则子句中对变量的使用不能出现在能确定它们的规则子句之前, 而 Soufflé 对此限制更宽
松, 允许此类规则的存在. 考虑如下以 Soufflé 语法写成的示例规则.
node_ord(?a, ?a_ord) :- ?a_ord = ord(?a), graph.edge(_, ?a).
这条示例规则表示 node_ord 关系需要增加来自于 graph.edge 关系中的元素?a 在被 ord 函数处理后的返回值.
其中 ord 函数可以对某一个确定性的字符串输入?a 返回一个运行中的唯一对应值. 对于 DDlog 的编译器, 在处理
规则的第 1 条子句?a_ord = ord(?a) 时, 它没能找到?a 的定义, 因而编译失败.
为了解决这个问题, 我们采用了规则重写方案. 通过分析规则子句中的 def-use 关系, 我们可以产生规则子句
的一个重排版本. 新的规则子句确保了每个变量在被使用之前都已经被确定. 重排后的规则如下.
node_ord(?a, ?a_ord) :- graph.edge(_, ?a), ?a_ord = ord(?a).
模块系统重写

2.2.4

组件系统是 Soufflé 在 Datalog 上为增强模块化而带来的一个语法糖, 而 DDlog 目前尚未支持模块化. 但在编
译过程中, Soufflé 编译器会在编译期首先进行解糖: 将模块化的规则进行“展开”. 因此, 我们的重写器针对模块化
的重写策略与之类似, 具体描述于算法 1 中.
算法 1. Soufflé 到 DDlog 的规则重写算法.
输入: A Soufflé program Ps;
输出: A DDlog program PD.
1. for each node v in AST(Ps) do
2.

if v is a component declaration then

3.

oc = new Component // 包含组件名称、类型参数列表、继承关系及组件体

4.

for each node u in AST(oc) do

5.

if u is an .override directive then
Mark relations that oc overrides // 被标记的关系不会输出到 PD 中

6.
7.

end if

8.

end for

9.

end if

10. end for
11. for each node v in AST(Ps) do
12.
13.

if v is a component instantiation of oc then
oc.instantiate() // 调用 Component 对象的 initantiate 方法
// 将组件的类型参数实例化为具体类型
// 实例化所有父组件并合并到当前组件实例中

14.

end if

15. end for
16. for each node v in AST(Ps) do
17.
18.
19.
20.

if v is a declaration in a component body then
vD ＝rewrite(v) // 将声明以 DDlog 语法重写
add vD to PD // 将重写后的声明添加到目标 DDlog 程序中
end if

21. end for
22. return PD

沈天琪 等: DDoop: 基于差分式 Datalog 求解的增量指针分析框架

13

针对算法 1 简要介绍如下.
1) 程序首先遍历 Soufflé 程序的抽象语法树, 并在遇到组件声明时创建一个 Component 对象.
2) 在第 2 次扫描的时候, 当程序遇到组件实例化时, 它会调用 Component 对象的 instantiate 方法. 这个方法负
责将组件的类型参数替换为实例化时提供的具体类型参数. 同时, 它还会处理组件的继承关系, 实例化所有父组
件, 并将它们的内容合并到当前组件实例中.
3) 在实例化过程中, 程序会处理.override 指令, 用于标记当前组件覆盖了哪些关系.
4) 最后, 程序将遍历组件体中的声明 (如关系、规则等), 并将它们重写为 DDlog 语法.
以 Soufflé 参考代码中的不包含继承关系的简单组件为例, 我们在扫描到.init graph = Graph 命令的时候, 我们
就会开始进行实例化, 由于这里没有.override 指令, 所以对 edge 的定义会被处理, 并且被实例化为 graph_edge, 之
后就是按照此前讨论的语法结构重写规则进行重写 (第 2.2.2 节).

3 实验评估
为了展示我们的 DDoop 增量指针分析框架在代码变更频繁的场景下的性能和对 Doop 框架现有指针分析规
则的兼容性, 我们在一组代码变更上对 DDoop 框架进行了实证评估, 并与原始 Doop 框架及当前尝试为 Doop 框
架添加增量支持的现有工作进行了实验对比. 我们的实验评估主要试图回答以下 3 个研究问题.
• RQ1: 我们的增量 DDoop 框架与原始的非增量 Doop 框架相比性能表现如何?
• RQ2: 我们的增量 DDoop 框架对 Doop 中现有指针分析规则的兼容性如何?
• RQ3: 我们在 DDoop 前端中进行的剪枝操作对分析精度的影响如何?
3.1 实验设置
实验平台. 我们的所有实验在一台配备 Intel(R) Xeon(R) Gold 6240 @ 2.60 GHz 处理器 (75 核), 285 GB 内存,
1 TB 磁盘的 Ubuntu 20.04.6 LTS 服务器上运行. 在实验评估中我们设置最大可用线程数为 8.
基线工具. 我们在实验评估中对比的基线工具分别是原始 Doop 框架以及两种带基础增量支持的 Doop 框架.
基线工具的详细信息如下.
• Doop: 基于 Soufflé (2.0.3) 的原始 Doop 框架 (4.24.10).
• Doop-SE: 基于 Soufflé-elastic (540cf8d) 的带基础增量支持的 Doop 框架 (4.24.10). 在 Soufflé-elastic 工作中
并未提供与 Doop 框架的整合. 这里的 Doop-SE 是我们框架前端的一个修改版 (修改了增量指令格式以适配 Souffléelastic) 与 Soufflé-elastic 增量引擎的一个简单组合.
• Doop-DDlog[17]: 初步整合了 DDlog 引擎 (0.4.0) 的 Doop 框架 (4.24.10).
我们的 DDoop 增量指针分析框架是基于 Doop (4.24.10) 和 DDlog (1.2.3) 进行实现的.
指针分析精度. 在实验中, 我们选择了来自 Doop 中的多种不同精度的指针分析规则, 旨在证明我们的框架对
于现有的 Doop 中丰富的指针分析规则的高度兼容性. 具体而言, 我们选择了如下几种精度: 上下文不敏感、k 对
象敏感 (k=1 或 2)、k 调用点敏感 (k=1)、k 类型敏感 (k=1 或 2)、选择性对象敏感.
实验基准集. 我们的增量指针分析框架针对的是在变更频繁且单次变更的变化相对局部的场景. 按照这个原
则, 我们选定的实验基准都是在实际中广泛使用的 Java 项目, 并且在其 GitHub 存储库中代码变更频繁, 如表 1 所
示. 我们的实验都集中在它们的 master 分支上, 对从表 1 中记录的起始提交开始, 按照提交历史中的顺序对连续
20 次代码提交进行增量分析. 当然, 在实际的增量分析场景下我们可能会针对一次 PR 甚至一次发布进行增量分
析, 但是我们同样可以将 PR 或者发布视为一次较大的代码提交.
我们对 5 个选定的 Java 项目做简要的介绍.
1) Jedis 是 Redis 的 Java 客户端, 专为性能和易用性而设计.
2) ErrorProne 是 Java 的静态分析工具, 可在编译时捕获常见的编程错误.
3) ZooKeeper 是一个分布式协调服务, 用于管理和维护分布式系统中的配置、命名等信息.

软件学报 ****年第**卷第**期

14

4) PMD 是一个源代码分析器, 用于检测和识别 Java 代码中的潜在问题和不规范编码风格.
5) Checkstyle 是一个用于检查 Java 源代码是否符合代码标准或验证规则集的工具.
表1
Project
Jedis
ErrorProne
ZooKeeper
PMD
Checkstyle

init
5359c37
3dd2abc
5b6823a
792fe44
7007bef

#commits
2 248
6 056
2 504
26 317
12 794

freq
3 per day
1 per day
1 per week
10 per week
10 per day

实验数据集
LOC
102k
317k
184k
224k
318k

ΔLOC (avg/max)
266/2 692
104/924
122/735
151/1 720
270/2 027

Δmet (avg/max)
45/387
11/77
11/74
2/21
7/73

Δfile (avg/max)
6/27
3/21
5/31
4/33
7/23

3.2 RQ1: 性能
为了评估我们的 DDoop 增量指针分析框架在代码变更频繁的场景下的性能, 我们将 DDoop 增量框架与原
始 Doop 框架进行对比. 具体而言, 对于 20 次连续代码提交编译生成的 20 个版本 jar 包, 我们的 DDoop 增量框架
和原始 Doop 框架将会依次分析每一个版本, 并且统计对每一个版本的分析所需要的时间. 对于我们的增量分析
框架, 对第 1 个版本 jar 包的分析是全量分析, 后续的 19 个版本则是增量分析; 而对于原始 Doop 框架, 它对于 20
个版本 jar 包的处理方式完全相同, 每一次都是从头开始全量分析.
图 7 展示了 DDoop 增量框架在开启了前端剪枝优化的情况下, 与原始 Doop 框架在 7 种指针分析精度设置、
5 个基准项目上的分析耗时结果, 其中纵坐标采用了对数轴. 图 7 分别给出了 Doop 框架在 20 个版本 jar 包上的平
均分析时间, DDoop 增量框架在第 1 个版本 jar 包上的全量分析时间以及在后续 19 个版本 jar 包上的增量分析平
均时间. 从图 7 我们可以发现, 虽然 DDoop 增量框架在第 1 个版本 jar 包上的分析耗时要比原始 Doop 框架更多,
但是在后续的增量分析过程中, DDoop 增量框架的耗时相比于原始 Doop 框架显著减少. DDoop 的第 1 轮全量分
析较慢主要包含以下两方面的原因: 首先, DDoop 增量框架前端需要记录增量信息, 这一过程会花费额外的时间.
这是因为我们的框架需要追踪和存储代码的变化, 以便在后续的分析中只关注这些变化部分. 这种增量信息的收
集和管理会增加一定开销. 其次, DDoop 增量框架后端所使用的 DDlog 引擎本身比非增量的 Soufflé 引擎要慢. 这
是因为 DDlog 需要额外的计算资源维护增量信息.
Doop 平均时间

DDoop 增量平均前端时间

DDoop 首次全量前端时间

DDoop 增量平均时间

DDoop 首次全量时间

运行时间 (s)

10 000

1 000

图7

Cl
1obj
2objH
1callsite
1type
2typeH
selective2objH
ZooKeeper

Cl
1obj
2objH
1callsite
1type
2typeH
selective2objH

ErrorProne

Cl
1obj
2objH
1callsite
1type
2typeH
selective2objH

Jedis

Cl
1obj
2objH
1callsite
1type
2typeH
selective2objH

Cl
1obj
2objH
1callsite
1type
2typeH
selective2objH

100

PMD

Checkstyle

DDoop 与 Doop 的实际运行时间对比 (开启剪枝选项)

图 8 展示了 DDoop 增量框架在开启了前端剪枝优化的情况下, 相比原始 Doop 框架的加速比, 参考值表示加
速比为 1. 从图 8 可以看出, 在增量分析场景下, DDoop 增量框架相比原始 Doop 框架的平均加速比约为 5. 在

沈天琪 等: DDoop: 基于差分式 Datalog 求解的增量指针分析框架

15

selective2objH 的精度设置下, DDoop 增量框架相比原始 Doop 框架的加速比最为明显. 在我们的实验评估中,
DDoop 增量框架在 selective2objH 的精度设置下可实现最高约 36×的分析加速 (Checkstyle).
40
参考值 1

30

20

Cl
1obj
2objH
1callsite
1type
2typeH
selective2objH

Cl
1obj
2objH
1callsite
1type
2typeH
selective2objH

Cl
1obj
2objH
1callsite
1type
2typeH
selective2objH

0

Cl
1obj
2objH
1callsite
1type
2typeH
selective2objH

10

Cl
1obj
2objH
1callsite
1type
2typeH
selective2objH

DDoop 对比 Doop 的加速比

DDoop 对比 Doop 的加速比

ErrorProne

ZooKeeper

PMD

Checkstyle

Jedis

图8

DDoop 对 Doop 的运行时间加速比

为了展示剪枝优化对分析效率的影响, 我们分别在图 7 (开启剪枝选项) 和图 9 (关闭剪枝选项) 中展示了
DDoop 增量框架在运行时的前端、后端更加细节的耗时信息. 我们可以发现, 在关闭剪枝优化选项时, 增量过程
中前端会更加耗时, 关闭剪枝优化选项的前端耗时平均比开启剪枝优化选项要多 41.3%. 这让 DDoop 框架在增量
时对比原版 Doop 框架的优势有所缩小. 此时, DDoop 增量框架相比原始 Doop 框架的平均加速比约为 4, 在
selective2objH 的精度设置下可实现最高约 27×的分析加速.
Doop 平均时间

DDoop 增量平均前端时间

DDoop 首次全量前端时间

DDoop 增量平均时间

DDoop 首次全量时间

运行时间 (s)

10 000

1 000

图9

Cl
1obj
2objH
1callsite
1type
2typeH
selective2objH
ZooKeeper

Cl
1obj
2objH
1callsite
1type
2typeH
selective2objH

ErrorProne

Cl
1obj
2objH
1callsite
1type
2typeH
selective2objH

Jedis

Cl
1obj
2objH
1callsite
1type
2typeH
selective2objH

Cl
1obj
2objH
1callsite
1type
2typeH
selective2objH

100

PMD

Checkstyle

DDoop 与 Doop 的实际运行时间对比 (关闭剪枝选项)

需要注意的是, 增量分析并不能保证在任何场景下、对任何项目的变更都能提供明显的性能优势, 特别是对
于可能影响到项目的较大范围的变更, 对其进行增量分析的耗时甚至可能会比重新全量分析更长. 在图 7 和图 9
中, 我们可以观察到这样一个稍显异常的例子: 在 ZooKeeper 代码库的 selective2objH 精度设置下, DDoop 增量框
架在增量过程中的平均消耗时间略差于 Doop 框架, 并且主要的耗时集中在后端评估部分, 我们对应给出了这种

软件学报 ****年第**卷第**期

16

情况下后端每次评估的耗时, 如图 10 所示. 从图 10 可以看出, 第 11 次提交对应的代码变更显著拉高了增量分析
耗时的平均值. 分析此次提交对应的代码变更可以发现, 在这一次提交中, 代码的变化引发了上下文方面的大范围
变化, 在相对更加复杂的上下文敏感性选项下, 这个问题会更加容易变得不可拓展. 但是从我们的实验评估中的所
有 665 次评估数据 (7 种精度设置下 5 个项目各有 19 次增量分析) 来看, 这种异常情况出现的概率 (2/665) 并不高.

10 000
1 000
100
10
1
0.1

iter 0
iter 1
iter 2
iter 3
iter 4
iter 5
iter 6
iter 7
iter 8
iter 9
iter 10
iter 11
iter 12
iter 13
iter 14
iter 15
iter 16
iter 17
iter 18
iter 19

单次后端计算时间 (s)

整体上看, DDoop 的增量化支持依旧可以在绝大多数场景下获得性能优势.

迭代次数

图 10

selective2objH 精度设置下 DDoop 在 ZooKeeper 上的后端耗时

实验结果表明, 我们的 DDoop 增量分析框架在代码频繁变更的场景下, 通过对代码变更进行增量分析, 在分
析耗时方面能够实现比现有先进的 Doop 框架的全量分析的显著加速. 并且在变更越频繁, 变更次数更多的场景
下, 我们的增量分析框架的累计耗时加速优势将更为显著.
3.3 RQ2: 兼容性
与 Doop 中现有指针分析规则兼容是 DDoop 增量框架设计中的一大核心目标. 为了评估 DDoop 增量框架对
Doop 框架中现有指针分析规则的兼容性, 在这部分的实验中, 我们将 DDoop 增量框架和两个现有的尝试为
Doop 框架提供基础增量支持的工作进行对比. 具体而言, 我们将 DDoop 框架与一个基于 Soufflé-elastic 实现基础
增量支持的 Doop-SE 框架以及另一个初步整合了 DDlog 引擎的 Doop-DDlog 框架进行兼容性测试. 基于实验设
置中选定的 7 种不同精度的指针分析规则以及 Doop 中提供的一种简化的指针分析规则 micro, 我们分别使用
DDoop 和 Doop-SE, 在我们的实验基准集中的 5 个真实世界大型 Java 项目上对 20 次连续的代码提交对应的 jar
包进行指针分析. 由于 Doop-DDlog 当前并无开放可获取的工具, 我们无法对其进行实际的实验评估, 因此我们这
里采用了其论文 [17]中报告的结果和结论.
表 2 展示了 DDoop、Doop-SE 和 Doop-DDlog 对 Doop 现有指针分析规则的兼容性情况. 从表 2 可以看出,
我们的 DDoop 能够兼容所有 8 种精度的指针分析规则, 而 Doop-SE 和 Doop-DDlog 都只能处理简化的 micro 分
析规则, 在其他复杂分析规则上的兼容性测试均无法正确运行. 对于 Doop-SE, 就目前 Soufflé-elastic 的实现而言,
在面对 Doop 中的绝大多数指针分析规则时, 它并不能将规则文件正确的编译或是解释执行, 对报错信息和实现
细节进行分析可以发现, 这个问题的直接原因是出在其中的规则重排器组件实现中. 此外, 对于 Doop 规则中的聚
合函数 (例如 sum()), 它实际上也并没有提出有效的增量解决方案, 它只提供了 Soufflé 中有限的语法结构的增量
能力. 对于 Doop-DDlog, 它试图在 Doop 框架中整合 DDlog 引擎以提供增量分析的能力, 然而和我们更为完善的
规则重写器相比, 它只实现了从 Soufflé 到 DDlog 的简单转换, 并且明确指出了其只支持了 Doop 中最简单的“selfcontained”分析和简化的 micro 分析. 此外, 根据其实验评估, 它只是在每一轮分析的时候将 Soufflé 引擎换成
DDlog 引擎, 实际运行的还是完整评估, 并未有效地利用 DDlog 引擎的增量能力.
表2
Analysis framework
DDoop
Doop-SE
Doop-DDlog

CI
√
×
×

lobj
√
×
×

2objH
√
×
×

实验数据集
1callsite
√
×
×

1type
√
×
×

2typeH
√
×
×

selective2objH
√
×
×

micro
√
√
√

沈天琪 等: DDoop: 基于差分式 Datalog 求解的增量指针分析框架

17

兼容性测试的实验评估结果表明, 我们的 DDoop 能够完全兼容所有精度的指针分析规则, 而 Doop-SE 和
Doop-DDlog 都只能处理简化的 micro 分析规则, 在其他复杂分析规则上均无法做到正确兼容. 此外, 由于我们的
规则重写器的通用性和完备性, 除了实验评估中用到的指针分析规则, Doop 中其他精度的指针分析规则我们也能
够兼容, 并且对于未来在 Doop 基础上开发的程序分析规则也能够继续提供兼容性.
3.4 RQ3: 精度
为了评估我们的增量指针分析框架前端采取的剪枝策略对最终分析结果的影响, 在这部分的实验中, 我们会
分别启用和关闭前端增量生成过程中的剪枝选项进行对比. 具体而言, 对于每一个不同的待分析程序和分析精度
的组合, 我们会分别运行两次依次分析每一个 jar 包的增量分析, 其中一次会在前端生成事实时开启剪枝, 另一次
则不开启剪枝. 在最后, 我们将统计两者最后分析结果的差异.
表 3 展示了前端剪枝选项在 5 个基准项目、7 种指针分析规则上对指针分析结果精度的影响. 其中使用了 3
种指标#vpt、#fpt 和#call-edge 来度量指针分析结果的精度, 分别对应 Doop 框架中定义的 3 种关系: VarPointsTo、
InstanceFieldPointsTo 和 CallGraphEdge. 需要注意的是, 在 Doop 框架定义中, 这些关系中引入了上下文信息, 为了
符合对指针分析算法精度评估的一般惯例, 我们使用一个简单的数据过滤程序过滤了其中仅上下文不同的数据,
得到了表 3 中现在的结果. 可以看出, 在我们实验基准集上, 在增量输入事实生成阶段引入剪枝策略在除了 Jedis
项目以外的其他仓库上都没有影响以上 3 个精度指标. 而在 Jedis 项目中, 剪枝后的版本在所有精度设置下的精度
指标都略微差于剪枝前, 整体差距约在 2% 左右.
表3
Project

Jedis

ErrorProne

ZooKeeper

PMD

Analysis
CI
lobj
2objH
1callsite
1type
2typeH
selective2objH
CI
lobj
2objH
1callsite
1type
2typeH
selective2objH
CI
lobj
2objH
1callsite
1type
2typeH
selective2objH
CI
lobj
2objH
1callsite
1type
2typeH
selective2objH

剪枝选项对指针分析精度的影响

Precision metrics (unpruned)
#vpt
#fpt
#call-edge
1 624 685
148 313
55 137
899 178
54 102
50 983
416 994
34 655
47 565
1 112 252
90 253
52 216
1 045 098
73 262
52 087
417 744
34 655
47 568
542 573
42 007
48 477
1 349 996
121 779
50 993
783 850
48 511
47 363
372 583
31 409
43 993
927 188
73 538
48 284
904 739
64 988
48 448
373 315
31 409
43 996
492 411
37 874
44 873
2 713 829
217 008
66 106
1 505 314
72 831
61 765
563 567
41 641
55 098
1 731 519
121 498
62 693
1 618 749
95 138
62 130
575 060
41 770
55 204
762 910
50 018
56 091
2 789 814
204 493
63 638
1 589 995
72 821
59 057
607 586
44 393
52 482
1 755 897
106 580
59 658
1 702 373
94 118
59 731
623 805
44 860
52 537
924 034
59 498
54 239

Precision metrics (pruned)
#vpt
#fpt
#call-edge
1 661 612
150 356
55 496
921 445
54 721
51 309
421 017
34 832
47 825
1 136 813
91 134
52 550
1 069 259
74 132
52 449
421 767
34 832
47 828
552 705
42 336
48 791
1 349 996
121 779
50 993
783 850
48 511
47 363
372 583
31 409
43 993
927 188
73 538
48 284
904 739
64 988
48 448
373 315
31 409
43 996
492 411
37 874
44 873
2 713 829
217 008
66 106
1 505 314
72 831
61 765
563 567
41 641
55 098
1 731 519
121 498
62 693
1 618 749
95 138
62 130
575 060
41 770
55 204
762 910
50 018
56 091
2 789 814
204 493
63 638
1 589 995
72 821
59 057
607 586
44 393
52 482
1 755 897
106 580
59 658
1 702 373
94 118
59 731
623 805
44 860
52 537
924 034
59 498
54 239

软件学报 ****年第**卷第**期

18

表3
Project

Analysis

Checkstyle

CI
lobj
2objH
1callsite
1type
2typeH
selective2objH

剪枝选项对指针分析精度的影响 (续)

Precision metrics (unpruned)
#vpt
#fpt
#call-edge
4 093 997
316 226
84 567
2 339 198
115 218
77 897
713 208
55 539
68 782
2 721 068
170 652
79 733
2 531 744
135 910
78 952
717 783
55 624
68 887
1 066 552
75 882
71 261

Precision metrics (pruned)
#vpt
#fpt
#call-edge
4 093 997
316 226
84 567
2 339 198
115 218
77 897
713 208
55 539
68 782
2 721 068
170 652
79 733
2 531 744
135 910
78 952
717 783
55 624
68 887
1 066 552
75 882
71 261

实验评估结果表明, 虽然在增量输入事实生成阶段引入剪枝策略可能会最终的指针分析结果带来轻微的精度
损失, 但影响极小, 我们设计的剪枝策略在现有的增量指针分析场景中仍可以保持相当高的精度.

4 讨

论

4.1 内存占用
静态程序分析没有银弹, 一种好的静态分析技术必然要在多个相互制衡的方面之间做出权衡取舍. 增量分析
虽然在分析效率上具有显著优势, 但这意味着其必然需要在其他方面付出一些代价——对于非增量的静态分析技
术而言, 在分析过程中, 它们往往可以通过删除在后续分析中不再需要的中间结果来节省内存使用; 而增量分析在
分析的全流程中必须维护所有与最终结果相关的中间结果, 以用于下一轮的增量分析, 这会导致增量分析的内存
使用量大幅增加. 目前, 我们的 DDoop 增量框架也存在内存占用相对较高的问题. 虽然实验已经证明其在流行的
Java 项目 (如 Jedis) 上能支持 Doop 中的许多指针分析精度, 但与原始的 Doop 相比, DDoop 的内存使用量仍然是
其数倍. 在这种情况下, 尽管我们在分析耗时上将可扩展性向前推进了一步, 但当前的内存问题成为了阻止我们
的 DDoop 框架进一步分析更大规模 (数百万行) 程序的主要障碍.
4.2 精度影响
在我们的框架前端中, 为了提高增量输入事实生成的效率, 引入了剪枝优化技术. 然而, 在提升输入事实生成
速度的同时, 剪枝优化技术也给指针分析的精度带来了挑战. 尽管我们的框架在第 1 轮全量生成中处理所有可能
引用的类库, 确保了初始的精度, 但具体到后续的增量事实生成前端中的剪枝操作, 精度的问题开始浮现. 具体而
言, 我们的剪枝操作会将基于类依赖关系加载类的过程“截断”到类依赖库类中的第 1 层, 这样, 如果程序的改变并
未导致引用类库的变化, 或者只是停止引用某些类库, 那么这种剪枝操作不会对精度产生负面影响. 这种情况下,
剪枝可以看作是一种优化策略, 既提高了分析效率, 又保持了精度. 然而, 当程序的变更引发新的依赖库类引用时,
剪枝操作可能会对精度产生负面影响. 因为这些新引用的依赖库类在静态分析层面对指针流数据流的完整建模可
能需要额外载入更多的依赖库类, 而剪枝操作已经将其截断, 于是我们的静态分析算法就无法通过获取经过库函
数相关的数据流建模, 从而影响精度. 在实验评估中, 我们也确实观察到了剪枝操作对精度的影响, 但总体上精度
损失非常细微, 处于可控范围之内. 此外, 在静态代码扫描的设置下, 更注重于尽早发现问题, 并快速地提供结果,
可以接受一定程度的误报或漏报, 对分析精度的要求并不如程序验证那么苛刻.
4.3 潜在适用性
我们的 DDoop 增量框架是针对 Java 指针分析框架 Doop 设计的, 因此, 目前的 DDoop 增量框架实现无法针
对其他更加广泛的编程语言编写的程序运行增量分析. 但是, DDoop 框架在理论上具有对于这些使用非 Java 语言
编写的程序的潜在分析能力. 具体而言, DDoop (Doop) 框架的指针分析在原理上就是将 Andersen 指针分析在
PAG 上计算不动点的问题转换为了在 Datalog 程序中不断迭代生成新的 facts 以达到不动点的问题. 显然, 从方法
的角度上来说, Andersen 算法部分是平台无关的, 只是在实现框架的过程中, 不同的语言对应的具体的和语言特性

沈天琪 等: DDoop: 基于差分式 Datalog 求解的增量指针分析框架

19

相关的工程实现细节和编程语言相关. 因此, DDoop 框架拥有对如 C++, Rust, Python 等其他语言的指针分析能力
的潜在适应性. 这为 DDoop 框架的未来发展提供了广阔的可能性. 此外, 我们的增量框架技术本身是框架无关的,
可以应用于其他基于 Datalog 的分析框架. 因此我们的技术具有更广阔的潜在适用性.

5 相关工作
在本节中, 我们主要通过以下 3 个方面来介绍和讨论我们的增量指针分析框架的相关工作: (1) 指针分析框架;
(2) 增量分析算法; (3) 增量计算.
5.1 指针分析框架
鉴于指针分析的基础分析地位及其重要性, 指针分析研究人员已经开发了一系列指针分析框架, 包含已有指
针分析算法的实现以及支持新的指针分析算法的开发.
Soot[22]中提供了两个指针分析框架 SPARK[23]和 Paddle[24], 其中 SPARK 是上下文不敏感的, 而 Paddle 支持多
种基于标签的上下文敏感指针分析. WALA[25]提供了一个 Andersen 风格的流不敏感指针分析框架. 目前已在其基
础上开发了一些新的指针分析工作, 例如 IPA[9]和 SHARP[10]. Doop[11]是一个基于 Datalog 的声明式指针分析框架,
其支持现有大多数指针分析算法, 被认为是当前最主流的指针分析框架. Qilin[26]是一个在 Soot 基础上开发的指针
分析框架, 其支持各种细粒度的 Java 上下文敏感指针分析. Tai-e[27]是最新提出的一个 Java 程序分析框架, 其精心
设计了一个易学、易用、高效的指针分析框架和基于指针分析的分析插件系统. CodeQL[28]是一个语义代码分析
引擎, CodeQL 支持对包括 Java 在内 11 种语言进行静态分析, 提供了基于 Steensgaard 算法的指针分析功能, 其底
层分析同样依赖于 Datalog 实现. 此外, 与本工作的思路类似, CodeQL 近期通过更换后端 Datalog 引擎, 实现了一
个支持增量分析的增量 CodeQL 原型框架 [29].
目前的这些主流的指针分析框架主要面向的是全程序指针分析, 对增量指针分析的支持较弱. 据我们所知, 其
中仅有在 Soot[22]之上基于 CFL 可达性的增量指针分析 [30], 以及在 WALA[25]之上的 IPA[9]和 SHARP[10]等少数工作
提出了增量指针分析算法, 而且它们面向的是特定指针分析算法的增量化, 并未从指针分析框架的层面上提供增
量分析的能力. 而 CodeQL 的原型增量框架虽然从框架层面支持了增量能力, 然而由于其算法层面仅提供了基础
的指针分析能力, 无法兼容和复用 Doop 中现有的大量指针分析算法实现, 特别是上下文敏感对指针分析精度至
关重要的部分. 本文提出的增量指针分析框架 DDoop 则是不仅从分析框架的层面解决了指针分析的增量化问题,
并且能够完全兼容 Doop 框架中现有已实现的各种上下文敏感性的大量指针分析算法.
5.2 增量分析算法
由于能够响应代码变更快速提供最新分析结果的优势, 近年来增量分析在静态分析领域受到了极大的关注.
针对各种静态分析技术, 研究人员分别提出了相应的增量化方案.
Reviser [31] 是一种针对基于 IDE/IFDS 的数据流分析框架的增量分析技术, 能够快速响应程序的增量变更.
Pacak 等人 [32]和 Zwaan 等人 [33]各自分别提出了自动推导增量类型检查器的工作, 能够在 IDE 等实时场景下对代
码变更立即产生反馈. Lu 等人 [30]基于 CFL 可达性提出了一种增量指针分析算法. Liu 等人 [9,10]针对上下文不敏感
指针分析和上下文敏感指针分析分别提出了相应的增量化方案. Zhao 等人 [34]在 CHA 调用图构建算法的基础上提
供了一种增量构建算法.
大多数现有的增量分析算法都源自 DRed (deletion-rederivation) 算法 [35]. 然而, 不同的分析算法中需要进行增
量处理的部分以及增量化的细节不尽相同, 这就导致需要为每一种静态分析算法单独设计相应的增量算法, 这个
过程非常冗长且极易出错. 此外, 基于 DRed 的增量分析算法通常会存在过度删除 (over-deletion) 的问题 [36], 即大
量数据可能先被删除, 但随后又被重新派生, 这可能会带来比较大的性能问题.
与这些传统的在分析算法层面进行增量化的工作相比, 我们的增量指针分析框架没有牺牲分析算法的透明
度, 可以兼容并复用现有的分析算法. 因此, 这种方式可以解决一大类 (可以用 Datalog 表述的) 程序分析算法的增
量化问题, 而无需为每个具体的分析问题实现特定的增量化方案. 此外, 在我们基于 DDlog 的增量分析框架中, 也

软件学报 ****年第**卷第**期

20

不存在 DRed 中的过度删除问题, 在这方面可以相比 DRed 可以取得更好的性能.
5.3 增量计算
随着输入变化而高效地更新计算输出, 而无需从头开始重新计算, 这一点对于许多应用来说可能很重要, 甚至
是必要的. 增量计算的基本想法是在静态算法执行期间维护某些信息, 这些信息可用于在输入变更时高效地更新
输出. 增量计算能够以一种对上层算法透明的方式为算法提供增量化的支持. 增量计算的早期工作包括静态依赖
图 [37], 函数缓存或记忆化 [38], 动态依赖图 [39]等. 而将动态依赖图与记忆化相结合的自调整计算 (self-adjusting computation)[40], 显著扩展了增量计算的适用性. Incoop[41]是自调整计算技术的代表工作之一, 它是一种基于 MapReduce 的
通用增量计算框架, 无需更改一行应用程序代码即可显著提高性能.
差分计算 (differential computation)[42]是由微软提出的一种新的增量计算模型, 以高效地支持迭代查询, 它扩展
了传统的增量计算以允许任意嵌套迭代. 基于差分计算模型的差分数据流 (differential dataflow, DDF) 现在是增量
Datalog 引擎的主流实现方式, 如 DDlog[17], Laddder[36]等. 我们的增量指针分析框架即是基于 DDlog 引擎来提供增
量计算支持, 而无需对上层的指针分析算法进行调整.
另一类增量计算模型是增量图计算 [ 4 3 ] . GraphBolt [ 4 4 ] 通过研究依赖驱动处理的方式来进行增量图计算.
Chronos[45]是一个专门为在时间图上运行内存中迭代图计算而设计和优化的图计算引擎, 其设计探索了局部性、
并行性和增量计算之间有趣的相互作用. iGraph[46]是一个针对持续更新的动态图的增量图处理系统. 静态分析中
的很多分析技术都是基于图形式进行计算的, 如数据流分析依赖于控制流图, 指针分析依赖于指针赋值图, 过程间
分析需要调用图. 因此增量图计算可能是未来的增量程序分析的一个重要的可探索方向. 目前 Gu 等人 [47]在这个
方向进行了初步的探索, 提出了 BigSpa, 这是一个结合了离线批量静态程序分析和在线增量静态程序分析的大数
据图处理系统, 对于小批量更新可以实现近乎实时的增量分析.

6 总结与展望
本文设计并实现了一个增量指针分析框架 DDoop, 旨在解决现有指针分析技术在处理大规模程序的连续代
码提交时耗时过长的问题. 基于带增量评估机制的 DDlog 引擎, DDoop 框架通过前端输入事实增量生成技术以及
后端兼容 Doop 中指针分析规则的自动化增量分析程序生成技术, 能够高效地处理代码变更, 尽可能地复用上一
次分析的结果, 从而降低计算量, 大大节省了指针分析过程所消耗的时间. 实验评估展示, 相较于非增量式的原始
Doop 框架, 我们的增量框架可以获得平均约 5×, 最高达 36×的加速效果.
本文的方法和框架实现目前仍存在一些不足, 这也是我们未来的研究方向: 增量分析的内存消耗较大, 我们未
来计划结合 DDlog 特性进一步优化重写器, 以及探索基于磁盘的中间结果存储策略, 以降低内存使用量; 此外, 我
们的输入事实增量生成的前端采取的剪枝优化技术会带来细微的精度损失, 未来我们计划对优化技术进一步改
进, 以消除精度损失. 我们的增量分析框架不仅探索了对指针分析技术的透明增量化支持, 同时也支持其他能基
于 Datalog 表述的一大类静态分析技术, 这将为软件开发 CI/CD 流程中的静态代码扫描提供了更加高效的解决方
案可能性. 除差分计算之外, 增量图计算也是一类重要的增量计算模型, 我们未来也计划探索基于增量图计算的静
态分析透明增量化技术.
References:
[1]

Cai YD, Yao PS, Zhang C. Canary: Practical static detection of inter-thread value-flow bugs. In: Proc. of the 42nd ACM SIGPLAN Int’l
Conf. on Programming Language Design and Implementation. New York: ACM, 2021. 1126–1140. [doi: 10.1145/3453483.3454099]

[2]

Grech N, Smaragdakis Y. P/Taint: Unified points-to and taint analysis. Proc. of the ACM on Programming Languages, 2017,
1(OOPSLA): 102. [doi: 10.1145/3133926]

[3]

Pradel M, Jaspan C, Aldrich J, Gross TR. Statically checking API protocol conformance with mined multi-object specifications. In: Proc.
of the 34th Int’l Conf. on Software Engineering (ICSE). Zurich: IEEE, 2012. 925–935. [doi: 10.1109/ICSE.2012.6227127]

[4]

Zhang QR, Lyu MR, Yuan H, Su ZD. Fast algorithms for Dyck-CFL-reachability with applications to alias analysis. In: Proc. of the 34th
ACM SIGPLAN Conf. on Programming Language Design and Implementation. New York: ACM, 2013. 435–446. [doi: 10.1145/

沈天琪 等: DDoop: 基于差分式 Datalog 求解的增量指针分析框架

21

2491956.2462159]

[5]

Smaragdakis Y, Balatsouras G. Pointer analysis. Foundations and Trends® in Programming Languages, 2015, 2(1): 1–69. [doi: 10.1561/
2500000014]

[6]

Tan T, Li Y, Xue JL. Efficient and precise points-to analysis: Modeling the heap by merging equivalent automata. In: Proc. of the 38th
ACM SIGPLAN Conf. on Programming Language Design and Implementation. Barcelona: ACM, 2017. 278–291. [doi: 10.1145/3062341.
3062360]

[7]

Li Y, Tan T, Møller A, Smaragdakis Y. Scalability-first pointer analysis with self-tuning context-sensitivity. In: Proc. of the 26th ACM
Joint Meeting on European Software Engineering Conf. and Symp. on the Foundations of Software Engineering. Lake Buena Vista:
ACM, 2018. 129–140. [doi: 10.1145/3236024.3236041]

[8]

Liu BH, Zhang H, Dong LM. Survey on state of DevOps in China. Ruan Jian Xue Bao/Journal of Software, 2019, 30(10): 3206–3226 (in
Chinese with English abstract). http://www.jos.org.cn/1000-9825/5796.htm [doi: 10.13328/j.cnki.jos.005796]

[9]

Liu BZ, Huang J, Rauchwerger L. Rethinking incremental and parallel pointer analysis. ACM Trans. on Programming Languages and
Systems, 2019, 41(1): 6. [doi: 10.1145/3293606]

[10]

Liu BZ, Huang J. SHARP: Fast incremental context-sensitive pointer analysis for Java. Proc. of the ACM on Programming Languages,
2022, 6(OOPSLA1): 88. [doi: 10.1145/3527332]

[11]

Bravenboer M, Smaragdakis Y. Strictly declarative specification of sophisticated points-to analyses. In: Proc. of the 24th ACM SIGPLAN
Conf. on Object Oriented Programming Systems Languages and Applications. Orlando: ACM, 2009. 243–262. [doi: 10.1145/1640089.
1640108]

[12]

Li Y, Tan T, Møller A, Smaragdakis Y. Precision-guided context sensitivity for pointer analysis. Proc. of the ACM on Programming
Languages, 2018, 2(OOPSLA): 1–29. [doi: 10.1145/3276511]

[13]

Ma WJ, Yang SY, Tan T, Ma XX, Xu C, Li Y. Context sensitivity without contexts: A cut-shortcut approach to fast and precise pointer
analysis. Proc. of the ACM on Programming Languages, 2023, 7(PLDI): 128. [doi: 10.1145/3591242]

[14]

Ryzhyk L, Budiu M. Differential Datalog. In: Proc. of the 3rd Int’l Workshop on the Resurgence of Datalog in Academia and Industry Colocated with the 15th Int’l Conf. on Logic Programming and Nonmonotonic Reasoning. Philadelphia: CEUR-WS.org, 2019. 56–67.

[15]

Jordan H, Scholz B, Subotić P. Soufflé: On synthesis of program analyzers. In: Proc. of the 28th Int’l Conf. on Computer Aided Verification. Toronto: Springer, 2016. 422–430. [doi: 10.1007/978-3-319-41540-6_23]

[16]

Zhao D, Subotic P, Raghothaman M, Scholz B. Towards elastic incrementalization for datalog. In: Proc. of the 23rd Int’l Symp. on
Principles and Practice of Declarative Programming. New York: ACM, 2021. 20. [doi: 10.1145/3479394.3479415]

[17]

Ritsogianni AA. Incremental static analysis with differential Datalog [BS. Thesis]. Athens: University of Athens, 2019.

[18]

Tan T, Ma XX, Xu C, Ma CY, Li Y. Survey on Java pointer analysis. Journal of Computer Research and Development, 2023, 60(2):
274–293 (in Chinese with English abstract). [doi: 10.7544/issn1000-1239.202220901]

[19]

Whaley J, Avots D, Carbin M, Lam MS. Using Datalog with binary decision diagrams for program analysis. In: Proc. of the 3rd Asian
Symp. on Programming Languages and Systems. Tsukuba: Springer, 2005. 97–118. [doi: 10.1007/11575467_8]

[20]

Antoniadis T, Triantafyllou K, Smaragdakis Y. Porting doop to Soufflé: A tale of inter-engine portability for Datalog-based analyses. In:
Proc. of the 6th ACM SIGPLAN Int’l Workshop on State of the Art in Program Analysis. Barcelona: ACM, 2017. 25–30. [doi: 10.1145/
3088515.3088522]

[21]

Use of Zipper in Doop. 2021. https://bitbucket.org/yanniss/doop/issues/41/use-of-zipper

[22]

Vallée-Rai R, Co P, Gagnon E, Hendren L, Lam P, Sundaresan V. Soot—A Java bytecode optimization framework. In: Proc. of the 1999
Conf. of the Centre for Advanced Studies on Collaborative research. Mississauga: IBM Press, 1999. [doi: 10.5555/781995.782008]

[23]

Lhoták O, Hendren L. Scaling Java points-to analysis using SPARK. In: Proc. of the 12th Int’l Conf. on Compiler Construction. Warsaw:
Springer, 2003. 153–169. [doi: 10.1007/3-540-36579-6_12]

[24]

Lhoták O, Hendren L. Evaluating the benefits of context-sensitive points-to analysis using a BDD-based implementation. ACM Trans. on
Software Engineering and Methodology, 2008, 18(1): 3. [doi: 10.1145/1391984.1391987]

[25]

WALA. Watson Libraries for Analysis (WALA). 2023. https://wala.sourceforge.net/

[26]

He DJ, Lu JB, Xue JL. Qilin: A new framework for supporting fine-grained context-sensitivity in Java pointer analysis. In: Proc. of the
36th European Conf. on Object-oriented Programming (ECOOP 2022). Berlin: Leibniz-Zentrum für Informatik, 2022. 30. [doi: 10.4230/
LIPIcs.ECOOP.2022.30]

[27]

Tan T, Li Y. Tai-e: A developer-friendly static analysis framework for Java by harnessing the good designs of classics. In: Proc. of the
32nd ACM SIGSOFT Int’l Symp. on Software Testing and Analysis. Seattle: ACM, 2023. 1093–1105. [doi: 10.1145/3597926.3598120]

[28]

CodeQL. 2024. https://codeql.github.com/

软件学报 ****年第**卷第**期

22

[29]

Szabó T. Incrementalizing production CodeQL analyses. In: Proc. of the 31st ACM Joint European Software Engineering Conf. and
Symp. on the Foundations of Software Engineering. San Francisco: ACM, 2023. 1716–1726. [doi: 10.1145/3611643.3613860]

[30]

Lu Y, Shang L, Xie XW, Xue JL. An incremental points-to analysis with CFL-reachability. In: Proc. of the 22nd Int’l Conf. on Compiler
Construction. Rome: Springer, 2013. 61–81. [doi: 10.1007/978-3-642-37051-9_4]

[31]

Arzt S, Bodden E. Reviser: Efficiently updating IDE-/IFDS-based data-flow analyses in response to incremental program changes. In:
Proc. of the 36th Int’l Conf. on Software Engineering. Hyderabad: ACM, 2014. 288–298. [doi: 10.1145/2568225.2568243]

[32]

Pacak A, Erdweg S, Szabó T. A systematic approach to deriving incremental type checkers. Proc. of the ACM on Programming
Languages, 2020, 4(OOPSLA): 127. [doi: 10.1145/3428195]

[33]

Zwaan A, Van Antwerpen H, Visser E. Incremental type-checking for free: Using scope graphs to derive incremental type-checkers. Proc.
of the ACM on Programming Languages, 2022, 6(OOPSLA2): 140. [doi: 10.1145/3563303]

[34]

Zhao ZL, Wang XZ, Xu ZG, Tang ZH, Li YC, Di P. Incremental call graph construction in industrial practice. In: Proc. of the 45th
IEEE/ACM Int’l Conf. on Software Engineering: Software Engineering in Practice (ICSE-SEIP). Melbourne: IEEE, 2023. 471–482. [doi:
10.1109/ICSE-SEIP58684.2023.00048]

[35]

Gupta A, Mumick IS, Subrahmanian VS. Maintaining views incrementally. ACM SIGMOD Record, 1993, 22(2): 157–166. [doi: 10.1145
/170036.170066]

[36]

Szabó T, Erdweg S, Bergmann G. Incremental whole-program analysis in Datalog with lattices. In: Proc. of the 42nd ACM SIGPLAN Int’l
Conf. on Programming Language Design and Implementation. New York, ACM, 2021. 1–15. [doi: 10.1145/3453483.3454026]

[37]

Demers A, Reps T, Teitelbaum T. Incremental evaluation for attribute grammars with application to syntax-directed editors. In: Proc. of
the 8th ACM SIGPLAN-SIGACT Symp. on Principles of Programming Languages. Williamsburg: ACM, 1981. 105–116. [doi: 10.1145/
567532.567544]

[38]

Pugh W, Teitelbaum T. Incremental computation via function caching. In: Proc. of the 16th ACM SIGPLAN-SIGACT Symp. on
Principles of Programming Languages. Austin: ACM, 1989. 315–328. [doi: 10.1145/75277.75305]

[39]
[40]

Carlsson M. Monads for incremental computing. ACM SIGPLAN Notices, 2002, 37(9): 26–35. [doi: 10.1145/583852.581482]
Anderson D, Blelloch GE, Baweja A, Acar UA. Efficient parallel self-adjusting computation. In: Proc. of the 33rd ACM Symp. on
Parallelism in Algorithms and Architectures. New York: ACM, 2021. 59–70. [doi: 10.1145/3409964.3461799]

[41]

Bhatotia P, Wieder A, Rodrigues R, Acar UA, Pasquin R. Incoop: MapReduce for incremental computations. In: Proc. of the 2nd ACM
Symp. on Cloud Computing. Cascais: ACM, 2011. 7. [doi: 10.1145/2038916.2038923]

[42]

McSherry F, Murray DG, Isaacs R, Isard M. Differential dataflow. In: Proc. of the 6th Biennial Conf. on Innovative Data Systems
Research. Asilomar: CIDR, 2013.

[43]

Fan WF, Liu MY, Tian C, Xu RQ, Zhou JR. Incrementalization of graph partitioning algorithms. Proc. of the VLDB Endowment, 2020,
13(8): 1261–1274. [doi: 10.14778/3389133.3389142]

[44]

Mariappan M, Vora K. GraphBolt: Dependency-driven synchronous processing of streaming graphs. In: Proc. of the 14th EuroSys Conf.
Dresden: ACM, 2019. 25. [doi: 10.1145/3302424.3303974]

[45]

Han WT, Miao YS, Li KW, Wu M, Yang F, Zhou LD, Prabhakaran V, Chen WG, Chen EH. Chronos: A graph engine for temporal graph
analysis. In: Proc. of the 9th European Conf. on Computer Systems. Amsterdam: ACM, 2014. 1. [doi: 10.1145/2592798.2592799]

[46]

Ju WY, Li JX, Yu WR, Zhang RC. iGraph: An incremental data processing system for dynamic graph. Frontiers of Computer Science,
2016, 10(3): 462–476. [doi: 10.1007/s11704-016-5485-7]

[47]

Gu R, Zuo ZQ, Jiang X, Yin H, Wang ZK, Wang LZ, Li XD, Huang YH. Towards efficient large-scale interprocedural program static
analysis on distributed data-parallel computation. IEEE Trans. on Parallel and Distributed Systems, 2021, 32(4): 867–883. [doi: 10.1109/
TPDS.2020.3036190]

附中文参考文献:
[8]

刘博涵, 张贺, 董黎明. DevOps 中国调查研究. 软件学报, 2019, 30(10): 3206–3226. http://www.jos.org.cn/1000-9825/5796.htm [doi:
10.13328/j.cnki.jos.005796]

[18]

谭添, 马晓星, 许畅, 马春燕, 李樾. Java 指针分析综述. 计算机研究与发展, 2023, 60(2): 274–293. [doi: 10.7544/issn1000-1239.
202220901]

沈天琪 等: DDoop: 基于差分式 Datalog 求解的增量指针分析框架

23

沈天琪(1999－), 男, 硕士生, CCF 学生会员, 主

宾向荣(2000－) 男, 博士生, CCF 学生会员, 主

要研究领域为程序分析.

要研究领域为程序分析.

王熙灶(1995－) 男, 博士生, CCF 学生会员, 主

卜磊(1983－), 男, 博士, 教授, 博士生导师, CCF

要研究领域为程序分析与验证, 程序设计语言.

杰出会员, 主要研究领域为模型检验, 形式化方
法, 信息物理系统, 复杂软件分析与验证.


软件学报 ISSN 1000-9825, CODEN RUXUEW
Journal of Software,2024,35(1):309−332 [doi: 10.13328/j.cnki.jos.006898]
©中国科学院软件研究所版权所有.

E-mail: jos@iscas.ac.cn
http://www.jos.org.cn
Tel: +86-10-62562563

*

DNS 信道传输加密技术: 现状、趋势和挑战
张 曼, 姚健康, 李洪涛, 董科军, 延志伟
(中国互联网络信息中心, 北京 100190)
通信作者: 姚健康, E-mail: yaojk@cnnic.cn

摘

要: DNS 作为重要的互联网基础设施, 其明文传输的特点带来很多隐私安全风险. DoH、DoT、DoQ 等 DNS

信道传输加密技术致力于防止 DNS 数据被泄露或篡改, 并保证 DNS 消息来源的可靠性. 首先从 DNS 消息格式、
数据存储和管理、系统架构和部署等 6 个方面分析明文 DNS 存在的隐私安全问题, 并对已有的相关技术和协议
进行总结. 其次分析 DNS 信道传输加密技术的实现原理及应用现状, 进而基于多角度评测指标对各加密协议在不
同网络条件下的性能表现进行讨论. 同时通过填充机制的局限性、加密流量识别和基于指纹的加密活动分析等方
向探讨 DNS 信道传输加密技术的隐私保护效果. 此外从部署规范、恶意流量对加密技术的利用和攻击、隐私和
网络安全管理之间的矛盾, 以及加密后影响隐私安全的其他因素等方面总结 DNS 信道传输加密技术存在的问题、
挑战和相关解决方案. 最后总结加密 DNS 服务的发现、递归解析器到权威服务器之间的加密、服务器端的隐私
保护、基于 HTTP/3 的 DNS 等后续需要着重关注的研究方向.
关键词: 隐私; 安全; QUIC; TLS 1.3; DoH; DoT; DoQ
中图法分类号: TP393
中文引用格式: 张曼, 姚健康, 李洪涛, 董科军, 延志伟. DNS信道传输加密技术: 现状、趋势和挑战. 软件学报, 2024, 35(1):
309–332. http://www.jos.org.cn/1000-9825/6898.htm
英文引用格式: Zhang M, Yao JK, Li HT, Dong KJ, Yan ZW. Encryption Technologies for DNS Channel Transmission: Status, Trends
and Challenges. Ruan Jian Xue Bao/Journal of Software, 2024, 35(1): 309–332 (in Chinese). http://www.jos.org.cn/1000-9825/6898.
htm

Encryption Technologies for DNS Channel Transmission: Status, Trends and Challenges
ZHANG Man, YAO Jian-Kang, LI Hong-Tao, DONG Ke-Jun, YAN Zhi-Wei
(China Internet Network Information Center, Beijing 100190, China)
Abstract: As critical Internet infrastructure, DNS brings many privacy and security risks due to its plaintext transmission. Many
encryption technologies for DNS channel transmission, such as DoH, DoT, and DoQ, are committed to preventing DNS data from leaking
or tampering and ensuring the reliability of DNS message sources. Firstly, this study analyzes the privacy and security problems of
plaintext DNS from six aspects, including the DNS message format, data storage and management, and system architecture and
deployment, and then summarizes the existing related technologies and protocols. Secondly, the implementation principles and the
application statuses of the encryption protocols for DNS channel transmission are analyzed, and the performance of each encryption
protocol under different network conditions is discussed with multi-angle evaluation indicators. Meanwhile, it discusses the privacy
protection effects of the encryption technologies for DNS channel transmission through the limitations of the padding mechanism, the
encrypted traffic identification, and the fingerprint-based encryption activity analysis. In addition, the problems and challenges faced by
encryption technologies for DNS channel transmission are summarized from the aspects of the deployment specifications, the illegal use of
encryption technologies by malicious traffic and its attack on them, the contradiction between privacy and network security management,
and other factors affecting privacy and security after encryption. Relevant solutions are also presented. Finally, it summarizes the highlights

*

基金项目: 北京市科技新星计划 (Z191100001119113)
收稿时间: 2022-08-02; 修改时间: 2022-09-16, 2022-11-26; 采用时间: 2023-01-05; jos 在线出版时间: 2023-06-28
CNKI 网络首发时间: 2023-06-29

软件学报 2024 年第 35 卷第 1 期

310

of future research, such as the discovery of the encrypted DNS service, server-side privacy protection, the encryption between recursive
resolvers and authoritative servers, and DNS over HTTP/3.
Key words: privacy; security; quick UDP Internet connection (QUIC); TLS 1.3; DNS over HTTPS (DoH); DNS over TLS (DoT);
DNS over QUIC (DoQ)

1 引

言

DNS 协议在设计之初并未考虑到安全性和隐私性, 其明文通信及缺少认证的特点被网络攻击、消息窃听和
用户活动分析等行为所利用. 根据安全公司 CrowdStrike 的报告, 广泛发生的 DNS 劫持事件使得多个行业的众多
组织曾受到影响, 其中包括不同国家的政府机构, 以及医疗健康、保险、民用航空、互联网服务等领域的基础设
施提供商 [1]. 美国实施的通信监听计划包含大量 DNS 数据, 其通过收集个人网络社交活动中的日志文件及身份信
息, 来进行数据挖掘和情报分析. 国际数据等公司发布的最新 DNS 威胁评估报告显示, 88% 的公司或组织曾遭遇
过 DNS 攻击, 并且各类型攻击 (例如 DNS 隧道、DNS 网络钓鱼、基于 DNS 的恶意软件) 的发生频率也都有所增
加 [2]. DNS 攻击造成的应用及服务宕机、客户敏感信息泄露等给各业务方造成了严重损失. 疫情加速了教育、医
疗、协同办公等不同领域的数字化转型进程, 但也为攻击者提供了新的目标和焦点, 根据报告显示在新冠疫情期
间医疗保健行业遭受了 DNS 攻击的严重影响, 给人民的生命和财产安全造成巨大威胁 [3]. 部署适当的 DNS 隐私
安全解决方案对提升网络安全性具有重要作用.
DNS 信道传输加密技术的提出致力于增强 DNS 消息的隐私安全性, 本文从技术原理、实施现状、性能表现、
隐私保护效果以及可能存在的问题和挑战等方面对 DNS 信道传输加密技术的研究工作进行分析总结. 第 2 节对
DNS 隐私安全问题和已有的相关研究进行分析. 第 3 节分析了 DNS 信道传输加密技术的技术实现原理. 第 4 节
分别从当前应用现状、加密技术的性能影响、隐私保护效果等方面对 DNS 信道传输加密技术进行总结. 第 5 节
讨论了 DNS 信道传输加密技术的问题和挑战. 第 6 节对未来的趋势和研究方向进行了展望. 最后第 7 节进行了全
文总结.

2 背景知识
2.1 DNS 存在的隐私安全问题
DNS 隐私安全威胁产生于以下几个方面: DNS 查询数据本身带来的信息泄露、通信路径和 DNS 服务器端的
数据隐私问题, 以及在数据存储和管理、系统架构和部署等方面存在的隐患, 具体分类如图 1 所示.
DNS 请求消息中包括的有关查询发起人和查询内容的数据会造成信息泄露, 如 QNAME 会泄露用户请求的
域名, 并且该字段中可能嵌入了使用的应用软件等标识信息; 客户端与递归解析器通信请求中的源 IP 地址字段包
含了用户自身地址; 客户端子网扩展则会泄露原始查询方的网络地址信息等.
在通信路径上中间攻击者可以监听、截获、篡改 DNS 消息, 一方面明文 DNS 消息中数据未加密, 路径中的
第三方可进行偷窥, 造成数据泄露; 另一方面传统 DNS 通信过程中缺少身份认证机制, 客户端无法验证响应来源
的真实性; 此外由于缺少数据完整性校验, DNS 消息可能面临被篡改的风险.
在服务器端同样存在隐私安全问题, 根据域名解析整体流程, DNS 服务器分为递归解析器、根域名服务器、
顶级域名服务器以及权威 (域名) 服务器. 递归解析器可由互联网服务运营商提供, 此外一些网络公司也对外发布
公共递归解析器, 以供用户选择. 基于网络活动中庞大的用户和查询体量, 递归解析器和权威服务器掌握大量
DNS 请求数据, 能够在此基础上进行信息观察、收集和分析处理, 或传输给第三方用于研究、安全分析和网络审
查等.
在数据存储和管理方面, 注册数据的公开一定程度上造成了域名注册方的信息泄露; 明文区传送使攻击者有
机会通过窃听网络连接来收集区内容; 美国封锁伊朗域名等事件也反映了在区文件的管理和修改权限设置机制方
面存在一定的不合理性.

张曼 等: DNS 信道传输加密技术: 现状、趋势和挑战

311

在系统架构和部署方面, 存在中心化、集中化、软件漏洞等问题. 虽然各级服务中设置了缓存机制, 但根服务
器作为 DNS 系统的中枢, 依然成为重要攻击目标; 近年来针对根服务器的大规模 DDoS 等攻击时有发生, 如攻击
者通过控制众多僵尸机器, 导致多个根服务器离线数小时 [4]. 在网络安全领域, 攻击方可利用网络中的受感染设备
获取攻击能力, 但防御方则需要付出数倍的成本来进行防范机制的部署, 虽然以往的攻击事件暂未导致大规模断
网事故, 但随着物联网的发展, 网络中将存在大量易受感染的设备, 使得未来潜在的攻击能力会迅速增长, 针对
DNS 的物联网僵尸网络的规模和复杂性将大大增加 [5]. 同时解析服务存在集中化问题, 头部的少数服务商占据了
巨大的市场份额, 掌握大规模的用户通信数据, 此外 DNS 服务器软件自身或系统中的其他网络设备可能存在安全
漏洞.
泄露用户请求的域名
QNAME
DNS 查询数据

源 IP 地址

可能嵌入软件等标识信息
用户客户端自身地址 (客户端-递归解析器)
递归解析器地址 (递归解析器-权威服务器)

EDNS0 扩展选项
客户端子网
客户端-递归解析器

数据未加密

传输路径

缺少消息完整性校验

递归解析器-权威服务器

缺少消息来源验证
递归解析器

缓存
解析器自身收集、分析

服务器端
权威服务器

隐私安全威胁
分类

传输给第三方用于研究、安全分析、审查等
隐私策略合规性

注册数据
域名注册方身份信息等数据泄露
数据存储和管理

DNS 区传送

区文件数据明文传输, 泄露给第三方

区文件管理修改权限

美国封锁伊朗域名事件

中心化

针对根服务器的攻击

集中化

少数服务器掌握大量用户数据

软件漏洞

Bind 等软件自身漏洞

系统架构与部署

收集查询数据、日志分析
被动
抓包分析 (DNSmezzo 、Tcpdump 等)

攻击
主动攻击

DDos 攻击、DNS 劫持、DNS 隧道
网络钓鱼、缓存中毒...

图1

DNS 隐私安全问题分析

针对 DNS 的攻击分为被动攻击和主动攻击, 被动攻击可进行数据偷窥和分析, 包括查询数据及日志的收集,
抓包分析 (例如 DNSmezzo[6]、Tcpdump[7]) 等, 而主动攻击则可通过对查询请求过程进行直接的更改和破坏来实
现攻击目的.
2.2 DNS 隐私安全增强相关研究
在 DNS 信道传输加密技术发布之前, 针对 DNS 存在的隐私安全问题 (参见第 2.1 节), 业内已开展了众多技
术研究和方案设计 [8,9]. 本节主要从解决的问题和技术架构实现等方面对已存在的技术协议进行分析和比较.

312

软件学报 2024 年第 35 卷第 1 期

在增强通信路径上的隐私保护方面, DNSCrypt[10]和 DNSCurve[11]对 DNS 消息进行加密, 并在收到响应时进行
来源验证, 保证 DNS 消息的机密性、真实性和完整性. DNSCurve 社区表示 DNSCurve 主要用于递归解析器和权
威服务器之间, DNSCrypt 则多用于客户端和递归解析器之间, 二者除了采用的密钥交换和加密算法有差异外, 具
体实现流程并无太大区别 [11]. DNSCurve 基于 NS 资源记录进行公钥发布, 其密钥交换和加密算法分别为 Curve25519
和 XSalsa20-Poly1305, DNSCrypt 不同版本之间支持多种实现算法. DNSCrypt 和 DNSCurve 需要使用方进行密钥
对的分发和管理, 部署成本较大, 并未被 IETF 标准化.
在缓解服务器端的数据收集方面, 应用较广泛的协议包括查询域名最小化 (query name minimization) 机制 [12].
DNS 请求过程中, 递归解析器发至根服务器、顶级域名服务器及权威服务器的消息中都包括完整的请求域名, 查
询域名最小化机制仅向各级服务器发送必要的信息 (例如在查询“www.example.com”时, 仅向根服务器发送“.com”
域名信息), 以减少上级名称服务器获得的信息量. 该方案与原有 DNS 协议兼容, 部署较为方便, 但作用有限.
在保证 DNS 数据未经篡改且来源正确方面, IETF 推出了 DNS 安全扩展协议 (DNS security extensions, DNSSEC)
机制 [13,14], 但它并未对消息进行加密. DNSSEC 利用非对称加密算法对 DNS 应答消息进行数字签名, 并通过父区
对子区公钥做信任背书的机制, 在域名体系内逐级构建起信任链. DNSSEC 实现了 3 种功能, 包括为接收到的
DNS 响应消息提供来源认证、对 DNS 数据进行完整性校验, 以及对于不存在的名字和记录类型进行否定存在验
证 [13] . DNSSEC 定义了新的资源记录类型以实现上述功能, 分别为资源记录签名 (resource record signature,
RRSIG)、DNS 公钥 (DNS public key, DNSKEY)、授权签名者 (delegation signer, DS)、NSEC (next secure). 其中
RRSIG 存放私钥对响应数据资源记录集的摘要信息进行签名后的结果. 在 DNS 区层次结构中, 每个区有一对公
私钥, 区的公钥信息保存在 DNSKEY 资源记录中. 同时子区公钥的摘要信息会存储在父区的 DS 资源记录中, 用
于逐级构建父区与子区之间的信任链. NSEC 通过指针方式对 DNS 区中存在的域名进行按序链接, 以证实某域名
或数据类型不存在, NSEC3 则通过哈希机制来防止 NSEC 机制下的区遍历. 就部署情况而言, 虽然约 91% 的顶级
域已部署 DNSSEC 验证服务, 但二级域部署率只占约 6%[15]; 此外虽然推出了 NSEC3, 但部分系统依旧部署先前
的 NSEC, 存在区文件被遍历的风险.
为了确保服务器证书的合法性, 基于 DNS 的命名实体认证 (DNS-based authentication of named entities,
DANE) 机制 [16]被提出. DANE 实现的功能包括以下几个方面, 首先其能对 CA 机构进行限制, 允许客户端只接受
特定 CA 机构签发的数字证书; 其次 DANE 能够对证书 (或公钥) 进行限制, 仅支持服务器使用特定的证书 (或公
钥); 最后 DANE 支持信任锚点声明 [16]. DANE 定义了 TLSA 资源记录类型, 其中包括 Cert.Usage (标识声明的类
型)、选择符、匹配类型和证书关联数据 4 个选项字段. DANE 的实现需要依赖 DNSSEC 对其消息进行签名,
DNSSEC 在二级域的低部署率对其推广应用产生了较大影响. 此外 DANE 会增加 TLS 连接过程的延时, 这也是阻
碍其在高实时性业务中部署推广的因素.
在注册数据的隐私保护方面, 最初域名注册信息通过 WHOIS 协议 [17]全部公开, 造成注册人敏感信息泄露, 改
进的域名注册数据访问协议 [18,19]中引入了多项安全机制. 首先, 其设立访问权限控制机制, 在匿名访问的基础上,
增加认证访问, 支持对客户端身份进行标识和验证, 同时对访问权限进行分级, 可根据用户身份和授权信息来实现
数据的差异化或分层访问. 其次该协议增加了可靠性保障 (例如限制客户端特定时间内的查询频率等), 以及数据
加密和完整性保护机制.
此外在新技术架构探索方面, 基于区块链的隐私安全机制也被用于名字空间领域. 它们利用区块链的合约、
共识等特性对域名系统整体架构或其中一部分 (如根区管理) 进行改进, 或在区块链平台上构建去中心化、匿名
性强的名字空间标识解析系统. 主要包括 Namecoin、Blockstack 名字系统、Handshake、以太坊名字服务等.
Namecoin [20] 基于比特币链实现, 其采用键值对形式将数据存储在链上, 并定义了 NAME_NEW、NAME_
FIRSTUPDATE、NAME_UPDATE 操作来完成域名注册流程, 其共识机制和挖矿算法设置与比特币类似. Namecoin
未开放顶级域申请, 仅支持“.bit”域下的域名注册. 它存在域名抢注等问题, 同时数据全部存储在链上, 对其性能和
扩展性也带来一定的影响.
Blockstack 名字系统 (Blockstack name system, BNS)[21,22]将区块链底层实现、业务逻辑处理和数据存储进行

张曼 等: DNS 信道传输加密技术: 现状、趋势和挑战

313

了解耦合, 底部区块链层负责存储基础的交易数据并实现共识等机制, 虚拟区块链层从底部区块链获取数据, 并进
行业务逻辑的处理. BNS 设计去中心化的存储系统, 用户可以自由选择不同类型的存储服务商, 数据需加密存储
并由所有者进行签名, 数据位置的指针和内容摘要信息保存在虚拟区块链层. 客户端发起访问请求后通过路由层
进行数据查询, 并按照指针指示的存储位置进行数据读取, 此时需完成签名验证、摘要比对等校验, 以保证数据的
安全和完整性. BNS 支持名字空间、各级子域的申请和注册, 但针对不同级别的域名, 其数据存储位置和管理机
制存在差异.
Handshake[23]基于区块链架构实现 DNS 根域部分的管理, 它与 DNS 兼容, 不会替代现有 DNS 系统. Handshake
设计 Urkel 树进行数据存储索引, 并定义了拍卖、公示、资金退回、转移、更新等一系列智能合约来完成域名的
拍卖注册和续签等流程. Handshake 支持个人注册顶级域名, 同时为了保持与现有 DNS 系统的兼容性, DNS 中已
存在的顶级域以及部分公司和组织机构的域名被禁止注册.
以太坊名字服务 (Ethereum name service, ENS)[24,25]用可读性强的名字替代以太坊中原有的随机字符串, 来表
示地址信息. 支持“.eth”顶级域下的域名注册管理, 同时 ENS 正在进行与 DNS 顶级域的集成, 集成完成后将支持
其他顶级域下的域名注册.
对新系统架构的探索不止局限在区块链领域, GNU 名字系统 [26,27]对 DNS 系统结构进行了重新设计, 致力于
提供去中心化、抗审查和隐私增强的方案, 并能与当前 DNS 系统兼容. GNU 名字系统中每个区与加密密钥对一
一对应, 密钥作为区的唯一标识; 同时其采用去中心化数据存储方式, 区内容经过加密和签名后以键值对的形式进
行存储. GNU 名字系统新定义了一系列资源记录类型用于域解析授权、重定向等操作, 同时为了支持与 DNS 的
互操作, 其定义了 GNS2DNS 资源记录, 以获取解析特定名字所需的 DNS 服务器信息.
上述技术的比较和总结如表 1 所示. 这些方案从不同方向对 DNS 隐私安全进行改进和探索, 部分协议由于推
广部署成本较大暂未得到大规模应用. DNS 作为重要的互联网基础资源, 规模结构庞大, 直接进行新架构的切换
难以实现, 基于区块链等的新架构研究也需要加强与现有 DNS 兼容整合的探索.
表1
类别

特点

技术/协议

DNS 隐私安全增强相关技术
目标

DNSCrypt
未标准化
DNSCurve
协议增强

Query name
minimization
标准化协议

DNSSEC
DANE
Namecoin

基于区块链
的架构
新架构探索

其他

描述
用于DNS客户端和递归解析器之间, 推广部署
成本较大, 未标准化

消息加密、利用签名验证服务器身
使用椭圆曲线加密算法, 基于NS资源记录进行
份, 并防止消息篡改
密钥分发; 安全、运算速度较快; 推广部署成本
较大, 未标准化
向各级服务器发送请求时仅提供当
域名最终仍会暴露, 不能避免第三方窥探和权
前步骤必需的域名标签集, 非必要情
威服务器端的隐私泄露
况下不发送完整域名信息
消息来源验证, 消息签名, 数据防篡
数据未加密
改; 否定存在验证
限制或声明证书、CA机构、信任锚 定义TLSA资源记录, 依赖DNSSEC, DNSSEC在
等的范围
二级域上的低部署率对其推广应用影响较大
基于区块链的名字标识解析系统

基于比特币链实现, 开放“.bit”域下域名注册, 不
支持顶级域申请, 存在域名抢注等问题

BNS

底层链和Stacks链两层区块链结构, 数据不全部
致力于利用区块链技术架构来重构
存储在链上; 支持名字空间及其下的子域申请,
DNS
并制定价格规则; 定义了去中心化存储系统

Handshake

不改变DNS架构, 仅对根区管理模块 基于区块链进行根区管理, 支持顶级域及子域
申请, 与DNS兼容, 保留DNS中已有的域名
进行优化

ENS

探索以太坊地址等资源标识解析

GNS

重新设计DNS架构, 构建去中心化、 可与DNS集成和兼容, 建立于GNUnet之上, 每
抗审查、提供隐私增强保护的名称 个区由区域键唯一标识, 定义了完整的密钥管
系统
理、加密、签名和域解析机制

ENS中正在添加对DNS中顶级域的支持

软件学报 2024 年第 35 卷第 1 期

314

3 DNS 信道传输加密技术原理
在第 2.1 节所述的威胁分类中, DNS 通信路径上的隐私安全保护具有比较重要的作用, 在客户端与递归解析
器以及递归解析器与权威服务器之间的通信过程中, 对 DNS 消息实施监听、截获、篡改是多种攻击类型得以实
现的落脚点. 即使 DNS 数据查询权限和结果本身是公开的, 但是用户的单个或一系列查询行为不应该公开 [28]. 表 2
介绍了 DNS 隐私安全相关的国际标准制定情况, 为了增强传输路径上的隐私保护, IETF 先后制定了基于 HTTPS
的 DNS (DNS over HTTPS, DoH), 基于 TLS 的 DNS (DNS over TLS, DoT) 和基于 QUIC 的 DNS (DNS over QUIC,
DoQ) 等 DNS 信道传输加密协议. DNS 信道传输加密旨在从隐私、身份认证、数据完整性等方面改善 DNS 数据
安全并最大限度地减少网络运营商等对 DNS 流量的分析. 其中隐私部分防范 DNS 数据被偷窥或泄露, 身份认证
机制确保数据在合法用户之间交互, 数据完整性验证防止消息被篡改.
表2
条目
RFC 7258
RFC 7816
RFC 7830
RFC 7858
RFC 7873
RFC 8094
RFC 8310
RFC 8467
RFC 8484
RFC 8932
RFC 9076
RFC 9103
RFC 9114
RFC 9250

日期
2014-05
2016-03
2016-05
2016-05
2016-05
2017-02
2018-03
2018-10
2018-10
2020-10
2021-07
2021-08
2022-05
2022-05

DNS 隐私安全协议
描述
Pervasive monitoring is an attack
QNAME minimization
The EDNS(0) padding option
DNS over TLS
Domain name system (DNS) cookies
DNS over DTLS
Usage profiles for DNS over TLS and DNS over DTLS
Padding policies for extension mechanisms for DNS (EDNS(0))
DNS over HTTPS
Recommendations for DNS privacy service operators
DNS privacy considerations
DNS zone transfer over TLS
HTTP/3
DNS over dedicated QUIC connections

3.1 DoH 技术原理
RFC 8484[29]定义了 DoH 的技术规范, DoH 使用 HTTPS 协议进行 DNS 消息数据的传输, 所有 DNS 请求和响
应都在 HTTP 数据包中进行编码, 其推荐的 HTTP 最低版本为 HTTP/2, 并使用 443 端口. DoH 中通过 HTTPS
URL 发送 DNS 请求, 服务器需要支持一个或多个 URI 模板. 此外所有 DoH 服务器和客户端必须支持 application/
dns-message 类型, 它本质上是 HTTPS 对 DNS 格式的封装. 另一种广泛支持的类型是 application/dns-json, 表示
JSON 格式的 DNS 消息, 但不强制要求服务器支持该格式.
在使用 DoH 时, 如果加密递归解析器的地址未知, 客户端与 DoH 服务器的通信过程将分为 URI 解析和双方
通信两个阶段, URI 解析阶段浏览器通过发送非加密 DNS 请求来获取 DoH 服务器的 IP 地址信息, Do53 所面临
的域名劫持等各种隐私安全风险在这个阶段同样存在. 双方通信阶段客户端与 DoH 服务器建立 TLS 连接, 通过
HTTPS 发送加密 DNS 查询请求.
DoH 不仅仅是基于 HTTP 的隧道, 其允许将 DNS 记录集成到具有缓存、重定向、代理、身份验证和压缩等
机制的 HTTP 生态系统中, 例如扩展 DoH 功能支持从服务器推送 DNS 数据选项, 此外递归解析器的选择权转移
到了 Web 浏览器等应用, 对底层操作系统透明. 一些公共服务器列表 [30]汇总了当前已部署的 DoH 公共递归解析
器, 用户可自行进行选择.
3.2 DoT 技术原理
DoT[31,32]是通过 TLS 连接发送 DNS 数据, 利用 TLS 实现 DNS 消息的加密和认证, 默认使用 853 端口. 由于
数据包通过专用端口发送, 因此网络管理员比较容易实现针对 DoT 流量的识别、阻止或过滤, RFC 标准中允许其

张曼 等: DNS 信道传输加密技术: 现状、趋势和挑战

315

使用 853 以外的端口. DTLS 在 UDP 之上实现, 基于 DTLS 的 DNS 能够避免 TCP 中的队头阻塞等问题, 已发布为
实验性协议 [33]. 此外, DNS 区传送原本以明文形式传输, 攻击者可通过窃听网络连接收集区文件内容; DNS TSIG
(transaction signature, 事务签名) 机制将直接的区域传输对象限制为仅授权的客户端, 但其未增加数据机密性保护.
基于 TLS 的区传送使用 TLS 来防止对区传送的被动监控和数据收集 [34].
DoT 和基于 DTLS 的 DNS 支持两种隐私配置模式, 如表 3 所示: 严格模式和机会主义隐私模式, 严格模式要
求加密和认证必须同时完成, 能够有效地防范主动和被动攻击者; 机会主义模式允许在加密和认证不能同时实现
时支持不经过身份认证直接进行消息加密, 进一步支持回退到明文传输. 仅加密模式能够提供针对被动攻击的保
护, 但并不能有效地防范主动攻击.
表3

机会加密设置机制

配置模式

支持的操作类型

严格模式

加密并经过身份认证的传输

机会加密模式

加密并经过身份认证的传输
加密传输, 未经过身份认证
明文传输

3.3 DoQ 技术原理
TCP 协议应用非常广泛, 但也存在一些问题, 包括建立连接的握手延迟、队头阻塞等, 但是由于涉及操作系统
内核层面, 同时网络中各种类型的设备需进行更新支持, 其功能改进和迭代都比较困难. HTTP/2 虽然通过多路复
用机制解决了应用层的队头阻塞问题, 但在传输层 TCP 协议中该问题依然存在. TCP 虽然存在一些问题, 但是创
建新的传输层协议并非易事, 牵一发而动全身, 需要网络系统中各方的支持, 涉及巨大的成本和工作量.
快速 UDP 网络连接 (quick UDP Internet connection, QUIC) 协议即在上述背景下提出, 一方面网络的场景和内
容越来越复杂, 另一方面当前广泛使用的协议中几个由来已久的问题和矛盾变得越来越突出, QUIC 希望能更好
地解决上面的问题, 进一步提升网络传输效率和响应速度. 它最初由 Google 提出, 该版本被称作 gQUIC, gQUIC
自定义了一套加密认证机制, 后在 IETF 进行整合改进. 2021 年 IETF 版本的 QUIC 发布为 RFC 9000, 并相继发布
QUIC 的丢包检测与拥塞控制、TLS 集成、头部压缩等系列标准 [35–37]. 由于新建传输层协议部署困难太大, QUIC
在 UDP 基础上实现了加密可靠传输, 通过内部集成 TLS 保证数据安全. 此外基于 QUIC 的 HTTP 被命名为 HTTP/3,
并于 2022 年发布为 RFC 9114[38].
QUIC 将多路复用的机制下移到传输层, 定义流 (stream) 表示连接中传输有序字节的单向或双向通道, 每个连
接同时支持多个流; 不同流之间相互独立, 一个数据包丢失仅会影响该包涉及的流, 不会对其他数据包中未涉及的
流产生影响. 流是一种逻辑概念, 数据携带由不同类型的帧 (frame) 完成, 例如 CRYPTO 帧 (类型=0x06) 用于传输
加密握手消息, STREAM 帧用来隐式的创建一条流并携带流数据发送. 同时帧会标识出自己属于哪个流, 多个帧
可能归属于一个逻辑流. 帧包含在 QUIC 数据包中, QUIC 数据包封装在 UDP 数据报中, 单个 UDP 数据报可以封
装一个或多个 QUIC 数据包, 其数据封装结构如图 2 所示. 同时 QUIC 设置了流和连接两个级别的流量控制机制,
通信双方可通过流量调节帧的交互, 动态的实现流量阈值管控的调整.
UDP header

QUIC header

Frame

...

数据
QUIC 数据包
(一个或多个)
UDP 数据报

图2

QUIC 数据传输结构

Frame

软件学报 2024 年第 35 卷第 1 期

316

QUIC 连接基于连接 ID 进行标识, 而非地址端口信息, 不与网络路径进行绑定. 客户端在不同网络之间 (例如
Wi-Fi 和蜂窝网络) 进行切换时支持连接迁移, 无需重新建立连接. 由于集成了 TLS 1.3, QUIC 在首次连接时支持
1-RTT 握手, 并在后续连接中可实现 0-RTT 握手, 不过 0-RTT 可能存在重放攻击等安全风险.
相比于 TCP, QUIC 拥塞控制的算法和参数选择机制更加灵活, TCP 中重传数据包跟原始数据包的序列号一
致, 接收端收到重传包后难以辨别该包是重传还是延迟过来的包, 造成往返时间 (round-trip time, RTT) 计算不准
确. 而 QUIC 中数据包的编号单一递增, 丢包后重传包与原包号不一致. 同时 QUIC 中已经确认的数据包不支持取
消确认, 并且支持更大的确认范围. 此外, TCP 中未计算接收端接收到数据包至发送确认之间的延迟时间, QUIC
中对这段时间进行了包含, 使得 RTT 计算更加准确.
基于 QUIC 的 DNS [39] 把 DNS 请求和响应消息在 QUIC 中进行映射, 例如一个 DNS 请求响应映射到一个
QUIC 流中, 利用 QUIC 为 DNS 提供隐私保护功能. 为了避免与明文 DNS 混淆, DoQ 禁止使用 53 端口, 其默认端
口为 853, 同时建议在客户端与递归解析器的通信中可协商使用 443 端口, 以避免基于端口号的流量拦截. 由于通
信过程中可能会在一个 QUIC 数据包中传输多个包含不同 DNS 消息的 STREAM 帧, 在进行消息填充时应按照数
据包粒度进行填充设置. 此外 QUIC 内部集成了 TLS, 如果 DoQ 连接建立失败, 它会首先尝试回退到 DoT, 如果还
不行, 再回退到明文传输.
3.4 TLS 1.3
DoH、DOT 和 DoQ 都建立在 TLS 的基础上, 2008 年 TLS 1.2[40]发布, 成为目前广泛部署的 TLS 版本, 2018
年 RFC 8446[41]推出 TLS 1.3 版本. 与 TLS 1.2 相比, TLS 1.3 简化了密码协商模型并且规范了密钥协商的选项, 例
如其将 Diffie-Hellman 算法的参数选择限制在已知的可选安全范围内, 客户端在发送第 1 条消息时便可以自己选
择密钥参数, 而不用像 TLS 1.2 需要先等服务器确认其支持的选项, 节省协商交互时间. 同时由于 RSA 密钥交换
不具有前向安全性, TLS 1.3 中已不再支持该算法. 此外 TLS 1.3 在密钥协商完成后即对后续握手消息和扩展选项
进行加密, 并在非首次连接时支持 0-RTT 数据传输. 但是 0-RTT 可能带来一些问题, 一方面攻击者可以重放
0-RTT 数据, 并根据接收服务器的行为推测其内容; 另一方面是 0-RTT 机制依靠 TLS 恢复, TLS 恢复会使连续的
客户端会话之间具有可链接性.
3.5 比较和总结
根据第 3.1–3.4 节中对各协议技术原理的分析, 在此基础上, 本节分别从架构层次、端口设置、流量隐蔽性、
协议特点、DNS 服务选择权、适用场景等方面对 DNS 信道传输加密协议进行比较 (参见图 3 和表 4).
DoH
应用层
DoT

HTTPS

DoQ

TLS

TLS

QUIC
TLS

TCP

TCP

UDP

传输层

网络层

IP

图3

DoH、DoT、DoQ 框架图

图 3 为各协议的架构层次图, DoH 和 DoT 使用的传输层协议为 TCP, 而 DoQ 则在 UDP 基础上实现. 如表 4
所示, 由于 DoH 使用与 HTTPS 相同的端口号, 相对于 DoT 流量, 其具有更高的隐蔽性. 传统 DNS 递归解析器的
选择在操作系统层面, DoH 将递归解析器的选择权转移到了浏览器等应用层面, 不同应用之间可以独立进行递归
解析器选择, 互联网公司、ISP 等机构对特定技术的选择和支持一定程度上会从自身利益出发.

张曼 等: DNS 信道传输加密技术: 现状、趋势和挑战

表4

317

协议比较

协议

DoH

DoT

DoQ

默认端口

443

853

853

隐秘性

与HTTPS具有相同的端
口号, 具有一定的隐蔽性

易被防火墙等屏蔽

易被防火墙等屏蔽(可协商使用443端口)

特点

需要URL标识

DNS服务选择权

浏览器等应用

操作系统层面

操作系统层面

传输层协议

TCP

TCP

UDP

加密与身份认证

TLS

TLS

TLS

目前应用场景
支持的场景

相较DoH, 无需架构在HTTPS应用层 多路复用、拥塞控制、丢包检测等方面进行
之上
了优化

客户端到递归解析器之
客户端到递归解析器之间的通信路径
间的通信路径

客户端到递归解析器之间的通信路径

客户端到递归解析器之 递归解析器到权威服务器之间的通信 递归解析器到权威服务器之间的通信路径、
间的通信路径
路径、区传送等
区传送等

DoH、DoT 和 DoQ 当前暂时多应用于 DNS 客户端和递归解析器之间的通信, 旨在防御路径上的攻击者, 并
未解决 DNS 服务器端的隐私安全问题. 各服务器仍能看到所有请求, 用户应谨慎选择具有良好隐私策略的提供
商. 此外 DoQ、DoT 等协议也支持递归解析器与权威服务器之间的通信保护.

4 DNS 信道传输加密技术分析
第 3 节总结了 DNS 信道传输加密技术的实现原理, 加密协议发布后引发关注的问题主要包括以下方面 (如
图 4 所示).
首先各技术推出之后包括 DNS 解析服务商在内的网络基础设施和资源提供方的态度如何, 以及新协议当前
的部署应用状态处于什么程度.
其次相对于传统 DNS, DoH 和 DoT 需要多次 TCP 及 TLS 握手来建立连接, 以保证安全可靠传输, 加密服务
部署后性能表现如何, 与传统 DNS 相比, 是否会有较为明显的响应延迟问题. DoQ 虽然基于 QUIC 实现, 理论上
从架构层面降低了握手等延迟, 那么相较于 DoH 和 DoT, DoQ 在实际表现中是否确实具有明显的优势.
此外加密协议致力于增强消息的机密性和安全性, 实际的隐私保护效果如何, 对加密消息的识别和分析是否
存在可能.
基于上述问题, 本节主要从实施现状、性能影响、隐私保护效果等方面对 DNS 信道传输加密技术进行分析
和讨论, 总体结构如后文图 4 所示.
4.1 现状分析
加密协议推出后其部署情况如何以及加密流量在网络中的占比也是研究者关注的方向. 2019 年初, Google 宣
布在其公共递归解析器中支持 DoT[42]. 2020 年 2 月, Firefox[43]开始默认为美国用户推出 DoH 服务, 提供 Cloudflare
(默认) 和 NextDNS 两个服务提供商. 随后 Chrome 浏览器 [44]等应用、Apple[45]、Android[46]及 Windows 操作系统 [47]
相继添加了对加密解析的支持.
探测公共加密递归解析器部署情况可采用的方法主要分为几类 (如表 5 所示), 首先技术人员维护了一些公共
加密服务器列表 (例如文献 [26]), 其中分类列举了已提供加密协议的厂商和对应的解析服务, 不过这些列表存在
更新延迟和覆盖不全面等问题. URL 推测方法主要用于 DoH, 利用服务提供商的 DNS 递归解析器域名和 RFC
8484 中建议的 URL 字段设置的格式特征, 有针对性地进行加密协议的探测.
利用现有非加密服务器可用来发现原服务器中是否添加了对加密协议的支持, 跨协议探测则是发现加密服务
器对其他协议的支持情况, 这两种方法都需要服务器通过特定机制 (如指定的资源记录等) 来发布自身属性信息,
并且通信双方就上述机制达成一致.
对加密服务部署情况的全面评估则多采用端口扫描法, 它基于加密协议的端口设置特征进行探测发现, 整体

软件学报 2024 年第 35 卷第 1 期

318

步骤如表 6 所示. 例如 DoT 主要是在 853 端口上进行全网扫描, 而 DoH 则是在 IPv4 地址空间中扫描发现 443 端
口打开的服务器, 同时借助反向 DNS 记录和被动 DNS 数据等得到递归解析器名等识别信息, 然后通过脚本对每
个 IP 地址执行多个 DoH 请求, 并对答案进行评估, 以此为依据辨别哪些是真正的 DoH 服务器. 此外有研究者在
请求步骤中同时设置了服务器名称标识 (server name indication, SNI) 字段值 [48], 以便能够正确访问多托管服务器.
最后通过响应消息验证等来确认各个服务器支持的 DoH 协议功能和属性.
现状及性能分析
(第 4 节)

部署情况分析

不同网络条件的影响

性能分析

多维度指标类型下的表现

加密流量占比分析

服务商和协议选择的影响
EDNS(0) 填充扩展选项

填充机制的局限性

QUIC 填充帧
隐私保护效果
分析 (第 4 节)

HTTPS 中识别 DoH

加密流量识别

其他加密协议的识别
网页指纹构建
加密流量分析

DNS 信道传输
加密隐私安全
技术分析

应用程序分析
服务器名称标识

其他影响隐私安全的因素

客户端子网

部署规范问题

无效证书
系统部署架构优化

加剧集中化

跨多解析器查询分发

问题和挑战分析
(第 5 节)

DoH 隧道等恶意流量探测
加密技术被恶意流量利用
僵尸网络检测
加密对网络安全管理的影响
隐私和网络安全管理的矛盾

解析器端操纵实现访问控制

降级攻击问题

机会性隐私设置被恶意利用
基于解析器的发现

加密解析器发现

基于网络的发现
趋势与研究方向
展望 (第 6 节)

递归解析器端的隐私保护

ODoH 设置中间代理, 防止将请求方
身份和请求内容关联

递归到权威服务器之间的加密

涉及根服务器、TLD 服务器、
二级权威服务器等多方

HTTP/3 和 DoH3

加密权威服务器信息的发布

图4

DNS 信道传输加密技术分析框架图

表5

公共加密 DNS 服务器探测方法分类

方法

描述

公共服务器列表

例如文献[26]

URL推测 (DoH)

基于URL探测网络中加密递归解析器和流量

根据现有非加密服务器信息探测加密服务器信息
已部署的加密服务器探测其他协议支持情况

服务器通过通信双方协商好的机制 (如指定的资源记录)
将自身属性信息发布给客户端

端口扫描后发送加密请求

见表6

张曼 等: DNS 信道传输加密技术: 现状、趋势和挑战

表6

319

端口扫描探测公共加密递归解析器流程

步骤

描述

端口扫描

IPv4地址空间内用默认端口进行全网扫描

反向解析

根据扫描得到的IP地址, 通过反向解析等方法得到对应域名信息

发送加密DNS请求

向端口扫描得到的IP地址发送加密DNS请求

加入SNI字段

发送请求时加入域名信息至SNI字段[48]

响应信息验证

对接收到的响应消息进行资源记录、证书等的校验

Lu 等人 [49]发现 2019–2020 年一年多的时间里 DoT 递归解析器的数目由 2 000 增至约 7 800, 分属 1 200
多家服务提供商, 在此期间 DoH 服务提供商的数目也增加了一倍多. García 等人 [50]通过全网扫描发现的 DoH
服务器比目前公共服务器列表中记录的多出近 4 倍 (按 IP 地址计算, 实际不同的 IP 可能对应同一个递归解析
器名).
Luo 等人 [48]在开放递归解析器的探测过程中, 通过端口扫描向潜在 IP 地址发送加密 DNS 请求, 并在请求中
将上述 IP 对应的域名信息加入 SNI 字段中, 结果共发现 5.7k 个 DoH 递归解析器和 9.6k 个 DoT 递归解析器, 但
约 30% 的解析器不能得到正确响应. Kosek 等人 [51]从 2021 年中到 2022 年初探测到 DoQ 递归解析器的数目增长
了 46%, 达到 1 217, 同时由于 QUIC 协议的 RFC 发布前历经众多版本的改进, 因此在探测期间服务的迭代和更新
波动比较频繁.
加密流量在整个网络中的占比方面, 2019 年 Cloudflare 公布其公共递归解析器上加密流量的占比为 8%. 有研
究者测量欧洲大型 ISP 服务商、大学、全球安全公司中的 DoH、DoT、DoQ 流量情况及分布, 发现上述 3 个组
织中 Do53 流量至少比加密 DNS 流量多 3 个数量级, 探测结果很少发现 DoQ 流量 [50].
在对公共加密递归解析器的评估及其支持的功能集方面, 虽然 DoH 中未强制要求使用特定的 URL 路径, 但
从服务运营商的选择趋势变化上, 大家更趋向于选择统一的基准路径段, 以避免太多的 URL 令人困惑并可能产生
配置错误 [52].
4.2 DNS 信道传输加密技术性能分析
DoT 和 DoH 都基于 TCP 和 TLS 运行, 需要多次握手协商, 此外加密协议需要 DNS 服务器对查询消息进行解
密, 并对待发出的响应消息进行加密, 这进一步增加了 DNS 服务器的开销. DNS 信道传输加密技术会对性能、服
务器负载和用户体验产生怎样的影响是值得关注的问题.
2018 年, Mozilla 基于真实浏览器用户数据对 Firefox 的 DoH 响应时间进行了大规模测量, 发现大多数查询
DoH 比 Do53 慢 6 ms[53]. 但是 Mozilla 的测试只是关于 Cloudflare 的 DoH 递归解析器, 未包括网络延迟、吞吐量
等因素对加密服务器性能影响的探索.
为了更加全面地对加密协议的性能特点进行分析, 研究者 [49,51−56]在网络环境选择方面覆盖了大学网络、代理
网络、云数据中心、家庭网路, 以及蜂窝网络和有损网路等. 性能分析指标分为传输数据量、加密协议连接建立
时间、DNS 查询的响应时间、网页加载时间等. 在实际的分析测试过程中, 连接重用的使用程度, 厂商和协议选
择之间的差异, 及测量时的网络状态等因素也会对性能结果产生一定的影响.
4.2.1

加密协议对响应时间和页面加载时间的影响

本节从数据包和传输字节的开销、队头阻塞问题的影响、解析响应时间及页面加载时间等方面分析评估加
密协议的属性特征和性能表现情况, 如表 7 所示.
在数据包和传输字节开销对比方面, Böttger 等人 [52]通过大学网络测试发现通常情况下访问网页时发送的总
DNS 查询数目是网页数量的 20 多倍, 单个 DoH 交换消耗的字节数是 Do53 的 30 倍以上, 数据包的数量大约是
Do53 的 15 倍左右. 连接复用情况下 DoH 消耗的字节和数据包数量仍约是传统 DNS 的 4 倍多.
通过分析研究者发现队头阻塞问题会为各协议带来一定的影响 [52], DoH 推荐的 HTTP 最低版本为 HTTP/2,
相比于 HTTP/1.1, HTTP/2 解决了应用层的队头阻塞, 但由于在传输层 TCP 协议感知不到上层协议的多路复用机

软件学报 2024 年第 35 卷第 1 期

320

制, 该问题并未彻底解决. 同时虽然不经常发生, 队头阻塞问题也存在于 TLS 中, QUIC 协议将多路复用机制下移
到传输层, 并将 TLS 的加密粒度改为按包加密, 以缓解上述问题.
表7
指标

条件

性能分析
分析

大学网络, Alexa全球排 字节数、数据包数目加密流量为Do53的十几倍至几十倍; 连接复用情况下降
数据包和传输字节开销
名前100 000的网页
至约4倍多
队头阻塞的影响
DNS响应时间
页面加载时间

HTTP/1.1受影响较大; HTTP/2、QUIC进行了改进
DoH、DoT、Do53
全球多点部署

由于建立连接、消息加解密等, DoH、DoT高于Do53
通过连接重用、技术优化等DoH、DoT与Do53页面加载时间的差异不太明显

网络条件的影响

模拟蜂窝4G网络、有
随着网络吞吐量的下降和损耗增加, DoT和DoH的性能表现有明显的下降;
损蜂窝4G网络及3G网
有损网络条件下DoH具有更高的失败率和错误率
络; 美国家庭网络

不同提供商、协议的
影响

Cloudflare、Google、
不同服务商之间以及同一服务商提供的不同协议的服务之间可能存在较大性
Quad9; Do53、DoT、
能差异; 同时性能表现存在明显的地区差异
DoH

DoQ与其他协议对比

DoQ、DoH、DoT
往返时间和握手时间

约20%需要1-RTT, 40%需要2个以上的RTT、相较DoT和DoH仍有较大优势

研究者发现由于建立连接、加密等的开销, DoH、DoT 的解析响应时间高于 Do53, 例如 Doan 等人 [55]发现包
含完整握手的 DoT 响应约比明文 DNS 慢 100 ms. 但在网页加载时间方面的差别并不明显, 在网络条件正常并选
择合适递归解析器的情况下, DoH 等在提供隐私保护的同时, 并不会使用户感知到明显的页面加载效率上的牺
牲 [49,52,54,56]. 这一方面是由于长连接机制可以一定程度上降低单个查询响应的连接开销, 另一方面提供商和应用厂
商在实现加密协议部署时会进行相应的技术优化, 例如 Firefox 可以一次并行解析多个域名, 其 Do53 和 DoT 中使
用了线程池进行同步解析, 而 DoH 实现是异步的, 并对 HTTP/2 协议进行了优化, 避免因为响应时间的延迟而过
多影响页面加载性能 [56]. 此外 Deccio 等人 [57]发现 TCP 快速打开 (TCP fast open, TFO) 在 DoH 和 DoT 递归解析器
中具有很低的支持率.
Kosek 等人 [51]通过比较 DoQ 和其他协议的往返时间和握手时间, 发现大约 20% 测量结果需要 1-RTT, 40%
的结果需要 2 个以上的 RTT, 虽然 DoQ 的性能表现不如预期, 但相较于 DoH 和 DoT, DoQ 仍表现出了较大的
优势.
4.2.2

网络条件对不同协议性能的影响

在不同网络条件带来的影响方面, 通过在模拟的蜂窝 4G 网络、有损蜂窝 4G 网络及 3G 网络中的测量,
Hounsel 等人 [56]发现在页面加载方面, 有损 4G 网络上 DoT 和 DoH 比 Do53 稍快, 他们分析原因是 TCP 和 UDP
之间处理 DNS 超时的差异. 对于 DoT 和 DoH, 由于 TCP 的缘故 DNS 数据包会在 2 倍往返时间的延迟后重传, 当
往返时间在数百毫秒数量级时, DoT 和 DoH 将比 Do53 更快地重传丢弃的数据包. 但是随着网络吞吐量的下降和
损耗增加, DoT 和 DoH 的性能表现有明显的下降, 这是由于加密协议发送的字节数更高, 导致大多数网站链接饱
和, 而且高延迟和随机丢包会进一步加剧对 TCP 的性能影响. 此外在有损网络条件下, DoH 具有更高的失败率和
错误率; 在对家庭网络的测量中, 随着递归解析器延迟的增加, 研究者也得到了相同的结果 [54].
4.2.3

服务商的选择对性能的影响

不同服务商之间以及同一服务商提供的不同协议之间可能存在很大的性能差异. 在页面加载时间方面, 对于
Cloudflare, 3 种协议的平均表现相似, 但是研究者发现 Google 的 DoH 和 Quad9 的 DoT 性能却明显较低, 这可能
跟缓存机制设置或连接到权威名称服务器失败后通过 Do53 触发重试有关 [56]. 同时由于较高比例的用户配置使用
Cloudflare Do53 和 Google Do53, 使得它们缓存了大量的热门域名, 二者的页面加载时间中位数比本地 Do53 递归
更短. 在针对美国两千多个家庭网络中应用的加密协议和传统 DNS 的性能测试实验中, 通过分析不同递归解析器
中 DoH、DoT 和传统 DNS 的连接建立时间开销、DNS 响应时间, 以及网络延迟和提供商选择对查询速率的影

张曼 等: DNS 信道传输加密技术: 现状、趋势和挑战

321

响, 研究者也得出了相同的结论, 发现不同递归解析器之间的性能差异较大 [54,58]. 此外在全球不同位置访问时, 部
分解析器性能表现出了明显的地区差异 [55]. DNS 客户端应定期进行延迟和响应时间测试, 以确定应选择哪种协议
和提供商, 没有单一的协议或服务能够在所有的客户端上表现最佳.
4.3 DNS 信道传输加密技术隐私保护效果分析
加密协议部署后其隐私保护效果如何, 对加密流量的识别和分析是否存在可能, 是值得关注的问题. 消息加密
后建议进一步通过填充机制来隐藏数据的真实属性信息 (参见第 4.3.1 节), 但填充机制也存在一定的局限性, 第
4.3.2 节探讨了对加密流量识别和分析的进展, 第 4.3.3 节对现有方案的不足进行了总结.
4.3.1

填充机制分析

即使 DNS 查询和响应消息都已加密, 但仍然可以使用元数据对这些消息进行识别和关联, 进而影响加密的
隐私效果, 例如加密 DNS 消息的大小和时间信息可以用来与递归解析器上游的未加密 DNS 请求相关联. DoH、
DoT 和 DoQ 都建议使用 RFC 7830[59]中定义的 EDNS(0) 填充扩展选项 (其结构如图 5 所示), 以允许客户端和服
务器通过可变长度的填充字节来手动增加 DNS 消息的长度, 如表 8 所示. 填充字节应设为 0x00, 但如果担心填充
部分在加密前被压缩可设为其他值. RFC 8467[60]进一步提出了填充数据的设置策略, 填充选项应放在 EDNS(0) 选
项空间的最后, 同时需要考虑填充消息大小和资源消耗的权衡, 以及不同传输协议的影响等. 其中列举了块长度填
充、最大长度填充、随机长度填充和随机块长度填充等填充机制, 并推荐使用块长度填充机制, 此外建议客户端
应将查询消息填充至最接近的 128 字节的倍数, 服务器端响应消息的填充块长度建议为 468 字节的倍数. 除 RFC
7830 中定义的填充选项外, QUIC 协议中定义了专门的填充帧, 使得 DoQ 还可以从数据包层面进行填充设置.
0

16
Option-code
Option-length
Padding

图5

EDNS(0) 填充扩展选项结构图
表8

4.3.2

填充机制

填充机制

描述

适用协议

EDNS(0)填充扩展选项

通过填充字节手动增加消息长度
填充数据的设置策略: 推荐块长度填充机制

DoH、DoT、DoQ

填充帧

QUIC中填充(PADDING)帧

DoQ

加密流量的识别和分析

影响加密协议隐私保护效果的关键因素是它们能否防止被识别和分析. 识别分为对正常和恶意加密流量的识
别, 例如从 HTTPS 流量中区分出哪些是 DoH, 识别隐藏在加密流量中的恶意活动等; 分析是在识别的基础上进一
步对用户活动细节进行探索, 例如对用户访问的网站、使用的客户端及具体应用进行破解.
填充机制虽然对加密流量的识别具有一定的抑制作用, 但其效果有限, 并不能完全避免对加密数据的分析 [61−64].
此外通过用一个递归解析器上的训练数据对另一个解析器上的流量进行识别, 研究者发现使用推荐的填充策略增
加了不同递归解析器之间流量的相似性 [61]. 同时通过测试发现有很高比例的 DoH 和 DoT 服务不支持填充功能.
从 HTTPS 流量中识别 DoH 数据这方面的研究主要基于二者的特征差异进行分析. 研究者发现相较于
HTTPS 流量, 单个 DoH 请求和响应至少包含多个数据包, 除重新连接及变更 DoH 服务器等特殊情况外, 浏览器
跟 DoH 服务器建立连接后会保持更长时间 [65]. 虽然文件下载、视频流传输时也会产生长连接, 但相比之下这些类
型的连接会在更短的时间内传输更多的数据.
Vekshin 等人 [65]利用 AdaBoost 决策树等 5 种机器学习算法从普通 HTTPS 流量中检测区分 DoH 通信, 并通

软件学报 2024 年第 35 卷第 1 期

322

过网络流量的行为特征来识别特定客户端. 他们用基尼指数计算后选取的 DoH 识别最重要的特征是流的持续时
间和平均包间延迟, 此外还包括响应数据包大小的方差、传入和传出数据量的对称性等. 在识别 DoH 客户端时选
取的主要特征是传入数据包大小的差异, 但是其能够很好分离客户端的原因在于不同浏览器对 EDNS 填充功能的
支持情况不同.
在对用户访问的网站进行识别方面, 主要方法是基于加密 DNS 信息构建网页指纹. Bushart 等人 [63]对 DNS 序
列进行建模, 通过请求间的依赖关系同时结合数据包大小和时间间隔等信息来识别用户访问的网站. 但他们的方
法也有一定的局限性, 在用户建模方面, 其未考虑特定网站加载时可能同步进行的其他 DNS 流量, 并且未考虑
DNS 和浏览器缓存的影响, 同时 DNS 消息可能在多个 TLS 记录中传输, 这种情况会影响消息大小提取的精确度.
Houser 等人 [64]对 DoT 的信息泄露进行评估, 其方法基于随机森林和 AdaBoost 分类器实现, 特征集由 DNS 消息的
时间序列构建, 包含时间戳、加密消息的长度和流量方向等, 并从中提取更高级别的特征, 例如查询或响应长度、
数据包总数、时间间隔或每秒查询数等, 进一步在此基础上计算一系列统计数据作为分类器参数. 孟德超等人 [66]
利用长短期记忆网络 (long short-term memory, LSTM) 对用户网站访问痕迹进行分析, 并且效果优于 Siby 等人 [67]
的方法.
Hoang 等人 [68]提出一种基于 IP 的网站指纹技术. 在指纹生成阶段, 选取共 220k 个热门网站和敏感网站, 分别
建立其主域名和从域名 (例如获取页面中的引用资源所涉及的域名) 集合, 以及对应的主从 IP 地址集合. 虽然访问
网页时部分域名请求顺序没有绝对固定的先后关系, 但根据浏览器的关键渲染路径, 某些域名请求之间仍会存在
依赖关系. Hoang 等人选取 domLoading、domContentLoaded 和 domComplete 这 3 个事件将域名和对应的 IP 地址
划分到 3 个不同的阶段以构建加强指纹. 在进行匹配时对于给定的 IP 地址序列, 先将主 IP 与所有指纹进行匹配,
获得候选集, 然后基于各个连接的时间序列利用 K-means 方法将其余 IP 进行聚类, 将分类后 3 个阶段的集合与候
选者集合分别取交集, 并计算信息熵, 值高者为最终结果. 最终识别的综合准确率为 91%, 未成功匹配的网站中有
不少候选域名都指向同一网站, 如同一公司在不同 TLD 下注册的域名或同一公司的不同名字. 上述部分方案的比
较如表 9 所示.
表9
方向

通过指纹构建分析用
户访问的网站

应用/客户端分析

4.3.3

协议
DoT[64]、DoH[63]

加密流量的分析
方法

特征

查询和响应长度及数目、查询和响应的累积字节数、
随机森林、AdaBoost分 数据包之间时间间隔、总传输时间、连续的查询或
响应组、单个TLS记录中的DNS消息数、每秒查询数、
类器等
接收前N字节的时间以及以上特征的统计值

DoH、DoT

基于IP构建网站指纹[68]

构建域名集合和IP集的对应关系, 并根据浏览器渲染
事件的逻辑关系进行阶段分类构建指纹

DoH、DoT

随机森林等[61]

TLS记录大小频率分布、DNS序列的距离、N-grams等

现有研究方案的局限和不足

之前已有较多网页指纹相关的研究, 但多是基于 HTTPS 进行构建, 相比之下利用 DNS 信息构建网站指纹可
能存在流量较小及消息大小区分度不明显等问题. 网页加载时会发送多个 HTTP 请求获取页面的所有对象, 由于
多个对象可能存储在同一主机上, 因此该过程中发送的 DNS 查询数目可能明显低于 HTTP 消息的数目; 另一方
面 HTTP 请求中字段设置更加多样化, 同时网页上不同资源对象大小差异较大, 可进一步增强其区分度.
此外网页指纹识别的准确率、稳定性、健壮性受到诸多方面因素的影响. 首先, 浏览器缓存会影响指纹识别
的准确率, 虽然较小的 TTL 值和浏览器的缓存分区策略使该影响有所减弱. 其次网站引用资源元素的改变或者域
名和 IP 映射关系的改变等因素会影响指纹的稳定性. 有研究者建议弱化网站的差异性, 如隐藏数据交换的数量、
为一组页面发送相同数目的查询或响应以及引入无关数据包调整时间间隔等, 但这可能会增加额外的网络负
担 [64]. 网站所有者和托管服务商可以从减小第三方资源的引用、增强网站内容的动态变化、广告拦截、增强域
名和 IP 对应关系的变化等方面加强网站的隐私防护.

张曼 等: DNS 信道传输加密技术: 现状、趋势和挑战

323

上述相关研究都对实验环境和威胁模型等条件进行了限制, 并且都在相对封闭的场景中进行指纹模型训练和
验证, 由于网站域名数据量巨大, 在完全开放的环境中进行域名识别比较困难. 此外主动攻击者在流量分析的基础
上可能会进一步创建有利于自身攻击的条件, 如通过发送 DNS 请求影响缓存等, 进一步增加加密流量被识别和分
析的风险. 当前研究在构建威胁模型时并未考虑将不同协议之间进行关联的场景, 主要基于加密 DNS 流量自身构
建网站指纹. 在 DNS 信道传输加密技术推出之前, 基于网站指纹已开展了大量的研究, 但主要关注 HTTPS 等协
议, 利用不同协议之间的数据关联能否创建更加准确的网站指纹是后续的一个探索方向.

5 DNS 信道传输加密技术的问题和挑战
本节主要对 DNS 信道传输加密技术可能存在的问题和挑战进行分析.
5.1 传输路径上其他影响隐私安全的因素
流量加密后仍存在其他可能泄露隐私的因素, 例如有研究者评估了使用加密解析代替明文传输能否避免路径
上的审查机制, 但单纯替换加密协议并没有取得理想效果 [69]. 因此加密协议需要与其他技术相互配合, 以更好地
保护隐私安全. 消息加密后仍存在隐私泄露的地方包括服务器名称标识 (第 5.1.1 节) 和客户端子网信息 (第 5.1.2
节) 等.
5.1.1

服务器名称标识

基于名称的虚拟主机是一种用于在单个服务器上托管多个域的方法, 因此在 TLS 握手完成之前服务器需要
一种机制来获取用户打算访问哪个域名, 以便提供正确的证书, 服务器名称标识 (server name indication, SNI) 扩展
被引入以解决上述问题. 由于当前 TLS 握手期间 SNI 扩展未加密, 路径上的观察者可以通过窃听 TLS 握手流量
来探测用户访问的域名. 加密 SNI (encrypted SNI, ESNI) 的草案 [70]正在推进, 以使 TLS 1.3 版本中支持 ESNI.
同时研究者对 DNS 信道传输加密技术和 ESNI 的部署能在多大程度上保护用户隐私进行了评估 [71], SNI 加
密后攻击者可以利用目标 IP 地址来推断用户访问的站点, 他们使用共同托管导致的 k 匿名度和 IP 地址变化的动
态程度两个指标来量化 ESNI 为不同托管和 CDN 服务商提供的隐私增益. 如表 10 所示, 通过在全球 9 个位置进
行测试, 发现部署加密 DNS 和 ESNI 后, 其选取的域中有约 20% 由于独占一个或多个 IP 地址从而不会获得任何
隐私优势, 约 30% 的 k 值大于 100 的域将获得显著的隐私优势, 隐私得到明显改善的域远不那么受欢迎. 同时约
81% 的多主机域共同托管在 30% 的 IP 地址上, 通过分析域名对应的 IP 地址的动态变化程度, 发现其中只有
7.7% 以天粒度更改托管的 IP 地址. 但是他们的测试结果中不考虑共同托管网站之间的可区分性 (例如人气排名、
网站敏感度和网络流量模式等), 以及利用特定于页面的属性构建的网页指纹等信息, 实际攻击者可以基于上述信
息做进一步分析.
表 10

加密机制和 ESNI 部署后域名获得的隐私保护效果分析
域名占比 占有IP的比例

域分类

特点

单一托管域

单个域名独占一个或多个IP

约20%

域名对应的IP托管域名数目大于1且小于100

约50%

域名对应的IP托管域名数目大于100

约30%

多托管域

5.1.2

70%
共30%

属性

隐私增益效果

热门域名

未获得隐私增强

非热门域名 获得一定程度的隐私增强
非热门域名

获得较明显的隐私增强

客户端子网

许多权威名称服务器根据感知的用户拓扑位置返回不同的响应, 然而递归解析器发送的源地址一般是其自身
地址, 并且很多递归解析器在拓扑上并不接近真实的用户查询源. ECS (EDNS client subnet, 客户端子网扩展) 选项
扩展 [72]使得递归解析器能够在请求中加入查询来源的原始地址信息. 递归与权威服务器通信时的源 IP 地址一定
程度上隐藏了真实的用户, 但客户端子网 EDNS0 选项则会暴露用户信息.
Poitrey[73]认为 ECS 的主要问题是隐私泄露和 DNS 缓存碎片, 同时发现排名前 100 万的支持 ECS 的域名中有
超过一半从不同位置的子网查询时都会返回相同的结果, 但却要按子网存储多个相同结果的副本. 他们提出的解

软件学报 2024 年第 35 卷第 1 期

324

决方案包括子网聚合后进行重新映射和建立白名单机制. 根据 GeoIP 将客户端原始的子网映射到基于位置的编码
块 (格式为 ASN:Country[:Metro Code]), 并以此作为 key, 相同的 key 映射到同一个随机选择的子网上, 以替代原来
的子网信息. 该机制避免客户端子网信息泄露, 同时一定程度上提升了缓存利用率.
5.2 部署规范和集中化加剧问题
递归解析器在部署时使用无效证书的问题较为突出, 2020 年的扫描中发现使用无效证书的 DoT 服务占比近
29%, 其中大部分为自签名证书, 除此之外还包括证书过期和信任链不完整等 [49] . 文献 [48] 发现 10.2% 的开放
DoH 递归解析器和 60.7% 的开放 DoT 递归解析器使用无效证书. 同时有较高比例的 DoH 和 DoT 服务不支持填
充功能.
递归解析器部署方面的另一问题是集中化, DNS 服务的集中化问题一直存在, 根据分析 5 个大型的云服务商
负责荷兰和新西兰两个 TLD 的 30% 的请求 [74]. 相较于传统 DNS, DoH 将递归解析器选择控制权转移到浏览器供
应商或其他应用提供商, 少数头部公司在服务器数目和流量占比上占有主导地位, 同时由于其在 Do53 中积累下
大量数据和用户优势, 更容易获得用户的选择. 部署集中化问题进一步对递归解析器端的用户数据隐私保护提出
了挑战.
美国电信协会等组织抗议谷歌在其浏览器和安卓系统中支持 DoH, 他们认为由于 Chrome 浏览器和安卓系统
具有强大的用户占有率, 谷歌对加密协议的部署将加剧 DNS 的集中化, 使得一个公司拥有大量的 DNS 数据, 损害
了广告和其他行业的竞争公平性 [75].
5.2.1

系统部署架构优化

Hounsel 等人 [76]认为 DNS 信道传输加密协议的部署在某些情况下加剧了 DNS 的集中化, 其对 DNS 递归解
析器架构的设计和实现进行重构, 提出去中心化的名称解析. 他们增加了额外的分发策略, 并将名称解析决策的控
制权从单个应用程序中转移出来, 设计支持基于规则的 DNS 服务选择策略 (例如匹配客户端 MAC 或 IP 地址等),
同时测试了基于 HASH 的分发、随机分发、循环分发等几种查询分发策略.
Hoang 等人 [77]提出一种 K-resolver DNS 解析机制, 利用桶哈希机制将 DNS 查询分散到多个 DoH 递归解析器
中, 避免单个递归解析器了解用户的整个 Web 浏览记录后构建完整的用户档案. 在性能方面, 由于所选 DoH 服务
器的地理位置原因, K-resolver 机制对 DNS 解析时间和网页加载时间有一些影响, 当有完善的任播服务器时, 该方
法产生的额外开销可大幅降低.
跨多个服务器分发 DNS 查询是改善互联网用户隐私的一个研究方向. 但在制定具体的分发策略时还有很多
问题需要注意, 单纯的循环、随机分发等并不能起到隐私保护的效果, 同时需要关注其对性能的影响, 以及是否会
对 CDN 本地化产生负面影响等. 此外访问特定网页时由于第三方资源引用等产生的多个域名查询请求能够被用
作网页指纹, DNS 预取也会向递归解析器泄露用户后续可能要访问的网页的域名, 制定分发策略时也需要将上述
因素考虑在内.
5.3 恶意流量对加密技术的利用和攻击
5.3.1

加密技术被恶意流量利用

加密技术在给合法用户提供隐私安全的同时其数据加密的特性也为恶意软件及攻击者提供了可乘之机. 例如
安全公司发现恶意软件 Godlua 利用 DoH, 把非法服务器 URL 存储在特定资源记录中, 并将该资源记录的获取过
程进行隐藏 [78]; 伊朗黑客组织 Oilrig 利用 DoH 防止其盗取数据的传输过程被监测 [79]. 流量加密使得原有针对明
文 DNS 的检测方法中使用的特征被隐藏 [80].
Patsakis 等人 [81]研究了如果恶意软件使用保护隐私的 DNS 服务来解析其 C&C 服务器是否会绕过当前的安
全机制, 并讨论了 DNS 信道传输加密对 DGA 检测带来的影响. 他们分析了不同数据集的响应数据包大小分布的
范围、标准差, 发现 DGA 生成静态长度或长度变化差异较小的域时, 基于数据包的流量分析可实现高准确率的
恶意流量区分. 进一步的他们将僵尸网络流量构建成时间序列变量, 使用 Hodrick-Prescott 过滤器去除流量噪声影
响, 在此基础上进行趋势和自相关性分析, 并利用自回归滑动平均模型系数来构建失陷指标, 结果表明该方法可以

张曼 等: DNS 信道传输加密技术: 现状、趋势和挑战

325

识别异常活动并标记僵尸网络感染, 但无法有效地将攻击归因于特定的僵尸程序. 张千帆等人 [82]选取时间序列等
特征, 通过 KNN 设计了在 DoH 中检测 DGA 流量的方法, 准确率为 79%.
Banadaki 等人 [83]构建一种两层模型来检测恶意 DNS 流量, 首先将 DoH 流量和非 DoH 流量进行分类, 在此基
础上在第 2 层进行恶意 DoH 和良性 DoH 的识别, 在该方法中选取源 IP、目的 IP 及数据包长度中位数等 34 种特
征, 并采用 XGBoost 等 6 种算法. MontazeriShatoori 等人 [84]同样基于两层模型构建分类器进行 DNS 隧道检测.
Singh 等人 [85]与 Banadaki 选用了相同的数据集, 但在算法的选择上有所不同, 其选择梯度提升树、随机森林等 4
种算法. Kwan 等人 [86]对一个支持加密 DNS 隧道的原型工具的抗审查性进行了评估, 并基于数据包大小、吞吐量
等特征构建基于阈值的 DNS 隧道检测机制.
Zhan 等人 [87] 通过客户端 TLS 指纹并基于流特征构建分类器来进行 DoH 隧道检测, 他们选取的特征分为
TLS 记录长度和时间间隔两大类, TLS 记录长度可以间接的反映域名长度信息, 同时相较 DNS 隧道流量, 正常
DNS 发送的请求随机性更高, 并且由于缓存机制收到响应的平均时间间隔更短. 上述两类特征分别包括查询、响
应和流上所有数据包的 TLS 记录长度, 以及两个连续查询/响应之间的时间间隔、相邻一对查询和响应之间的时
间间隔和对应的统计值. 通过对准确率、精确率、召回率、F1-score 等方面的评估, 上述方法取得了较好的实验
结果. 由于机器学习方法中特征的提取和构建复杂度高, Ding 等人 [88]利用变分自编码器和双向 GRU 网络来构建
能够自动进行特征学习的 DNS 隧道检测方法, Nguyen 等人 [89]在企业网络中构建基于 Transformer 的 DoH 隧道攻
击检测系统. 上述方法的比较和总结如表 11 所示.
表 11
方向
HTTP流量中检
测DoH

协议
DoH

加密流量识别

方法

特征

AdaBoost决策树、C4.5决策树等5 流的持续时间、平均包间延迟、响应数据包大小的方差、
种机器学习算法[65]
传入和传出数据量的对称性等
两层模型; XGBoost、LightGBM、 源IP、目的IP、端口、数据包长度、时间戳、持续时间、
梯度提升树、随机森林等[83,85]
发送和接收字节量、方差标准差等统计指标

查询、响应流上所有数据包的TLS记录长度; 两个连续查
Boosting决策树、随机森林、逻辑
询/响应之间的时间间隔; 相邻一对查询和响应之间的时间
[87]
恶意流量识别 恶意DoH检测 回归
间隔; 每个流级特征的最小值、最大值、平均值、标准差
变分自编码器和双向GRU网络[88];
特征自学习; 连接时长、发送接收字节数及速率、数据包
基于Transformer构建DNS隧道检测
长度及响应时间差异等统计指标
[89]
系统

5.3.2

降级攻击问题

DNS 信道传输加密技术各协议都支持机会主义隐私设置方案, 浏览器等应用在采用加密 DNS 解析时大多默
认开启了该机制. 机会主义隐私设置的初衷是允许在某些隐私性要求不高或条件不具备的场景下能正常为用户提
供服务, 但是为了规避加密协议提供的保护, 攻击者可能利用该机制对加密服务进行降级. 如第 3.1 节所述, DoH
通信过程中, 在 URI 解析阶段攻击者可以对 DNS 消息进行过滤和拦截, 或通过缓存中毒的方法使用虚假 IP 地址
替换目标 DoH 服务器的 IP 地址, 进而影响后续客户端与 DoH 递归解析器的正常连接. 双方通信阶段攻击者可以
通过 TCP 流量数据拦截或篡改等使 DoH 回退到传统 DNS. 攻击者可获取 TCP 报头中的序列号和确认号, 并向受
害者或 DoH 递归解析器发送伪造的 TCP 重置数据包, 以诱使他们切断 TCP 连接 [90].
Huang 等人 [90]对 DoH 的降级攻击进行了分析, 他们选取了 Chrome 等 6 款常用的浏览器, 并选用连续请求周
期、请求间隔时间设置、最大时间间隔 3 个属性测试面对不同攻击方法时各浏览器的反应. 发生降级后没有任何
浏览器会尝试通知用户, 部分浏览器需要很长时间才能恢复到 DoH.
5.4 隐私和网络安全管理的矛盾
章坚武等人 [91]和胡宁等人 [92]对 DNS 各种攻击类型和检测技术进行了总结和分析, 当前家长控制、企业防火
墙、病毒防护等安全软件都依赖未加密的 DNS 通信来过滤流量或实施安全防护策略, 加密协议的应用使得网络

软件学报 2024 年第 35 卷第 1 期

326

管理员和技术团队对网络安全的精准管控变得困难. 隐私和网络安全的平衡位置应在哪里各方持不同态度, 完全
加密的机制对网络安全管理造成了一定的负面影响.
部分研究者建议设置代理系统作为中间人解密 HTTPS 流量, 以对流量执行深度数据包检查, 但是该方法同样
面临一些问题, 首先涉及开放网络中的证书部署及身份认证信息的管理, 此外 TLS 会话密钥等数据的存储及会话
解密将消耗大量的存储和计算资源, 同时对性能也会产生一定的影响 [80].
此外信道传输加密协议对客户端到递归解析器之间的通信过程进行加密, 但递归解析器端仍可对响应进行查
看甚至修改, 以实现访问控制和安全管理. Jin 等人 [69]通过大规模的测试来评估加密协议中 DNS 操纵的情况. 研究
者基于数千个递归解析器及 740 万次 DNS 查询测试, 发现 1.66% 的 DoT 响应和 1.42% 的 DoH 响应进行了 DNS
操纵. 最严重的 DoT 递归解析器操纵了 728 个域名的响应. 并且被操纵的响应来自超过 2/3 的 DoT 和 DoH 递归
解析器, 与之前研究 11% 的测试结果 [93]相比, DNS 操纵在加密协议中比在传统 DNS 更普遍. 同时同一提供商下
不同递归解析器审查域的数量可能存在很大的差异. 对 DNS 信道传输加密技术存在的问题和挑战的分类总结如
表 12 所示.
表 12
问题和挑战

DNS 信道传输加密技术的问题和挑战

表现

服务器名称标识泄露目标
加密后其他影响隐 服务器名字
私安全的因素
客户端子网
部署规范问题
部署集中化问题

相关研究/改进方案
对服务器名称标识进行加密; SNI加密后共同托管的匿名度和IP地址动态变化
对隐私增益的影响[71]
客户端子网信息进行重新映射, 缓解信息泄露[73]

无效证书的问题; 部分不支
存在自签名证书、证书过期、信任链不完整等[49,48]
持填充机制
递归解析器的选择配置权
限转移等因素加剧集中化

恶意流量利用加密技术隐
恶意流量对加密技 藏自身的攻击行为
术的利用和攻击
降级攻击
隐私和网络安全管 加密对网络安全防护技术
理的矛盾
提出了新的要求

跨递归解析器进行DNS查询的分发[76,77]
对利用加密协议进行隐蔽通信的僵尸网络(例如利用加密协议隐藏与C&C服
务器的通信等)、DGA等进行检测[81]; 机器学习、神经网络等方法识别恶意加
密DNS流量[83−88], 恶意流量检测两层模型(首先识别DoH流量, 进而区分良性和
恶意DoH流量)[83,84]
机会主义隐私配置支持仅加密无身份认证, 或退至明文传输, 机会加密配置被
攻击者利用[90]
加密技术部署后攻击检测技术、安全监管和网络管理机制需要进行改进; 加
密协议部署后递归解析器端的DNS操纵[69]

6 趋势与研究方向展望
DNS 信道传输加密协议推出后, 未来待解决的问题和后续其他研究方向主要包括以下方面.
6.1 加密 DNS 服务器发现
由于明文 DNS 传输已广泛部署多年, 加密协议推出后如何在当前传统递归解析器广泛部署的环境下进行过
渡和转换是需要解决的问题. 首先客户端如何发现支持加密机制的 DNS 递归解析器, 以及上述解析器的参数信息
如何传递给客户端用于其选择决策. 其次非加密递归解析器升级或服务商推出了新的加密递归解析器后, 用户如
何能够及时地发现并更换使用新的加密服务. 同时某些应用软件或 APP 厂商, 以及一些公司可能希望用户通过特
定的加密递归解析器来访问自己的应用或域名. 最后通过其他协议或公共列表获取到加密递归解析器名字后, 用
户需要能够获取该解析器更加详细的信息, 并对其进行身份验证. 此外权威服务器支持加密后也面临如何进行信
息发布的问题.
加密递归解析器发现的相关研究包括两个方向, 基于递归解析器的发现和基于网络的发现. Pauly 等人 [94]提
出一种利用 SVCB 资源记录来发现 DNS 信道传输加密配置的机制, 该方法支持从非加密递归解析器探测对应的
加密递归解析器, 同时可以发现和选择该解析器支持的多种加密协议, 其适用于非加密和对应的加密递归解析器

张曼 等: DNS 信道传输加密技术: 现状、趋势和挑战

327

由同一实体管理的场景. Boucadair 等人 [95] 通过定义新的 DHCP、DHCPv6 和 IPv6 路由通告选项来发现加密
DNS 递归服务器, 定义的选项扩展字段中包括加密递归解析器的完整域名、IPv6 地址及一些服务参数信息. 针对
当前提出的加密递归解析器发现机制不支持客户端通过本地转发器与上层解析器进行通信的场景, Schwartz 等
人 [96]提出本地转发器通过私有 IP 地址指定时, 采用基于加密 DNS 的机会隐私配置来执行宽松的验证策略, 去除
证书校验过程. 同时某些 DNS 转发器会起到阻止访问恶意软件或其他威胁域的作用, 并在学校、公司等环境下对
提供特定服务的域进行禁止, 应确保此服务不会因跨转发器升级到对应的加密解析服务而丢失 [96].
6.2 递归解析器到权威服务器之间的加密
虽然 DoQ 等协议也支持递归解析器到权威服务器之间的隐私安全保护, 但相较于客户端到递归解析器的通
信路径, 递归解析器和权威服务器之间信道传输加密技术的应用面临更多困难, 也是未来值得关注和研究的方向.
一方面 DNS 解析时若未命中缓存, 递归解析器会依次向根服务器、顶级域服务器、二级域权威服务器发送查询
请求, 为增强隐私保护防止消息 (例如查询的目标域名等) 泄露, 上述路径都需要实现加密机制, 同时隐私保护效
果跟权威服务器服务域名的数目有关. 另一方面由于下一级查询依赖于上一级查询的响应, 如果当前路径未部署
加密认证机制, 若遭受攻击导致响应被篡改为攻击者服务器, 则下游路径即便已部署加密协议其安全性也无法保
证. 上级路径未部署时缺少相应的机制来向下游通信路径传达自身响应的可信性证明.
此外在加密权威服务器的属性等信息如何告知递归解析器方面, 可通过服务端发布或递归解析器端主动查询
等方式, 但都存在实际的问题, 虽然已定义了服务绑定资源记录, 但 EPP 等协议暂未提供对其的支持, 一定程度上
阻碍了加密协议的应用部署进程.
2021 年根服务器运营商在针对加密机制发布的声明 [97]中表述了对加密、连接状态维护等带来的性能影响和
可能引发的新型拒绝服务攻击的担忧, 同时表示根服务器运营商不愿成为加密机制的早期实践方, 并建议可采用
其他机制 (例如维护本地根区副本 [98]) 提高隐私保护.
基于上述问题, Gillmor 等人 [99]提出递归解析器和权威服务器通信时使用单边部署支持机会加密配置, 由于
DoH 中需要知道服务器的 URL, 目前暂未有机制使得客户端能单方面探测到 DoH 权威服务器, 因此研究者选用
了 DoT 和 DoQ 协议.
6.3 DNS 服务器端的隐私保护
DoH、DoT 和 DoQ 避免了路径上的窥探和数据篡改, 但是主要的加密递归解析器掌握在少数几家服务提供
商 (例如 Cloudflare、Google、Quad9 等) 手中, 部署的集中化使得服务器掌握大量数据, 能够方便地进行数据收集
和分析, 进而挖掘用户的敏感信息. 递归解析器端隐私泄露问题的一种解决途径是建立数据管理维护的规范和机
制, 例如 Mozilla 围绕数据保留、汇总、销售或转让等操作定义了一套限制条件, 要求浏览器的 DoH 递归解析器
遵守他们制定的可信递归解析器策略文档 [100]中概述的隐私要求. 这种方法以政策和合约等为基础建立信任机制,
但它们难以验证且缺乏执行手段, 探索有效的协议和机制来加强递归解析器和权威服务器端的隐私保护也是未来
的研究方向.
ODoH[101,102]在 DoH 的基础上进行了扩展, 通过增加代理来避免任何一方同时掌握客户端 IP 地址以及对应
的 DNS 请求和响应的内容, 防止递归解析器利用信息关联进行用户活动分析. ODoH 中引入了目标对象的概念,
它表示递归解析器或转发器. 客户端将 DNS 查询用 HTTP POST 请求形式发送给代理, 其中通过定义的目标主机
和目标路径两个字段指示将消息发送至哪个目标对象. 代理收到客户端发来的请求后根据上述信息与目标对象建
立连接. 客户端通过代理发送给目标对象的 DNS 查询消息由公钥进行加密, 确保只有指定的目标对象才能解析客
户端的请求内容. 目标对象发送的响应也通过代理返回给客户端, 整个过程中代理不应将可能会泄露客户端身份
的有关信息发送给目标对象. 同时有研究者提出 DoHoT (DNS over HTTPS over Tor, 基于 Tor 的 DoH), 但其在
DNS 响应时间和网页加载时间等方面对性能有较大的影响 [103].
6.4 HTTP/3 和 DoH3
DoH 协议提出时 HTTP/3 尚未发布, HTTP/3 基于 QUIC 和 UDP 实现, QUIC 致力于替代 TCP 协议并且发展

软件学报 2024 年第 35 卷第 1 期

328

迅速, 目前已被国内外众多公司部署应用, 例如其在 Facebook 中的流量占比已超过 75%[104], 并在音视频、直播、
图文下载及不稳定的网络环境下明显的改善错误率和卡顿率等指标. NextDNS 已尝试推出 DoH3 服务, 今年 7 月
Google 宣布在 Android 平台支持 DoH3[105]. 但由于 HTTP/3 刚正式发布, 大部分服务提供商暂未提供对 DoH3 功
能的支持. 未来 QUIC 协议及 HTTP/3 的发展是否会在一定程度上推动 DoQ 和 DoH3 的部署, 以及其性能和隐私
安全表现及优化等也是值得关注的方向.

7 总

结

对明文 DNS 的隐私和安全保护机制一直在迭代探索中发展, DNS 信道传输加密协议自发布后得到了广泛的
关注和大规模的应用部署. 本文分析了 DNS 信道传输加密协议的技术原理及应用现状, 并从不同的指标、网络设
施环境等方向对性能影响进行讨论; 进而对信道传输加密技术的隐私保护效果进行分析, 最后从传输路径上其他
影响隐私安全的因素、部署规范和集中化加剧、恶意流量对加密技术的利用和攻击、隐私和网络安全管理的矛
盾等方面分析总结了 DNS 信道传输加密技术所面临的问题和挑战及对应的解决方案, 并从加密解析器发现、递
归解析器端的隐私保护、递归解析器到权威服务器之间的加密等方面对未来的趋势和研究方向进行展望.
References:
[1]

CrowdStrike. Widespread DNS hijacking activity targets multiple sectors. 2019. https://www.crowdstrike.com/blog/widespread-dnshijacking-activity-targets-multiple-sectors/

[2]
[3]

Fouchereau R. IDC 2022 global DNS threat report. 2022. https://www.efficientip.com/resources/idc-dns-threat-report-2022/
Help Net Security. Healthcare suffering from DNS attacks more than other industries. 2021. https://www.helpnetsecurity.com/2021/07/
15/healthcare-dns-attacks/

[4]

McCarthy K. Internet’s root servers take hit in DDoS attack. 2015. https://www.theregister.com/2015/12/08/internet_root_servers_ddos/

[5]

Hesselman C, Kaeo M, Chapin L, Claffy K, Seiden M, Mcpherson D, Piscitello D, Mcconachie A, April T, Latour J, Rasmussen R. The
DNS in IoT: Opportunities, risks, and challenges. IEEE Internet Computing, 2020, 24(4): 23–32. [doi: 10.1109/MIC.2020.3005388]

[6]

DNSmezzo. DNSmezzo form AFNIC project DNSwitness. 2022. https://github.com/dsutto/DNSmezzo

[7]

The Tcpdump Group. Tcpdump. 2021. https://www.tcpdump.org/

[8]

Wang WT, Hu N, Liu B, Liu X, Li SD. Survey on technology of security enhancement for DNS. Ruan Jian Xue Bao/Journal of
Software, 2020, 31(7): 2205–2220 (in Chinese with English abstract). http://www.jos.org.cn/1000-9825/6046.htm [doi: 10.13328/j.cnki.
jos.006046]

[9]

Huang K, Kong N. Research on status of DNS privacy. Computer Engineering and Applications, 2018, 54(9): 28–36 (in Chinese with
English abstract). [doi: 10.3778/j.issn.1002-8331.1801-0101]

[10]

DNSCrypt. 2021. https://dnscrypt.info/

[11]

DNSCurve: Usable security for DNS. 2021. https://dnscurve.org/

[12]

Bortzmeyer S. RFC 7816 DNS query name minimisation to improve privacy. 2016. https://www.rfc-editor.org/info/rfc7816

[13]

Arends R, Austein R, Larson M, Massey D, Rose S. RFC 4033 DNS security introduction and requirements. 2005. https://www.rfceditor.org/info/rfc4033

[14]

Arends R, Austein R, Larson M, Massey D, Rose S. RFC 4034 Resource records for the DNS security extensions. 2005. https://www.rfceditor.org/info/rfc4034

[15]

DNSSEC deployment report. 2023. https://rick.eng.br/dnssecstat/

[16]

Hoffman P, Schlyter J. RFC 6698 The DNS-based authentication of named entities (DANE) transport layer security (TLS) protocol:
TLSA. 2012. https://www.rfc-editor.org/info/rfc6698

[17]

Daigle L. RFC 3912 WHOIS protocol specification. 2004. https://www.rfc-editor.org/info/rfc3912

[18]

Newton A, Ellacott B, Kong N. RFC 7480 HTTP usage in the registration data access protocol (RDAP). 2015. https://www.rfc-editor.
org/info/rfc7480

[19]

Hollenbeck S, Kong N. RFC 7481 Security services for the registration data access protocol (RDAP). 2015. https://www.rfc-editor.org/
info/rfc7481

[20]

Loibl A. Namecoin. In: Proc. of the 2014 Seminars FI/IITM SS Network Architectures and Services. 2014. 107–113. [doi: 10.2313/NET2014-08-1_14]

张曼 等: DNS 信道传输加密技术: 现状、趋势和挑战

[21]

329

Ali M, Nelson J, Shea R, Freedman MJ. Blockstack: A global naming and storage system secured by blockchains. In: Proc. of the 2016
USENIX Annual Technical Conf. Denver: USENIX Association, 2016. 181–194.

[22]

Bloackstack name service. 2021. https://docs.blockstack.org/core/naming/introduction.html

[23]

Handshake. 2021. https://handshake.org/files/handshake.txt

[24]

Ethereum Name Service Document. 2021. https://docs.ens.domains/

[25]

The Ethereum Name Service Constitution. 2021. https://ensdao.eth.limo/constitution.pdf

[26]

Schanzenbach M, Grothoff C, Fix B. The GNU name system. 2022. https://datatracker.ietf.org/doc/html/draft-schanzen-gns-19

[27]

The GNU name system. 2022. https://www.gnunet.org/en/gns.html

[28]

Wicinski T. RFC 9076 DNS privacy considerations. 2021. https://www.rfc-editor.org/info/rfc9076

[29]

Hoffman P, McManus P. RFC 8484 DNS Queries over HTTPS (DoH). 2018. https://www.rfc-editor.org/info/rfc8484

[30]

Curl. Publicly available servers. 2021. https://github.com/curl/curl/wiki/DNS-over-HTTPS

[31]

Hu Z, Zhu L, Heidemann J, Mankin A, Wessels D, Hoffman P. RFC 7858 specification for DNS over transport layer security (TLS).
2016. https://www.rfc-editor.org/info/rfc7858

[32]

Dickinson S, Gillmor D, Reddy T. RFC 8310 usage profiles for DNS over TLS and DNS over DTLS. 2018. https://www.rfc-editor.org/
info/rfc8310

[33]

Reddy T, Wing D, Patil P. RFC 8094 DNS over datagram transport layer security (DTLS). 2017. https://www.rfc-editor.org/info/
rfc8094

[34]

Toorop W, Dickinson S, Sahib S, Aras P, Mankin A. RFC 9103 DNS zone transfer over TLS. 2021. https://www.rfc-editor.org/info/
rfc9103

[35]

Iyengar J, Thomson M. RFC 9000 QUIC: A UDP-based multiplexed and secure transport. 2021. https://www.rfc-editor.org/info/rfc9000

[36]

Thomson M, Turner S. RFC 9001 using TLS to secure QUIC. 2021. https://www.rfc-editor.org/info/rfc9001

[37]

Iyengar J, Swett I. RFC 9002 QUIC loss detection and congestion control. 2021. https://www.rfc-editor.org/info/rfc9002

[38]

Bishop M. RFC 9114 HTTP/3. 2022. https://www.rfc-editor.org/info/rfc9114

[39]

Huitema C, Dickinson S, Mankin A. RFC 9250 DNS over dedicated QUIC connections. 2022. https://www.rfc-editor.org/info/rfc9250

[40]

Dierks T, Rescorla E. RFC 5246 The transport layer security (TLS) protocol version 1.2. 2008. https://www.rfc-editor.org/info/rfc5246

[41]

Rescorla E. RFC 8446 The transport layer security (TLS) protocol version 1.3. 2018. https://www.rfc-editor.org/info/rfc8446

[42]

Duckett C. Google public DNS gets DNS-over-TLS treatment. 2019. https://www.zdnet.com/article/google-public-dns-gets-dns-over-tlstreatment/

[43]

Deckelmann S. Firefox continues push to bring DNS over HTTPS by default for US users. 2020. https://blog.mozilla.org/blog/2020/02/
25/firefox-continues-push-to-bring-dns-over-https-by-default-for-us-users/

[44]

Chromium blog: A safer and more private browsing experience with secure DNS. 2020. https://blog.chromium.org/2020/05/a-safer-andmore-private-browsing-DoH.html

[45]

Apple enable encrypted DNS. 2020. https://developer.apple.com/videos/play/wwdc2020/10047/

[46]

DNS over TLS support in Android developer preview. 2018. https://android-developers.googleblog.com/2018/04/dns-over-tls-support-inandroid-p.html

[47]

Improving DNS configuration in Settings. 2020. https://blogs.windows.com/windows-insider/2020/08/05/announcing-windows-10insider-preview-build-20185/

[48]

Luo M, Yao YP, Xin LL, Jiang ZW, Wang QY, Shi WC. Measurement for encrypted open resolvers: Applications and security.
Computer Networks, 2022, 213: 109081. [doi: 10.1016/j.comnet.2022.109081]

[49]

Lu CY, Liu BJ, Li Z, Hao S, Duan HX, Zhang MM, Leng CY, Liu Y, Zhang ZF, Wu JP. An end-to-end, large-scale measurement of
DNS-over-encryption: How far have we come? In: Proc. of the 2019 Internet Measurement Conf. Amsterdam: ACM, 2019. 22–35. [doi:
10.1145/3355369.3355580]

[50]

García S, Hynek K, Vekshin D, Čejka T, Wasicek A. Large scale measurement on the adoption of encrypted DNS. arXiv:2107.04436,
2021.

[51]

Kosek M, Doan TV, Granderath M, Bajpai V. One to rule them all? A first look at DNS over QUIC. In: Proc. of the 23rd Int’l Conf. on
Passive and Active Network Measurement. Springer, 2022. 537–551. [doi: 10.1007/978-3-030-98785-5_24]

[52]

Böttger T, Cuadrado F, Antichi G, Fernandes EL, Tyson G, Castro I, Uhlig S. An empirical study of the cost of DNS-over-HTTPS. In:
Proc. of the 2019 Internet Measurement Conf. Amsterdam: ACM, 2019. 15–21. [doi: 10.1145/3355369.3355575]

[53]

Firefox nightly secure DNS experimental results. 2018. https://blog.nightly.mozilla.org/2018/08/28/firefox-nightly-secure-dnsexperimental-results/

软件学报 2024 年第 35 卷第 1 期

330

[54]

Hounsel A, Schmitt P, Borgolte K, Feamster N. Can encrypted DNS be fast? In: Proc. of the 22nd Int’l Conf. on Passive and Active
Network Measurement. Springer, 2021. 444–459. [doi: 10.1007/978-3-030-72582-2_26]

[55]

Doan TV, Tsareva I, Bajpai V. Measuring DNS over TLS from the edge: Adoption, reliability, and response times. In: Proc. of the 22nd
Int’l Conf. on Passive and Active Network Measurement. Springer, 2021. 192–209. [doi: 10.1007/978-3-030-72582-2_12]

[56]

Hounsel A, Borgolte K, Schmitt P, Holland J, Feamster N. Comparing the effects of DNS, DoT, and DoH on Web performance. In:
Proc. of the 2020 Web Conf. Taipei: ACM, 2020. 562–572. [doi: 10.1145/3366423.3380139]

[57]

Deccio C, Davis J. DNS privacy in practice and preparation. In: Proc. of the 15th Int’l Conf. on Emerging Networking Experiments and
Technologies. Orlando: ACM, 2019. 138–143. [doi: 10.1145/3359989.3365435]

[58]

Borgolte K, Chattopadhyay T, Feamster N, Feamster N, Kshirsagar M, Holland J, Hounsel A, Schmitt P. How DNS over HTTPS is
reshaping privacy, performance, and policy in the Internet ecosystem. In: Proc. of the 47th Research Conf. on Communications,
Information and Internet Policy. 2019. [doi: 10.2139/ssrn.3427563]

[59]

Mayrhofer A. RFC 7830 The EDNS(0) padding option. 2016. https://www.rfc-editor.org/info/rfc7830

[60]

Mayrhofer A. RFC 8467 Padding policies for extension mechanisms for DNS (EDNS(0)). 2018. https://www.rfc-editor.org/info/rfc8467

[61]

Mühlhauser M, Pridöhl H, Herrmann D. How private is Android’s private DNS setting? Identifying apps by encrypted DNS traffic. In:
Proc. of the 16th Int’l Conf. on Availability, Reliability and Security. New York: ACM, 2021. 1–10. [doi: 10.1145/3465481.3465764]

[62]

Hynek K, Cejka T. Privacy illusion: Beware of unpadded DoH. In: Proc. of the 11th IEEE Annual Information Technology, Electronics
and Mobile Communication Conf. Vancouver: IEEE, 2020. 621–628. . [doi: 10.1109/IEMCON51383.2020.9284864]

[63]

Bushart J, Rossow C. Padding ain ’t enough: Assessing the privacy guarantees of encrypted DNS. In: Proc. of the 10th USENIX
Workshop on Free and Open Communications on the Internet. Santa Clara: USENIX Association, 2020.

[64]

Houser R, Li Z, Cotton C, Wang HN. An investigation on information leakage of DNS over TLS. In: Proc. of the 15th Int’l Conf. on
Emerging Networking Experiments and Technologies. Orlando: ACM, 2019. 123–137. [doi: 10.1145/3359989.3365429]

[65]

Vekshin D, Hynek K, Cejka T. DoH insight: Detecting DNS over HTTPS by machine learning. In: Proc. of the 15th Int ’l Conf. on
Availability, Reliability and Security. ACM, 2020. 87. [doi: 10.1145/3407023.3409192]

[66]

Meng DC, Zou FT. DNS privacy protection security analysis. Communications Technology, 2020, 53(2): 445–449 (in Chinese with

[67]

Siby S, Juarez M, Diaz C, Vallina-Rodriguez N, Troncoso C. Encrypted DNS→privacy? A traffic analysis perspective. In: Proc. of the

English abstract). [doi: 10.3969/j.issn.1002-0802.2020.02.028]
27th Annual Network and Distributed System Security Symp. San Diego: The Internet Society, 2020. [doi: 10.14722/ndss.2020.24301]

[68]

Hoang NP, Niaki AA, Gill P, Polychronakis M. Domain name encryption is not enough: Privacy leakage via IP-based website
fingerprinting. Proc. on Privacy Enhancing Technologies, 2021, 2021(4): 420–440. [doi: 10.2478/popets-2021-0078]

[69]

Jin L, Hao S, Wang HN, Cotton C. Understanding the impact of encrypted DNS on internet censorship. In: Proc. of the 2021 Web Conf.
Ljubljana: ACM, 2021. 484–495. [doi: 10.1145/3442381.3450084]

[70]

Rescorla E, Oku K, Sullivan N, Wood CA. TLS encrypted client hello. 2022. https://datatracker.ietf.org/doc/html/draft-ietf-tls-esni-14

[71]

Hoang NP, Niaki AA, Borisov N, Gill P, Polychronakis M. Assessing the privacy benefits of domain name encryption. In: Proc. of the
15th ACM Asia Conf. on Computer and Communications Security. Taipei: ACM, 2020. 290–304. [doi: 10.1145/3320269.3384728]

[72]

Contavalli C, van der Gaast W, Lawrence D, Kumari W. RFC 7871 client subnet in DNS queries. 2016. https://www.rfc-editor.org/info/
rfc7871

[73]

How we made DNS both fast and private with ECS. 2021. https://medium.com/nextdns/how-we-made-dns-both-fast-and-private-withecs-4970d70401e5

[74]

Moura GCM, Castro S, Hardaker W, Wullink M, Hesselman C. Clouding up the Internet: How centralized is DNS traffic becoming? In:
Proc. of the 2020 ACM Internet Measurement Conf. ACM, 2020. 42–49. [doi: 10.1145/3419394.3423625]

[75]

Final DoH letter. 2019. https://www.ncta.com/sites/default/files/2019-09/Final%20DOH%20LETTER%209-19-19.pdf

[76]

Hounsel A, Schmitt P, Borgolte K, Feamster N. Encryption without centralization: Distributing DNS queries across recursive resolvers.
In: Proc. of the 2021 Applied Networking Research Workshop. ACM, 2021. 62–68. [doi: 10.1145/3472305.3472318]

[77]

Hoang NP, Lin I, Ghavamnia S, Polychronakis M. K-resolver: Towards decentralizing encrypted DNS resolution. In: Proc. of the 2020
NDSS Workshop on Measurements, Attacks, and Defenses for the Web. San Diego: Internet Society. 2020. [doi: 10.14722/madweb.
2020.23009]

[78]

Turing A, Ye GS. An analysis of Godlua backdoor. 2019. https://blog.netlab.360.com/an-analysis-of-godlua-backdoor-en/

[79]

Cimpanu C. Iranian hacker group becomes first known APT to weaponize DNS-over-HTTPS (DoH). 2020. https://www.zdnet.com/
article/iranian-hacker-group-becomes-first-known-apt-to-weaponize-dns-over-https-doh/

[80]

Bumanglag K, Kettani H. On the impact of DNS over HTTPS paradigm on cyber systems. In: Proc. of the 3rd Int’l Conf. on Information

张曼 等: DNS 信道传输加密技术: 现状、趋势和挑战

331

and Computer Technologies. San Jose: IEEE, 2020. 494–499. [doi: 10.1109/ICICT50521.2020.00085]

[81]

Patsakis C, Casino F, Katos V. Encrypted and covert DNS queries for botnets: Challenges and countermeasures. Computers & Security,
2020, 88: 101614. [doi: 10.1016/j.cose.2019.101614]

[82]

Zhang QF, Guo XJ, Zhou PJ. DGA identification method based on DoH traffic. Computer Technology and Development, 2021, 31(12):
122–127 (in Chinese with English abstract). [doi: 10.3969/j.issn.1673-629X.2021.12.021]

[83]

Banadaki YM. Detecting malicious DNS over HTTPS traffic in domain name system using machine learning classifiers. Journal of
Computer Sciences and Applications, 2020, 8(2): 46–55. [doi: 10.12691/jcsa-8-2-2]

[84]

MontazeriShatoori M, Davidson L, Kaur G, Lashkari AH. Detection of DoH tunnels using time-series classification of encrypted traffic.
In: Proc. of the 2020 IEEE Int ’l Conf. on Dependable, Autonomic and Secure Computing, Int ’l Conf. on Pervasive Intelligence and
Computing, Int ’l Conf. on Cloud and Big Data Computing, Int ’l Conf. on Cyber Science and Technology Congress. Calgary: IEEE,
2020. 63–70. [doi: 10.1109/DASC-PICom-CBDCom-CyberSciTech49142.2020.00026]

[85]

Singh SK, Roy PK. Detecting malicious DNS over HTTPS traffic using machine learning. In: Proc. of the 2020 Int ’l Conf. on
Innovation and Intelligence for Informatics, Computing and Technologies. Sakheer: IEEE, 2020. 1–6. [doi: 10.1109/3ICT51146.2020.
9312004]

[86]

Kwan C, Janiszewski P, Qiu SL, Wang C, Bocovich C. Exploring simple detection techniques for DNS-over-HTTPS tunnels. In: Proc.
of the 2021 ACM SIGCOMM Workshop on Free and Open Communications on the Internet. New York: Association for Computing
Machinery, 2021. 37–42. [doi: 10.1145/3473604.3474563]

[87]

Zhan MQ, Li Y, Yu GX, Li B, Wang WP. Detecting DNS over HTTPS based data exfiltration. Computer Networks, 2022, 209: 108919.
[doi: 10.1016/j.comnet.2022.108919]

[88]

Ding S, Zhang DQ, Ge JG, Yuan XW, Du XH. Encrypt DNS traffic: Automated feature learning method for detecting DNS tunnels. In:
Proc. of the 2021 IEEE Int’l Conf. on Parallel & Distributed Processing with Applications, Big Data & Cloud Computing, Sustainable
Computing & Communications, Social Computing & Networking. New York: IEEE, 2021. 352 –359. [doi: 10.1109/ISPA-BDCloudSocialCom-SustainCom52081.2021.00056]

[89]

Nguyen TA, Park M. DoH tunneling detection system for enterprise network using deep learning technique. Applied Sciences, 2022,
12(5): 2416. [doi: 10.3390/app12052416]

[90]

Huang Q, Chang DL, Li Z. A comprehensive study of DNS-over-HTTPS downgrade attack. In: Proc. of the 10th USENIX Workshop on
Free and Open Communications on the Internet. USENIX Association, 2020. 1–8.

[91]

Zhang JW, An YJ, Deng HY. A survey on DNS attack detection and security protection. Telecommunications Science, 2022, 38(9):
1–17 (in Chinese with English abstract). [doi: 10.11959/j.issn.1000-0801.2022248]

[92]

Hu N, Deng WP, Yao S. Issues and challenges of Internet DNS security. Chinese Journal of Network and Information Security, 2017,
3(3): 13–21 (in Chinese with English abstract). [doi: 10.11959/j.issn.2096-109x.2017.00154]

[93]

Pearce P, Jones B, Li F, Ensafi R, Feamster N, Weaver N, Paxson V. Global measurement of DNS manipulation. In: Proc. of the 26th
USENIX Conf. on Security Symp. Vancouver: USENIX Association, 2017. 307–323.

[94]

Pauly T, Kinnear E, Wood CA, McManus P, Jensen T. Discovery of designated resolvers. 2022. https://www.ietf.org/archive/id/draftietf-add-ddr-10.html

[95]

Boucadair M, Reddy T, Wing D, Cook N, Jensen T. DHCP and router advertisement options for the discovery of network-designated
resolvers. 2023. https://www.ietf.org/archive/id/draft-ietf-add-dnr-16.html

[96]

Schwartz B, Box C. Discovery of designated resolvers in the presence of legacy forwarders. 2021. https://www.ietf.org/archive/id/draftschwartz-add-ddr-forwarders-01.html

[97]

Statement on DNS Encryption. 2021. https://root-servers.org/media/news/Statement_on_DNS_Encryption.pdf

[98]

Kumari W, Hoffman P. RFC 8806 running a root server local to a resolver. 2020. https://www.rfc-editor.org/info/rfc8806

[99]

Gillmor DK, Salazar J, Hoffman P. Unilateral opportunistic deployment of encrypted recursive-to-authoritative DNS. 2023. https://www.
ietf.org/archive/id/draft-ietf-dprive-unilateral-probing-06.html

[100]

Security/DoH-resolver-policy. 2021. https://wiki.mozilla.org/Security/DOH-resolver-policy

[101]

Kinnear E, McManus P, Pauly T, Verma T, Wood CA. RFC 9230 oblivious DNS over HTTPS. 2022. https://www.rfc-editor.org/info/
rfc9230

[102]

Singanamalla S, Chunhapanya S, Hoyland J, Vavruša M, Verma T, Wu P, Fayed M, Heimerl K, Sullivan N, Wood C. Oblivious DNS
over HTTPS (ODoH): A practical privacy enhancement to DNS. Proc. on Privacy Enhancing Technologies, 2021, 2021(4): 575–592.
[doi: 10.2478/popets-2021-0085]

[103]

Alecmuffett. Dohot: Making practical use of DNS over HTTPS over Tor. 2020. https://github.com/alecmuffett/dohot/blob/master/papers/

软件学报 2024 年第 35 卷第 1 期

332

no-port-53-who-dis-paper-3.1.pdf

[104]

How Facebook is bringing QUIC to billions. 2020. https://engineering.fb.com/2020/10/21/networking-traffic/how-facebook-is-bringingquic-to-billions/

[105]

DNS over HTTP3 in Android. 2022. https://security.googleblog.com/2022/07/dns-over-http3-in-android.html

附中文参考文献:
[8]

王文通, 胡宁, 刘波, 刘欣, 李树栋. DNS安全防护技术研究综述. 软件学报, 2020, 31(7): 2205–2220. http://www.jos.org.cn/1000-9825/
6046.htm [doi: 10.13328/j.cnki.jos.006046]

[9]
[66]
[82]

黄锴, 孔宁. DNS隐私问题现状的研究. 计算机工程与应用, 2018, 54(9): 28–36. [doi: 10.3778/j.issn.1002-8331.1801-0101]
孟德超, 邹福泰. DNS隐私保护安全性分析. 通信技术, 2020, 53(2): 445–449. [doi: 10.3969/j.issn.1002-0802.2020.02.028]
张千帆, 郭晓军, 周鹏举. 基于DoH流量的DGA识别方法. 计算机技术与发展, 2021, 31(12): 122–127. [doi: 10.3969/j.issn.1673-629X.
2021.12.021]

[91]
[92]

章坚武, 安彦军, 邓黄燕. DNS攻击检测与安全防护研究综述. 电信科学, 2022, 38(9): 1–17. [doi: 10.11959/j.issn.1000-0801.2022248]
胡宁, 邓文平, 姚苏. 互联网DNS安全研究现状与挑战. 网络与信息安全学报, 2017, 3(3): 13–21. [doi: 10.11959/j.issn.2096-109x.2017.
00154]

张曼(1992－), 女, 工程师, 主要研究领域为互联

董科军(1977－), 男, 博士, 正高级工程师, CCF

网基础资源治理, 网络安全.

高级会员, 主要研究领域为互联网基础资源管
理, 云计算, 分布式系统, 网络协同技术.

姚健康(1978－), 男, 博士, 研究员, CCF 专业会

延志伟(1985－), 男, 博士, 研究员, CCF 专业会

员, 主要研究领域为 DNS 等互联网基础技术资

员, 主要研究领域为互联网名址协议, 网络安全,

源, 网络安全, 互联网治理.

下一代网络架构.

李洪涛(1977－), 男, 正高级工程师, CCF 高级会
员, 主要研究领域为计算机应用技术, 下一代互
联网架构, 互联网基础资源新型解析技术, 大数
据分析.


软件学报 ISSN 1000-9825, CODEN RUXUEW
Journal of Software,2024,35(3):10901106 [doi: 10.13328/j.cnki.jos.007072]
©中国科学院软件研究所版权所有.

GPPR: 跨域分布式个性化 PageRank 算法

E-mail: jos@iscas.ac.cn
http://www.jos.org.cn
Tel: +86-10-62562563



陈子俊 1, 马德龙 1, 王一舒 1, 袁 野 2
1

(东北大学 计算机科学与工程学院, 辽宁 沈阳 110169)

2

(北京理工大学 计算机学院, 北京 100081)

通信作者: 袁野, E-mail: yuanye@mail.neu.edu.cn

要: 个性化 PageRank 作为大图分析中的基本算法, 在搜索引擎、社交推荐、社区检测等领域具有广泛的应

摘

用, 一直是研究者们关注的热点问题. 现有的分布式个性化 PageRank 算法均假设所有数据位于同一地理位置, 且
数据所在的计算节点之间具有相同的网络环境. 然而在现实世界中, 这些数据可能分布在跨洲的多个数据中心中,
这些跨域分布(cross-geo-distributed)的数据中心之间通过广域网连接, 存在网络带宽异构、硬件差异巨大、通信
费用高昂等特点. 分布式个性化 PageRank 算法需要多轮迭代, 并在全局图上进行随机游走. 因此, 现有的分布式
个性化 PageRank 算法不适用于跨域环境. 针对此问题, 提出了 GPPR (cross-geo-distributed personalized PageRank)
算法. 该算法首先对跨域环境中的大图数据进行预处理, 采用启发式算法映射图数据, 以降低网络带宽异构对算
法迭代速度的影响; 其次, GPPR 改进了随机游走方式, 提出了基于概率的 Push 算法, 通过减少工作节点之间传输
数据的带宽负载, 进一步减少算法所需的迭代次数. 基于 Spark 框架实现了 GPPR 算法, 并在阿里云中构建真实的
跨域环境, 在 8 个开源大图数据上, 与现有的多个代表性分布式个性化 PageRank 算法进行了对比实验. 结果显示,
GPPR 的通信数据量在跨域环境中比其他算法平均减少 30%. 在算法运行效率方面, GPPR 比其他算法平均提升了
2.5 倍.
关键词: 跨域分布式; 个性化 PageRank; 近似计算
中图法分类号: TP301
中文引用格式: 陈子俊, 马德龙, 王一舒, 袁野. GPPR: 跨域分布式个性化 PageRank 算法. 软件学报, 2024, 35(3): 1090–
1106. http://www.jos.org.cn/1000-9825/7072.htm
英文引用格式: Chen ZJ, Ma DL, Wang YS, Yuan Y. GPPR: Cross-geo-distributed Personalized PageRank Algorithm. Ruan Jian
Xue Bao/Journal of Software, 2024, 35(3): 10901106 (in Chinese). http://www.jos.org.cn/1000-9825/7072.htm

GPPR: Cross-geo-distributed Personalized PageRank Algorithm
CHEN Zi-Jun1, MA De-Long1, WANG Yi-Shu1, YUAN Ye2
1

(School of Computer Science and Engineering, Northeastern University, Shenyang 110169, China)

2

(School of Computer Science and Technology, Beijing Institute of Technology, Beijing 100081, China)

Abstract: Personalized PageRank, as a basic algorithm in large graph analysis, has a wide range of applications in search engines, social
recommendation, community detection, and other fields, and has been a hot problem of interest to researchers. The existing distributed
personalized PageRank algorithms assume that all data are located in the same geographic location and the network environment is the
same among the computing nodes where the data are located. However, in the real world, these data may be distributed in multiple data
centers across continents, and these cross-geo-distributed data centers are connected to each other through WANs, which are characterized


基金项目: 国家重点研发计划(2022YFB2702100); 国家自然科学基金(61932004, 62225203, U21A20516); 中央高校基本科研

业务专项资金(N232405-16)
本文由“面向多模态数据的新型数据库技术”专题特约编辑彭智勇教授、高云君教授、李国良教授、许建秋教授推荐.
收稿时间: 2023-07-17; 修改时间: 2023-09-05; 采用时间: 2023-10-24; jos 在线出版时间: 2023-11-08
CNKI 网络首发时间: 2023-12-25

陈子俊 等: GPPR: 跨域分布式个性化 PageRank 算法

1091

by heterogeneous network bandwidth, huge hardware differences, and high communication costs. The distributed personalized PageRank
algorithm requires multiple iterations and random wandering on the global graph. Therefore, the existing distributed personalized
PageRank algorithms are not applicable to the cross-geo-distributed environment. To address this problem, the GPPR (cross-geodistributed personalized PageRank) algorithm is proposed in this study. The algorithm first preprocesses the big graph data in the
cross-geo-distributed environment and maps the graph data by using a heuristic algorithm to reduce the impact of network bandwidth
heterogeneity on the iteration speed of the algorithm. Secondly, GPPR improves the random wandering approach and proposes a
probability-based push algorithm to further reduce the number of iterations required by the algorithm by reducing the bandwidth load of
transmitting data between working nodes. The GPPR algorithm is implemented based on the Spark framework and a real
cross-geo-distributed environment in AliCloud is built to conduct experiments on eight open-source big graph data compared with several
existing representative distributed personalized PageRank algorithms. The results show that the communication data volume of GPPR is
reduced by 30% on average in the cross-geo-distributed environment compared with other algorithms. In terms of algorithm running
efficiency, GPPR improves by an average of 2.5 times compared to other algorithms.
Key words: cross-geo-distributed; personalized PageRank; approximate calculation

图是表达对象与对象之间关系的基本数据结构, 也是对现实世界中对象及其关系的一种抽象 [1]. 一方面,
在大数据时代, 当人们意识到数据蕴含的巨大价值以后, 对图数据的分析需求变得越来越迫切, 如网页推荐、
生物基因分析以及信息检索等 [2]; 另一方面, 随着互联网及物联网的应用范围持续扩大, 图数据的规模也在
不断扩张 [3], 传统的图分析算法在这样蕴含大量信息的大图数据上产生了各方面的局限, 如节点间通信成本
高、计算资源或存储容量不足. 因此, 如何高效地对真实大图数据进行分析, 并获取数据背后的价值信息, 一
直是大图数据分析中的研究热点.
个性化 PageRank 算法(personalized PageRank)是图搜索和分析领域的基本算法, 是 PageRank 算法的一个
重要变体. PageRank 算法最初由 Brin 和 Page 在 1998 年提出, 并引入了 Google 的 Web 搜索引擎[4], 作为其引
擎内部用于网页推荐的基础算法. 与传统在数据提取领域用的模式匹配算法不同, PageRank 算法完全依赖于
基础的图数据来确定图中节点的重要性程度 [5]. 所以, PageRank 算法可以给出图上任意一个节点在全局环境
下的重要性分数, 进而可以根据重要性分数的高低得到该节点相对全图的重要性程度, 在文献计量学、社交
网络分析、道路交通网络分析等领域有广泛的实际应用[6], 是现代搜索服务的基础算法. 个性化 PageRank 算
法是 PageRank 算法的一个重要变体, 个性化 PageRank 算法与 PageRank 算法不同体现在: 个性化 PageRank
算法给出的图上每一个节点的重要性分数是相对某一特定节点, 而不是针对图数据中所有节点的, 是一种个
性化的重要性程度. 与 PageRank 算法类似, 个性化 PageRank 算法也在诸多领域有广泛的应用, 如信息提取、
推荐系统、知识发现等[7].
现有的个性化 PageRank 算法主要包括两大类,分别是单机和分布式. 单机个性化 PageRank 算法进一步可
以概括为两个研究方向: 矩阵迭代和本地推送 [8]. 采用矩阵迭代的方式进行计算, 算法迭代时间随着数据量
的增加成指数级增长, 不适用于现在的大图数据; 本地推送算法计算得到的结果的是个性化 PageRank 算法的
无偏估计, 不是个性化 PageRank 算法的准确值, 但由于应用时需要的往往只是节点重要性程度间的比较, 因
此, 本地推送算法可以在一定误差界限下极大地减少算法的运行时间. 分布式个性化 PageRank 算法通过在更
改本地推送算法以适用分布式环境, 并进一步增加蒙特卡洛模拟来减少算法的运行时间, 可以通过增加计算
节点的方式, 利用算法的可扩展性来加快算法迭代, 减少算法的运行时间. 但是现有的分布式个性化
PageRank 算法大都假设所有计算节点之间的带宽是相同的, 这样的假设在跨域环境下并不成立, 由此会产生
诸多问题. 如某个数据中心节点传输效率低拖慢算法整体迭代速度, 从而导致算法出现单点瓶颈问题[9].
用户数据产生于世界各地的服务集群, 如 Amazon 现在在全球范围内有 15 个集群[10], Bing 提供的搜索来
自 8 个不同地理分布的地区 [11]. 由于服务集群间数据传输成本过高, 单机房难以处理大量数据, 因此很少会
将数据传输到同一机房来计算, 所以为了低成本并高效地计算个性化 PageRank 算法, 跨国公司会将数据分散
在地理分布的各个数据中心内.
Google 依托超过 20 个地理分布的数据中心, 通过内部优化过的 PageRank 算法来提供搜索服务[12]等. 在

1092

软件学报 2024 年第 35 卷第 3 期

这样的跨域环境下, 相对于分布式环境来说有以下几方面的局限: (1) 由于大图数据的数据量级过高, 通常有
数十亿个节点和数万亿条边 [13,14], 并且跨域环境下的网络传输效率低, 所以难以将数据传输到单一主机上进
行汇总处理; (2) 由于地理分布的数据中心之间使用的广域网带宽通常是数据中心内部局域网带宽的千分之
一[15], 但是价格却是局域网的数倍, 个性化 PageRank 算法通常会进行多轮迭代, 因此产生高昂的数据传输成
本; (3) 由于不同数据中心之间的带宽是异构的, 同一数据中心的上传和下载带宽也是不同的[16], 所以个性化
PageRank 算法通常会因为某个数据中心的低传输效率, 拖慢算法的整体传输效率, 进而遭遇单点性能瓶颈问
题. 如表 1 所示, 3 个地区的下载带宽比上传带宽高几倍, 不同数据中心之间的上传和下载带宽都是不同的,
新加坡的上传和下载带宽分别比悉尼的高 17%和 40%.
表1

3 个地区的 Amazon EC2 实例的上传/下载带宽(GB/s)

带宽
上行带宽
下载带宽

美国
0.52
2.8

新加坡
0.55
3.5

悉尼
0.48
2.5

基于以上挑战, 本研究提出了跨域个性化 PageRank 算法(cross-geo-distributed personalized PageRank,
GPPR), 解决跨域环境下分布式个性化 PageRank 算法的相关问题, 主要工作如下.
(1)

首次从跨域环境下带宽异构的角度出发, 提出了算法数据的预处理步骤, 通过基于节点度数和带宽
的启发式算法来确定数据和工作节点的对应关系, 可以有效地解决跨域环境下的带宽异构引发的
算法单点瓶颈问题, 弥补了现有分布式个性化 PageRank 算法跨域环境下的不足.

(2)

提出了基于概率的 Push 算法, 结合蒙特卡罗模拟来计算节点的个性化 PageRank 值, 减少个性化
PageRank 算法所需要的迭代次数, 从而降低跨域环境下带宽使用成本, 并支持水平扩展, 可以通过
增加工作节点来满足大图数据下的计算需求.

(3)

通过搭建真实的跨域环境, 在 5 个真实数据集上, 通过和现有分布式个性化 PageRank 算法进行多组
对比实验, 进一步验证和分析了算法的有效性.

本文首先简要介绍个性化 PageRank 算法和跨域数据处理的相关工作(第 1 节); 并给出必要的基本概念与
问题定义(第 2 节); 然后介绍基于带宽和节点度数的启发式算法, 用于算法的数据预处理阶段, 解决算法后续
迭代计算可能遇到的单点瓶颈问题(第 3 节); 进一步提出基于概率的 Push 算法, 保证结果在一定误差范围的
前提下, 减少算法所需要的迭代次数, 降低算法所需要的数据传输量, 并给出结果正确性的相关证明(第 4 节);
为了说明算法的性能, 在 5 个真实数据集上对算法进行实验评估和结果分析(第 5 节); 最后, 对全文整体工作
进行总结(第 6 节).

1

相关工作
本节主要针对与单机和分布式环境下个性化 PageRank 算法以及跨域环境下的数据处理密切相关的工作

进行简要介绍与总结.
1.1 个性化PageRank算法
1.1.1

单机个性化 PageRank 算法
个性化 PageRank 算法主要有以单一源节点为基准(简称单源节点)、以单一目的节点为基准(简称单目的

节点)两大类. 单源节点类别意味着求图中所有节点针对某一个选定源节点的重要性程度, 可以抽象成一对多
的关系; 而单目的节点类别则是求选取的目的节点对图中所有其他节点的重要性程度, 是多对一的关系 [17,18],
本研究主要研究单源节点条件下的个性化 PageRank 算法相关问题.
个性化 PageRank 算法默认的解决方案是通过矩阵迭代的方式进行计算, 通过个性化 PageRank 的定义推
导出的矩阵公式来进行计算, 如公式(1)所示.




r   Pr  (1   ) s

(1)

陈子俊 等: GPPR: 跨域分布式个性化 PageRank 算法

1093

其中, P 为转移矩阵, P 矩阵定义如公式(2)所示.
 1
, j  O( ni )

pijT   | O(ni ) |
(2)
0,
otherwise


为随机游走的终止概率, 当等式收敛时, r 的每个值为对应节点相对于源节点 s 的个性化 PageRank 值 .
根据值的大小, 可以得到对于源节点 s 相对更重要的那些点. 但是, 此算法在大图数据场景下的计算效率并不
高. 因此, 后续一些工作主要关注如何提高矩阵的计算速度来加快单机环境下的个性化 PageRank 算法的迭代
效率[1921]:

1.1.2

分布式个性化 PageRank 算法
分布式环境下的个性化 PageRank 算法有两个主要分支. 一方面, 一些研究通过本地推送的方式来解决个

性化 PageRank 算法计算的问题, 如 Forward Push 算法[22]和 Backward Push 算法[23,24]; 另一方面, 有很多工作
将 Forward Push 方法和蒙特卡洛方法结合来提高算法的运行效率[25,26]. 其中 : Bahmani 提出了 Doubling 算
法[27], 通过合并短的随机游走分片组成新的更长的随机游走分片来加快算法速度; Lin 通过引入 pipeline 机制
来提高算法的效率[28]; Sarma 主要关注如何在拥塞模型上进行优化[29]; Massively 主要通过引入预采样算法来
增加随机游走的复用 [30]. 但是这些算法都默认工作节点之间的带宽是相同的 , 没有考虑跨域环境下的相关问
题 .在跨域环境下 , 工作节点之间带宽是异构的 , 相同的数据传输量会因带宽不同而有相异的传输时间 , 进而
导致算法出现单点瓶颈问题.

Forward-Push[26]算法如算法 1 所示, Forward-Push 算法通过读取图数据 G=(V,E)、源节点 s、 一个随机游
走的结束概率 和一个残留量阈值 rmax. 对于图中的每一个节点 v, 算法为其维护一个残留量 rs(v)和一个储备
量 ˆ s (v) , 其中 , 储备量就是图上随机游走在当前节点停止的部分 , 残留量就是图上随机游走在当前节点还未
结束的部分. 当所有节点的残留量都为 0 时, 节点的储备量就是改节点的准确 PPR 值. 在算法的初始化部分,
首先将源节点的残留量设置为 1, 其余节点的残留量都设置为 0, 而所有节点的储备量都设置为 0 (算法 1 的
第 1、 2 行). 从 s 开始激活随机游走, 对于满足条件 rs(v)/|Nout(v)|>rmax 的所有节点 v, 将 v 节点残留量 rs(v)的
加到 v 节点的储备量, 代表到达 v 的随机游走中有 rs(v)停止在 v 节点 ; 随机游走中的另外 rs(v)(1)部分将
继续进行, 向 v 节点的每一个出度邻居节点传递(1)rs(v)/Nout(v)部分, 最后清空 v 节点的残留量(算法 1 的
第 48 行 ).
算法 1. Forward-Push.
输入: 图 G=(V,E), 源节点 s, 结束概率, residue 阈值 rmax.
输出: 集合 V 中所有节点 v 的 ˆ s (v) 和 rs(v).

1: rs(v)1; rs(v)0 for all vs;
2: ˆ s (v)  0 for all v;
3: while vV such that rs(v)/|Nout(v)|>rmax do
4:

for each uNout(v) do
rs(u)rs(u)+(1)rs(v)/Nout(v)

5:
6:
7:

end for
ˆ s (v)  ˆ s (v)    rs (v);

8:

rs(v)0;

9: end while
如果能耗尽所有节点的残留量, 那么将能得到所有节点的准确个性化 PageRank 值, 但这将会导致巨大的
计算成本, 所以在 Forward-Push 算法中, 通过一个阈值 rmax, 仅当节点的残留值不小于 rmax 和节点出度的乘积
时才会继续传递. 通过这样的策略, 可以极大地降低算法的计算成本, 在每轮迭代中 , Forward-Push 都会保存
如下不变量.

1094

软件学报 2024 年第 35 卷第 3 期


 s (t )   s (t )   rs (v)   v (t )

(3)

vV

等式中的 v(t)通过另一个随机游走阶段来得到, 通过从每个节点 v 取样 v=rs(v)(2/3+2)log2(2/pf)/2个
随机游走 , 然后用其中停在 t 的随机游走比例作为  v ( t ) 的估计值 v (t ) . 最后 , 可通过公式 (3) 得到每个节点
满足定义 1 的近似个性化 PPR 值.
目前 , 分布式环境下求解个性化 PageRank 算法的最新研究为 Delta-Push[30] 算法 , Delta-Push 算法基于

Spark 框架的 MPC (massively parallel computing, 大规模并行计算)模型, 在分布式环境下, 实现高效的计算个
性化 PageRank 算法的迭代计算. 但是其未考虑跨域环境下的带宽异构导致的单点计算瓶颈问题, 并且因为其
进行全局的随机游走 , 所以算法会消耗更多的计算时间 . 本文在该算法的基础上改进全局随机游走为基于概
率的随机游走, 降低算法的运行时间, 并在算法中提出图预处理步骤来优化跨域环境下算法的运行效率问题.
1.2 跨域数据预处理
当前 , 跨域数据处理研究大都是在已有分布式数据处理框架上进行修改 , 以适应跨域带宽延迟高、带宽
异构、带宽费用高等问题.

Pu 等人[31]在 Spark[32]的基础上开发了 Iridium 框架, 主要关注如何最小化地理分布环境下的分析查询操作
的响应时间 . 框架主要包括全局管理器和本地管理器两部分组件 : 通过全局管理器协调数据中心之间的查询
问题 , 并分配任务给本地管理器 ; 本地管理器负责管理数据中心内部的资源 , 并执行全局管理器下发的工作
任务 . 框架充分考虑广域网的带宽异构问题 , 通过贪婪启发式算法来优化数据和任务的分配方式 , 缩短数据
中心间的数据传输时间, 并降低总体的带宽使用量.

Zhou 等人[33]在 PowerGraph[34]框架的基础上开发了 Geo-Cut 图分区框架, Geo-Cut 主要包括两个优化阶段:
首先 , 以流式启发算法划分图 , 划分时充分考虑数据中心的上行带宽和下行带宽的异构 ; 其次 , 通过多次切
换分区的方法来适应数据中心之间的异构带宽环境 , 并通过分辨瓶颈数据中心来平衡负载 , 进一步减少数据
传输时间.

Yuan 等人[35,36]在 Giraph[37,38]的基础上开发了 GeoGraph 算法, 一个用来在跨域环境下处理图查询的算法,
通过组合数据中心内部数据进行同步计算、协调数据中心之间进行异步计算两个模式来高效、可靠地进行图
相关算法的计算[39].
上述的跨域图处理系统都充分考虑了跨域环境, 但是在个性化 PageRank 算法的求解上, 均使用的矩阵迭
代的算法, 利用框架提供的分布式能力, 并没有针对分布式个性化 PageRank 算法进行优化, 不能进一步提高
性能, 在跨域环境下会产生大量的带宽消耗, 不适用于大图环境下的个性化 PageRank 问题的求解.

2

问题定义与 GPPR 算法框架
本节将对一些基本的问题进行梳理, 对所面向的研究对象个性化 PageRank 算法进行介绍, 对基本的概念

做出定义, 并在表 2 中对本文所常用到的一些符号的意义进行简要说明.
表2

符号说明

符号
G=(V,E)
n, m
P

描述
图数据 G, 节点集合 V, 边集合 E
节点和边的数量
集群处理器的数量

M
Ur/Dr



随机游走的终止概率

I vr

在数据中心 r 上是否存在节点 v

Nout(v)
s(v)
 s (v)

节点 v 的出度
v 相对于 s 的准确个性化 PageRank 值

Rv
ˆ s (v)

至少有 1 个节点 v 的数据中心集合
节点 v 对节点 s 的 reserve 值

v 相对于 s 的近似个性化 PageRank 值

rs(v)

节点 v 对节点 s 的 residue 值

2.1 GPPR算法框架

GPPR 算法总体共分为 3 步.

符号



描述
相对错误界限
地理分布的数据中心数量
数据中心 r 的上传/下载带宽

陈子俊 等: GPPR: 跨域分布式个性化 PageRank 算法

(1)

1095

图预处理阶段 , 通过启发式算法 , 将图数据与工作节点相映射 , 根据带宽异构来平衡数据传输负载 ,
然后进行随机游走的采样.

(2)

通过基于概率的 Push 算法结合蒙特卡洛模拟 , 综合第 (1)步采样的随机游走 , 来计算所有图上节点

(3)

针对输入的每一次查询的源节点, 重复算法的第 (2)步, 直到返回所有节点及与每个源节点相关的个

相对于当前源节点的个性化 PageRank 值.
性化 PageRank 值.
2.2 问题定义
为简化问题 , 假设所有计算节点处于无拥塞网络环境 , 计算节点间的上行下行带宽的巨大差异是算法的
主要网络瓶颈.
定义 1((, )-近似单源个性化 PageRank). 给定一个源节点 s, 一个阈值 , 一个错误边界(0,1]和一个失
败概率 Pf, 一个近似单源个性化 PageRank 查询将会返回每个节点的个性化 PageRank 值的近似值 s (v) , 并且
在至少 1Pf 的概率下满足公式(4)和公式 (5).
|  s (v)  s (v) |≤    s (v),  s (v) ≥ 
|  s (v)  s (v) |≤    ,  s (v)  

(4)
(5)

定义 2(蒙特卡洛随机游走). 给定一个含有 n 个节点的有向图, 其中, 节点表示状态, 有向边表示状态之
间的转移 . 假设从一个节点到通过有向边相连的所有节点的转移概率相等 , 每次可以一个节点转移到任意一
相连节点(随机游走). 从一个给定的源节点 S 执行 次随机游走, 并记录随机游走停在节点 t 的概率.
如图 1 所示, 模型中的某一个当前停在节点 0 上的随机游走, 可以进一步走到节点 1、节点 2、节点 5; 停
在节点 5 的随机游走可以进一步走到节点 2、节点 7、节点 6.

7

图1

5

6

2

0

1

4

3

随机游走示例

定义 3(-随机游走). 给定一个参数[0,1), 默认为 0.2[4]. 假设当前节点为 v(初始化时为 s), 在每一步随
机游走中, 有 的可能性停在 v, 1可能性移动到下一节点. 其中, 下一节点的选择有两种可能性: 如果当前
节点的 Nout, 则在节点 v 进行蒙特卡洛随机游走, 即等可能选择一个出度邻居; 否则, 跳转至节点 s.
定义 4(带宽异构). 在一个通信系统或网络中, 存在多个不同的带宽参数(如传输速率、数据传输能力等),
这些带宽参数在时间和空间上可以变化 , 且它们的分布不一定遵循特定的概率分布 . 带宽异构可以在多个层
次上存在, 包括网络连接、链路、设备或应用程序层面.
本文问题定义为: 给定分布在跨域环境下 k 个工作节点的图 G、一个图内点 s, 设每个图内点相对于 s 的
近似单源个性化 PageRank 值为 s (v) , 工作节点所需要的带宽消耗为 Bi, 通过随机游走模型, 在保证 s (v) 准
k

确率的情况下, 最小化算法整体带宽消耗  B j .
j

3

图预处理
如第 1 节所述, 已有的分布式个性化 PageRank 算法主要有两类, 分别是矩阵迭代和蒙特卡洛模拟. 对于

矩阵迭代相关算法来说 , 对单机的计算能力和存储能力有极高的要求 , 无法适用在分布式环境下 . 基于蒙特
卡洛模拟的方法是现有分布式个性化 PageRank 算法的主流选择, 主要由 Forward-Push 算法和蒙特卡洛模拟两
部分组成, 其中, Forward-Push 算法细节如第 1.1.2 节所示 .

1096

软件学报 2024 年第 35 卷第 3 期

但是在跨域环境下, 通过 Forward-Push 算法求解个性化 PageRank 近似值会有这样几方面的局限: 首先是
由于带宽的异构, 不合适的工作节点与图数据方式会使得 Forward-Push 算法产生低效的计算节点, 拖慢整个
算法的运行速度 ; 其次 , 由于随机游走是动态的 , 且每次都从同一源节点开始 , 会造成图中所有边的数据传
输负载不平衡, 导致很难构建一个满足跨域环境的分布式算法.
本研究的预处理算法由两部分组成 : 首先是通过启发式算法解决图数据的分布问题 ; 其次 , 通过全局预
取样算法来保证算法每条边上的负载大致相同.
在跨域环境下 , 广域网带宽比 CPU、内存等本地计算资源更稀缺 , 所以跨域环境下算法的瓶颈会出现在
数据中心之间的数据传输上 . 事实上 , 许多数据中心的拥有者希望将在全球范围内提供服务 , 所以会构建自
己的基础网络设施 , 因此假设数据中心之间的网络连接是拥塞避免网络 , 网络的瓶颈只会出现在数据中心的
上行/下行链路上.
由于不同工作节点之间带宽是异构的, 所以 M 个地理分布的工作节点之间是互异的. 将 M 个图分区映射
到 M 个地理分布的数据中心是一个经典的组合 NP-hard 问题, 并且解空间为 O(M !)[33]. 这个问题之所以是 NP-

hard 的原因, 是因为需要探索出 M 个图分区映射到 M 个数据中心的所有可能方式, 而随着分区数量 M 的增
加 , 可能的映射数量呈阶乘增长 (O(M !)), 解空间的指数增长使得在合理的时间内找到大规模问题的最优解成
为不可行的计算任务. 当 M 的值很小时, 可以使用广度优先遍历或深度优先遍历来找到一种更低的数据中心
间数据传输时间的映射方案 . 但是当 M 的值很大时 , 广度优先或深度优先遍历的方法需要更长的计算时间 ,
无法适用, 通常做法采用启发式算法、近似算法等方法在合理的时间内找到近似最优解. 因此, 这里提出一个
高效的算法来快速找出最佳的映射方案.
由于算法运行在 MPC 模型上 , 所以会在每一轮迭代后进行一次数据同步, 每个工作节点会有两部分数据
同步, 分别是上行数据和下行数据, 其中, 上行数据受限于工作节点的上传带宽, 下行数据受限于工作节点的
下载带宽. 由于个性化 PageRank 算法的特点, 每个节点的上行数据与此工作节点内部所存储的子图数据的总
出度成正比 ; 同理 , 每个节点的下行数据与此工作节点内部的子图数据的总入度成正比 . 所以 , 将每个节点
上行和下行数据的时间用如下公式(6)、公式(7)表示, 则每一次数据同步的耗时如公式(8)所示.
r
T (i)  Tup (i )  Tdown (i )  max Tupr (i )  max Tdown
(i)

Tupr (i) 
r
Tdown
(i ) 





  din (u )   I vr 

uRu



Ur



r





  dout (v)   Iur 

uRv



r



Dr

(6)
(7)
(8)

二次选择迭代 [40] 技术已经被证明可以高效地在任务调度问题中找到近似最优调度方案 , 并且耗时较低 .
最近的研究发现 : 在有长尾分布的情况出现时 , 如果有更多的选择 , 该技术的性能可以得到一定的提升 [41].
如何把部分图数据和工作节点相映射 , 也可以模拟成一个类似的任务调度问题 , 所以提出如下算法来快速得
到映射方案.
算法 2. Partition Mapping Algorithm.
输入: Pinit: 初始分区映射方案.
输出: Popt: 最优分区映射方案.

1:

Popt=Pinit;

2:

repeat

3:

continue=false;

4:

随机取样 d 对分区映射;

5:

Let Gb=0 and pb=0;

6:
7:

for i=1 to d do
通过交换 pairi 的映射来产生新的映射计划 p;

陈子俊 等: GPPR: 跨域分布式个性化 PageRank 算法

8:

G=EstimateTime(P)ExtimateTime(p);

9:

if G>0
交换 pairi 的映射;

10:
11:

Continue=true;

12:
13:

1097

end if
end for

14: until !continue
15: return Popt
算法从一个初始化默认分区映射方案开始, 随机选择 d 对工作节点及其内部数据(算法 2 第 4 行), 遍历每
一对分区 , 通过根据上面提到的公式来估测交换前后的数据传输时间是否有提升 , 来决定是否交换两个工作
节点内部的数据. 重复以上算法, 直到没有更多的提升. 其中, d 的取值默认取 2, 当工作节点之间的网络性能
差异过大时, 可以适当提高.
为了更好地解释映射算法, 这里给出一个示例. 如图 1 所示的图数据, 与现有分布式个性化 PageRank 算
法一样 , 首先采用点切分和最长处理时间调度算法来初始划分图数据 [42], 最长处理时间调度算法主要用来平
衡各机器的负载. 针对图 1 的图结构数据, 初始化分结果如图 2(a)所示, 其中, 实线点为工作节点所存数据节
点 , 虚线点为需要同步数据的节点以及其相应地数据传输方向 . 两个工作节点的上行带宽和下行带宽如表 3
所示 , 假设图中所有边上的数据传输量相同且为 1, 可以通过公式 (6)计算得到每一轮迭代后所需要的数据同
步时间为 3.95 s. 使用算法 2 的分区映射方案交换两个工作节点的分区数据后, 可以得到如图 2(b)所示的优化
结果, 根据公式可以计算得到对应的数据同步时间为 2.8 s, 所以交换后的数据与工作节点的映射方案可以更
好地适应当前的异构带宽环境, 降低每一轮数据同步所需要的时间, 进而提高算法整体的迭代速度.

7

5

6

5

6

2

0

2

0

1

4

1

4

DC1

3

5

6

2

0

1

4

3

7

6

2

0

1

4

DC2

DC1

DC2

5

EstimateTime = max(2/10,3/15) + max(3/0.8, 2/2) = 3.95

EstimateTime = max(2/15,3/10) + max(3/2, 2/0.8) = 2.8

(a) 初始分区映射方案

(b) 最优分区映射方案

图2
表3
计算节点
DC1
DC2

分区映射方案选择
计算节点带宽(MB/s)
上行带宽
2.0
0.8

下行带宽
15
10

通过引入预取样算法[30], 可以保证图上的每一条边的所需数据传输量的负载均衡. 算法 3 展示了整个预
取样阶段的伪代码 . 首先 , 通过上述公式来进行图数据的划分 ; 然后 , 从每个节点开始 p=m/n个随机游走 ,
在每轮迭代中, 通过 MPC 模型进行随机游走的模拟. 具体来说, 使用 P= s,v来表示一次从节点 s 开始的现在
停在 v 的随机游走, 然后, 每一次随机游走都有的可能性在当前节点结束, 并记录在集合 wt 中; 另外, 1的
可能性继续传播到当前节点的出邻居并更新相应的键值对, 并被记录在集合 Wa 中. 算法会重复以上循环, 直
到所有的随机游走都结束.
算法 3. Pre-Sample.
输入: 图 G=(V,E), 结束概率, Popt.

1098

软件学报 2024 年第 35 卷第 3 期

输出: 预取样的随机游走集合 wt.

1:

pm/n; wt=;

2:

for 1,…,R do

3:
4:

for each vV in parallel do
for 1,…, p/P do

wa0 (v)  wa0 (v)   v, v ;

5:
6:
7:
8:
9:

end for
end for
Wa   vV Wa0 (v) ;

while Wa do

10:

Wt*  Wa filter{b  ( Random(0,1))   } ;

11:

Wt  Wt  Wt* ;

12:

Wa  Wa \ Wt* ;

13:

$
Wa  Wa map -value{v  u 
 N out (v)}

14:

end while

15: end for
16: return wt

4

基于概率的 Push 算法
对于真实世界的图数据 , 常常呈现出一个特别的分布特征, 即幂律分布 (或称长尾分布 ). 在这种分布特征

下 , 少数节点具有非常高的度数 ( 即相邻邻居数量 ), 而大多数节点的度数相对比较小 . 这意味着这样一些 “中
心节点”在图数据中具有很高的连通性, 而其他节点有相对较低的连通性. 如算法 1 所述, 在 Forward-Push 算
法的每一个 push 操作都会将到达当前节点的随机游走的部分推送给他的出度邻居. 对于这样度数较高的 “中
心节点”来说, 一方面, 他传递给出度邻居的残留值 rs(v)会因为 d 的值过大而变得很小, 此时对下游节点的影
响很小 ; 另一方面 , 由于算法的局限 , 即使传递的值很小也需要一次完整的数据传输 , 所以中心节点的每次
传输需要消耗大量的带宽 . 基于以上两方面的观察 , 为了减少这样的随机游走数量 , 可以把随机游走的部分
推送给出度邻居的一个小的随机子集. 根据证明, 这样的随机化 push 可以得到结果是准确值的一个有界无偏
估计, 如公式 (9)所示.








d out (v) ≤ d out (u )  (1   )  rs (v) /(   ) ,


push  d out (v)  d out (u )  (1   )  rs (v) /(   ) and d out (v) ≤

d out (v)  d out (u )  (1   )  rs (v) /(r     ) ,

确定性推送





d out (u )  (1   )  rs (v) /(r     ) , 随机推送



(9)

不推送

这里给出一个示例, 根据上述条件, 可以将不同出度数的节点分为 3 类, 分别是确定性推送、随机推送、
停止推送, 如图 3 所示. 针对确定性推送, 如图 4 节点 x, 其向下一节点推送的残留量为(1)rs(v)/dout(v); 对
于随机推送节点来说, 如图 3 中的节点 u 和 z, 其向下一节点推送的残留量为    d out (u ) , 但是否推送取决于
公式(9)中的随机值 r; 对于停止推送的节点来说, 如图 3 中的节点 y, 其不会向后续节点进行推送, 因为这样
的节点通常出度很大 , 并且由于残留值是均分给后续出度节点的 , 所以每一个后续出度节点都会得到很小的
残留值 , 较小的残留值对算法的终值影响很小 . 传统的方法会向每一个出度节点传送残留值 , 在残留值很小
的时候也会占用同等的带宽, 导致算法的整体带宽使用量过高.

陈子俊 等: GPPR: 跨域分布式个性化 PageRank 算法

图3

1099

Random 推送示例

首先给出 GPPR 算法的整体框架, 如算法 4 所示.
算法 4. GPPR.
输入: 图 G=(V,E), 查询集合 Q, 结束概率, 相对精度 , 最优分区映射方案 Popt.
输出: 近似单源个性化 PageRank 值  .

3:

1/n; pf1/n;
ppm/n/p;
,(2/3+2)log(2/pf)/2;

4:
5:

for each sQ do
rs[s,1]; ˆ s  ;

1:
2:

6:

rsum1; rmax1;

7:

while rmax> p/, do

8:
9:

rsum(1)rsum
(rmax , rs , ˆ s )  Random-Push(G, , rs , ˆ s ) ;

10:

end while

11:

for each v,rs(v)rs in parallel do

12:

vrs(v)/rsump

13:

$
sr (v)  [ti 
 wt (v), rs (v) / v  | i  1,..., v ]

14:

end for

15:

s*   vV sr (v)

16:

s  ˆ s  s*reduce-by -key{}
return 

17:
18: end for

算法读取预处理阶段返回的最优映射方案以及一个查询集合 Q, Q 中包含着所要查询的所有源节点. 首先
初始化阈值, 其中, ,是蒙特卡洛方法满足定义 1 近似个性化 PageRank 所需的随机游走数量(算法 4 第 3 行);
然后, 针对查询集合 Q 中的每一个查询源节点 s, 算法首先向 rs 中增加键值对s,1, 表示每次从 s 节点开始随
机游走(算法 4 第 5 行 ), 并通过 rmax> p/ ,条件来保证算法最终结果的准确性, 其中, p 是预取样阶段每个节
点取样的随机游走数量. 在迭代的过程中, 引入基于概率的 Push 算法(算法 5), 算法 5 主要通过公式(9)来进行
全局基于概率的随机游走(算法 5 解释见下), 并在迭代结束后, 通过公式 (10)聚合算法 3 和算法 5 的中间结果 ,
并得到最终的个性化 PageRank 值:

1100

软件学报 2024 年第 35 卷第 3 期

s (t )  ˆ s (t )   rs (v)   v (t )

(10)

vV

算法 5. Random-Push.
输入: 图 G=(V,E), 查询集合 Q, 结束概率, 相对精度保证 ;
输出: 最大残留量 rmax, 残留量 rs, 储备量 ˆ s .

1:

 s*  ramp-value{rs (v)    rs (v)} ;

2:

for each v,rs(v)rs in parallel do

3:

if 满足确定性推送条件

rs* (v)  [u ,(1   )  rs (v) / d out (v) | u  N out (v)]

4:
5:

else if 满足随机性推送条件

rs* (v)   u ,  


6:
7:

d out (v) | u  N out (v) 


else

rs* (v)  

8:
9:

end for

10:

rs 

11:

ˆ s  ˆ s  

12:
13:

rmaxrsreduce-value{max}
return rmax, rs, ˆ s



*
uv s

r (v )



*
s

算法 5 实现了公式(9)所定义的基于概率的随机 Push 算法 , 算法首先读取当前图数据, 针对每一个节点 v,
算法根据公式(9)所定义的 3 类推送条件, 依据其出度的大小来确定该节点的 push 操作(算法 5 的第 38 行 ). 在
这样的条件下 , 针对出度大的节点 , 可以大幅减少其进行随机游走所需要传送的数据量 , 进而降低整个算法
的通信量, 提高算法的迭代速度. 最后更新并记录最大的 rs(v), 返回该值以供上层算法 4 来确定算法中循环的
停止条件.
基于概率的 Push 算法, 其结果的无偏性证明如下.
证明: 在第 i 次进入循环时, 边(u,v)的一次 push 操作, 用 Xi+1(u,v)来表示 rˆi 1 ( s, v) 在此次 push 的增加量, 根
据 算 法 , 当 (1   )rˆi ( s, u ) / | N out (u ) |≥ 

X i 1 (u , v)  (1   ) rˆi ( s, u )/ | N out (u ) |;

N out (u ) 时 ,

N out (u )(1   ) rˆ( s, u )  | N out (u ) | 的 概 率 为 

N out (u ) ,

否 则 , Xi+1(u,v) 有

其 他 情 况 为 0. 这 里 使 用 {rˆi ( s )} 表 示

 rˆi (s, u ),

vV

Xi+1(u,v)关于 {rˆi ( s )} 的条件期望可以写成 E[ X i 1 (u , v ) | {rˆi ( s )}]  (1   ) rˆ( s, u ) / | N out (u ) | .
因为 rˆi 1 ( s, u ) 



vN out ( u )

X i 1 (u , v), 所以 rˆi 1 ( s, u ) 关于 {rˆi ( s )} 的条件期望是:
E[rˆi 1 ( s, u ) | {rˆi ( s )}] 

基于前一个等式可以得到 E[rˆi 1 ( s, u ) | rˆi ( s )] 



vN out ( u )

E[ X i 1 (u , v) | {rˆi ( s )}].

 ((1   )rˆi (s, u ) / | N out (u ) |).

u N in

又因为 E[rˆi 1 ( s, u )]  E[ E[ rˆi 1 ( s, u ) | {rˆi ( s )}]], 所以可以得到 E[rˆi 1 ( s, u )] 
由归纳法可知 E[rˆi ( s, u )]  ri ( s, u ) , 由此可推得 E[rˆi 1 ( s, u )] 

 {(1   ) E[rˆi (s, u )]/ | N out (u ) |}.

u Nin

 ((1   ) E[rˆi ( s, u )]/ | N out (u ) |)  ri 1 (s, u ),

即无

u Nin

偏性成立.
个性化 PageRank 计算的误差证明如下.
证明: 考虑从节点 v 产生  v 个随机游走,定义一个伯努利变量 X vi (t ) (当第 i 次随机游走停止在 t 节点上则
为 1, 否则为 0), 可得其期望为  (v,t). 如果一个随机游走在 t 点结束, 则该节点的 PPR 估计值  s (v) 会有一个

陈子俊 等: GPPR: 跨域分布式个性化 PageRank 算法

1101



增量为 r s (v)/ v . 令增加值为  v , 则可以得到等式 E   i v1 ( v  X vi (t ))   rs (v)   v (t ), 其中 ,  i v1 ( v  X vi (t )) 是


s (v) 从节点 v 获取的增量 . 将这个增量表示为  v , 可得等式 E   vV ( v )    vV rs (v)   v (t ). 此外 , 引入

FORA[26]所提出的定理, 即: 在每个 vV 节点产生 v=rs(v),个随机游走的环境下, s (v) 和  vV ( v ) 组合所
得到的个性化 PageRank 估计值 s (v) 满足定义 1. 由于本文算法已保证 r max ≤ p /  ,  成立 , 可进一步得到

rmax ,≤ p, 即, 任意一个节点 v 都已经预先存储了足够数量的随机游走用于保证 FORA 定理的成立. 综上 ,
本算法满足定义 1.

5

实

验

本节将通过多组实验验证评估本算法在跨域环境下的有效性 . 首先对实验数据集、对比算法和参数设置
进行描述, 在此基础上进行多组对比实验, 并对实验结果进行详细的分析.
5.1 实验设置



实验环境

算法使用 Scala 语言, 基于 Spark 框架实现. 使用 20 台分布在各大洲(共选取 10 个地理分布的数据中心,
分别为中国张家口、中国香港、中国成都、英国伦敦、德国法兰克福、美国弗吉尼亚、美国硅谷、日本东京、
澳大利亚悉尼、阿联酋迪拜, 默认选择其中相距较远的 6 的数据中心进行实验)的阿里云服务器进行实验, 每
台云服务器拥有 16 核 64 GB 内存. 为模拟跨域环境, 限制每台服务器的上行和下行带宽, 最大相差 8 倍. 数
据集使用 GrQc、 DBLP、 Stanford、 Pokec、 LJ、 Orkut、Twitter、 Friendster 共 8 个图数据集, 其中, 顶点数和
边数见表 4, 并以 k=103, M=106, B=109 的方式呈现, 详细数据可以在 KONECT[43]项目找到并下载.
表4
名称
GrQc
DBLP
Stanford
Pokec
LJ
Orkut
Twitter
Friendster



顶点数
5.2k
613.6k
281.9k
1.6M
4.8M
3.1M
41.7M
65.6M

实验数据集属性
边数
29.0k
2.0M
2.3M
30.6M
69.0M
117.2M
1.5B
1.8B

平均度数
2.77
6.6
16.4
37.5
28.2
76.3
70.5
75.7

图类型
无向
无向
有向
有向
有向
无向
有向
无向

对比算法

Spark 框架提供的 Power 算法、Delta-Push 算法和 DistPPR 算法. 其中, Power 算法使用迭代的方式来实现,
利用框架所提供的分布式能力, 并限制算法在保证绝对误差不超过 1020 的情况下停止迭代; Delta-Push 算法
使用全局 Push 算法, 并且实现了 Map-Reduce 模型, 适用于分布式环境; DistPPR 算法使用 pipeline 机制, 通过
多路并行机制来实现算法的迭代加速, 可以直接得出图上所有节点之间的个性化 PageRank 值, 但是算法运行
时间长.



参数设置

实验中的相关参数设置遵循 Delta-Push[30]算法, 设置  =1/n, pf =1/n 来保证算法求出的个性化 PageRank 值
维持在一定的误差范围内, 并满足定义 1 所述. 参照默认实现[4], 设置  =0.2,  =0.5.
5.2 实验结果

5.2.1

运行时间
第 1 个实验检查了在相同实验条件下, 4 个算法在不同数据集中的运行时间; 并且为了便于展示, 纵坐标

以对数形式呈现. 如图 4 所示, 在分布式环境下, 本研究提出的 GPPR 算法相对于分布式下的 Power 算法可以
有 35 倍的提升, 与 Delta-Push 算法相比有少量提升. 这主要是因为相对于 Delta-Push 算法来说, GPPR 算法

1102

软件学报 2024 年第 35 卷第 3 期

虽然加快了迭代的效率, 但是增加了数据的预处理步骤 , 增加了算法的耗时. DistPPR 算法非常耗时, 并且在
后 4 个数据集上的运行时间超过了 24 h, 在这里不再记录.

图4

5.2.2

平均运行时间

总通信成本
第 2 个实验检查了在相同实验条件下, 4 个算法在不同数据集中的总通信成本, 如图 5 所示. 由于 GrQc

的通信成本过低, 暂不考虑. 可以得出: Power 算法会产生最大的总通信成本, 这是因为其采用的矩阵迭代方
法会增加每一对工作节点之间的通信量 ; Delta-Push 虽然使用了预采样算法 , 但由于跨域环境带宽的异构问
题, 其通信成本也比较高; DistPPR 的通信成本与其他算法相比相对不高, 但是其运行时间随图数据量级增长
得过快, 不适用于大图环境. 由于其在后 4 个数据集上运行时间超过 24 h, 所以这里不再记录其通信成本.本
研究提出的 GPPR 算法可以显著地降低跨域环境下计算个性化 PageRank 算法的总通信成本.

图5

5.2.3

总通信成本

算法运行平均轮次
第 3 个实验说明了在相同实验条件下, 4 个算法在不同数据集中的平均迭代轮次, 结果如图 6 所示.

图6

平均轮次

陈子俊 等: GPPR: 跨域分布式个性化 PageRank 算法

1103

可以看出: 4 个算法中, DistPPR 算法需要最多的迭代次数, 所以在数据量大的后 4 个数据集上运行时间会
很长; Power 算法因为使用了矩阵迭代的思想, 为了达到近似条件, 需要进行更多轮迭代; Delta-Push 算法和

GPPR 算法迭代次数相差不大, 由于 GPPR 在 Delta-Push 算法的基础上加了随机 Push 算法, 所以可以一定程
度上降低算法的迭代次数, 降低算法的运行时间.

5.2.4

参数变化对算法的影响
第 4 个实验说明了不同参数  的情况下, 本研究提出的 GPPR 算法在 Pokec 和 LJ 上的运行时间变化. 如

图 7 的两条曲线所示, 可以看出: 参数 越大, 算法所需要的耗时越少. 因为当 变大时, 增加了每一次随机游
走停止的概率, 减少了算法每次迭代所需要的时间, 从而导致计算的总体耗时减少.

图7

5.2.5

 变化

算法的可扩展性
第 5 个实验说明了在集群数量不变、计算节点数量不同的情况下, 本研究提出的 GPPR 算法以及 Power

算法、 Delta-Push 算法在 Pokec 和 LJ 两个数据集上的运行时间, 如图 8(a)和 (b)所示, 分别选取集群工作节点
总数量为 20, 30, 40 和 50. 从图中数据可以得知: 随着计算节点数量的增加, 4 个算法的耗时不断减少; 但是当
计算节点数量增加到一定程度之后 , 对算法运行时间的降低量不再显著 . 这是因为更多的计算节点数量会导
致在算法的每一轮迭代过程中都产生更多的通信数据量 , 进一步由于跨域环境带宽的特殊性 , 更低的广域网
带宽会进一步拖慢算法的迭代速度. 根据 3 个算法的趋势对比可以看出: 本研究提出的 GPPR 算法不会影响
算法的可扩展性, 在增加计算节点的情况下, 可以降低算法的运行耗时, 提升算法的迭代效率.

(a) Pokec

(b) LJ

图8

5.2.6

工作节点数量变化

集群数量敏感性
第 5 个实验说明了在集群数量不变的情况下, 增减单个集群内计算节点观察到的算法扩展能力相关实验;

实验 6 通过设定每个集群内计算节点数量不变的情况下, 增加集群数量所带来的算法性能变化情况(如图 9 所

1104

软件学报 2024 年第 35 卷第 3 期

示). 通过设定算法的集群数量为 2, 4, 6, 8, 10 (每组数据总计算节点数量为 20), M 相应为 2, 4, 6, 8, 10. 当 M
比较小时, 图分区映射更建议使用简单的 BFS/DFS 来找到最佳执行计划(如第 3 节所述); 当 M 的数量超过地
理分布数据中心的数量时 , 即一个地理位置有两个以上相互独立的数据中心时 , 算法耗时会增加 . 因为相对
于一个地理位置一个数据中心来说 , 更多的数据中心需要额外映射计算开销 , 进而增加算法整体耗时 . 根据
数据可以得出: 集群数量为 6 时, 算法效果最好, 因为 6 正好是地理分布的数据中心个数(即中国、欧洲、美
国、日本、澳大利亚、阿联酋). 因此, 当 M 为地理分布的数据中心个数时, 算法可以得到最优的运行效率.

(a) 算法耗时

(b) 局域网使用

图9

6

总

集群数量变化

结

为了解决跨域环境下分布式个性化 PageRank 算法所存在的通信量高、单点瓶颈问题, 本文提出了 GPPR
算法 . 通过基于带宽和节点度数的启发式优化算法 , 综合考虑通信数据量和通行时间 , 在保证算法迭代运行
时间的前提下, 有效降低了算法的通信数据量和通信代价. 同时, 引入了基于概率的 Push 算法利用图的结构
信息 , 减少了算法的迭代次数 , 加快了算法所需的执行时间 . 实验结果表明 : 该方法在保证结果准确性的前
提下, 显著减少了通信数据量和通信时间.
References:
[1]

Sahu S, Mhedhbi A, Salihoglu S, et al. The ubiquity of large graphs and surprising challenges of graph processing. Proc. of the
VLDB Endowment, 2017, 11(4): 420431.

[2]

Angles R, Gutierrez C. Survey of graph database models. ACM Computing Surveys (CSUR), 2008, 40(1): 139.

[3]

Adoni HWY, Nahhal T, Krichen M, et al. A survey of current challenges in partitioning and processing of graph-structured data in
parallel and distributed systems. Distributed and Parallel Databases, 2020, 38(2): 495530.

[4]

Page L, Brin S, Motwani R, et al. The PageRank citation ranking: Bringing order to the Web. Technical Report, Stanford
University, 1998.

[5]

Chung F. A brief survey of PageRank algorithms. IEEE Trans. on Network Science & Engineering, 2014, 1(1): 3842.

[6]

Yuan Y, Wang G, Chen L, et al. Efficient subgraph similarity search on large probabilistic graph databases. arXiv:1205.6692,
2012.

[7]

Gleich DF. PageRank beyond the Web. SIAM Review, 2015, 57(3): 321363.

[8]

Park S, Lee W, Choe B, et al. A survey on personalized PageRank computation algorithms. IEEE Access, 2019, 7: 163049163062.

[9]

Dolev S, Florissi P, Gudes E, et al. A survey on geographically distributed big-data processing using MapReduce. IEEE Trans. on
Big Data, 2017, 5(1): 6080.

[10]

Aws global infrastructure. 2019. https://aws.amazon.com/about-aws/global-infrastructure/

[11]

Windows azure regions. 2019. https://azure.microsoft.com/en-us/regions/

[12]

Google datacenter locations. 2019. https://www.google.com/about/datacenters/inside/locations/index.html

[13]

Yuan Y, Wang G, Chen L, et al. Efficient keyword search on uncertain graph data. IEEE Trans. on Knowledge and Data
Engineering, 2013, 25(12): 27672779.

陈子俊 等: GPPR: 跨域分布式个性化 PageRank 算法

[14]

1105

Yuan Y, Wang G, Wang H, et al. Efficient subgraph search over large uncertain graphs. Proc. of the VLDB Endowment, 2011,
4(11): 876886.

[15]

Nawab F, Agrawal D, El Abbadi A. The challenges of global-scale data management. In: Proc. of the 2016 Int’l Conf. on
Management of Data. 2016. 22232227.

[16]

Dolev S, Florissi P, Gudes E, et al. A survey on geographically distributed big-data processing using MapReduce. IEEE Trans. on
Big Data, 2017, 5(1): 6080.

[17]

Fujiwara Y, Nakatsuji M, Yamamuro T, et al. Efficient personalized PageRank with accuracy assurance. In: Proc. of the 18th ACM
SIGKDD Int’l Conf. on Knowledge Discovery and Data Mining. 2012. 1523.

[18]

Bi X, Nie HJ, Zhang GL, Hu L, Ma YL, Zhao XG, Yuan Y, Wang GR. Boosting question answering over knowledge graph with
reward integration and policy evaluation under weak supervision. Information Processing & Management, 2023, 60(2): Article No.
103242.

[19]

Maehara T, Akiba T, Iwata Y, et al. Computing personalized PageRank quickly by exploiting graph structures. Proc. of the VLDB
Endowment, 2014, 7(12): 10231034.

[20]

Shin K, Jung J, Lee S, et al. Bear: Block elimination approach for random walk with restart on large graphs. In: Proc. of the 2015
ACM SIGMOD Int’l Conf. on Management of Data. 2015. 15711585.

[21]

Zhu F, Fang Y, Chang KCC, et al. Incremental and accuracy-aware personalized PageRank through scheduled approximation.
Proceedings of the VLDB Endowment, 2013, 6(6): 481492.

[22]

Andersen R, Chung F, Lang K. Local graph partitioning using PageRank vectors. In: Proc. of the 2006 47th Annual IEEE Symp. on
Foundations of Computer Science (FOCS 2006). IEEE, 2006. 475486.

[23]

Andersen R, Borgs C, Chayes J, et al. Local computation of PageRank contributions. In: Proc. of the WAW, Vol. 4863. 2007.
150165.

[24]

Jeh G, Widom J. Scaling personalized Web search. In: Proc. of the 12th Int’l Conf. on World Wide Web. 2003. 271279.

[25]

Wang S, Tang Y, Xiao X, et al. HubPPR: Effective indexing for approximate personalized PageRank. Proc. of the VLDB

[26]

Wang S, Yang R, Xiao X, et al. FORA: Simple and effective approximate single-source personalized PageRank. In: Proc. of the

Endowment, 2016, 10(3): 205216.
23rd ACM SIGKDD Int’l Conf. on Knowledge Discovery and Data Mining. 2017. 505514.
[27]

Bahmani B, Chakrabarti K, Xin D. Fast personalized PageRank on MapReduce. In: Proc. of the 2011 ACM SIGMOD Int’l Conf. on
Management of Data. 2011. 973984.

[28]

Lin W. Distributed algorithms for fully personalized PageRank on large graphs. In: Proc. of the World Wide Web Conf. 2019.
10841094.

[29]

Das Sarma A, Molla AR, Pandurangan G, et al. Fast distributed PageRank computation. In: Proc. of the 14th Int’l Conf. on

[30]

Hou G, Chen X, Wang S, et al. Massively parallel algorithms for personalized PageRank. Proc. of the VLDB Endowment, 2021,

Distributed Computing and Networking (ICDCN 2013). Mumbai: Springer, 2013. 1126.
14(9): 16681680.
[31]

Pu Q, Ananthanarayanan G, Bodik P, et al. Low latency geo-distributed data analytics. ACM SIGCOMM Computer

[32]

Zaharia M, Chowdhury M, Franklin MJ, et al. Spark: Cluster computing with working sets. In: Proc. of the HotCloud. 2010.

[33]

Zhou AC, Shen B, Xiao Y, et al. Cost-aware partitioning for efficient large graph processing in geo-distributed datacenters. IEEE

Communication Review, 2015, 45(4): 421434.

Trans. on Parallel and Distributed Systems, 2019, 31(7): 17071723.
[34]

Gonzalez JE, Low Y, Gu H, et al. PowerGraph: Distributed graph-parallel computation on natural graphs. In: Proc. of the Presented
as part of the 10th USENIX Symp. on Operating Systems Design and Implementation (OSDI 2012). 2012. 1730.

[35]

Yuan Y, Ma D, Wen Z, et al. Efficient graph query processing over geo-distributed datacenters. In: Proc. of the 43rd Int’l ACM
SIGIR Conf. on Research and Development in Information Retrieval. 2020. 619628.

[36]

Yuan Y, Wang G, Chen L, et al. Efficient keyword search on uncertain graph data. IEEE Trans. on Knowledge and Data
Engineering, 2013, 25(12): 27672779.

[37]

Martella C, Shaposhnik R, Logothetis D, et al. Practical Graph Analytics with Apache Giraph. Berkeley: Apress, 2015.

1106

[38]

软件学报 2024 年第 35 卷第 3 期

Bi X, Nie HJ, Zhang XY, Zhao XG, Yuan Y, Wang GR. Unrestricted multi-hop reasoning network for interpretable question
answering over knowledge graph. Knowledge-based Systems, 2022, 243: Article No. 108515.

[39]

Yuan Y, Chen L, Wang G. Efficiently answering probability threshold-based shortest path queries over uncertain graphs. In: Proc.

[40]

Ren X, Ananthanarayanan G, Wierman A, et al. Hopper: Decentralized speculation-aware cluster scheduling at scale. In: Proc. of

of the 15th Int’l Conf. on Database Systems for Advanced Applications (DASFAA 2010). Tsukuba: Springer, 2010. 155170.
the 2015 ACM Conf. on Special Interest Group on Data Communication. 2015. 379392.
[41]

Mitzenmacher M. The power of two choices in randomized load balancing. IEEE Trans. on Parallel and Distributed Systems, 2001,

[42]

Williamson DP, Shmoys DB. The Design of Approximation Algorithms. Cambridge University Press, 2011.

[43]

Kunegis J. KONECT: The Koblenz network collection. In: Proc. of the 22nd Int’l Conf. on World Wide Web. 2013. 13431350.

12(10): 10941104.

陈子俊(1999－), 男, 硕士生, CCF 学生

王一舒(1993－), 女, 博士, 讲师, CCF 专

会员, 主要研究领域为地理分布式计算.

业会员, 主要研 究领域为 图数据 管理与
分析, 时空数据管理与分析.

马德龙(1990－), 男, 博士生, 主要研究

袁 野 (1981－), 男 , 博 士 , 教 授 , 博 士 生

领域为 分布 式大 图数据 处理 与分 析, 跨

导师, CCF 杰出会员, 主要研究领域为大

域计算.

数据管理 与分析, 基于大数据的 人工智
能, 分布式大数据计算.


软件学报 ISSN 1000-9825, CODEN RUXUEW
Journal of Software,2024,35(3):1280−1306 [doi: 10.13328/j.cnki.jos.006813]
©中国科学院软件研究所版权所有.

MSA-Lab: 模型驱动的微服务集成设计平台

E-mail: jos@iscas.ac.cn
http://www.jos.org.cn
Tel: +86-10-62562563

*

熊靖浏 1, 任秋蓉 1, Shmuel TYSZBEROWICZ1,2, 刘志明 1,3, 刘 波 1
1

(西南大学 计算机与信息科学学院 软件研究与创新中心, 重庆 400715)

2

(Software Engineering Department, Afeka Academic College of Engineering, Tel-Aviv 6998812, Israel)

3

(西北工业大学 软件学院, 陕西 西安 710129)

通信作者: 刘波, E-mail: liubocq@swu.edu.cn

摘

要: 从单体系统迁移到微服务系统是当前业界对遗留系统实施再工程化的主流选项之一, 基于单体遗留系统

的微服务体系架构重构则是实现该迁移的关键步骤. 目前学界多集中在微服务识别方法的研究上; 业界虽有许多
面向微服务架构的遗留系统重构的实践, 但缺乏系统性的方法及高效鲁棒的工具. 鉴于此, 在微服务识别与模型驱
动开发方法前期研究的基础上, 研发一种模型驱动的、可用于单体遗留系统微服务化重构的集成设计平台 MSA-Lab.
它通过分析单体遗留系统运行日志中的方法调用序列, 对其中的类和数据表进行类型识别和聚类以构造抽象微服
务, 同时生成包括微服务图和微服务序列图在内的系统架构设计模型. 它包括用于微服务自动识别与设计模型自
动生成的核心部件 MSA-Generator, 以及用于微服务静态结构模型与动态行为模型可视化展现、交互式建模、模
型语法约束检验的核心部件 MSA-Modeller. 在 MSA-Lab 平台上, 通过对 4 个开源项目实施有效性、鲁棒性、功
能转换完备性等实验以及对 3 个同类型工具实施性能对比实验, 结果表明: 所提平台拥有很好的有效性、鲁棒性
及实现面向日志的功能转换完备性, 且性能更加优越.
关键词: 微服务架构; 服务识别; 设计模型生成; 交互式建模工具; 软件设计评估
中图法分类号: TP311
中文引用格式: 熊靖浏, 任秋蓉, Shmuel TYSZBEROWICZ, 刘志明, 刘波. MSA-Lab: 模型驱动的微服务集成设计平台. 软件学报,
2024, 35(3): 1280–1306. http://www.jos.org.cn/1000-9825/6813.htm
英文引用格式: Xiong JL, Ren QR, Tyszberowicz S, Liu ZM, Liu B. MSA-Lab: Integrated Design Platform for Model-driven
Development of Microservices. Ruan Jian Xue Bao/Journal of Software, 2024, 35(3): 1280–1306 (in Chinese). http://www.jos.org.cn/
1000-9825/6813.htm

MSA-Lab: Integrated Design Platform for Model-driven Development of Microservices
XIONG Jing-Liu1, REN Qiu-Rong1, Shmuel TYSZBEROWICZ1,2, LIU Zhi-Ming1,3, LIU Bo1
1

(Center for Research and Innovation in Software Engineering, College of Computer and Information Science, Southwest University,
Chongqing 400715, China)

2

(Software Engineering Department, Afeka Academic College of Engineering, Tel-Aviv 6998812, Israel)

3

(School of Software, Northwestern Polytechnical University, Xi’an 710129, China)

Abstract: Migrating from monolithic systems to microservice systems is one of the mainstream options for the industry to realize the
reengineering of legacy systems, and microservice architecture refactoring based on monolithic legacy systems is the key to realizing
migration. Currently, academia mainly focuses on the research on microservice identification methods, and there are many industry
practices of legacy systems refactored into microservices. However, systematic approaches and efficient and robust tools are insufficient.

*

基金项目: 国家自然科学基金 (62032019, 61732019, 61872051); 西南大学国家人才建设项目 (SWU116007); 重庆市自然科学基金面上
项目 (CSTB2022NSCQ-MSX0437)
收稿时间: 2022-03-03; 修改时间: 2022-06-30, 2022-09-13; 采用时间: 2022-10-11; jos 在线出版时间: 2023-05-24
CNKI 网络首发时间: 2023-05-26

熊靖浏 等: MSA-Lab: 模型驱动的微服务集成设计平台

1281

Therefore, based on earlier research on microservices identification and model-driven development method, this study presents MSA-Lab,
an integrated design platform for microservice refactoring of monolithic legacy systems based on the model-driven development approach.
MSA-Lab analyzes the method call sequence in the running log of the monolithic legacy system, identifies and clusters classes and data
tables for constructing abstract microservices, and generates a system architecture design model including the microservice diagram and
microservice sequence diagram. The model has two core components: MSA-Generator for automatic microservice identification and design
model generation and MSA-Modeller for visualization, interactive modeling, and model syntax constraint checking of microservice static
structure and dynamic behavior models. This study conducts experiments in the MSA-Lab platform for effectiveness, robustness, and
function transformation completeness on four open-source projects and carries out performance comparison experiments with three sametype tools. The results show that the platform has excellent effectiveness and robustness, function transform completeness for running logs,
and superior performance.
Key words: microservice architecture; service identification; design model generation; interactive modeling tool; software design evaluation

近年来, 微服务架构 (microservice architecture, MSA) 因其在模块化设计、故障隔离、技术异构性支持、独立
开发部署等方面的优势, 成为替代传统单体架构 (monolithic architecture) 用以开发和运维复杂软件系统的主流选
择 [1]. 诸多重要的互联网企业如 Alibaba、Netflix、Amazon 等工业界大量业务服务系统的拥有者已经或正在将单
体架构系统迁移到微服务架构系统. 微服务架构的核心思路是将系统构造为一系列适当粒度的、高内聚低耦合的、
分布式自治的功能单元 (即微服务)[2]. 各微服务实现相对单一的业务功能; 它们边界清晰并运行在专属的进程中;
它们能被独立设计、开发、部署和运维, 并通过定义良好的接口以轻量级的通信方式 (如 RESTful style) 交互和协
作 [3]. 其中, 如何从单体架构迁移到微服务架构是工业界和学术界共同关心的问题 [4].
在工业实践中, 除建造全新的微服务架构系统, 更多决策者采取了对遗留系统实施微服务化再工程的策略,
而基于遗留系统现有的软件制品 (如代码、日志) 实施微服务体系架构重构则尤为重要 [5]. 微服务体系架构重构
设计包含两个方面: (1) 系统分解与微服务识别. (2) 微服务体系架构的结构与行为特征刻画. 微服务识别的基本
思想来自于传统的系统构件分解 [6] 与组合 [7] 设计, 即: 按照高内聚低耦合 [8] 的原则及其他非功能或服务质量
(quality of service, QoS) 属性 [9], 对关联紧密的功能元素 (如类) 进行聚类. 而对微服务体系架构的静态结构和动
态行为进行建模, 则是模型驱动开发方法在软件系统设计阶段的主要工作. 所谓模型驱动开发 (model-driven
development, MDD) 即使用模型作为软件系统开发各阶段的关键输入和核心产出, 通过在各阶段针对不同关注
点的模型的构建、转换和精化来严格地构造软件系统, 并可借助模型检验等形式化方法来确保软件系统的正确
性 [10,11]. MDD 被证明能成功地应用于传统的软件密集型系统开发 [12]以及全新构造微服务系统的研发中 [12−15]. 然
而, 工业界有许多事实上正在运行的微服务系统虽是由遗留系统重构而来, 但其再工程化过程缺乏系统性方法
的指导及高效工具的支持, 以致其微服务化重构设计主要依靠设计师的个人经验, 并多采用试错法完成, 其方法
易错、低效且不可重复.
学术界对微服务体系架构重构设计的工作主要集中在系统分解和微服务识别方面. 相关工作尚未完全解决好
以下问题: (1) 通过聚类能明确系统分解后的微服务集合及每个微服务所含类的集合, 但微服务内外部结构特征并
未明确. (2) 许多基于运行日志的解析工具能根据方法调用自动构造出对象行为模型 (对象序列图), 但缺乏支持对
象行为模型向微服务行为模型转换和精化的工具. 显然, 仅明确微服务内部所含类的集合, 但缺少相应结构和行为
模型的微服务体系架构设计方案是不完整的. (3) 除了极少量研究工作 [16], 绝大部分微服务识别的研究工作要么需
要人工选择微服务数量, 要么生成微服务重构设计方案后便无法进行交互式修改, 导致任何设计变更都必须重新
执行一遍其给出的重构方法. 此外, 即便是支持交互式微服务设计的工作, 也仅是在可视化外观层面提供微服务静
态结构的调节, 没有系统性的设计模型自动重构机制及动态评估机制与之适配.
显然, 学术界和工业界均需要一种能够面向遗留系统的、高效且鲁棒地执行微服务体系架构重构设计的方法
和支持工具. 鉴于此, 我们在前期工作 [17]完成微服务体系架构设计模型相关元模型定义的基础上, 进一步研发了
一个模型驱动的、可用于单体遗留系统微服务化重构的集成设计平台 MSA-Lab. 我们的主要贡献如下: (1) 研发
了一个可用于微服务自动识别与设计模型自动生成的核心部件 MSA-Generator; 它可通过对单体遗留系统日志中
执行轨迹 (execution traces) 的扫描, 自动完成类和数据表的类型识别和聚类以构造抽象微服务; 在此基础上根据

1282

软件学报 2024 年第 35 卷第 3 期

对象间的方法调用序列, 自动解析得到微服务内外部的静态结构模型及动态行为模型. (2) 研发了一个可支持微服
务静态结构模型 (微服务图) 与动态行为模型 (微服务序列图) 可视化展现、交互式建模、模型语法约束检验的核
心部件 MSA-Modeller; 它可载入 MSA-Generator 中自动生成的微服务设计模型, 在集成设计环境中可视化展现并
供开发者按需进行二次建模; 并对二次建模结果, 提供基于对象约束语言 (object constraint language, OCL) 的语法
检验和基于通用微服务划分合理性指标 [8,16,18,19]的量化评估.
本文第 1 节综述相关工作. 第 2 节介绍 MSA-Lab 平台技术框架. 第 3 节和第 4 节分别介绍 MSA-Lab 核心部
件 MSA-Generator 和 MSA-Modeller 的工作原理. 第 5 节是工具的评估实验及实验结果讨论. 第 6 节总结全文并
介绍未来的工作.

1 相关研究综述
软件重构是软件工程领域的重要研究与实践方向之一 [6], 将传统的单体架构重构为 MSA 已引起学术界和工
业界的广泛关注. 一些技术框架和原型工具也在不断推出, 我们将其划分为: (1) 基于系统分解与聚类方法的微服
务设计工具; (2) 基于模型驱动的微服务设计工具. 下文分别对相关工作进行综述.
1.1 基于系统分解与聚类方法的微服务设计工具
能否高效和自动的帮助工程师进行系统拆分和微服务识别是工具和框架的一个重要特点, 故我们对这类工具
按自动化支持程度进行划分. 早期大多数工具仅支持半自动的微服务设计. 例如: Levcovitz 等人 [20]根据代码静态
依赖图, 实施自底向上的子系统划分与聚类, 并通过数据表的人工识别完成微服务的构造设计. Chen 等人 [21]提出
一种半自动框架: 即首先人工分析业务需求并绘制数据流图, 继而将具有相同输出数据的操作和数据划分为一个
微服务. 类似的工作还有 Li 等人 [22]提出的数据流驱动的半自动分解框架. 这些半自动的框架对大型系统改造而
言, 时间成本是难以承受的.
近年来, 也有学者提出了一些全自动工具和框架. 例如: Jin 等人 [8]提出的框架将在同一方法执行路径上一起
出现的类认为是一组功能相关单元, 按此思想通过定义评估函数将功能相关的类聚集成微服务; 但他们所提出的
框架手工配置复杂 (服务生成数量、算法迭代次数等重要参数需要手工设置), 并且没有对数据库进行划分. 李杉
杉等人 [23]提出了一个基于优化的数据流驱动的微服务拆分工具, 采用两阶段的聚类算法来对数据表和类实现微
服务拆分; 但他们的工具目前只支持服务拆分的核心功能, 并且参数设置需要工程师的先验知识. Kalia 等人 [16]根
据遗留系统的静态和运行时信息提出了一个基于 AI 技术的拆分设计工具, 最后得出微服务所包含的类的集合; 但
他们忽略掉了对数据表的拆分且也需事先预设服务生成数量. Abdullah 等人 [24]使用系统访问日志和无监督的机器
学习方法将具有相似性能和资源要求的 URL 组作为一个微服务, 但基于 URL 拆分的结果可能存在大量的代码重
复, 不方便后面维护. Mazlami 等人 [19]将数据库中的表当作普通的类来进行划分解决数据库的划分问题, 但没有考
虑微服务数据的共享或划分问题. Santos 等人 [18]提出了一系列指标来组合衡量微服务拆分成本, 以可视化的形式
给出了类的拆分结果, 但他们没有对持久层的类进行划分.
不过, 大多数现有方法仅关注如何从遗留系统中获取信息以生成微服务划分结果这一过程. 其方法和工具得
到的设计产出, 仅可回答每个微服务由哪些功能单元 (例如类) 组成, 却往往缺乏对可指导微服务系统后续开发工
作的 MSA 静态结构及动态行为特征的刻画. Zhang 等人 [9]使用功能信息和性能信息对遗留系统进行拆分, 给出的
拆分结果只包括包含类名的结果. Jin 等人 [8]给出的拆分结果也只包括类名, 但将类分为了接口类和业务类. Kalia
等人 [16]给出了拆分结果的可视化展现, 其中刻画了类与类的调用链路, 但没有给出微服务重构后微服务之间的结
构及行为特征的描述.
此外, 大多数研究中的工具和框架给出的拆分结果一旦生成, 就不允许工程师再进行手动的修改. 工业实践表
明: 有经验的工程师对系统实施的交互式设计工作往往能优化和提升自动构造的微服务设计方案. 在支持交互式
设计调整方面, 丁丹等人 [25]提出的 MSDecomposer 可将拆分过程可视化, 并能够手动调整数据表的归属. 李杉杉
等人 [23]的工具可以手动将类的归属改在另一个微服务下. 但这些交互式设计调整仅限于对各微服务所含功能单

熊靖浏 等: MSA-Lab: 模型驱动的微服务集成设计平台

1283

元 (类) 的归属进行修改, 尚不涉及微服务架构模型 (包括静态结构模型及动态行为模型) 的调整. 同时, 作为支持
微服务架构交互式设计的工具, 还需要相应的动态语法检查与指标评估机制, 以便对调整后得到的不同划分方案
给出语法正确性检验和基于通用合理性指标的评估数据对比.
1.2 基于模型驱动的微服务设计工具
与基于系统分解与聚类方法的微服务设计方法不同, 基于模型驱动的微服务设计方法从用户需求开始, 对微
服务静态结构和动态行为进行严格的刻画; 并利用模型精化和模型转换构造不同层级、不同视角下的设计模型,
直至获得微服务程序 (程序代码本身也是严格的模型); 同时, 基于模型检验与分析的工具也可用于保障微服务模
型的一致性、正确性等性质. 近年来, 模型驱动的 MSA 系统设计方法研究已引起广泛关注 [12,15,26], 但主要用于全
新微服务系统的构建. 而单体遗留系统的模型驱动 MSA 重构设计方法和工具研究仍处于初级阶段.
在面向全新微服务系统模型驱动开发的研究工作中, 基于领域驱动设计 (domain-driven design, DDD) 的模型
驱动方法最具代表性 [27] . Gysel 等人 [13] 提出了一个能基于 16 个标准, 从用户输入文件进行微服务拆分的工具
Service Cutter. 每个微服务由数据、操作和工件 (操作在数据上作用后产生的结果) 组成. 在基于 Service Cutter 的
基础上, Kapferer 等人还提出了一种基于 DDD 的微服务拆分工具 Context Mapper[15], 其微服务模型由有界上下文
表示, 并使用上下文关系和对称关系来描述有界上下文之间的关系. 但这两个工具要求用户编写所有的输入文件,
时间成本高, 不适用于大型系统改造. Rademacher 等人 [26]使用 UML profile 来进行领域驱动设计里的概念建模.
Profile 中主要包含聚合根、实体、仓库和服务等元素. Schneider 等人 [28] 和 Petrasch 等人 [14] 也同样使用 UML
profile 来扩展领域驱动设计的建模元素, 并开发了手工图形化建模工具. 但他们的 profile 中没有定义元模型的约
束条件, 以便检查模型语法避免模型误用.
此外, 还有许多非 DDD 系列的建模工具和框架也被提出. Sorgalla 等人 [12]提出了名为 AjiL 的手工图形化微
服务建模工具, 将每个微服务定义为实现业务能力的功能微服务和基础设施服务, 微服务拥有接口, 实体端口等属
性. 这些模型是与 Spring Cloud 相关的, 工具可以根据这些模型和其对应的属性生成平台相关的 Spring Cloud 代
码, 这种平台相关性模型难以进行迁移和复用. Terzić等人 [29]提出了用于对遵循 RESTful 风格的微服务领域建模
语言, 使用户通过提供图像和文字输入来生成微服务代码. 但这些模型往往只局限在微服务的结构和微服务之间
的关系, 忽视了对微服务的行为的描述. Tyszberowicz 等人 [30]基于模型从自然语言需求中提取系统功能来进行微
服务拆分, 但这种框架对需求的要求高, 结果由系统功能对应的文字组成, 不好对应代码实现.
支持遗留系统向 MSA 系统迁移的模型驱动工具尚缺较为成熟工作. Escobar 等人 [31]提出了一个以 Java 企业
版应用模型为中心的流程来分析可视化业务层和数据层之间的当前结构和依赖关系, 并以图像的结果给出微服务
拆分结果, 但这种图像只是一种辅助理解的可视化展现, 不能支持模型驱动的开发过程.
总之, 现有的工具和框架尚不能提供一种高效鲁棒的集成设计平台, 以便对遗留系统进行解析并自动生成包
含微服务识别及微服务系统架构模型在内的设计方案; 且支持设计方案的可视化展现需通过图形用户接口 (GUI)
进行交互式调整, 而不必重新运行自动识别与模型生成算法, 也无需事先指定微服务数量等“超参数”.

2 MSA-Lab 的技术框架
MSA-Lab 的当前版本致力于提供一种模型驱动的、可用于单体遗留系统微服务化重构的集成设计平台, 其
技术框架如图 1 所示. 它包含两个用于 MSA 结构与行为建模的微服务元模型, 即: 微服务图元模型 (meta-model
for microservice diagram) 和微服务序列图元模型 (meta-model for microservice sequence diagram); 以及两个用于微
服务及设计模型自动生成与交互式建模的核心功能部件, 即: 微服务及设计模型生成器 (MSA-Generator) 和微服
务建模工具 (MSA-Modeller). 微服务元模型及核心功能部件分别从建模元素和建模工具两方面支持构建微服务体
系架构的两种设计模型, 即: 刻画微服务内外部结构 (intra- and inter-structure of microservices) 特征的微服务图
(microservice diagram) 和描述微服务间交互行为的微服务序列图 (microservice sequence diagram).
微服务元模型旨在定义可用于刻画微服务体系架构两种设计模型的建模元素及模型约束. 相似地, 例如为构

软件学报 2024 年第 35 卷第 3 期

1284

建表达面向对象方法中概念模型的类图, 需定义类 (含类名, 属性, 方法定义) 和关系 (含类型, 角色, 重数等属性的
定义) 等建模元素的元模型; 事实上 UML 标准规范已提供这些元模型的标准定义. 但对于微服务这类新型系统构
件而言, UML 规范尚无标准元模型予以支持. 故在前期工作 [17]中, 我们利用 profile 机制扩展定义了适用于刻画微
服务体系架构静态结构和动态行为模型的元模型. 其本质是通过对标准 UML 类型的继承、聚合和关联等操作来
定义和添加新的构造型 (stereotype); 新的构造型即自定义的建模元素, 它包含建模元素相应的图形表示、附加属
性, 及利用对象约束语言 (OCL) 等方法定义的语法约束 (以避免构造型的误用). MSA-Lab 当前版本支持的两种微
服务元模型所定义的新构造型如下.

MSA-Lab
用于单体遗留系统微服
务化重构的集成设计平台

微服务元模型
可扩展集成设计平台

微服务结构与行为
核心功能部件

组成

MSA-Modeller

MSA-Generator

微服务图

微服务序列图

调用序列日志

服务识别

模型集成开发平台

导入或新建模型

微服务模型

建模元素生成

图1

MSA-Lab 平台的技术框架

(1) 微服务图元模型 (meta-model for microservice diagram): 定义了构造微服务图 (microservice diagram) 所需
的各种新构造型 (stereotypes) 及 OCL 语法约束 (如图 2 所示). 这些构造型 (stereotypes) 包括: Microservice (微服
务)、Controller Class (CC, 控制类)、General Subordinate Class (GSC, 一般附属类)、Data Access Class (DAC, 数据
访问类)、Data Entity Class (DEC, 数据实体类)、MSInterface (Interfaces, 微服务接口). 利用这些新构造型建立的微
服务图的具体案例可见本文第 4.2 节.
(2) 微服务序列图元模型 (meta-model for microservice sequence diagram): 定义了构造微服务序列图 (microservice sequence diagram) 所需的各种新构造型 (stereotypes) 及 OCL 语法约束 (如图 3 所示). 这些构造型
(stereotypes) 包括: MSInteraction 和 MSMessage. 其中, MSInteraction 由 UML 元模型 Interaction (序列图) 继承而
来, 我们添加新的 3 个标注 (MSs 代表此场景下所参与的微服务; traceNum 代表当前交互所对应的运行日志中的
Trace; actors 代表此场景下所参与的角色, 其 OCL 约束定义分别是 MSs_Constraint, TraceNum_Constraint 和
Actor_Constraint), 用来验证微服务序列图是否满足规范, 同时验证对象序列图向微服务序列图转换的过程中是否
出错. MSMessage 则是由 UML 元模型 Message 继承而来; 为其新添加的标注为 Interface, 用来记录此消息对应的
微服务接口; 其 OCL 约束 Interface_Constraint 限制每个 MSMessage 只有一个 MSInterface 与之对应. 利用这些新
构造型建立的微服务序列图的具体案例可见本文第 4.2 节.
MSA-Lab 当前版本提供的用于微服务及设计模型自动生成与交互式建模的核心部件如下.
(1) 微服务及设计模型生成器 (MSA-Generator): 支持微服务自动识别与设计模型自动生成的核心部件. 它可
从遗留系统运行日志中提取对象类型信息及交互信息, 自动完成类和数据表的识别和聚类, 以构造抽象微服务; 在
此基础上根据对象间的方法调用序列, 自动解析得到包括微服务内外部静态结构模型及动态行为模型在内的微服

熊靖浏 等: MSA-Lab: 模型驱动的微服务集成设计平台

1285

务设计模型.
(2) 微服务建模工具 (MSA-Modeller): 支持可视化展现与交互式建模的核心部件. 它可载入自动生成的微服务
设计模型, 在集成设计环境中可视化展现并供开发者按需进行二次建模; 并对二次建模结果提供基于 OCL 的语法
检验和基于通用微服务划分合理性指标的量化评估反馈.
我们将在接下来的第 3 节和第 4 节分别详述以上两个核心部件.
CC_Constraint
{{OCL} self.CC->size()=1}
《stereotype》
MSInterface

《stereotype》
Microservice

+isRaised: Boolean[1]
1..*

1

+CC: Controller Class[1]
+GSCs: General Subordinate Class[*]
+DACs: Data Access Class[*]
+Interfaces: MSInterface[1..*]

《metaClass》
Interface

《stereotype》
Controller Class

DAC_Constraint
{{OCL} self.DACs->size()>=0}

GSC_Constraint
{{OCL} self.SCs->size()>=0}

1

1

《metaClass》
Component
1..*
1

《stereotype》
Data Entity Class

1
0..*

0..*
《stereotype》
General Subordinate Class

《stereotype》
Data Access Class
+DECs: Data Entity Class[1..*]

1
《stereotype》
Subordinate Class

《metaClass》
UMLClass

Entitys_Constraint
{{OCL} self.DECs->notEmpty()}

Interface_Constraint
{{OCL} self.interfaces->notEmpty()}

图2
TraceNum_Constraint
{{OCL}
self.traceNum>=−1}

MSs_Constraint
{{OCL}
self.MSs->size>=1}

微服务图的元模型与约束

《stereotype》
MSInteraction
+MSs: Microservice [1..*]
+traceNum: Integer [1]
+actors: Actor [1..*]

《metaClass》
Interaction

图3

《stereotype》
MSMessgae

Interface_Constraint
{{OCL}
self.interface->size()=1}

+interface: MSInterface

《metaClass》
Message

Actor_Constraint
{{OCL}
self.actor->size()>=1}

微服务序列图的元模型与约束

3 MSA-Generator
MSA-Generator 是一个 Web 工具, 它以单体遗留系统的方法调用日志为输入, 进行微服务识别与设计模型自
动生成, 并提供设计方案的量化评估.
MSA-Generator 采用前后端分离的方式开发, 前端采用 Vue 开发, 后端采用 Java 和 Golang 开发. 其中 Java 开
发的后端主要负责生成供 MSA-Modeller 导入的微服务模型对应文件. Golang 开发的后端主要负责服务识别、设
计评估与 MSA-Modeller 前端建模相关的初级建模功能, 以及将生成模型文件所需的数据传给 Java 后端. MSAModeller 是一个微服务模型的集成开发环境, 在第 4 节会对其进行介绍.
图 4 是 MSA-Lab 平台进行微服务重构的工作原理和流程, 可分为 5 个部分: (1) 功能单元获取, 对收集的日志
进行分析, 提取用于聚类的功能原子. (2) 调用关系分析, 对日志中的调用关系进行分析, 得到关系矩阵. (3) 抽象微

软件学报 2024 年第 35 卷第 3 期

1286

服务识别, 使用不同聚类函数对功能原子进行聚类, 并识别出抽象微服务. (4) 微服务模型与重构设计生成, 生成微
服务重构设计和对应的微服务模型. 在这个过程中, 开发者可根据对设计的评价结果以及业务需求对微服务设计
进行调整, 工具可根据调整结果生成新的设计与模型. (5) 微服务模型使用, 将从 MSA-Generator 中自动生成的模
型文件导入进 MSA-Modeller, 在集成环境下进行模型驱动的开发.
MSA-Generator 遗留系统迁移
微服务模型与重构设计生成
根据设计方案结合实际,
手动调整类组成
设计方案指标评价
功能单元获取

测试用例

收集日志

单体系统

类型识别

监控日志

调用关系分析

抽象微服务识别

建立矩阵

类划分, 接
口识别

控制类、其余附属类、
数据访问类、数据实
体类对象

关系矩阵

微服务模型使用
未来计划融入的工具:
MSA-Designer 微服务新系统构造

根据方案进
行评价计算
微服务模 微服务图
型生成

抽象微服务

微服务序列图

可视化
微服务推荐
设计方案

模型驱动开发
加载进
UML 模型 Papyrus 供 Papyrus 加载的微
代码等其
服务模型对应文件
他结果
应用 profile 根据
方案进行建模
MSA profile

MSA-Coder 微服务代码生成器

MSA-Modeller 模型集成开发环境

图4

MSA-Lab 中的微服务重构流程

我们将使用 JPetStore [32]作为辅助案例来介绍工具的工作原理; JPetStore 是一个单体架构的小型宠物商店系
统, 包括账户管理, 宠物品种管理, 购物车管理, 订单管理等用例. 本节接下来的部分将介绍 MSA-Generator 的主要
功能: 抽象微服务识别和微服务模型生成; 并讨论微服务识别结果评估指标的设计和使用.
3.1 抽象微服务生成
在获得微服务模型之前, 我们需要获取抽象微服务, 这部分的过程对应图 4 中的前 3 步: (1) 对遗留系统进行
日志的收集, 分析其中的方法调用序列, 识别对象及其对应类的所属类型, 得到组成抽象微服务的功能单元 (被识
别了类型的类). (2) 通过分析对象所对应的类之间的关系, 得到代表功能单元关系的关系矩阵. (3) 根据功能单元
的关系矩阵及接口复杂度、用例专注度信息, 使用改进的 NSGA-II 算法 [33]对功能单元进行聚类, 得到每个微服务
的类组成. 通过类组成进一步识别微服务接口, 最后得到抽象微服务. 抽象微服务包括一个微服务的所有类和接
口, 并识别了每个类的类型, 是生成微服务图和微服务序列图的关键.
3.1.1

功能单元获取

我们通过收集测试用例并在遗留系统上运行, 得到方法调用序列作为工具平台的输入, 我们选用 Kieker 来收
集所需要的方法调用序列. Kieker 是一个开源的执行日志收集和分析工具 [34], 它能够对 Java 系统通过动态代理的
方式, 得到代码执行路径, 并将对应的方法调用序列写入日志当中. 在本文中, 我们使用 Kieker 对遗留 Java Web 系
统进行日志的收集. Web 系统通常由于其业务的迭代频繁而难以维护和扩展, 具有代表性. 我们选取不同大小、不
同领域的 Web 系统进行实验, 以验证我们方法的鲁棒性. 对于其他语言的遗留系统, 可以使用 SkyWalking [35] ,
Pinpoint[36]等不同的工具进行收集. 我们的工具平台运行所需要的遗留系统信息, 是包含数据库调用信息的方法序

熊靖浏 等: MSA-Lab: 模型驱动的微服务集成设计平台

1287

列, 这些方法调用序列是由开发者提供的. 根据本文以 Kieker 收集日志并进行分析的思想, 也可迁移至不同语言
系统的日志进行处理.
Kieker 的标准版本 (v1.13) 不能跟踪方法调用与数据库的交互. 因此我们改进了其运行时插入到被代理方法
前后的探针 (Probe), 通过提取被代理方法中直接调用的 SQL 语句来支持这一点. Kieker 生成的执行日志使用
Trace 作为基本单元, Trace 包含由 Web 请求 (例如, 单击网页中的一个按钮) 触发的方法调用序列. 方法调用序列
中的方法调用分为以下 3 种类: (1) 构造方法 (创建新对象); (2) 持久层方法 (对数据实体执行 CRUD 操作); (3) 通
用方法 (表示所有不同于以上两种的其他方法). 每个方法调用都由一对 Line 表示: 一个开始 Line 和一个结束
Line. Line 的属性在表 1 中描述. 特别的, 构造方法中的属性类型可以是 BeforeConstructorEvent 或 AfterConstructorEvent; 持久化方法可以是 BeforeOperationEventWithSql 或 AfterOperationEventWithSql; 通用方法可以是
BeforeOperationEvent 或 AfterOperationEvent.
表1
名字
type
generation time
trace id

日志行结构

描述
BeforeOperationEvent, AfterOperationEvent, BeforeConstructorEvent,
AfterConstructorEvent BeforeOperationEventWithSql, AfterOperationEventWithSql
该行生成的时间
用以标识唯一请求的全局唯一ID

order id

Trace中的顺序ID

method identifier
class identifier
object id

被调用方法的完备签名
被调用类的完备签名
用以标识运行时对象的唯一ID

table name

持久层方法特有属性, 标识SQL语句中所涉及的数据表名

为了提取功能单元和进一步设计模型生成所需的信息, 我们将日志中的每个方法调用重新组织为 MessageRecord, 全部的调用关系被表达为一个 MessageRecord 的集合, 记作{MessageRecords}, MessageRecord 被描述为:
MessageRecord =<SenderObject, ReceiverObject, MethodSig, MethodType>, 其中,
• SenderObject =<OID, ClassSig>, 是调用关系之中, 调用发出对象的信息, 包括对象唯一 ID 和对象所对应的
类签名.
• ReceiverObject =<OID, ClassSig>, 是调用关系之中, 调用接收对象的信息, 包括对象唯一 ID 和对象所对应的
类签名.
• MethodSig, 是调用关系之中, 调用接收对象的被调用的方法签名, 其中构造方法的签名统一为<init> 函数.
• MethodType, 是被调用方法的类型, 分别是 CREATE, UPDATE, READ, DAOCALL 代表着创造对象的构造
方法, 改变对象状态的更新方法, 不改变对象状态的读方法以及使用数据访问对象的方法. 通过 MethodSig 的关键
字, 能够确定前 3 种方法的类型. 例如 CREATE 类型的方法签名是<init>, UPDATE 类型和 READ 类型的方法签
名通常包含 set 和 get 关键字, DAOCALL 类型则由方法调用是否直接触发 SQL 语句执行进行识别.
基于{MessageRecords}, 我们设计了一种算法 1, 将对象自动划分为 4 种类型: 控制对象、一般附属对象、
数据访问对象和数据实体对象. 这些对象所属的类与第 2 节中的控制类、一般附属类、数据访问类、数据实
体类等构造型一一对应, 他们所属的类被称为功能单元. 算法 1 中自动识别的检查指标是按照类对应对象的
特征来进行识别, 并能够对系统中所有类进行类型划分: 控制对象作为模块和外部通讯的唯一出入口, 是一个
永久对象 [37]. 意味它在执行测试用例前已经存在, 同时不在执行中被销毁; 数据访问对象下的方法会直接调
用 SQL 语句; 数据实体对象对应 SQL 语句中的数据表; 一般附属对象为所有对象集合除去以上 3 种对象的补
集. 通过算法 1, 能够对系统中的所有类进行类型划分. 这些识别出类型的类, 作为功能原子用于后续的微服
务聚类当中.

软件学报 2024 年第 35 卷第 3 期

1288

算法 1. 对象类型识别算法.
输入: MessageRecord 的集合{MessageRecords}; 记录每个 DAO 能够访问的所有 DEO 的 Map DAO2Tables (一个
DEO 对应着数据库中的一个 Table. Tables 为数据表的集合, 在预处理 Trace 时获得. 如果日志行结构 Line 的 table
name 属性不为空, 则将 Line 对应方法所属对象为键, table name 下的所有数据表为值添加或修改进 Map 中);
输出: 4 种类型的对象集合.
1. Function IdentifyObjectType({MessageRecords}, DAO2Tables)
2. {OBJECTs} //记录所有对象的集合
3. {COs} //控制对象集合
4. {GSOs} //一般附属对象集合
5. {DAOs} //数据访问对象集合
6. {DEOs} //数据实体对象集合
7. For MessageRecord in {MessageRecords}
8.

Add MessageRecord.SenderObject and MessageRecord.ReceiverObject to {OBJECTs}

9.

If MessageRecord.MethodType == CREATE

10.

//控制对象的特征: 此对象是单例模式; 向其他对象发送过消息; 被与系统业务无关的框架类创建;

11.

//根据控制对象特征, 检测 Object 是否符合控制对象的规范 CheckController (Object)

12.

If CheckController (MessageRecord.receiverObject)

13.

Add MessageRecord.receiverObject to {COs}

14.

End if
Else MessageRecord.MethodType == DAOCALL //如果方法类型是 DAOCALL, 调用了 SQL 语句

15.

Add MessageRecord.receiverObject to {DAOs} //方法所属对象是 DAO, 加入{DAOs}

16.
17.

//根据 DAO2Tables 查找当前 DAO 能够访问的所有 Tables, 将所有 Table 转换为对象, 添加进{DEOs}

18.

Add DAO2Tables.get(MessageRecord.receiverObject) to {DEOs}

19.

End if

20. End for
21. //{GSOs}为{OBJECTs}与{COs}、{DAOs}和{DEOs}的补集合
22. {GSOs} = {OBJECTs}∁({COs} ∪ {DAOs} ∪ {DEOs})
23. Return {COs}, {GSOs}, {DAOs}, {DEOs}
3.1.2

调用关系分析

这一步计算对象所对应的类之间的关系. 建立控制类与一般附属类之间的关系矩阵—业务矩阵 (控制类与一
般附属类都是业务类, 执行业务逻辑但不进行数据存储), 以及数据访问类与业务类之间的关系矩阵—数据访问类
使用矩阵. 在后续会使用业务矩阵对业务类进行聚类, 得到微服务的业务类. 最后根据每个微服务的业务类组成,
使用数据访问类使用矩阵得到微服务数据存储相关的类.
控制类与一般附属类决定了微服务接口以及具体的业务逻辑, 在每个微服务的业务逻辑确立后, 才能进行数
据相关的类的划分. 我们使用{MessageRecords}从功能联系 (类与类之间的具体调用关系, 体现为一个类对另一个
类进行了多少次什么样的操作) 的角度度量控制类和一般附属类之间的关系. 建立控制类和一般附属类的关系矩
阵 (业务矩阵, 如表 2), 并称控制类和一般附属类为业务类 (Business Class).
Ri j 是对第 i 个控制类和第 j 个一般附属类之间函数关系的量化, 定量公式定义如下:
Ri j = IStrc × (IFrec )kc + IStru × (IFreu )ku + IStrr × (IFrer )kr

(1)

IStrc , IStru , IStrr 常量被用于定义不同类型方法的调用强度, 构成调用强度常量组 (InvocStren). 定义 InvocStren

熊靖浏 等: MSA-Lab: 模型驱动的微服务集成设计平台

1289

的原因是不同类型的方法对对象之间的度量关系应该有不同的影响, 一个对象创建另一个对象比读取另一个对象
的关系强度要强上许多, 需要区分如下 3 个常量.
• IStrc , CREATE 类型方法的强度常量, 两个对象之间的最强的关联, 应该对度量关系有着最大的影响.
• IStru , UPDATE 类型方法的强度常量, 代表的关联其次.
• IStrr , READ 类型方法的强度常量, 代表的关联最弱.
IFrec , IFreu , IFrer 常量构成调用频度常量组 (InvocFreq), 这 3 个常量用以定义不同类型方法的调用频度. 定

义 InvocFreq 的原因是随着调用数量的上升, 调用数量应该越容易占据主导地位. 对象之间的频繁读操作也应该尽
量放在一个微服务内, 而不是跨服务读取数据提高通信代价, 需要区分如下 3 个常量.
• IFrec , CREATE 类型方法的频度常量.
• IFreu , UPDATE 类型方法的频度常量.
• IFrer , READ 类型方法的频度常量.
表2

业务矩阵
一般附属类

控制类
GSC1

GSC2

GSC3

GSC4

…

CC1

R11

R12

R13

R14

…

CC2

R21

R22

R23

R24

…

CC3

R31

R32

R33

R34

…

…

…

…

…

…
…

在 InvocStren 和 InvocFreq 中, 常数值的定义依据如下原则.
(1) 在量化过程之中, 调用强度相比调用频度应该占据主导地位. 因此, CREATE 方法的调用强度 IStrc 应该被
设置为高于 UPDATE 方法的调用强度 IStru 一个数量级的值. IStru 和 IStrr 之间的关系也遵循该理念 (本文实验设
置为 IStrc = 100, IStru = 10, IStrr = 1 , 设置依据见第 5.1 节参数设置).
(2) 尽管调用强度常量在关系度量函数中占主导地位, 但调用数量达到一定多时, 调用频度的影响也需要有机
会成为主要因素. 调用频度常量都被设置为稍微大于 1.0 的值, 并且 IFrec > IFreu > IFrer (本文实验设置为 1.03,
1.02 和 1.01, 设置依据见第 5.1 节参数设置).
最后, 由 kc 、 ku 和 kr 组成调用数量参数组, 分别是第 i 个控制类实例化的对象与第 j 个一般附属类实例化的
所有对象之间存在的 CREATE 方法、UPDATE 方法的数量和 READ 方法的数量.
在业务矩阵建立后, 我们接着分析数据访问类与业务类之间的关系. 我们建立数据访问类使用矩阵, 以此统计
每个业务类所实例化的所有对象对不同的数据访问类下的所有对象的使用次数, 如表 3 所示. 其中, S i j 代表
{MessageRecords}中 BC i 实例化的所有对象调用 DAC j 实例化的所有对象的次数.
表3

数据访问类使用矩阵
业务类

数据访问类

3.1.3

BC 1

BC 2

BC 3

BC 4

…

DAC 1

S 11

S 12

S 13

S 14

…

DAC 2

S 21

S 22

S 23

S 24

…

DAC 3

S 31

S 32

S 33

S 34

…

…

…

…

…

…
…

抽象微服务识别

一个抽象微服务定义为 microservicei =< CCi , {GSCs}i , {DACs}i , {DECs}i , {I s}i > , 其中, i ∈ [1, |{CC s }|] , 上界| {CC s } |
表示生成的抽象微服务的数量应该等于控制类的数量.
我们首先使用基于 NSGA-II[33]改进的遗传算法将所有控制类与一般附属类聚集成|{CCs}|个业务集群. 一个业

软件学报 2024 年第 35 卷第 3 期

1290

务集群 (BusinessCluster) 对应着一个抽象微服务下的控制类和一般附属类, 负责完成每个抽象微服务的业务逻辑,
但不进行数据的存储. 所有业务集群称为业务集群候选集合 ({BusinessClusters}).
针对一个业务集群候选集合, 一组目标函数被提出以评估该集合, 它们分别是功能联系 (类与类之间的具体调
用关系, 体现为一个类对另一个类进行了多少次什么样的操作) 度 (function connection objective), 接口复杂度
(interface params complexity objective), 业务专注度 (business focus objective). 同时我们也为算法预留了接口, 使工
程师可以自行添加或修改目标函数.
FO 是对于一个 {BusinessClusters} 在功能联系视角下的度量, 其中的 n 是 {BusinessClusters} 中业务集群的数
量, Fc (BusinessClusteri ) 是 BusinessClusteri 中 CC i 和 {GS Cs}i 之间 Ri j 的累加和, 累加和可以由业务矩阵查出.
FO 的数值越大, 意味该抽象微服务候选集合表现出更好地高内聚低耦合的特性.
∑n
max FO =
Fc (BusinessClusteri )
i=1
∑n
min IO =
Ic (BusinessClusteri )
i=1
∑n
min BO =
Bc (BusinessClusteri )
i=1

(2)
(3)
(4)

IO 是对于一个 {BusinessClusters} 在接口复杂度视角下的度量. Ic 是业务集群 BusinessClusteri 中所有接口方
法的传入参数与返回参数复杂度的总和, 反映了该业务集群接口的复杂度, 其中的 k 是业务集群中接口的总数.
(
)
(
)
complexity param j 和 complexity return j 用于计算业务集群中第 j 个接口的参数复杂度, 其中 j ∈ [1, k] . 为了区分
不同类型的参数, 定义了两种不同的参数复杂度. 类和 List 的复杂度为 10, 基本数据类型的复杂度为 1. 当 IO 的值
较小, 意味着最终在 {BusinessClusters} 上形成的抽象微服务接口的设计更简单, 符合迪米特法则. 而在服务之间
进行通信时, 接口的参数越简单, 越有利于降低服务之间的通信成本.
∑k
(
)
(
)
Ic (BusinessClusteri ) =
complexity param j + complexity return j
j=1

(5)

BO 是对于一个 {BusinessClusters} 在用例视角下的度量, Bc (BusinessClusteri ) 计算了一个业务集群在代表
性 Trace 集合 (第 3.2.2 节讲述) 中参与了多少条 Trace. 如果一个业务集群涉及很多不同的 Trace, 这意味着该抽象
微服务有更大的可能性参与到每个用例中, 由此产生的微服务的可重用性很低. 根据单一责任原则, 每个微服务的
责任应尽可能单一, 并集中在单一业务领域. 当 BO 的值较小时, 意味着最终在 {BusinessClusters} 候选集合上形成
的抽象微服务更专注于自己的业务.
在识别出 {BusinessClusters} 后, 我们已经知道每个抽象微服务下负责业务流程的业务类组成, 并根据业务类
来确定数据访问类的归属. 在功能单元识别阶段, 我们记录了每个数据访问对象所访问的数据表 DAO2Tables (数
据表 Table 对应数据实体类). 我们查询数据访问类使用矩阵, 将每个数据访问类分配给使用其次数最多的业务集
群, 并将数据访问类访问的所有数据实体类也分给该业务集群. 一个数据实体类可能同时被多个抽象微服务所拥
有. 我们允许这种代码重复是因为在单体架构系统下, 通常只会访问一个数据库. 而将数据库进行拆分后, 由于单
体系统设计不规范, 很可能出现几个微服务同时依赖相同数据表的情况.
得到每个抽象微服务的类组成后, 我们可以通过{MessageRecords}中的消息去识别微服务接口. 如果消息的
发送对象和接收对象属于不同微服务, 消息应该被识别为消息接收者所属微服务的接口. 如果消息接收者的方法
不在所属微服务的控制类中, 则将消息对应的方法签名添加到控制类, 然后让控制类将消息转发到真正的消息接
收类中. 这就是我们所谓的附属类的一个方法提升到控制类中, 如果发生了这种提升或接口原本属于控制类, 此接
口所属的 isRaised 属性被置为 true. 否则, 此接口所属的 isRaised 属性被置为 false. 设计这种提升动作是为了满足
迪米特法则, 使控制类来负责接口的请求转发, 其目的是降低类之间的耦合度, 提高模块的相对独立性.
3.2 微服务模型生成
在得到抽象微服务后, 我们可以根据抽象微服务和运行日志, 进一步生成微服务图及微服务序列图, 这些模型
都是通过 MSA-Generator 自动生成并可在 MSA-Modeller 中加载. 使用微服务图及微服务序列图能够帮助工程师
了解迁移后的 MSA 系统的静态结构以及动态行为.

熊靖浏 等: MSA-Lab: 模型驱动的微服务集成设计平台

3.2.1

1291

微服务图生成

在得到每个抽象微服务后, 可以根据每个抽象微服务来进一步生成在模型章节中所定义的微服务图. 我们设
计了相应的算法, 来为每个抽象微服务生成微服务图来描述微服务静态结构. 对于一个抽象微服务 microservicei =
< CCi , {GSCs}i , {DACs}i , {DECs}i , {I s}i > , 算法根据其属性值来生成 UML 模型.

(1) UML 组件模型生成规则: 根据 microservicei 中的 CCi 属性生成一个组件, 组件名由 CCi 属性值确定. 该组
件所适用的构造型为 Microservice.
(2) UML 类模型生成规则: 为 CCi 属性以及 {GSCs}i , {DACs}i , {DECs}i 属性集合中的每个类生成 UML 模型中
的类, 类名为类的全限定类名. 这些类所适用的构造型则由其所属的抽象微服务属性类型决定 (Controller Class,
General Subordinate Class, Data Access Class, Data Entity Class).
(3) UML 接口模型生成规则: 为 {I s}i 数组中的每个微服务接口生成一个接口模型, 接口名由微服务接口本身
名字确定, 接口参数值类型由微服务接口的参数值类型确定. 该接口所适用构造型为 MSInterface, 构造型标注
isRaised 的值由微服务接口的 isRaised 属性确定.
MSA-Generator 会为这些生成的 UML 模型自动适配上 MSA profile, 即可得到微服务构造型、控制类构造型
等描述微服务静态结构的构造型. 这些构造型可被引入类图和构件图中, 按照 MSA profile 的约束对微服务的内部
和外部结构进行建模, 进而得到微服务图. 在实际的实现过程中, MSA-Generator 在抽象微服务生成完后, 会将抽
象微服务的数据存储在 JSON 当中 (也可使用 XML 等其他存储格式). MSA-Generator 按照上述的规则, 自动为每
个抽象微服务的 JSON 数据生成对应微服务构造型的微服务图对应文件. 一个微服务图会对应 3 种文件, *.di 文
件、*.uml 文件和*.notation 文件. 当把这些文件导入进 MSA-Modeller 后, 工程师可依据 UML 2.5.0 等其他标准进
一步对这些微服务图进行可视化交互建模.
3.2.2

微服务序列图生成

微服务序列图根据每个微服务构造型以及代表性 Trace 集合{RTs}(Representative Traces set) 生成. 如果任意
两个 Trace 具有不同的对象调用序列, 但具有相同的类调用序列, 则将这两个 Trace 视为相同的 Trace. {RTs}是根
据上述规则对 Trace 集合中的 Trace 进行过滤得到的, 这样能够完整地反映系统行为, 并去掉系统行为中的冗余信
息. 对于{RTs}中的每一条 Trace, 可以根据其方法调用序列生成对象序列图. 为了获得对象调用序列, 需要迭代地
访问{RTs}中的 Trace, 然后将其转换为调用树. 调用树的节点 (treenode) 表示 Trace 中的一个方法调用, 并包含被
调用的方法 (treenode.Method), 类 (treenode.ClassSignature) 和对象 Id (treenode.ObjectId). 我们用 UML 中的消息
(Message) 来表达对象与对象之间的方法调用, 因为调用存在的方式是多样的 (同步、异步) 等, 因此使用消息做统
一抽象. 一个调用树节点的父节点为调用者, 其孩子节点们为方法中直接调用的其他方法, 由左往右顺序为调用的
先后顺序. 因此调用树的深度优先遍历即为对象的调用顺序, 并根据调用的方法将其转换为对应的消息. 为了在
MSA-Generator 上进行序列图的可视化展示, 我们使用 PlantUML 生成图片. PlantUML 是一种开源工具, 允许用户
利用纯文本语言创建 UML 图. 图 5 的左下角图片和右边图片是 MSA-Generator 根据一个调用树所生成的 HTML
纯本文以及此纯文本对应的对象序列图.
在通过 Trace 得到对象序列图后, 我们设计了一个算法将对象序列图转换为微服务序列图. 微服务序列图的
相关定义在第 2 节中已经给出, 每个微服务序列图对应着系统的一个功能, 描述了此功能下服务之间的交互和交
互的发生顺序. 在对象序列图中, 一条生命线可以代表一个参与者 (角色), 一个 (特定类的) 对象. 在微服务序列图
中, 一条生命线可以代表一个参与者 (角色), 一个特定的微服务. 在对象序列图中, 使用消息统一作为对象之间调
用的一种表达. 在微服务序列图中, 使用微服务消息 (第 2 节定义的 MSMessage) 来表示服务之间的接口调用. 我
们设计了对象序列图转换为微服务序列图的转换规则, 此转换规则的主要思想是按照对象序列图中的消息发送顺
序, 对每一条消息进行验证. 如果消息的发送者与接收者的所属微服务不同, 则消息对应方法是微服务接口之间的
调用, 应创建新的微服务生命线并生成相应的微服务消息. 对于对象序列图中的每条消息, 按照其发生顺序进行遍
历并进行以下操作.

软件学报 2024 年第 35 卷第 3 期

1292

图5

对象序列图及其对应 HTML 文本和微服务序列图

(1) 对于第 1 条消息, 这个消息是角色和微服务启动交互所产生的消息. 根据这条消息生成新的微服务消息
(微服务消息名与对象消息名相同), 微服务消息的发送者为一个新生成的角色生命线 (与对象序列图中角色生命
线相同). 微服务消息的接收者为一个新生成的微服务生命线, 此微服务生命线对应消息的接收者对象所属类的所
属微服务构造型. 如果消息不是第 1 条消息, 则跳转至步骤 (2).
(2) 根据消息的接收对象和发送对象对应类查找分别所属的微服务构造型, 判断两者所属的微服务构造型是
否相同. 如果相同, 跳过此消息, 否则跳转至步骤 (3).
(3) 根据这条消息生成新的微服务消息 (微服务消息名与对象消息名相同), 查找消息的发送者和接收者所属
类的所属微服务构造型, 是否在此微服务序列图中存在对应的微服务生命线. 如果存在则将消息关联到对应的微
服务生命线, 不存在则生成新的微服务生命线并关联.
待对所有消息进行验证后, 微服务序列图的生命线和微服务消息已生成完毕, 统计所有生命线以此设置微
服务序列图的 Actors 及 MSs 属性, 设置 traceNum 为此微服务序列图对应的对象序列图所持有的 Trace 序号.
图 5 左上角的图片为依照上述规则所生成的微服务序列图 (PlantUML 绘制). 同样与第 2 节中生成微服务图的
方法相似, MSA-Generator 也会为每个微服务序列图生成*.di 文件、*.uml 文件、*.notation 文件供加载到 MSAModeller 中.
3.3 微服务识别结果的评价指标
目前大多数工作仅评价针对业务类聚类的微服务识别结果, 忽视了对微服务所拥有数据相关类的划分和评
价. 本文结合在工业界中常用的微服务提取指标 [38]以及考虑在黑盒测试情况下所能得到的信息, 提出了 5 个对微
服务设计方案的评估指标. 一共由 5 部分组成: 统计指标、服务接口内聚度、服务模块度、请求接口复杂度和数
据库区分度. 基于这些评价指标, 将能够对 MSA-Generator 自动微服务识别结果的优劣进行量化评估, 还可支持
MSA-Modeller 实施设计方案动态调整后所得结果的对比评估.

熊靖浏 等: MSA-Lab: 模型驱动的微服务集成设计平台

3.3.1

1293

统计指标

统计指标统计了此次拆分结果和日志输入的数量相关的信息, 包括工具运行时间、生成微服务的数量、类的
数量、日志中的 Trace 数量、代表性 Trace 集合中的 Trace 数量和接口数量. 图 6 是 JPetStore 在 MSA-Generator
上首次运行后生成的设计方案的各项指标. 可以看到此次上传到 MSA-Generator 中的日志共有 2 761 条 Trace, 经
过去重后获得 35 条代表性 Trace. MSA-Generator 根据这份日志将此系统下的 23 个类 (不包含数据实体类) 划分
成了 3 个微服务 (一个微服务平均 7 个类), 所有的微服务一共暴露了 38 个接口.
Target project name: JPetStore
Running time: 22.280 846 133 second
Microservices number

Total classes number

Representation trace number

3

23

35

Average class

Total interface

Trace number

7

38

2 761

MSIC

MSN

RISC

MSDD

0.107 125 48

0.453 041 47

183.942 857 14

1.000 000 00

图6
3.3.2

评价指标

服务接口内聚度

在黑盒的情况下, 无法从代码的角度度量内聚性. 因此考虑从接口的角度来衡量内聚度, 即根据接口的领域概
念的重叠程度来衡量接口之间的相似性. 对于一个微服务的接口内聚度衡量, 首先根据接口所属的类对所有接口
进行分类. 根据单一责任原则, 每个类下的函数应完成一个职责. 对于同一类下的接口, 我们不度量它们的域相似
性. 我们应该衡量的是所属不同类的接口领域相似性, 以判断这些类是否能够合作完成一系列相关职责. 设一个微
服务中有 n 个类, Ii−k 为第 i 个类下的第 k 个接口, Numi 为第 i 个类下的接口的数量. 使用微服务接口内聚度 (microservice interface cohesion, MIC) 衡量一个微服务的接口集合内聚程度, 它根据 Athanasopoulos 等人 [39] 提出的
LoC msg 修改而来, 其取值范围在 [0, 1]. 从公式 (6) 可以看出, 当一个微服务中只有一个类时, 微服务的内聚性为 1

(最大值). 如果微服务下有多个类, 则两两比较不同类下的接口内聚度:


1,
if n = 1




∑
∑

Num
Num

i
j
(
)

S I Ii−k , I j−m
MIC = 

k=1


∑n m=1
i < j, if n , 1




Numi × Num j

(6)

i=1

S I 函数是一个衡量两个接口相似性的公式:
S I (Ik , Im ) =

| fterm (sigk ) ∩ fterm (sigm )|
| fterm (sigk ) ∪ fterm (sigm )|

(7)

fterm (sig) 对每个接口中的类名, 方法名和所需参数类型名进行领域单词提取 (即有意义的业务单词提取, 注

意提取过程中需要排除包名), 该函数还过滤掉了停止词 (英语中没有意义的单词, 如代词、介词、数组和单个字母).
MSIC (microservice set interface cohesion) 定义为一个微服务候选集合的平均 MIC, 衡量了整个系统的接口内
聚度, 它的取值范围在 [0, 1] 间, 值越大, 说明内聚程度越高:
n
∑
MSIC =

3.3.3

MICi

i=1

n

(8)

服务模块度

服务模块度 (microservice set modurality) 根据 Mitchell 等人 [40] 提出的模块值修改而来, 其计算基于依赖图
(DG). 依赖图是在{RTs}下的每个类之间的依赖关系. DG = (V, E) , V 为目标系统的类集, E 为类间调用关系集.
E = {ei | ei =< v x , vy , wi >, v x ∈ V, vy ∈ V, wi ∈ [1, 5]} . 当{RTs}中有一个从 vx 到 vy 的调用时, 依赖强度值设为 wi , 有 ei =< vx ,

软件学报 2024 年第 35 卷第 3 期

1294

vy , wi > . wi 的值是由功能场景 (考虑类所负责的功能) 中 vx 和 vy 的间隔层数决定的. 在功能场景中, 我们将每个微服
务看作 3 层架构, 第 1 层由控制类组成, 负责公开所有外部接口和请求转发; 第 2 层由一般附属类组成, 负责完成业

务流程; 第 3 层由数据访问类组成, 负责与数据库交互. 如果 vx 和 vy 所属微服务相同, wi 被设置为 vx 与 vy 之间差值
的绝对值. 如果不同, wi 被设置为 vx 和 vy 所处的层数的总和相加. 提出功能场景的原因是因为服务之间正常的调用
是通过微服务接口进行调用. 例如, 当微服务 A 中的控制类直接调用微服务 B 中的数据访问类和微服务 B 中的控制
类相比. 两者之间的耦合关系是完全不同的. 前者的调用显然不符合设计模式, 可以认为是紧耦合, 而后者是松耦合.
微服务候选集合将依赖图划分为 n 个群 (cluster), 每个群对应一个候选微服务中的所有类. 聚类因子 (cluster
factor, CF) 定义为群的内部边的权重和与群的内部边的权重和加上群外部边 (连接该群和其他群的边) 权重和的
二分之一的比值. 使用外部边权重的一半是用来对外部边相连的两个群产生同样的负面影响. 对于第 i 个微服务
的集群因子 CF i 定义如下:



0,
µi = 0






2µ
CFi = 

∑n i (


) , µi , 0


ϵi, j + ϵ j,i
 2µi +

(9)

j=1, j,i

其中, µi 为第 i 群的内部边的权重和, 而连接第 i 群和第 j 群的边的权重和被记为 ϵi, j 和 ϵ j,i . 从公式上来看, 一个群
外部耦合关系越少, 内部聚合关系越多, 其群因子值越大, 越能满足低耦合设计原则. 对于服务模块度的计算, 即每
个微服务聚类因子相加取平均值. 它的取值范围在 [0, 1] 间, 该值越大, 说明系统越能满足低耦合设计原则:
n
∑
CFi
MSM =

3.3.4

i=1

n

(10)

请求接口复杂度

通信成本是重构后的微服务之间通信时间相关的负面影响, 如网络条件和服务器之间的物理距离等. 在目前
的条件下, 我们无法直接衡量微服务系统是否具有较低的通信成本. 因此, 考虑从微服务接口的参数复杂度角度出
发去片面的描述通信成本. 这是因为在微服务接口调用时会通过 HTTP 等协议传输参数, 当参数涉及的内容较多
时, 对一些需要低延迟的系统会产生很大的负面影响.
对于一个 Web 请求 (在日志中对应一个 Trace) 下的请求接口复杂度 (request interface complexity, RIC) 的计
算如公式 (11) 所示. 设第 i 个 Trace ( tracei ) 下存在 n 次接口的调用, RIC (tracei ) 定义如下, 它描述了对微服务系统
发出一个 Web 请求后, 需要访问的所有微服务接口参数有多复杂.
∑n
(
)
(
)
RIC (tracei ) =
complexity param j + complexity return j
j=1

(11)

其中, param j 是第 j 次接口调用所需要的参数, return j 是接口所返回的参数, complexity () 在前面已经定义. 通过
公式 (11), 我们可以得到一个 Trace 下的请求接口复杂度, 最后将代表微服务系统业务逻辑的{RTs}中所有 Trace
的接口复杂度结果求和取平均值, 得到微服务系统的请求接口复杂度 (request interface set complexity, RISC):
k
∑
RIC (tracei )
RISC =

i=1

k

(12)

其中, RISC 的取值范围不限, 且越大说明系统的请求接口设计越复杂, 通信的代价越高.
3.3.5

数据库区分度

理想的设计是每个微服务都有自己单独的数据库, 并可以通过其余微服务提供的接口修改其余微服务的数据
库. 如果两个微服务之间的数据共享十分频繁, 不断的服务调用无疑会对增加服务的负担. 这时就需要通过建立共
享表, 或者主、从表等方式去减轻对系统服务的负担, 但这无疑会增加分布式事务的难度.
我们通过数据库区分度 (database discrimination, DD) 计算两个不同微服务数据库中的数据表集合的差别, 该
公式由 Jaccard 距离计算 [41]公式改进得到. {Tablesi } 代表着第 i 个微服务数据库下的数据表集合. 公式的取值在 [0, 1]
内, 值越大, 两个数据库之间的区别就越大, 数据表重复越少:

熊靖浏 等: MSA-Lab: 模型驱动的微服务集成设计平台

1295



1,
if T ablesi == ∅ and T ables j == ∅




DD = 
|
T
ables
∪
T
ables
|
−
|T
ables
∩
T
ables
|
i
j
i
j



, else

| T ablesi ∪ T ables j |

(13)

最后, 我们将任意两个不同微服务对应的 DD 值相加取平均值, 即可得到整个微服务系统的数据库区分度
(microservice set database discrimination, MSDD), 即:
n
∑
(
)
DD T ablesi , T ables j
MSDD =

i=1

(n − 1)!

, i< j

(14)

其中, MSDD 的取值范围在 [0, 1] 间, 值越大说明系统的数据库设计越不存在重复. 当值为 1 时, 说明每个微服务
的数据库中的数据表没有重复.

4 MSA-Modeller
核心部件 MSA-Modeller 的当前版本包含两个建模辅助工具.
(1) MSA-Modeller 前端建模工具: 提供基于网页的服务概览、服务上下文展现, 及服务调整功能. 但需指出的
是, 前端工具仅提供 MSA 建模的初级功能, 即它支持对各微服务下类的归属进行动态调整, 但不支持调整后得到
的模型进行基于 OCL 的语法检验. 故需要借助 MSA-Generator 提供的模型导出功能 (model download), 导出调整
后的模型文件并载入到 MSA-Modeller 桌面端建模工具中进行语法检验.
(2) MSA-Modeller 桌面建模工具: 它基于 Eclipse 的 Papyrus 插件进行开发. Papyrus 可支持 UML 2.5.0 规范,
默认支持 SysML 建模. 我们则在 Papyrus 加入了我们定义的微服务元模型, 并对相关建模元素和编辑器进行了二
次开发, 使之能支持面向微服务体系架构设计模型的可视化展现、交互式建模及语法检查. 当然, 后续版本将持续
开发提供如模型检测、代码生成等模型驱动开发设施.
4.1 MSA-Modeller 前端建模工具
前端工具基于 Web 开发, 与 MSA-Generator 共用前端界面, 在 MSA-Generator 运行完后即可进行使用. 旨在
提供 MSA 模型查看及交互式建模的初级功能, 包含: 服务概览、服务上下文展现及服务调整. 服务概览模块能够
让工程师快速对每个微服务的组成 (类, 接口) 和交互 (序列图) 有直观的了解; 服务上下文模块展示微服务化后类
与类在日志中的直接调用次数; 服务调整能够交互式的供工程师按照微服务模型约束进行微服务下类的调整, 并
根据调整结果生成新的微服务模型. 这种交互式功能是大部分微服务拆分工具 [9,13,16]所不支持的, 其他拆分工具往
往只支持对结果的可视化而不允许用户进行交互式调整与新结果生成.
4.1.1

服务概览

使用 MSA-Generator 对 JPetStore 系统进行分析后, 会得到微服务对应的模型文件, 将这些文件导入 MSAModeller 后会得到 3 个微服务图和 35 个微服务序列图. 这 3 个微服务分别是账号微服务, 商品目录微服务, 订单
微服务. 服务概览模块负责展示每个微服务的可视化元素组成与图片形式的对象序列图和微服务序列图, 能够快
速地让工程师对每个微服务的组成和交互有直观的了解. 可视化元素组成包括此微服务具体的类、接口; 图片形
式的序列图包括此微服务相关联的 Trace 所转换成的对象序列图和微服务序列图, 也可进入序列图对应的 Trace
模块快速查看此 Trace 所关联的所有微服务. 后文图 7(a) 展示了商品目录微服务下的可视化元素组成, 图 7(b) 展
示了商品目录微服务下的部分对象序列图和微服务序列图, 这些序列图都对应着日志中的某一个 Trace.
4.1.2

服务上下文展现

借助 MSA-Generator 统计代表性 Trace 集合中每个类与其他类之间的调用次数. 在 MSA-Modeller 前端建模
工具中, 工程师可以看到每个微服务下的类与其他类的相互调用情况. 进而得知在原始业务逻辑下微服务之间以
及微服务内部的类的依赖程度. 如图 8 所示, 图中的节点代表一个类, 有向边上的权值代表所连接的两个节点所对
应的类在日志中存在的总调用次数. 同属于一个微服务的类被一个圆圈所包括, 每个微服务下 3 种不同颜色区分
各个类的所属类别 (控制类, 一般附属类, 数据访问类).

软件学报 2024 年第 35 卷第 3 期

1296

(a) 组成

(b) 交互

图7

图8
4.1.3

商品目录微服务的组成与交互

服务上下文中的各个微服务的调用关系

服务调整

基于 MSA-Generator 生成的微服务拆分设计, 开发者能够使用 MSA-Modeller 前端建模工具对设计方案进行
修改. 图 9 为供开发者进行服务调整的页面, 一个微服务下有许多的类, 从上到下 3 种不同颜色的类对应着此微服
务的控制类, 一般附属类, 数据访问类. 开发者可拖动微服务下的类到另一个微服务中, 从而改变此类的所属微服

熊靖浏 等: MSA-Lab: 模型驱动的微服务集成设计平台

1297

务. 此模块允许工程师能够更改微服务中一般附属类与数据访问类的归属, 但不能修改控制类的归属. 这是因为对
于每个微服务来说, 控制类暴露此微服务的业务功能, 不能对其进行更改, 否则会违背单一职责原则. 在调整完毕
后, 每个微服务下的数据实体类, 接口, 微服务序列图等都会自动重新调整并生成新的设计方案, 工具会再次对新
的设计方案进行评估并生成其对应的模型文件.
service.CatalogService

service.OrderService

service.AccountService

Adjust service composition

图9

对微服务下的类归属进行调整

4.2 MSA-Modeller 桌面建模工具
MSA-Modeller 桌面建模工具提供面向微服务图和微服务序列图的可视化展现和综合编辑功能, 是一个集成
的微服务模型开发环境.
下面介绍如何使用微服务图来刻画 MSA 静态结构模型.
(1) 对于一个微服务的内部结构, 我们基于类图加上前文定义的微服务元模型中的构造型, 对手动建模 (或来
自 MSA-Modeller 前端建模工具生成) 的微服务进行可视化展现和交互式建模. 一个微服务主要由类构成, 类图可
以描述微服务中存在的类、类的内部结构以及类与类之间的关系等. 如图 10 中, 账号微服务专门负责对账号进行
相关操作, 我们对此微服务进行建模. 从图 10 中可以看到账号微服务下的具体的类, 接口以及其中各个模型的关
系, 并且最下面的窗口中也显示了微服务构造型的相关标注.
(2) 对于一个微服务外部的结构, 我们基于组件图的模型表达以及微服务构造型对微服务外部结构进行模型
表达; 其中的微服务构造型继承自组件标准元模型; 原有的 UML 组件和微服务共享一些公共概念, 例如 requiredinterface、provided-interface、端口、连接器、依赖关系和 usage 等. 在此基础上对生成的微服务构造型进行交互
式建模, 建立微服务之间的接口依赖等关系. 假设订单微服务需要通过账号微服务的接口 getAccount 来获取账户
数据, 则可建模如图 11 所示, 账号微服务通过 Port1 暴露此接口, OrderService 通过 Port1 端口进行依赖.
此外, 为表达微服务之间交互行为模型的微服务序列图: 我们基于对象序列图, 并引入 MSA profile 的微服务
序列图元模型来构建微服务序列图. 序列图能显示对象按时间顺序排列的相互作用, 它描述了场景中涉及的对象
以及执行场景功能所需的对象之间交换的消息序列. 而其扩展而成的微服务序列图则能描述微服务之间通过接口
的交互, 以及与运行日志中 Trace 的对应关系. 图 12 展示了 Trace 1 中 User 用户通过账号微服务创建账户后, 根
据商品的不同类别开始浏览对应类别下的商品的业务流程. MSInteraction 对应的标注值表明了此 diagram 对应的
Trace, 以及参与的微服务和 actor, 并能通过 OCL 验证来确定微服务序列图建模是否正确.

软件学报 2024 年第 35 卷第 3 期

1298

图 10
《component》
《Microservice》
AccountService

账号微服务模型

《use》

《component》
《Microservice》
OrderService

+ Port1: getAccount [1]

图 11

图 12

订单微服务调用账号微服务接口

Trace 1 对应的微服务序列图

熊靖浏 等: MSA-Lab: 模型驱动的微服务集成设计平台

1299

对于建模完成后的微服务图和微服务序列图, 我们可以通过 MSA-Modeller 提供的 OCL 验证来检测微服务
模型是否符合 MSA profile 中所定义的约束. 这样可确保工程师按照 MSA 的规范来进行建模, 以确保后面的模型
驱动开发正确. 在工程师对微服务模型修改并验证完成后, 还可根据 Eclipse 的各种第三方插件进行模型驱动开
发. 例如可以使用 Papyrus Designer 为微服务模型生成平台相关性的代码 (C++, Java 等). 目前我们也正在研发代
码生成工具 MSA-Coder 为微服务模型生成 Java 相关的微服务框架 Spring Cloud 代码. 当然, 如果工程师想进行其
他模型驱动的开发, 也可使用 Papyrus 本身所含的基于 Moka 的模型执行插件, 集成安全技术插件等.

5 实验研究与案例分析
我们一共进行了 3 组实验. 第 1 组是工具有效性与鲁棒性实验, 通过此实验证明 MSA-Generator 在日志输入
不完整的情况下, 其拆分结果仍然是有效的, 体现了工具的鲁棒性; 第 2 组是工具对比实验, 通过与其他微服务拆
分工具和框架对比. 证明 MSA-Generator 在性能上更加出色, 能够提供更多的信息给工程师作为拆分参考. 在与其
他工具和框架的拆分对比中, MSA-Generator 的拆分结果同样有效, 且部分指标表现更加优秀; 第 3 组是工具的功
能转换完备性实验, 通过数据分析的方式证明我们的工具在转换过程中不会造成功能损失. 最后, 讨论部分指出了
我们的工具平台的实用性与局限性. 我们一共选取了 4 个开源软件系统进行实验, 研究系统分属电商、社交博客、
线上考试和 ERP (企业资源计划). JPetStore 是一个简单且架构清晰的小型系统, 使用 Spring, MyBatis 等常用框架
开发; Zb-blog 整合了 Redis, SpringBoot, Shiro 等常用组件; Exam++是一个采用 SSM 框架开发的数据驱动系统, 提
供了可灵活配置的功能用于不同考试领域; JeeSite 是一个企业信息化领域开发平台, 整合了 Spring Boot, Shiro 等
大量常用组件. 它们的详细信息如表 4 所示, 展示了每个项目的类的数量, 项目的有效代码行数, 包的数量, 数据表
的数量, 项目中初始化数据库的 SQL 语句的行数.
表4

实验项目详细信息

项目名
JPetStore

类数量
39

有效代码行数
4 932

包数量
4

数据表数量
12

SQL行数
282

Zb-blog[42]

198

10 277

14

15

1 154

Exam++[43]

81

7 757

7

19

663

JeeSite[44]

292

25 366

6

55

9 162

5.1 参数设置
我们的参数设置有两方面, 一方面是对不同类型方法的调用强度和调用频度的设置, 另一方面是对 NSGA-II
中参数的设置. 我们参考了 Akoka 等人 [45]对关系强度的设置, 将强度常量组中的 IStrc , IStru , IStrr 设置为 100, 10,
1, 以此来遵循第 3.1.2 节中的策略以突出方法调用强度的不同影响. 同时, 为了将方法发生的频度也纳入到度量过
程之中, 并使数量足够时调用频度的影响能够超越调用强度. 调用频度常量组中 IFrec , IFreu , IFrer 被依次设置为
1.03, 1.02 和 1.01. 我们在结构设计分明的项目 JPetStore 进行了多次实验, 确保了此参数下, 每次微服务拆分结果
稳定, 能够较好地刻画类之间的功能联系度.
对于面向微服务识别的 NSGA-II 算法, 其参数设置是基于标准遗传算法的一般建议, 并综合考虑在代码量最
大的项目中 JeeSite 上进行的多轮实验结果进行设置. 我们的依据是如果在大型项目下此参数对应的实验结果能
表现稳定, 那么在小型项目下也一定适用, 以此避免实验结果的不稳定性. 最终, 种群被设置为 200, 交换和变异概
率被设置为 0.001 和 0.2. 遗传算法最多迭代次数被设置为 300.
5.2 工具有效性及鲁棒性实验
为了证明我们的工具即使在日志输入不完整的情况下也具有鲁棒性, 且拆分结果仍然具有效性. 我们进行了
敏感性实验. 我们的假设是, 根据我们的评价指标, 能够评价此次微服务拆分结果的各个性质的好坏. 我们以每个
项目下的收集的一份较为完整的日志为样本, 并设此时所得到的拆分结果的评价指标是有效的. 之后取日志长度

软件学报 2024 年第 35 卷第 3 期

1300

的 2/3, 1/2 来进行实验, 将这两次实验的评价指标与完整长度日志的评价指标进行对比. 在截取日志的过程中, 我
们根据 Trace 编号的倍数进行截取. 例如截取 2/3 长度的日志时, 即去掉按序编号后编号为 3 的倍数的 Trace. 这是
因为在 Trace 的收集过程中, 我们按照一系列测试用例来进行收集, Trace 的顺序也对应着测试用例的顺序. 而顺
序相邻的测试用例可能存在一定业务逻辑联系, 例如订单相关的用例是连续执行的, 且生成订单用例一定在删除
订单用例之前. 按照倍数进行截取, 能保证业务逻辑的覆盖不会太分散 (订单相关的用例对应 Trace 仍然存在), 又
减少了对相同业务逻辑方面的深入 (某个订单相关的用例被截取掉). 这种截取方法我们称为业务截取法. 我们对
每份日志都按照业务截取法进行了 20 次实验, 将每次实验所获取的结果相加取平均值, 以此来避免偶然性. 结果
如表 5 所示. 每一行的数据代表一份日志下的 20 次实验的平均结果. 展示了 4 个评价指标 MSIC, MSM, RISC,
MSDD 下的得分, Trace 总数量, 代表性 Trace 数量. 除了业务截取法, 我们也尝试了按照 2/3, 1/2 的比例随机截取
日志. 但这可能会造成日志覆盖业务场景减少过多 (表现为代表性 Trace 数量急剧减少), 漏掉微服务之间交互的
关键业务场景, 导致一部分识别的微服务不符合规范 (没有接口). 在这种业务场景不足以提供自动微服务识别的
情况下, 开发者可以重新规划用例设计, 或凭借业务理解通过手工调整来达到拆分.
表5
项目名称
JPetStore

Zb-blog

Exam++

JeeSite

MSIC
0.107 12
0.1976 4
0.116 90
0.068 11
0.072 28
0.071 42
0.075 29
0.066 09
0.070 34
0.031 36
0.032 52
0.031 41

MSM
0.453 04
0.411 38
0.366 55
0.140 90
0.146 80
0.149 18
0.161 51
0.175 64
0.175 17
0.016 06
0.014 26
0.013 80

有效性及鲁棒性实验
RISC
183.942 85
164.053 22
206.320 00
821.690 13
831.465 51
847.933 62
1 524.606 71
1 520.388 54
1 312.912 79
6 466.750 00
5 211.771 12
2277.616 58

MSDD
1.000 00
0.666 66
1.000 00
0.807 69
0.807 69
0.846 15
0.900 00
0.900 00
0.900 00
0.973 76
0.971 79
0.973 02

Trace数量
2 761
1 886
1 449
8 307
5 677
4 361
3 337
2 280
1 751
40 188
27 462
21 099

代表性Trace数量
35
31
25
76
58
58
67
48
43
272
232
208

注: 项目指标MSIC, MSM, MSDD值越大越好, RISC越小越好; 每个项目的3次日志完整度从上到下分别是1, 2/3, 1/2. 加粗数据为
异常数据, 其余数据波动都在一定范围内. 如Zb-blog的RISC指标在日志缺少情况下, 相比完整日志情况都变化不大

根据 Trace 总数量和代表性 Trace 数量可以知道此次实验所获取的日志的有效信息的大小. 从表 5 的每个项
目下的 3 次实验的总体情况来看, 我们的方法在每个项目下日志不同的缺失情况下的差距不是太大, 具有一定的
鲁棒性. 在对于 JPetStore 的第 2 次实验中, 其 MSIC 和 MSDD 发生了很大的变化. 这是因为日志的缺少引起了业
务逻辑的减少, 本应属于 CatalogService 的一些类划分到了 OrderService 中. MSIC 指标较其他两次实验的急剧提
高是因为 OrderService 中聚集了比以前更多的类, 虽然提高了其内聚度, 但违反了“一个微服务只解决一个问题”
的原则. 由于两个微服务的业务逻辑的重合加重, 因此数据库设计也变得更差. Zb-blog 项目的第 2 次实验和第 3
次实验中的代表性 Trace 数目相同, 因此各指标的变化相差不大 (但总 Trace 数量不同也会给各指标带来影响, 动
态关系分析时会考虑调用次数). Exam++项目的第 2 次实验中 MSIC 明显下降也是因为由于日志的缺少引起业务
逻辑的变化, 导致部分微服务下类的拆分发生了很大的改变. 这些微服务的内聚程度相比其他实验会发生很大的
变化, 进而影响到整个系统的 MSIC 指标. JeeSite 项目的每次实验的指标相对稳定, 变化较小. 第 3 次实验的 RISC
急剧减小是因为 JeeSite 本身就是一个比较大型的系统, 日志减少过多后, RISC 伴随业务逻辑减少而下降.
经过实验得知, 我们的拆分方法依赖于日志的完整情况. 但方法能在缺失日志的情况下尽力生成当前情况下
各指标最好的微服务, 而不会因为日志的一些缺少, 引起微服务拆分结果的整体指标急剧下降. 即使日志不全, 在
获得新的日志后可以及时投入进行弥补. 此外, MSA-Generator 能够在短时间内生成结果并允许工程师进行手工
的修改. 我们的评价指标也能够对当前生成的设计方案进行一个综合的评价, 有助于使工程师对设计方案有个直

熊靖浏 等: MSA-Lab: 模型驱动的微服务集成设计平台

1301

观的认识并及时进行反馈调整.
5.3 工具对比实验
为了证明本文的工具能够显著加快微服务拆分决策的速度, 并且所得到的微服务拆分结果是准确的. 我们设
计了性能对比实验, 拆分结果对比实验, 来与其他工具和框架进行对比.
5.3.1

性能对比实验

实验目标是根据遗留系统来得到其重构为微服务的设计方案, 且设计方案应尽可能地包含更多的架构信息
(例如评价指标, 模型信息等). 我们选用一个与我们的工具有着类似特性的传统微服务拆分工具 MSDecomposer[25], 以及一个模型驱动的微服务拆分工具 AjiL[12]来与我们的工具进行对比. MSDecomposer 工具是采用遗
留系统的运行日志作为输入来得到微服务拆分结果, 其使用过程与 MSA-Generator 工具的使用过程相同: 包括
Kieker 配置, 测试用例分析, 日志收集和工具分析. 在熟练使用 Kieker 工具后, 工具分析比以前的时间花费会更
少. AjiL 是一个手工图形化微服务建模工具, 需要人工分析遗留系统, 并根据人工拆分结果对新的微服务系统进
行建模. 使用 AjiL 的实验是让几位有微服务研究背景和开发经验的硕士研究生, 让他们对表 6 中的 4 个实验系
统一起进行人工分析并给出类和数据表的拆分方案, 并记录下每个人对每个项目得出拆分方案所得出的平均时
间而来.
表6

拆分方法过程时间记录 (min)

Kieker配置、场景 MSDecomposer MSA-Generator MSDecomposer MSA-Generator
人工分析时间 AjiL建模时间
整理、日志收集
分析时间
分析时间
分析总时间
分析总时间
JPetStore
90
3
0.4
93
90.4
210.5
86.5
Zb-blog
120
5
0.9
125
120.9
266.1
312.6
Exam++
120
5
1.5
125
121.5
247.2
292.3
JeeSite
170
8
5.2
178
175.2
305.5
770.2

项目名称

从表 6 中可以看出, 人工拆分的平均总耗时都是大于工具拆分的总耗时. 并且如果还需要在建模工具上进行
手动建模, 其时间人力成本将会大大提高. 而 MSA-Generator 则可以自动生成模型文件, 提高工程师建模的效率.
MSA-Generator 和 MSDecomposer 一样, 在上传日志后即可自动开始运行并给出拆分结果. 除了像 MSDecomposer 中的微服务拆分结果外, MSA-Generator 还给出了评价指标的计算, 直接能被载入的模型文件等, 这些结果
能够带给工程师更多的设计参考. 在 MSDecomposer 与 MSA-Generator 对比中, MSA-Generator 在运行时间上相
对较短. 这是因为通过分布式开发, 只要 MSA-Generator 获得抽象微服务后, 设计评估及可视化生成就能和模型生
成并行执行. MSA-Generator 分析的时间主要聚集在遗传算法的迭代运行上, MSDecomposer 也同样需要 GA 算法
不断迭代进行聚类. 但 MSDecomposer 的可视化与存储都需通过图数据库 Neo4j 来配合执行, 这可能也是增加
MSDecomposer 运行时间的原因. MSA-Generator 的存储直接通过 JSON 文件存储, 并通过 PlantUML 并行进行每
条序列图的可视化, 降低了运行时间. 如果工程师需要对 MSA-Generator 此次生成的结果进行修改, 也可通过
MSA-Modeller 前端工具直接进行交互式修改, 并生成新的设计方案与对应模型. 以上的实验结果表明 MSA-Generator
运行效率高, 更有利于给工程师提供参考, 并且结合 MSA-Modeller 后能提升模型驱动开发的速度.
5.3.2

拆分结果对比实验

我们使用与 MSA-Generator 中的微服务识别方法相似的 FoSCI 框架中的方法和 MSDecomposer 中的方法来
进行对比实验, 以证明我们的拆分结果更加合理. 为了达到方法的对比公平, 我们对 3 种方法都使用同一份日志,
将微服务生成数量设置为 MSA-Generator 识别的微服务数量, 并进行类与数据表的划分. (1) FoSCI 以从遗留系统
的动态方法调用序列提取到的功能原子 (function atom), 作为遗传算法的基本单元进行聚类. 由于无法拿到源码,
我们复现了他们的微服务内部动态调用函数和微服务间动态调用函数进行聚类. (2) MSDecomposer 以遗留系统
的动态方法调用序列进行微服务识别, 使用 G-N 算法对数据表进行聚类, 再由数据表根据方法调用序列搜索所关
联的类得到微服务. 为了和其余方法同样只使用日志进行类与数据表的拆分, 我们根据源码并使用调用链构建数

软件学报 2024 年第 35 卷第 3 期

1302

据表权重矩阵, 使用模块度作为指标函数进行微服务聚类.
表 7 是对比实验结果, 我们将每个方法对每个项目进行 10 次实验, 并取最好的一次. 对于 MSIC 指标,
MSDecomposer 表现较好, 特别是在 JeeSite 项目下. 一方面 MSDecomposer 采用自底向上搜索的策略能直接找到
原始的接口类, 对接口识别有着积极的影响. 另一方面 MSDecomposer 加入了部分人工拆分的结果. MSDecomposer
无法对和多个数据表关联的类进行自动拆分, 需要开发者自行进行拆分, 这种类常常也是微服务识别的困难所在.
而人工拆分的类如果和微服务接口相关的话, 则会影响 MSIC 指标和 RISC 指标. 对于 MSM 指标, MSA-Generator
在所有项目比其余方法表现都好, MSDecomposer 每次比 MSA-Generator 稍差一点. 这可能是因为 MSAGenerator 通过对动态调用关系的分类划分和业务专注度提高了微服务的模块度. 而 FoSCI 在这一指标表现较差,
在 JPetStore 上比其他方法差了一倍左右, 只对调用次数的考虑并不能做到对模块度的很好划分. 对于 RISC 指标,
MSA-Generator 在 Zb-blog 和 Exam++上比其他方法好. MSA-Generator 对接口的参数复杂度做出度量, 降低了
RISC 指标, 使微服务之间的通信代价变得更低. 对于 MSDD 指标, MSDecomposer 表现的最好, 因为 MSDecomposer
采用数据表聚类做到了对数据表的不重复划分. 而 FoSCI 无法对和数据操作的有关类和数据表进行监控, 因此无
法进行与 MSDD 指标的计算. 从拆分结果对比可以看出, MSA-Generator 与其他工具同样能对微服务进行有效拆
分, 虽然在 MSIC 指标和 MSDD 指标上没有达到最好, 但在 MSM 和 RISC 指标均表现比其余方法好.
表7
项目序号
JPetStore

Zb-blog

Exam++

JeeSite

MSIC
0.107 51
0.130 64
0.125 99
0.082 77
0.043 51
0.104 12
0.103 69
0.050 57
0.098 30
0.034 23
0.030 19
0.044 59

拆分结果有效性实验

MSM
0.453 04
0.203 97
0.438 28
0.150 54
0.067 92
0.139 00
0.191 43
0.036 34
0.161 01
0.015 15
0.009 92
0.014 27

RISC
183.942 85
179.514 28
181.800 00
840.381 57
1 161.289 45
940.934 21
1 576.641 79
2 696.641 79
1 350.731 34
6 273.783 08
6 532.136 02
6 519.738 97

MSDD
1.000 00
不支持
1.000 00
0.807 69
不支持
1.000 00
0.900 00
不支持
1.000 00
0.970 44
不支持
1.000 00

方法种类
MSA-Generator
FoSCI
MSDecomposer
MSA-Generator
FoSCI
MSDecomposer
MSA-Generator
FoSCI
MSDecomposer
MSA-Generator
FoSCI
MSDecomposer

注: 项目指标MSIC, MSM, MSDD值越大越好, RISC越小越好; 加粗数据代表在此项目下表现最好的结果

5.4 工具功能转换完备性实验
我们通过设置功能转换完备性实验 (MSA-Lab 工具与实验数据相关网址: https://gitee.com/liubo-rise/msa-lab),
来证明从单体到微服务的迁移不会造成功能损失. 我们将功能转换完备性定义为: 遗留系统中对象之间的每个方
法调用路径 (Trace), 总是可以在功能上找到一个对应的微服务内部, 或微服务之间的可替换调用路径. 我们使用
基于数据的分析 (Trace 覆盖) 而不是理论分析来证明功能转换完备. 我们通过使用 Trace 覆盖率指标来评估功能
转换完备性. 它代表了遗留系统日志中的对象之间的调用路径 (对象序列图) 在多大程度上可以在工具生成的微
服务调用路径 (微服务序列图) 中找到可替换调用路径.
为了更好地进行分析, 我们标出了每个方法所属的微服务序号 (如图 13(a) 所示). 并对微服务间的接口方法采
用“!!!”的标记进行标注. 我们以 Trace 25 来说明如何对一个 Trace 进行功能转换完备性验证: 在对象序列图中, 用
户调用的第 1 个方法和接下来的微服务间的方法都应该作为接口在微服务序列图中出现, 并且接口调用后的同属
于一个微服务的所有方法应该都包含在此接口调用内 (因为在同一个微服务下发生, 且由接口调用触发). 图 13(a)
和图 13(b) 分别是 Trace 25 的对象序列图和微服务序列图, 左边的对象序列图中 removeItemFromChart 和 removeItemByID 这两个方法符合接口定义, 应该出现在微服务序列图中, 且 getItem 方法应包含在 removeItemByID 接口

熊靖浏 等: MSA-Lab: 模型驱动的微服务集成设计平台

1303

调用内. 这两个接口分别属于 2 号微服务与 1 号微服务, 那么微服务序列图应该是用户调用 2 号微服务, 2 号微服
务调用 1 号微服务. 在我们的拆分结果中 2 号微服务对应 CatologService, 1 号微服务对应 OrderService. 而图 13(b)
的微服务图刚好符合上述描述的序列, 因此判定功能转换完备.

CartActionBean(39)
OBJECT

User

Cart(40)
OBJECT

Cartitem(50)
OBJECT
User

CatalogService
SERVICE 2

OrderService
SERVICE 1

removeltemFromCart()!!! 2()
removeltemFromCart()

removeltemByld()!!!1()

removeltemByld()

getltem()1()
User

CartActionBean(39)
OBJECT

Cart(40)
OBJECT

Cartitem(50)
OBJECT

User

(a) 对象序列图

图 13

CatalogService
SERVICE 2

OrderService
SERVICE 1

(b) 微服务序列图

Trace 25 对应的被标注对象序列图与微服务序列图

根据上述分析方法, 我们提取并使用代表性 Trace 集合 (第 3.2.2 节讲述) 作为输入并进行分析. 如表 8 所示,
可以证明在案例项目中工具迁移的功能转换完备性, 所有 Trace 覆盖率均为 100%.
表8
项目名称
JPetStore
Zb-blog
Exam++
JeeSite

5.5 讨

功能转换完备性实验

Trace数量
2 761
8 307
3 337
40 188

代表性Trace数量
35
76
67
272

Trace覆盖率 (%)
100
100
100
100

论

上述实验和案例证明了本文的工具平台的实用性: (1) 能够与其他工具一样做到对遗留系统的有效拆分, 在模
块度和接口复杂度上表现更好, 并且在拆分过程中不会发生功能的丢失. (2) 能在更短的时间内生成更多的微服务
重构设计, 以及对应的微服务模型, 减少了人工建模的时间. (3) 即使在日志不足的情况下, 也能达到一定的鲁棒性
与有效性, 通过添加新日志或人工调整微服务结构来快速进行弥补. (4) MSA-Generator 能够与 MSA-Modeller 能
够相互配合, 提供模型驱动开发的集成微服务设计平台.
但是, 本文所进行的实验和工具平台依然存在一些局限性: (1) MSA-Generator 的日志解析功能只支持日志收
集工具 Kieker 的日志解析, 而 Kieker 目前只支持 Java 系统. 应该添加其他语言的日志解析功能, 但分析动态方法
调用序列的思想是通用的. (2) 自动生成的微服务模型和重构设计信息目前只来源于方法调用序列, 应添加相关的
静态代码信息进一步补全模型, 如类中具体的属性, 方法. 进一步减轻人工建模的负担. (3) 微服务拆分算法的关键
步骤在于对控制类的识别和控制类与其他类之前的关系的衡量. 案例中对于 JPetStore 这种结构清晰的小型系统,
工具平台能够清晰地划分出服务边界. 但对于 JeeSite 这种规模大且结构复杂的系统, 如果原始设计中某个类作为
业务核心与多个类关联, 且不符合控制类的识别指标, 那么这些类将会被拆分到关联程度稍弱的其他微服务中. 而
实际生产的系统设计可能更加复杂, 业务核心类具体有什么样的特征, 类与类之间的关系是否需要从静态结构、
调用顺序等多方面去考虑也有待研究. 工具平台在这种情况下支持的模型和拆分算法是否满足生产需求仍然未
知. 但从模型驱动的角度去重构微服务, 屏蔽了各种语言系统以及结构的差异, 通过模型驱动也能指导后续开发,
以及带来更多的自动化操作, 是一个值得尝试的方向.

软件学报 2024 年第 35 卷第 3 期

1304

6 总

结

基于遗留系统的微服务体系架构重构是实现单体系统到微服务系统迁移的关键, 目前大多数研究都集中在微
服务识别方法上. 这些研究给出的设计方案缺乏对微服务之间和其内部以及动态行为的描述, 设计方案一旦生成
就难以进行二次修改. 学术界和工业界缺乏一种能够基于遗留系统高效鲁棒地执行微服务体系架构重构设计的方
法和支持工具. 为了解决以上问题, 我们在前期工作中主要完成了微服务体系架构设计建模的模型元素语法与语
义的定义; 在此基础上, 我们进一步研发了一种模型驱动的、可用于单体遗留系统微服务化重构的集成设计平台
MSA-Lab. 它主要提供两个用于微服务及设计模型自动生成与交互式建模的核心功能部件, 即: 微服务及设计模型
生成器 (MSA-Generator) 和微服务建模工具 (MSA-Modeller), 分别用于支持微服务自动识别与设计模型自动生成,
以及支持微服务静态结构模型 (微服务图) 与动态行为模型 (微服务序列图) 可视化展现、交互式建模、模型语法
约束检验等. 通过对 4 个开源项目实施有效性与鲁棒性实验; 与 3 个相同特性的工具进行性能和拆分结果对比实
验; 功能转换完备性实验. 结果表明: 本平台拥有很好的有效性、鲁棒性及实现面向日志的功能转换完备性, 且性
能更加优越.
未来针对 MSA-Lab 平台的研发工作, 主要集中在强化 MSA-Generator 工具以支持更多类型的项目和使用不
同语言开发的系统; 以及在 MSA-Modeller 中增加更多设计模型的支持、表达和语法检验. 此外, 为微服务元模型
加入更严格的形式化语义以支持模型检测; 开发新的核心部件 MSA-Coder 以支持代码生成; 以及开发新的核心部
件 MSA-Designer 以支持全新 (Greenfield) 项目的模型驱动开发也在我们未来的研发计划中.
References:
[1]

Zimmermann O. Microservices tenets: Agile approach to service development and deployment. Computer Science-research and
Development, 2017, 32(3): 301–310. [doi: 10.1007/s00450-016-0337-0]

[2]

Sampaio AR, Kadiyala H, Hu B, Steinbacher J, Erwin T, Rosa N, Beschastnikh I, Rubin J. Supporting microservice evolution. In: Proc. of
the 2017 IEEE Int’l Conf on Software Maintenance and Evolution. Shanghai: IEEE, 2017. 539–543. [doi: 10.1109/ICSME.2017.63]

[3]

Lewis J, Fowler M. Microservices: A definition of this new architectural term. 2014. https://martinfowler.com/articles/microservices.html

[4]

Taibi D, Lenarduzzi V, Pahl C. Processes, motivations, and issues for migrating to microservices architectures: An empirical
investigation. IEEE Cloud Computing, 2017, 4(5): 22–32. [doi: 10.1109/MCC.2017.4250931]

[5]

Zhang H, Li SS, Jia ZJ, Zhong CX, Zhang C. Microservice architecture in reality: An industrial inquiry. In: Proc. of the 2019 IEEE Int’l
Conf. on Software Architecture. Hamburg: IEEE, 2019. 51–60. [doi: 10.1109/ICSA.2019.00014]

[6]

Parnas DL. On the criteria to be used in decomposing systems into modules. In: Broy M, Denert E, eds. Pioneers and Their Contributions
to Software Engineering. Berlin: Springer, 1972. 479–498. [doi: 10.1007/978-3-642-48354-7_20]

[7]

Mancoridis S, Mitchell BS, Chen Y, Gansner ER. Bunch: A clustering tool for the recovery and maintenance of software system
structures. In: Proc. of the 1999 IEEE Int ’l Conf. on Software Maintenance. Oxford: IEEE, 1999. 50 –59. [doi: 10.1109/ICSM.1999.
792498]

[8]

Jin WX, Liu T, Cai YF, Kazman R, Mo R, Zheng QH. Service candidate identification from monolithic systems based on execution
traces. IEEE Trans. on Software Engineering, 2021, 47(5): 987–1007. [doi: 10.1109/TSE.2019.2910531]

[9]

Zhang YK, Liu B, Dai LY, Chen K, Cao XL. Automated microservice identification in legacy systems with functional and non-functional
metrics. In: Proc. of the 2020 IEEE Int’l Conf. on Software Architecture. Salvador: IEEE, 2020. 135–145. [doi: 10.1109/ICSA47634.2020.
00021]

[10]

Selic B. The pragmatics of model-driven development. IEEE Software, 2003, 20(5): 19–25. [doi: 10.1109/MS.2003.1231146]

[11]

Liu ZM, Chen XH. Model-driven design of object and component systems. In: Engineering Trustworthy Software Systems. Cham:
Springer, 2016. 152–255. [doi: 10.1007/978-3-319-29628-9_4]

[12]

Sorgalla J, Wizenty P, Rademacher F, Sachweh S, Zündorf A. AjiL: Enabling model-driven microservice development. In: Proc. of the
12th European Conf. on Software Architecture. Madrid: ACM, 2018. 1. [doi: 10.1145/3241403.3241406]

[13]

Gysel M, Kölbener L, Giersche W, Zimmermann O. Service Cutter: A systematic approach to service decomposition. In: Proc. of the 5th
IFIP WG 2.14 European Conf. on Service-oriented and Cloud Computing. Vienna: Springer, 2016. 185–200. [doi: 10.1007/978-3-31944482-6_12]

[14]

Petrasch R. Model-based engineering for microservice architectures using enterprise integration patterns for inter-service communication.

熊靖浏 等: MSA-Lab: 模型驱动的微服务集成设计平台

1305

In: Proc. of the 14th Int’l Joint Conf. on Computer Science and Software Engineering. Nakhon Si Thammarat: IEEE, 2017. 1–4. [doi: 10.
1109/JCSSE.2017.8025912]

[15]

Kapferer S, Zimmermann O. Domain-driven architecture modeling and rapid prototyping with Context Mapper. In: Proc. of the 8th Int’l
Conf. on Model-driven Engineering and Software Development. Valletta: Springer, 2020. 250 –272. [doi: 10.1007/978-3-030-674458_11]

[16]

Kalia AK, Xiao J, Lin C, Sinha S, Rofrano J, Vukovic M, Banerjee D. Mono2Micro: An AI-based toolchain for evolving monolithic
enterprise applications to a microservice architecture. In: Proc. of the 28th ACM Joint Meeting on European Software Engineering Conf.
and Symp. on the Foundations of Software Engineering. ACM, 2020. 1606–1610. [doi: 10.1145/3368089.3417933]

[17]

Liu B, Xiong JL, Ren QR, Tyszberowicz S, Yang Z. Log2MS: A framework for automated refactoring monolith into microservices using
execution logs. In: Proc. of the 2022 IEEE Int’l Conf. on Web Services. Barcelona: IEEE, 2022. 391–396. [doi: 10.1109/ICWS55610.
2022.00065]

[18]

Santos N, Silva AR. A complexity metric for microservices architecture migration. In: Proc. of the 2020 IEEE Int’l Conf. on Software
Architecture. Salvador: IEEE, 2020. 169–178. [doi: 10.1109/ICSA47634.2020.00024]

[19]

Mazlami G, Cito J, Leitner P. Extraction of microservices from monolithic software architectures. In: Proc. of the 2017 IEEE Int’l Conf.
on Web Services. Honolulu: IEEE, 2017. 524–531. [doi: 10.1109/ICWS.2017.61]

[20]

Levcovitz A, Terra R, Valente MT. Towards a technique for extracting microservices from monolithic enterprise systems. arXiv:
1605.03175, 2016.

[21]

Chen R, Li SS, Li Z. From monolith to microservices: A dataflow-driven approach. In: Proc. of the 24th Asia-Pacific Software
Engineering Conf. Nanjing: IEEE, 2017. 466–475. [doi: 10.1109/APSEC.2017.53]

[22]

Li SS, Zhang H, Jia ZJ, Li Z, Zhang C, Li JQ, Gao QY, Ge JD, Shan ZH. A dataflow-driven approach to identifying microservices from
monolithic applications. Journal of Systems and Software, 2019, 157: 110380. [doi: 10.1016/j.jss.2019.07.008]

[23]

Li SS, Rong GP, Gao QY, Shao D. Optimized dataflow-driven approach for microservices-oriented decomposition. Ruan Jian Xue
Bao/Journal of Software, 2021, 32(5): 1284–1301 (in Chinese with English abstract). http://www.jos.org.cn/1000-9825/6233.htm [doi: 10.
13328/j.cnki.jos.006233]

[24]

Abdullah M, Iqbal W, Erradi A. Unsupervised learning approach for Web application auto-decomposition into microservices. Journal of
Systems and Software, 2019, 151: 243–257. [doi: 10.1016/j.jss.2019.02.031]

[25]

Ding D, Peng X, Guo XF, Zhang J, Wu YJ. Scenario-driven and bottom-up microservice decomposition method for monolithic systems.
Ruan Jian Xue Bao/Journal of Software, 2020, 31(11): 3461–3480 (in Chinese with English abstract). http://www.jos.org.cn/1000-9825/
6031.htm [doi: 10.13328/j.cnki.jos.006031]

[26]

Rademacher F, Sachweh S, Zündorf A. Towards a UML profile for domain-driven design of microservice architectures. In: Proc. of the
2017 SEFM Int’l Conf. on Software Engineering and Formal Methods. Trento: Springer, 2017. 230–245. [doi: 10.1007/978-3-319-747811_17]

[27]

Evans E. Domain-driven Design: Tackling Complexity in the Heart of Software. Boston: Addison-Wesley Professional, 2003.

[28]

Schneider M, Hippchen B, Giessler P, Irrgang C, Abeck S. Microservice development based on tool-supported domain modeling. In:
Proc. of the 5th Int’l Conf. on Advances and Trends in Software Engineering. Varese, 2019.

[29]

Terzić B, Dimitrieski V, Kordić S, Milosavljević G, Luković I. Development and evaluation of MicroBuilder: A model-driven tool for the
specification of REST microservice software architectures. Enterprise Information Systems, 2018, 12(8–9): 1034–1057. [doi: 10.1080/
17517575.2018.1460766]

[30]

Tyszberowicz S, Heinrich R, Liu B, Liu ZM. Identifying microservices using functional decomposition. In: Proc. of the 4th Int’l Symp.
on Dependable Software Engineering: Theories, Tools, and Applications. Beijing: Springer, 2018. 50–65. [doi: 10.1007/978-3-319-999333_4]

[31]

Escobar D, Cárdenas D, Amarillo R, Castro E, Garcés K, Parra C, Casallas R. Towards the understanding and evolution of monolithic
applications as microservices. In: Proc. of the 2016 XLII Latin American Computing Conf. Valparaiso: IEEE, 2016. 1–11. [doi: 10.1109/
CLEI.2016.7833410]

[32]

JPetStore. 2007. https://blog.csdn.net/pcfan1/article/details/1724521

[33]

Deb K, Pratap A, Agarwal S, Meyarivan T. A fast and elitist multiobjective genetic algorithm: NSGA-II. IEEE Trans. on Evolutionary
Computation, 2002, 6(2): 182–197. [doi: 10.1109/4235.996017]

[34]

van Hoorn A, Waller J, Hasselbring W. Kieker: A framework for application performance monitoring and dynamic software analysis. In:
Proc. of the 3rd ACM/SPEC Int’l Conf. on Performance Engineering. Boston: ACM, 2012. 247–248. [doi: 10.1145/2188286.2188326]

[35]

SkyWalking. 2017. https://skywalking.apache.org/

软件学报 2024 年第 35 卷第 3 期

1306

[36]
[37]

Pinpoint. 2023. https://github.com/pinpoint-apm/pinpoint
Li D, Li XS, Liu ZM, Stolz V. Interactive transformations from object-oriented models to component-based models. In: Proc. of the 8th
Int’l Workshop on Formal Aspects of Component Software. Oslo: Springer, 2011. 97–114. [doi: 10.1007/978-3-642-35743-5_7]

[38]

Carvalho L, Garcia A, Assunção WKG, de Mello R, de Lima MJ. Analysis of the criteria adopted in industry to extract microservices. In:
Proc. of the 7th IEEE/ACM Joint Int’l Workshop on Conducting Empirical Studies in Industry (CESI) and the 6th Int’l Workshop on
Software Engineering Research and Industrial Practice. Montreal: IEEE, 2019. 22–29. [doi: 10.1109/CESSER-IP.2019.00012]

[39]

Athanasopoulos D, Zarras AV, Miskos G, Issarny V, Vassiliadis P. Cohesion-driven decomposition of service interfaces without access to
source code. IEEE Trans. on Services Computing, 2015, 8(4): 550–562. [doi: 10.1109/TSC.2014.2310195]

[40]

Mitchell BS. A heuristic search approach to solving the software clustering problem [Ph.D. Thesis]. Philadelphia: Drexel University,
2002.

[41]

Hamers L, Hemeryck Y, Herweyers G, Janssen M, Keters H, Rousseau R, Vanhoutte A. Similarity measures in scientometric research:
The Jaccard index versus Salton’s cosine formula. Information Processing & Management, 1989, 25(3): 315–318. [doi: 10.1016/0306-457
3(89)90048-4]

[42]
[43]
[44]
[45]

Zb-blog. 2009. https://gitee.com/skyblue7080/zb-blog
Exam++. 2009. https://gitee.com/ocelot/examxx/
JeeSite. 2023. https://github.com/thinkgem/jeesite/
Akoka J, Comyn-Wattiau I. Entity-relationship and object-oriented model automatic clustering. Data & Knowledge Engineering, 1996,
20(2): 87–117. [doi: 10.1016/S0169-023X(96)00007-9]

附中文参考文献:
[23]

李杉杉, 荣国平, 高邱雅, 邵栋. 一种优化的数据流驱动的微服务化拆分方法. 软件学报, 2021, 32(5): 1284–1301. http://www.jos.org.
cn/1000-9825/6233.htm [doi: 10.13328/j.cnki.jos.006233]

[25]

丁丹, 彭鑫, 郭晓峰, 张健, 吴毅坚. 场景驱动且自底向上的单体系统微服务拆分方法. 软件学报, 2020, 31(11): 3461–3480. http://
www.jos.org.cn/1000-9825/6031.htm [doi: 10.13328/j.cnki.jos.006031]

熊靖浏(1998－), 男, 硕士生, 主要研究领域为微

刘志明(1961－), 男, 博士, 教授, 博士生导师,

服务, 软件工程.

CCF 杰出会员, 主要研究领域为高可信软件, 形
式化方法, 人机物融合, 可解释的人工智能.

任秋蓉(1997－), 女, 硕士生, 主要研究领域为智

刘波(1981－), 男, 博士, 副教授, CCF 专业会员,

能软件工程.

主要研究领域为模型驱动智能化软件工程, 可信
微服务, 区块链系统.

Shmuel TYSZBEROWICZ(1951－), 男, 博士,
教授, 博士生导师, 主要研究领域为软件工程, 软
件验证, CPS, 可信微服务系统.


软件学报 ISSN 1000-9825, CODEN RUXUEW
Journal of Software,2024,35(3):11941206 [doi: 10.13328/j.cnki.jos.007074]
©中国科学院软件研究所版权所有.

Navi: 基于自然语言交互的数据分析系统

E-mail: jos@iscas.ac.cn
http://www.jos.org.cn
Tel: +86-10-62562563



谢宇鹏 1, 骆昱宇 2, 冯建华 3
1

(青海大学 计算机技术与应用系, 青海 西宁 810016)

2

(香港科技大学(广州), 广东 广州 511400)

3

(清华大学 计算机科学与技术系, 北京 100084)

通信作者: 冯建华, E-mail: fengjh@tsinghua.edu.cn

摘

要: 随着大数据时代的到来, 数据分析的作用日益显著. 它能够从海量数据中发现有价值的信息, 从而更有

效地指导用户决策. 然而, 数据分析流程中存在三大挑战: 分析流程高耦合、交互接口种类多和探索分析高耗时.
为了应对上述挑战, 提出了基于自然语言交互的数据分析系统 Navi. 该系统采用模块化的设计原则, 抽象出主流
数据分析流程的 3 个核心功能模块: 数据查询、可视化生成和可视化探索模块, 从而降低系统设计的耦合度. 同
时, Navi 以自然语言作为统一的交互接口, 并通过一个任务调度器实现了各功能模块的有效协同. 此外, 为了解决
可视化探索中搜索空间指数级和用户意图不明确的问题, 提出了一种基于蒙特卡洛树搜索的可视化自动探索方
法, 并设计了基于可视化领域知识的剪枝算法和复合奖励函数, 提高了搜索效率和结果质量. 最后, 通过量化实
验和用户实验验证了 Navi 的有效性.
关键词: 数据分析; 数据查询; 可视化; 自然语言; 蒙特卡洛树搜索
中图法分类号: TP311
中文引用格式: 谢宇鹏, 骆昱宇, 冯建华. Navi: 基于自然语言交互的数据分析系统. 软件学报, 2024, 35(3): 1194–1206.
http://www.jos.org.cn/1000-9825/7074.htm
英文引用格式: Xie YP, Luo YY, Feng JH. Navi: Data Analysis System Powered by Natural Language Interaction. Ruan Jian Xue
Bao/Journal of Software, 2024, 35(3): 11941206 (in Chinese). http://www.jos.org.cn/1000-9825/7074.htm

Navi: Data Analysis System Powered by Natural Language Interaction
XIE Yu-Peng1, LUO Yu-Yu2, FENG Jian-Hua3
1

(Department of Computer Technology and Applications, Qinghai University, Xining 810016, China)

2

(The Hong Kong University of Science and Technology (Guangzhou), Guangzhou 511400, China)

3

(Department of Computer Science and Technology, Tsinghua University, Beijing 100084, China)

Abstract: With the advent of the big data era, the significance of data analysis has increasingly come to the forefront, showcasing its
ability to uncover valuable insights from vast datasets, thereby enhancing the decision-making process for users. Nonetheless, the data
analysis workflow faces three dominant challenges: high coupling in the analysis workflow, a plethora of interactive interfaces, and a
time-intensive exploratory analysis process. To address these challenges, this study introduces Navi, a data analysis system powered by
natural language interaction. Navi embraces a modular design philosophy that abstracts three core functional modules from mainstream
data analysis workflows: data querying, visualization generation, and visualization exploration. This approach effectively reduces the
coupling of the system. Meanwhile, Navi leverages natural language as a unified interactive interface to seamlessly integrate various
functional modules through a task scheduler, ensuring their effective collaboration. Moreover, in order to address the challenges of
exponential search space and ambiguous user intent in visualization exploration, this study proposes an automated approach for


基金项目: 国家自然科学基金(61925205, 62232009, 62102215); 中国铁路总公司科技研究开发计划(K2022S005)
本文由“面向多模态数据的新型数据库技术”专题特约编辑彭智勇教授、高云君教授、李国良教授、许建秋教授推荐.
收稿时间: 2023-07-17; 修改时间: 2023-09-05; 采用时间: 2023-10-24; jos 在线出版时间: 2023-11-08
CNKI 网络首发时间: 2023-12-22

谢宇鹏 等: Navi: 基于自然语言交互的数据分析系统

1195

visualization exploration based on Monte Carlo tree search. In addition, a pruning algorithm and a composite reward function, both
incorporating visualization domain knowledge, are devised to enhance the search efficiency and result quality. Finally, this study validates
the effectiveness of Navi through both quantitative experiments and user studies.
Key words: data analysis; data query; visualization; natural language; Monte Carlo tree search

步入大数据时代, 数据分析在商业智能、科学研究等诸多领域发挥着重要的作用[1]. 然而, 用户在进行数
据分析过程中仍面临着数据分析流程高耦合、交互接口种类多、探索分析高耗时等挑战.
如图 1 所示, 主流的数据分析流程包括以下关键步骤: (1) 用户通过 SQL 查询或 Python 等方式从数据库
中查询用于分析的数据子集; (2) 用户可以通过表格或者可视化的方式理解数据集蕴含的数据规律, 并使用可
视化结果进行分析结果的呈现与交流; (3) 在可视化阶段, 用户通常使用可视化查询语言如 Vega-Lite[2]或交互
式可视化工具如 Tableau[3]创建相应的可视化结果. 如果可视化结果不能满足用户的数据分析需求, 则用户可
能会重复上述若干步骤.

图1

基于自然语言交互的数据分析系统 Navi 框架图

由此可见, 数据分析过程中的各个步骤相互依赖, 通常需要反复迭代以进行探索式分析, 分析流程耦合
且耗时. 此外, 数据分析作为一项专业且复杂的任务, 对用户的专业技能和可视化分析能力的要求较高.
为降低用户进行数据分析的门槛和耗时并提高数据分析的质量和效率, 研究人员对数据分析的各流程进
行了优化, 以提高人机协作的效率. 如表 1 所示, 现有工作主要优化数据分析流程中的某一环节, 难以同时应
对分析流程高耦合、交互接口种类多和探索分析高耗时这三大挑战.
表1
系统
RATSQL[4]
SEQ2VIS[5]
DeepEye[6]
KG4VIS[7]
Navi

与现有工作比较

数据查询
✔
✗
✗
✗
✔

可视化生成
✗
✔
✗
✗
✔

为应对上述挑战, 如图 2 所示, 本文提出了以下研究目标.

可视化探索
✗
✗
✔
✔
✔

1196

(1)

软件学报 2024 年第 35 卷第 3 期

分析流程模块化. 为提高数据分析流程的灵活性和可复用性, 本文将数据分析过程中的各个环节进
行了抽象和模块化设计, 降低各环节之间的耦合度. 通过模块化设计, 能够灵活地利用不同的数据
查询、可视化和交互方法, 实现多样化和定制化的数据分析需求.

(2)

交互接口统一化. 数据分析是一个涉及多种不同任务类型的复杂过程, 每个任务都具有特定的交互
接口, 用户需要在不同任务之间进行转换. 为了应对交互接口种类多的挑战, 本文旨在设计一个统
一的自然语言查询交互接口, 该接口能够有效地整合各类数据分析任务(如数据查询、可视化生成、
可视化探索等), 以提高协作效率.

(3)

探索分析自动化. 探索式分析是数据分析的重要步骤, 传统的方式通常需要用户主动生成和选择可
视化图表, 并通过不断迭代来优化分析结果. 上述过程繁琐、复杂、耗时且对用户专业技能要求高.
为了应对这些问题, 本文提出一种探索分析自动化的方法, 基于蒙特卡洛树搜索(Monte Carlo tree
search, MCTS)[8]算法, 结合库内数据特征、可视化领域知识和用户分析偏好, 以自动地探索数据集
并生成有价值的可视化图表, 而无须用户过度的干预和反馈, 有效地提高了探索分析的效率和
质量.

图2

本文的研究目标和系统设计

为了实现上述研究目标, 本文设计并实现了一个基于自然语言交互的数据分析系统 Navi, 如图 1 所示.
Navi 采用模块化的设计原则, 将主流的数据分析流程[9]抽象为 3 个功能模块: 数据查询模块、可视化生成模
块和可视化探索模块(实现目标(1)). 并以自然语言查询作为统一的交互接口, 设计了一个任务调度器, 负责
解析用户输入的自然语言查询, 并将其分发给相应的功能模块(实现目标(2)). 如图 2 的系统设计所示: Navi 系
统的各模块均基于深度学习实现, 数据查询模块的关键技术是将自然语言查询转换为对应的 SQL 查询
(natural language to SQL, NL2SQL), 可视化生成模块的关键技术是将自然语言查询转换为对应的可视化查询
(natural language to visualization, NL2VIS). 对于可视化探索模块, 本文提出了基于蒙特卡洛树搜索的方法,
以高效地探索可视化空间(实现目标(3)).
因此, Navi 可为用户提供一个基于自然语言查询交互的数据分析系统, 实现数据查询、可视化生成和可视
化探索功能. 综上, 本文的主要贡献如下.
(1)

采用模块化的数据分析系统设计原则, 将主流的数据分析流程抽象为 3 个常见的模块, 降低了系统
设计的耦合度(第 1 节).

(2)

提出使用自然语言查询接口作为用户统一的交互接口, 并实现了基于自然语言的数据查询、可视化
生成和探索模块(第 2 节).

(3)

提出一种基于 MCTS 的可视化自动探索方法, 实现对数据集的自动探索和可视化, 并设计基于可视
化领域知识的剪枝算法和复合奖励函数, 从而提高探索效率和分析质量(第 3 节).

(4)

基于上述技术, 本文实现了端到端的基于自然语言交互的数据分析系统 Navi, 并通过实验验证了其
有效性(第 4 节).

谢宇鹏 等: Navi: 基于自然语言交互的数据分析系统

1

1197

Navi 系统概述

1.1 模块化设计
Navi 采用模块化的设计原则, 包括数据查询、可视化生成和可视化探索模块这 3 个功能模块, 如图 3 所
示. 模块化设计降低了系统设计的耦合度. 以下是各个模块的功能介绍.


数据查询模块. 该模块能够将用户输入的自然语言查询转换为 SQL 查询, 并返回查询结果, 如图 3③
所示. 该模块支持对数据表进行选择和连接以及对数据进行筛选、排序和聚集等操作, 旨在返回用户
感兴趣的数据子集, 提供了便捷的数据查询体验. 用户还可以对数据子集进行收藏或切换操作, 以便
基于所需分析的数据子集进行进一步的探索, 如图 3④所示.



可视化生成模块. 该模块根据用户输入的自然语言查询分析用户意图, 并根据数据特征和分析目标
生成相应的可视化结果, 如图 3⑤所示.



可视化探索模块. 该模块主要协助用户理解数据集. 通过结合数据特征生成多种可视化结果, 展示数
据规律, 如图 3⑥所示. 该模块适用于用户分析意图不明确的场景, 能够为用户提供一个分析的起点.

图3

Navi 系统概览

这种模块化的设计原则使 Navi 具备灵活性、可扩展性和兼容性: 灵活性体现在用户可以根据自己的需求,
选用不同的功能模块, 并且这些模块之间可以无缝切换, 通过协作的方式完成整个数据分析任务; 可扩展性
体现在 Navi 可以随时替换或更新某个模块, 以适应技术的发展; 而兼容性体现在 Navi 能够与其他遵循相同交
互接口的系统进行模块复用, 实现功能的互通.
1.2 任务调度器
任务调度器将各功能模块有效地组合起来, 其作用是解析用户输入的自然语言查询, 并将其分发给相应
的功能模块. 本节把任务调度器视为一个多分类器, 并介绍其实现步骤.


数据集构建. 本节构建了一个包含大量自然语言查询及其对应模块标签的数据集, 用于训练和验证
任务调度器. 该数据集涵盖了各类查询, 包括 NL2SQL、NL2VIS 和可视化探索. 本节从现有的
NL2SQL 基准 Spider[10]和 NL2VIS 基准 nvBench[5]中提取自然语言查询, 并为其打上相应的标签. 同
时, 还将 nvBench 中的自然语言查询转化为适用于可视化探索模块的查询, 这些查询更简洁且模糊,
不包含明确的可视化类型、排序要求等详细信息.



模型训练. 任务调度器采用预训练的 DistilBERT[11]模型作为基础模型, 该模型基于 Transformer[12]架

1198

软件学报 2024 年第 35 卷第 3 期

构, 具有较少的参数和较低的计算复杂度, 但在性能上仍然优异. 本节用 Hugging Face Transformers
库中的 AutoTokenizer 将查询文本转换为 DistilBERT 输入格式, 并将数据集划分为训练集、验证集和
测试集. 为构建任务特定的调度器, 本节在 DistilBERT 的基础上添加了一个具有 3 个输出单元的线性
分类器, 使用交叉熵损失作为损失函数, 并采用了带权重衰减的 AdamW[13]优化器进行权重更新.
引入任务调度器, 使得 Navi 具有全面性和易用性. Navi 通过灵活组合功能模块, 以满足用户对数据查询、
可视化生成和探索的需求. 同时, 它采用自然语言查询作为交互接口, 简化用户的操作, 实现与数据的“对话”
式探索.

2

基于自然语言的数据查询和可视化生成
数据查询和可视化生成本质上同属一类任务, 它们都涉及将输入序列(例如自然语言查询)转换成输出序

列 ( 例 如 SQL 或 Vega-Lite[2] 查 询 ). 为 了 解 决 此 类 问 题 , 本 节 采 用 序 列 到 序 列 (Sequence-to-Sequence,
Seq2Seq)[14]模型进行具体实现.
Seq2Seq 模型一般由编码器和解码器两部分组成, 可以采用不同的神经网络实现. 如图 4 所示, 编码器的
任务是理解输入序列并生成一个较小的向量 h 来表示输入, 而解码器的任务是根据 h 生成一系列输出. 在本
节中, 自然语言查询作为输入是由令牌(或单词)组成的序列[q1,q2,…,ql]Vin, 而 SQL 查询或 Vega-Lite[2]查询作
为输出也是由令牌组成的序列[y1,y2,…,yk]Vout. 这里, Vin 和 Vout 分别代表输入和输出的词汇表. 接下来, 本节
将基于 Transformers[12]介绍编码器和解码器网络的实现.


基于 Transformer 的编码器. 给定输入序列 nV=[q1,q2,…,ql], 本节将 nV 与数据库表结构信息 A=[a1,a2,…,
am]连接起来. 连接后的输入序列为
X=[x1,x2,…,xn]=[q1,q2,…,ql,a1,a2,…,am]
其中, n=l+m. 然后, 本节使用预训练的全局词嵌入(GloVe)

[15]

(1)

将每个令牌 xi 映射为其向量表示. 完成

令牌嵌入后, 本节将嵌入的令牌输入到双向 Transformer 网络中, 输出一系列编码向量:
h=[h1,h2,…,hn]


基于 Transformer 的解码器. 解码器同样采用基于 Transformer 的架构, 并引入了注意力机制

(2)
[16]

. 根据

隐藏状态 h, 解码器生成输出序列:
Y=[y1,y2,…,yk]

(3)

在每个时间步 t, 解码器根据当前状态 si、先前的令牌和注意力向量 ci 来预测 SQL 查询令牌 yt.

图4

序列到序列模型

基于该模型, 本章只需对参数进行适当调整, 然后利用大量与任务相关的输入序列和输出序列样本对作
为训练数据, 即可分别训练得出数据查询和可视化生成模型. 具体而言, 数据查询模块的训练采用了跨领域
NL2SQL 基准 Spider[10]中的自然语言查询和 SQL 查询(NL,SQL)样本对. 例如: 对于自然语言查询输入: “fetch
all COVID-19 cases”, 相应的 SQL 查询输出为: “select * from COVID-19”. 而可视化生成模块的训练则使用
NL2VIS 基准 nvBench[5]中的自然语言查询和可视化查询(NL,VIS)样本对. 例如: 一个示例的自然语言查询输
入是: “draw a line chart to show the trend of numbers of cases by each case type in Utah”, 其对应的输出类似于
Vega-Lite[2], 如: “mark line encoding x date y aggregate none number color cases transform filter states=‘Utah’”.

3

基于 MCTS 的可视化自动探索方法
可视化探索的目标是根据用户需求和数据特征, 自动生成并展示合适的可视化结果. 实现该目标面临两

谢宇鹏 等: Navi: 基于自然语言交互的数据分析系统

1199

大挑战: 一是如何在庞大的可视化设计空间中搜索和评估候选的可视化结果; 二是如何在用户意图不明确的
情况下, 根据用户提供的自然语言查询推测出用户偏好, 从而生成出最符合用户意图的可视化结果.
为应对上述挑战, 本节提出了一种基于蒙特卡洛树搜索的可视化自动探索方法. 该算法利用 MCTS 的高
效搜索策略, 能够有效地生成并评估不同的可视化查询, 从而推荐出高质量的可视化结果. 可视化查询是一
种用于描述可视化图表的语言, 它由一系列可视化子句组成, 每个子句描述了一个可视化图表的某个方面,
如数据列、图表类型、聚集操作等. 在 MCTS 树 T 中, 每个节点 v 对应一个可视化子句 c. 例如: 在图 5 中,
第二层的节点 bar 就对应了图表类型子句. 因此, 可视化查询 Q 可以被表示为从根节点到叶节点的一条路径
Q=[v1,v2,…,vn], 其中, n 是查询中子句的数量.

图5

蒙特卡洛树搜索的流程

针对第 1 个挑战, 本节采用了数据集解析和逐步搜索的方法构建了一个包含多个可视化子句的搜索树.在
搜索过程中, 本节引入了基于可视化领域知识的剪枝算法和复合奖励函数, 旨在引导系统以更高效的方式选
择最佳的可视化结果.
为应对第 2 个挑战, 本节对用户输入的自然语言查询进行意图提取, 以获取与分析任务(如分布、趋势、
对比)和可视化子句(如数据列名、数据聚集方式)相关的关键词. 基于这些关键词, 本节能够在搜索树节点的
扩展阶段根据用户偏好限定节点的扩展方向, 使得搜索树向用户偏好的方向进行搜索, 并进一步对搜索空间
进行剪枝操作. 如图 5 所示, 该模块具体包括以下 4 个步骤.
(1)

选择. 在选择阶段, 算法会从根节点出发, 逐步递归地寻找最优的子节点, 一直持续到抵达一个尚
未完全展开的节点为止. 如图 5 所示, 蓝色节点代表本次选择阶段所选择的节点. 为了更好地利用
奖励函数提供的反馈信息, 并平衡探索和利用的关系, 以避免陷入局部最优解, 本节采用了上置信
界(upper confidence bound, UCB)[17]算法. 该算法的核心思想是: 在每次迭代中, 选择具有最高置信
上界的子节点进行下一步的探索. UCB 算法的具体选择策略可以使用以下公式表示:
UCB(i )  X i  C

ln N
ni

(4)

其中, UCB(i)是第 i 个子节点的上置信界, X i 是第 i 个子节点的平均奖励值, N 是当前节点的访问次
数, ni 是第 i 个子节点的访问次数, C 是一个控制探索与利用平衡的参数. 通过这种选择策略, UCB 算
法可以确保在有限的模拟次数内, 既能充分探索节点空间, 又能有效利用已知信息.
(2)

扩展. 在扩展阶段, 为了高效地缩减搜索范围, 本节引入了基于可视化领域知识的剪枝算法. 此算
法综合考虑了数据类型、图表类型以及编码规则等多个维度的信息, 以筛选出具有较高潜在价值的
候选操作.

(3)

模拟. 在模拟阶段, 算法选取当前节点作为模拟的起点, 并在时间限制的条件下, 依据预设的约束
规则对其进行扩展. 在更新模拟节点的过程中, 算法将优先选取潜在价值最高的操作, 并计算模拟
结果的奖励值以优化决策过程.

(4)

反向传播. 在反向传播阶段, 算法会沿着已确定的路径, 更新节点的访问次数和累积奖励, 如图 5

1200

软件学报 2024 年第 35 卷第 3 期

所示. 其中, 每个可视化结果的得分都将通过奖励函数进行计算.


奖励函数设计

由于可视化没有像围棋那样清晰的奖惩规则, 且单一评判标准得出的奖励可能存在偏差, 因此, 本节考
虑了数据特征、可视化领域知识和用户偏好这 3 个方面, 综合评价一个可视化结果的质量.
(1)

在数据特征方面, 本节通过 LambdaMART[18] 实现评分函数. 这种方法可以从数据中提取有用的特
征(例如最大值、最小值、唯一值的比率等等), 并利用机器学习为可视化图表评定分数.

(2)

在可视化领域知识方面, 本节采用了基于规则的评分策略. 这些领域知识规则旨在确保所生成的可
视化结果能够与人类的视觉感知相适应, 同时, 真实地展现数据的核心属性. 以柱状图为例, 一个
含有超过 50 根柱子的图表可能会导致用户难以捕捉重要信息. 因此, 通过整合领域知识作为约束,
本章可以为不同类型的可视化设置不同的规则, 从而更有效地指导探索的方向.

(3)

在用户偏好方面, 本节将其划分为两个部分: 起始阶段的可视化初始评分和交互阶段的当前用户评
分. 在进行可视化探索的起始阶段, 由于缺乏当前用户的偏好信息, 本章采用生成对抗网络 [19] 训练
了一个用户模型并进行评估. 该模型的训练数据来源于 Plotly 社区的可视化语料库[20], 其中包含了
真实用户的历史交互记录. 通过这些记录, 模型能够学习到用户偏好和行为特征的共性, 进而对可
视化结果给出更为准确的初始评价. 在起始阶段, 该模型的评价结果被视为主要的评判标准.

在交互阶段, 系统会实时记录用户的交互行为, 并引入推荐系统 [21]中的实时反馈机制 [22]来动态调整用户
评分. 针对不同的用户操作行为, 本节设计了一套奖励机制. 当用户对某个可视化结果执行点击、保存或编辑
等操作时, 系统会根据预设的规则, 将这些操作转换为具体的数值, 并动态地调整相应可视化结果和子句的
得分. 通过这种方式, 系统能够为用户感兴趣的部分赋予更高的奖励分数. 进入下一轮蒙特卡洛树搜索时, 算
法会倾向于探索用户感兴趣的方向, 从而生成更符合用户实际需求的可视化结果 [23,24]. 在整合用户偏好评分
方面, 系统采用加权组合的方式, 将初始评分和当前用户评分相结合, 并为当前用户评分赋予更高的权重. 这
样做旨在更好地满足用户当前的偏好, 并鼓励他们进行更深入的数据探索.
接下来, 本节将介绍如何利用这 3 个部分来综合评判一个可视化结果的好坏. 对于一个可视化结果, 本节
首先根据领域知识进行初步筛选: 若不符合领域知识, 则直接赋予较低的分数; 若符合, 则进一步根据数据
特征进行评分, 以得到数据特征分. 随后, 根据用户模型和当前用户偏好计算得出用户偏好分. 最后, 通过对
这两个分数进行加权平均, 计算出该可视化结果的最终分数, 其中, 权重值是基于经验进行设定的.
综上所述, 本节提出了一种基于 MCTS 的可视化自动探索方法, 该方法结合了基于可视化领域知识的剪
枝算法和复合奖励函数, 提高了搜索的准确性和效率.

4

实验评测

4.1 量化实验
4.1.1

数据查询模块的有效性评测
本实验采用 Spider[10]基准数据集来评测数据查询模块的有效性. Spider 是一个大规模、复杂和跨领域的自

然语言查询到 SQL 查询的数据集, 它包含了 10 181 个(NL,SQL)样本对, 这些样本对分布在 200 个数据库中,
覆盖了 138 个不同的领域. 本实验在其测试集上进行评估, 并确保训练集和测试集之间没有自然语言查询和
数据库的重复.
本实验通过两个指标来量化模型的性能 [25]: 精确集合匹配准确率和执行准确率. 精确集合匹配准确率是
指生成的 SQL 查询与真实的 SQL 查询在标准化后的数据结构上是否完全一致, 执行准确率是指生成的 SQL
查询在数据库上执行后得到的结果是否与真实的 SQL 查询得到的结果相同.
实验结果见表 2: 数据查询模块在两个指标精确集合匹配准确率和执行准确率上都取得了较高的分数,
分别为 71.08%和 74.37%. 与现有工作 BRIDGE[26]和 RATSQL[4]相比, 数据查询模块有显著改进, 在精确集合
匹配准确率方面分别提升了 5.06%和 2.95%, 在执行准确率方面分别提升了 7.19%和 5.16%.

谢宇鹏 等: Navi: 基于自然语言交互的数据分析系统

表2

1201

数据查询模块的实验结果(%)

系统
BRIDGE[26]
RATSQL[4]
数据查询模块

精确集合匹配准确率
66.02
68.13
71.08

执行准确率
67.18
69.21
74.37

这表明数据查询模块可以更有效地将自然语言转换为正确且可执行的 SQL 查询, 并更好地适应不同领域
和复杂度的数据库.
4.1.2

可视化生成模块的有效性评测
本实验采用 nvBench[5]基准数据集来评测可视化生成模块的有效性. nvBench 是一个大规模、复杂和跨领

域的 NL2VIS 任务的数据集, 由 750 张关系表和 25 750 个(NL,VIS)样本对组成, 涵盖了 105 个不同的领域. 本
实验在其测试集上进行评测, 并使用准确率作为评测指标. 准确率是指生成的可视化查询与真实的可视化查
询是否完全匹配, 即 Accuracy=N/M, 其中, N 是匹配的结果数量, M 是测试样本的数量.
实验结果见表 3: 可视化生成模块的准确率达到了 76.78%, 显著优于的现有工作 NL2Viz[27]和 SEQ2VIS[5],
在准确率方面分别提升了 18.13%和 12.66%. 这说明可视化生成模块可以灵活地处理不同类型和复杂度的自
然语言查询, 并准确地生成符合用户意图和数据特征的可视化图表. 这证明可视化生成模块能够很好地实现
可视化生成的目标.
表3

可视化生成模块的实验结果(%)
系统
NL2Viz[27]
SEQ2VIS[5]
可视化生成模块

4.1.3

准确率
58.65
64.12
76.78

可视化探索模块的有效性评测
本实验采用 KG4VIS[7]的数据集来评测可视化探索模块的有效性. KG4VIS 是一个基于知识图谱的可视化

探索方法, 该工作从 VizML[28]中筛选出 88 548 个(数据集,可视化集)样本对, 并将其按 7:3 的比例分为训练集
和测试集. 本实验使用其测试集进行评测, 并沿用了 KG4VIS 的评价指标, 包括:


平均排名: 表示正确的可视化设计选择在所有生成结果中的平均位置, 越低越好;



Hits@2: 表示正确的可视化设计选择出现在前两个生成结果中的概率, 越高越好;



轴准确率: 表示生成结果中轴属性(即 x 轴或 y 轴)与真实结果一致的概率, 越高越好.

本次实验着重评估系统首次生成的可视化结果的质量. 因此, 奖励函数的用户偏好部分主要以可视化的
初始评分作为评判标准. 实验结果见表 4: 本文的可视化探索模块在 3 个指标上都取得了较好的成绩, 分别为
1.726 8, 87.22%和 98.12%. 与现有工作 KG4VIS[7]和 DeepEye[6]相比, 本文的可视化探索模块有明显的优势,
在平均排名方面分别降低了 0.341 4 和 0.277 6, 在 Hits@2 方面分别提高了 12.15%和 9.94%, 在轴准确率方面
分别提高了 38.11%和 4.93%. 这说明可视化探索模块可以更智能地根据数据特征和用户意图生成合适的可视
化图表, 并更精确地确定轴属性和数据映射. 这证明了可视化探索模块能够较好地实现可视化探索的目标.
表4
系统
KG4VIS[7]
DeepEye[6]
可视化探索模块



可视化探索模块的实验结果
平均排名
2.068 2
2.004 4
1.726 8

Hits@2 (%)
75.07
77.28
87.22

轴准确率(%)
60.01
93.19
98.12

消融实验

可视化探索模块中的复合奖励函数综合了数据特征、可视化领域知识和用户偏好这 3 个方面的评分. 其
中, 用户偏好部分包括了起始阶段的可视化初始评分和交互阶段的当前用户评分两部分. 为验证本文提出的
复合奖励函数的有效性, 本节设计了 5 种 Navi 的变种实现, 如下所示.
(1)

Navi: 评测 Navi 在不考虑当前用户评分的情况下, 首次生成的可视化结果的质量.

1202

软件学报 2024 年第 35 卷第 3 期

(2)

Navi 除去可视化领域知识的评分.

(3)

Navi 除去数据特征的评分.

(4)

Navi 除去用户偏好的初始评分.

(5)

Navi+: 评测当用户进行多轮交互后, 结合当前用户评分情况下, 所生成的可视化结果的质量.

表 5 展示了消融实验的结果, 从表中可以看出: 当用户参与交互后, 考虑当前用户的评分时所得到的可
视化结果质量是最高的, Hits@2 和轴准确率分别为 90.36%和 98.83%; 相比之下, 如果不考虑当前用户评分,
只评估 Navi 首次生成的可视化结果质量, 则 Hits@2 和轴准确率分别降低了 2.64%和 1.02%; 此外, 如果只评
估 Navi 首次生成的可视化结果的质量, 并分别去掉可视化领域知识评分、数据特征评分或用户偏好的初始评
分中的任意一个, 则 Hits@2 和轴准确率都会有所下降. 这些实验结果表明: 复合奖励函数中考虑的 3 个方面
都对可视化探索模块的性能有正向影响; 而且, 通过记录用户的交互行为, 并根据其调整奖励函数的评分,
可以进一步提升生成的可视化结果的质量. 因此, 本文设计的复合奖励函数能够更全面地评价不同可视化结
果的优劣, 并更有效地指导可视化探索模块生成高质量的可视化结果.
表5

复合奖励函数的消融实验结果(%)

方法
Navi 除去可视化领域知识的评分
Navi 除去数据特征的评分
Navi 除去用户偏好的初始评分
Navi(无用户交互)
Navi+(有用户交互)

4.1.4

Hits@2
78.56
77.82
76.78
87.72
90.36

轴准确率
92.15
89.31
91.22
97.81
98.83

任务调度器的有效性评测
本实验在第 1.2 节任务调度器所述的测试集上进行评测, 该测试集是基于 Spider[10]和 nvBench[5]数据集构

造的. 评测的目的是: 检验任务调度器能否准确地识别用户输入的自然语言查询, 并将其分配到合适的功能
模块. 本实验采用 3 个评价指标, 即准确率、召回率和 F1 值: 准确率表示任务调度器正确分配查询到相应模
块的比例; 召回率表示正确分配给相应模块的查询数占总应分配数的比例; F1 值表示准确率和召回率的调和
平均值, 反映了两者之间的平衡.
表 6 展示了任务调度器的实验结果, 任务分类模型在测试集上达到了 98.22%的准确率、98.23%的召回率
和 98.55%的 F1 值. 这些结果表明: 本文的任务调度器能够有效地识别不同类型的分析任务, 并将其准确地分
配到相应的模块中.
表6

任务调度器的实验结果(%)

模块
任务调度器

准确率
98.22

召回率
98.23

F1 值
98.55

4.2 用户实验
本节邀请了 5 名专家用户和 15 名普通用户参与用户实验, 评测了 Navi 端到端的效果.


用户任务. 本实验为每位用户设计了涉及数据查询、可视化生成和可视化探索这 3 个方面的不同数据



用户评价. 在完成每个任务后, 本实验邀请用户填写一个问卷, 对系统在易用性、响应速度、结果质

分析任务. 用户通过自然语言查询与系统交互, 并根据系统返回的结果完成相应的目标.
量方面进行评分. 同时, 本节也记录了用户完成每个任务所花费的时间和操作日志.


实验结果. 图 6(a)展示了 Navi 在各个评价维度上的用户满意度, 其平均得分均超过了 4 分(满分为 5
分). 这一结果充分体现了其在数据分析支持上的出色表现, 为用户带来了便捷且高效的使用体验.
图 6(b)显示了不同类型的用户在使用 Navi 时的交互时间, 可以看出, 部分普通用户的交互时间已经
接近专家用户的水平, 说明 Navi 能够有效地提升用户的数据分析能力和效率. 而从图 6(c)中可以观
察到: 在进行可视化探索任务时, 用户的平均交互时间最短, 这不仅凸显了可视化探索模块的高效

谢宇鹏 等: Navi: 基于自然语言交互的数据分析系统

1203

交互时间(s)

性, 同时也说明了 Navi 在帮助用户快速揭示数据规律方面的有效性.

交互时间(s)

(a) 用户评价

(b) 用户交互时间

图6

5

(c) 不同操作类型的交互时间

用户实验结果

相关工作


NL2SQL

在当今大数据时代, 从庞大的数据集中高效地检索出有用的信息显得尤为重要 [29,30]. NL2SQL 技术应运
而生, 它允许用户通过自然语言查询的交互方式, 更方便且高效地从结构化数据中检索相关信息. 近年来, 基
于深度学习的 NL2SQL 模型在数据库和自然语言处理领域取得了显著的研究进展[3136]. 典型的 NL2SQL 模型
如 RESDSQL[32]、RASAT[33]以及 Graphix-T5[34], 通常都采用基于微调(fine-tuning)的方式进行训练. 这些模型
需要大量的(NL,SQL)样本对来进行训练, 并在诸如 Spider[10] 等基准数据集上进行效果评估. 近期, 随着大语
言模型(large language model, LLM)的兴起, 一些研究者开始探索其在 NL2SQL 任务上的应用潜力. 通过零样
本[35]和少样本提示, 模型如 C3[35]和 DIN-SQL[36]在 NL2SQL 任务上均展现出较好的效果. 然而, 当前大语言
模型在处理数据库中多表和多列间关系时仍然面临挑战, 这导致它在处理复杂查询任务时的性能不如传统的
基于微调的方法[32].


NL2VIS

NL2VIS 的实现方法主要分为基于规则的方法和基于深度学习的方法 [37,38], 其中, 基于规则方法的代表
性研究工作有 NL4DV[39]、QRec-NLI[40] 和 NL2Viz[27]. 此类方法主要基于语义解析器(如 NLTK[41]、Stanford
CoreNLP[42]和 NER[43])对自然语言查询进行解析, 进而提取其中的词性、命名实体等语言特征, 最终通过启发
式规则转换为对应的可视化查询. 但这类方法在处理自然语言的模糊性上存在局限性, 鲁棒性有待加强.
近年来, 深度学习在自然语言处理领域的突破性进展[37]为 NL2VIS 提供了新的技术路径. 研究者开始将
深度学习引入 NL2VIS 任务, 其中具有代表性的研究工作有 ADVISor[44]、ncNet[45]和 Chat2vis[46]. ADVISor 采
用了基于 BERT[47] 的框架, 先完成 NL2SQL 的映射, 再基于规则生成可视化查询. ncNet 则是一个基于
Transformer[12]实现的端到端模型, 它使用了 NL2VIS 的首个公共基准数据集 nvBench[5]进行训练, 该数据集汇
集了各种领域与场景下的(NL,VIS)样本对, 为深度学习模型的训练与评估提供了标准. Chat2vis 则巧妙地将大
语言模型(如 Codex[48] 和 GPT-3[49])与提示工程 [50]相结合, 实现了自然语言查询到可视化查询的转换, 推进了
NL2VIS 研究的发展.


可视化探索

创建符合用户分析意图的可视化结果是一项充满挑战的任务, 它需要用户了解数据、明确分析意图并熟
悉 可 视 化 技 术 . 因 此 , 研 发 能 够 自 动 进 行 可 视 化 探 索 的 数 据 分 析 系 统 变 得 尤 为 重 要 . 例 如 , Draco[51] 和
DeepEye[6,52,53]都能为特定数据集提供排序后的可视化结果. 其中, Draco 采用硬约束与软约束相结合的方法,
以识别并排序出优质的可视化结果; 而 DeepEye 则采用了决策树模型, 评估可视化图表的质量, 并利用
Learning-to-rank[54] 技术进行图表排序. 为了使可视化探索过程更加贴切用户的分析意图, 一些研究工作尝试
引入更先进的技术. 例如: LineNet[55]采用基于 Vision Transformer[56]的 Triplet Autoencoder 结构, 能够根据用户
提供的折线图图像, 自动推荐相似的折线图结果; Lux[57]是一个可集成在 Jupyter Notebook 中的工具, 能够根
据用户的实际分析意图, 实时地提供数据分析支持; Sevi[58] 则尝试采用语音交互方式来获取用户的分析意图,

1204

软件学报 2024 年第 35 卷第 3 期

使用户能够通过语音交互方式来便捷地进行可视化的创建和探索.

6

结论与未来的工作
本文设计并实现了一个基于自然语言交互的数据分析系统 Navi, 其采用模块化的设计原则, 集成了数据

查询、可视化生成和可视化探索这 3 个功能模块, 并由任务调度器统一管理. 量化实验和用户实验表明, Navi
能够有效地支持用户进行数据分析操作. 未来的工作主要包括: 首先, 支持多轮对话, 融合上下文信息, 使用
户能够持续地进行数据分析操作; 其次是探索与大语言模型的结合, 利用大语言模型在自然语言处理方面的
优势, 提高 Navi 在各个任务上的泛化能力和效果.
References:
[1]

Ward MO, Grinstein G, Keim D. Interactive Data Visualization: Foundations, Techniques, and Applications. CRC Press, 2010.

[2]

Satyanarayan A, Moritz D, Wongsuphasawat K, et al. Vega-Lite: A grammar of interactive graphics. IEEE Trans. on Visualization
and Computer Graphics, 2016, 23(1): 341350.

[3]
[4]

Tableau. 2023. https://www.tableau.com/
Shi P, Ng P, Wang Z, et al. Learning contextual representations for semantic parsing with generation-augmented pre-training. Proc.
of the AAAI Conf. on Artificial Intelligence, 2021, 35(15): 1380613814.

[5]

Luo Y, Tang N, Li G, et al. Synthesizing natural language to visualization (NL2VIS) benchmarks from NL2SQL benchmarks. In:
Proc. of the 2021 Int’l Conf. on Management of Data. 2021. 12351247.

[6]

Luo Y, Qin X, Chai C, et al. Steerable self-driving data visualization. IEEE Trans. on Knowledge and Data Engineering, 2020,
34(1): 475490.

[7]

Li H, Wang Y, Zhang S, et al. KG4Vis: A knowledge graph-based approach for visualization recommendation. IEEE Trans. on
Visualization and Computer Graphics, 2021, 28(1): 195205.

[8]

Browne CB, Powley E, Whitehouse D, et al. A survey of Monte Carlo tree search methods. IEEE Trans. on Computational
Intelligence and AI in Games, 2012, 4(1): 143.

[9]
[10]

Munzner T. Visualization Analysis and Design. CRC Press, 2014.
Yu T, Zhang R, Yang K, et al. Spider: A large-scale human-labeled dataset for complex and cross-domain semantic parsing and
text-to-SQL task. arXiv:1809.08887, 2018.

[11]

Sanh V, Debut L, Chaumond J, et al. DistilBERT, a distilled version of BERT: Smaller, faster, cheaper and lighter. arXiv:1910.
01108, 2019.

[12]

Vaswani A, Shazeer N, Parmar N, et al. Attention is all you need. In: Advances in Neural Information Processing Systems, Vol. 30.
2017. 59986008.

[13]

Loshchilov I, Hutter F. Decoupled weight decay regularization. arXiv:1711.05101, 2017.

[14]

Sutskever I, Vinyals O, Le QV. Sequence to sequence learning with neural networks. In: Advances in Neural Information

[15]

Pennington J, Socher R, Manning CD. Glove: Global vectors for word representation. In: Proc. of the 2014 Conf. on Empirical

Processing Systems, Vol. 27. 2014. 3104–3112.
Methods in Natural Language Processing (EMNLP). 2014. 15321543.
[16]

Bahdanau D, Cho K, Bengio Y. Neural machine translation by jointly learning to align and translate. arXiv:1409.0473, 2014.

[17]

Auer P, Cesa-Bianchi N, Fischer P. Finite-time analysis of the multiarmed bandit problem. Machine Learning, 2002, 47: 235256.

[18]

Wu Q, Burges CJC, Svore KM, et al. Ranking, boosting, and model adaptation. Technical Report, MSR-TR-2008-109, Microsoft
Research, 2008.

[19]

Goodfellow I, Pouget-Abadie J, Mirza M, et al. Generative adversarial networks. Communications of the ACM, 2020, 63(11):
139144.

[20]

Qian X, Rossi RA, Du F, et al. Personalized visualization recommendation. ACM Trans. on the Web (TWEB), 2022, 16(3): 147.

[21]

Afsar MM, Crump T, Far B. Reinforcement learning based recommender systems: A survey. ACM Computing Surveys, 2022,
55(7): 138.

谢宇鹏 等: Navi: 基于自然语言交互的数据分析系统

[22]

1205

Chen M, Chang B, Xu C, et al. User response models to improve a reinforce recommender system. In: Proc. of the 14th ACM Int’l
Conf. on Web Search and Data Mining. 2021. 121129.

[23]

Baghi V, Motehayeri SMS, Moeini A, et al. Improving ranking function and diversification in interactive recommendation systems
based on deep reinforcement learning. In: Proc. of the 26th Int’l Computer Conf., Computer Society of Iran (CSICC). IEEE, 2021.
17.

[24]

Xiao T, Wang D. A general offline reinforcement learning framework for interactive recommendation. Proc. of the AAAI Conf. on

[25]

Zhong RQ, Yu T, Klein D. Semantic evaluation for text-to-SQL with distilled test suites. In: Proc. of the 2020 Conf. on Empirical

Artificial Intelligence, 2021, 35(5): 45124520.
Methods in Natural Language Processing (EMNLP). 2020. 396411.
[26]

Lin XV, Socher R, Xiong C. Bridging textual and tabular data for cross-domain text-to-SQL semantic parsing. arXiv:2012.12627,
2020.

[27]

Wu Z, Le V, Tiwari A, et al. NL2Viz: Natural language to visualization via constrained syntax-guided synthesis. In: Proc. of the
30th ACM Joint European Software Engineering Conf. and Symp. on the Foundations of Software Engineering. 2022. 972983.

[28]

Hu K, Bakker MA, Li S, et al. Vizml: A machine learning approach to visualization recommendation. In: Proc. of the 2019 CHI
Conf. on Human Factors in Computing Systems. 2019. 112.

[29]

Qin X, Chai C, Luo Y, et al. Interactively discovering and ranking desired tuples by data exploration. The VLDB Journal, 2022,
31(4): 753777.

[30]

Chai C, Liu J, Tang N, et al. GoodCore: Data-effective and data-efficient machine learning through coreset selection over
incomplete data. Proc. of the ACM on Management of Data, 2023, 1(2): 127.

[31]

Kim H, So BH, Han WS, et al. Natural language to SQL: Where are we today? Proc. of the VLDB Endowment, 2020, 13(10):
17371750.

[32]

Li HY, Zhang J, Li CP, et al. RESDSQL: Decoupling schema linking and skeleton parsing for text-to-SQL. Proc. of the AAAI Conf.

[33]

Qi J, Tang J, He Z, et al. RASAT: Integrating relational structures into pretrained Seq2Seq model for text-to-SQL. In: Proc. of the

on Artificial Intelligence, 2023, 37(11): 1306713075.
Conf. on Empirical Methods in Natural Language Processing. 2022. 32153229.
[34]

Li J, Hui B, Cheng R, et al. Graphix-T5: Mixing pre-trained transformers with graph-aware layers for text-to-SQL parsing. In: Proc.

[35]

Dong X, Zhang C, Ge Y, et al. C3: Zero-shot text-to-SQL with ChatGPT. arXiv:2307.07306, 2023.

[36]

Pourreza M, Rafiei D. Din-SQL: Decomposed in-context learning of text-to-SQL with self-correction. arXiv:2304.11015, 2023.

[37]

Shen L, Shen E, Luo Y, et al. Towards natural language interfaces for data visualization: A survey. IEEE Trans. on Visualization

of the AAAI. 2023. 1307613084.

and Computer Graphics, 2023, 29(6): 31213144.
[38]

Luo YY, Qin XD, Xie YP, Li GL. Intelligent data visualization analysis techniques: A survey. Ruan Jian Xue Bao/Journal of
Software, 2024, 35(1): 356404 (in Chinese with English abstract). https://www.jos.org.cn/1000-9825/6911.htm [doi: 10.13328/j.
cnki.jos.006911]

[39]

Narechania A, Srinivasan A, Stasko J. NL4DV: A toolkit for generating analytic specifications for data visualization from natural
language queries. IEEE Trans. on Visualization and Computer Graphics, 2020, 27(2): 369379.

[40]

Wang X, Cheng F, Wang Y, et al. Interactive data analysis with next-step natural language query recommendation. arXiv:2201.
04868, 2022.

[41]

Bird S. NLTK: The natural language toolkit. In: Proc. of the COLING/ACL 2006 Interactive Presentation Sessions. 2006. 6972.

[42]

Manning CD, Surdeanu M, Bauer J, et al. The Stanford CoreNLP natural language processing toolkit. In: Proc. of the 52nd Annual
Meeting of the Association for Computational Linguistics: System Demonstrations. 2014. 5560.

[43]

Finkel JR, Grenager T, Manning CD. Incorporating non-local information into information extraction systems by Gibbs sampling.

[44]

Liu C, Han Y, Jiang R, et al. Advisor: Automatic visualization answer for natural-language question on tabular data. In: Proc. of

In: Proc. of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL 2005). 2005. 363370.
the 2021 IEEE 14th Pacific Visualization Symp. (PacificVis). IEEE, 2021. 1120.

1206

[45]

软件学报 2024 年第 35 卷第 3 期

Luo Y, Tang N, Li G, et al. Natural language to visualization by neural machine translation. IEEE Trans. on Visualization and
Computer Graphics, 2021, 28(1): 217226.

[46]

Maddigan P, Susnjak T. Chat2vis: Generating data visualisations via natural language using ChatGPT, codex and GPT-3 large

[47]

Kenton JDMWC, Toutanova LK. BERT: Pre-training of deep bidirectional transformers for language understanding. In: Proc. of

language models. arXiv:2302.02094v2, 2023.
the NAACL-HLT. 2019. 41714186.
[48]

Chen M, Tworek J, Jun H, et al. Evaluating large language models trained on code. arXiv:2107.03374, 2021.

[49]

Brown T, Mann B, Ryder N, et al. Language models are few-shot learners. In: Advances in Neural Information Processing Systems,

[50]

Zhou Y, Muresanu AI, Han Z, et al. Large language models are human-level prompt engineers. arXiv:2211.01910, 2022.

[51]

Moritz D, Wang C, Nelson GL, et al. Formalizing visualization design knowledge as constraints: Actionable and extensible models

[52]

Luo Y, Qin X, Tang N, et al. DeepEye: Towards automatic data visualization. In: Proc. of the 2018 IEEE 34th Int’l Conf. on Data

Vol. 33. 2020. 18771901.

in Draco. IEEE Trans. on Visualization and Computer Graphics, 2019, 25(1): 438448.
Engineering (ICDE). IEEE, 2018. 101112.
[53]

Luo Y, Qin X, Tang N, et al. DeepEye: Creating good data visualizations by keyword search. In: Proc. of the 2018 Int’l Conf. on

[54]

Burges C, Shaked T, Renshaw E, et al. Learning to rank using gradient descent. In: Proc. of the 22nd Int’l Conf. on Machine

Management of Data. 2018. 17331736.
Learning. 2005. 8996.
[55]

Luo Y, Zhou Y, Tang N, et al. Learned data-aware image representations of line charts for similarity search. Proc. of the ACM on

[56]

Liu Z, Lin Y, Cao Y, et al. Swin transformer: Hierarchical vision transformer using shifted windows. In: Proc. of the IEEE/CVF

Management of Data, 2023, 1(1): 129.
Int’l Conf. on Computer Vision. 2021. 1001210022.
[57]

Lee DJL, Tang DX, Agarwal K, et al. Lux: Always-on visualization recommendations for exploratory dataframe workflows. Proc.

[58]

Tang J, Luo Y, Ouzzani M, et al. Sevi: Speech-to-visualization through neural machine translation. In: Proc. of the 2022 Int’l Conf.

of the VLDB Endowment, 2021, 15(3): 727738.
on Management of Data. 2022. 23532356.

附中文参考文献:
[38] 骆昱宇, 秦雪迪, 谢宇鹏, 李国良. 智能数据可视分析技术综述. 软件学报, 2024, 35(1): 356404. https://www.jos.org.cn/10009825/6911.htm [doi: 10.13328/j.cnki.jos.006911]

谢宇鹏(1996－), 男, 硕士生, CCF 学生

冯 建 华 (1967－), 男 , 博 士 , 教 授 , 博 士

会员, 主要研究领域为数据可视化.

生导师, CCF 杰出会员, 主要研究领域为
数据库, 数据安全与隐私保护, 信息
检索.

骆 昱 宇 (1996 － ), 男 , 博 士 , 副 研 究 员 ,
CCF 专业会员, 主要研究领域为智能数
据管理与可视分析.


软件学报 ISSN 1000-9825, CODEN RUXUEW
Journal of Software, [doi: 10.13328/j.cnki.jos.007116]
©中国科学院软件研究所版权所有.

UEFI 固件的启发式逆向分析与模糊测试方法

E-mail: jos@iscas.ac.cn
http://www.jos.org.cn
Tel: +86-10-62562563

∗

林欣康, 顾匡愚, 赵 磊
(空天信息安全与可信计算教育部重点实验室,武汉大学国家网络安全学院,湖北 武汉
通讯作者: 赵

摘

要:

430079)

磊, E-mail: leizhao@whu.edu.cn

统一可扩展固件接口(Unified Extensible Firmware Interface,简称 UEFI),作为新一代固件接口标准，

广泛应用于现代计算机系统,但其漏洞可能引发严重安全威胁.为了减少 UEFI 漏洞引发的安全问题,需要进行
漏洞检测.而第三方安全测试场景下的模糊测试是检测的主要手段.但符号信息的缺失影响了测试效率.本文提
出了一种启发式的 UEFI 逆向分析方法,恢复固件中的符号信息,改进模糊测试并实现了原型系统 ReUEFuzzer.
通过对来自 4 个厂商的 525 个 EFI 文件进行测试,证明了逆向分析方法的有效性.ReUEFuzzer 可以提升函数测
试覆盖率,并在测试过程中发现了一个零日漏洞,已报告给国家信息安全漏洞共享平台以及公共漏洞和暴露系
统. 实验证明,本文方法在 UEFI 漏洞检测方面具有有效性,可以为 UEFI 安全提供一定的保障.
关键词:

统一可扩展固件接口;逆向工程;模糊测试;静态分析;固件安全

中文引用格式: 林欣康,
1000-9825/7116.htm

顾匡愚,

赵磊. UEFI 固件的启发式逆向分析与模糊测试方法.软件学报. http://www.jos.org.cn/

英文引用格式: Lin XK, Gu KY, Zhao L. UEFI fuzz testing system and method based on heuristic reverse analysis. Ruan Jian Xue
Bao/Journal of Software. http://www.jos.org.cn/1000-9825/7116.htm

UEFI fuzz testing system and method based on heuristic reverse analysis
LIN Xin-Kang1,2,

GU Kuang-yu1,2,

ZHAO Lei1,2

1

(School of Cyber Science and Engineering, Wuhan University, Wuhan 430072, China)

2

(Key Laboratory of Aerospace Information Security and Trusted Computing Ministry of Education, Wuhan University, Wuhan 430072,
China)

Abstract:

As a next-generation firmware interface standard, the Unified Extensible Firmware Interface (UEFI) has been widely used in

modern computer systems. However, UEFI vulnerabilities have also brought serious security threats. To avoid security problems caused by
UEFI vulnerabilities as much as possible, vulnerability detection is needed, in which, fuzzing under third-party security testing scenarios is
mainly used. However, the absence of symbolic information affects the efficiency of testing. This paper proposes a heuristic UEFI reverse
analysis method, which recovers the symbolic information within the firmware, improves fuzz testing, and implements a prototype system,
ReUEFuzzer. Through testing 525 EFI files from four manufacturers, the effectiveness of the reverse analysis method is demonstrated.
ReUEFuzzer can enhance the function test coverage and has identified an unknown vulnerability during the testing process, which has
been reported to China National Vulnerability Database and The Common Vulnerabilities and Exposures (CVE) system. Empirical
evidence shows that the method presented in this paper is valid for UEFI vulnerability detection and can provide a certain degree of
security guarantee for UEFI.
Key words:

∗
∗

UEFI(Unified Extensible Firmware Interface); reverse engineering; fuzzing; static program analysis; firmware security

林欣康和顾匡愚为共同第一作者
基金项目: 国家自然科学基金(62172305)
收稿时间: 2023-09-10; 修改时间: 2023-10-30; 采用时间: 2023-12-15; jos 在线出版时间: 2024-01-05

统一可扩展固件接口(Unified Extensible Firmware Interface,简称 UEFI)[1] 是一种用于加载和启动操作系统,
并向操作系统提供丰富的运行时服务的固件接口.由于其灵活性和可扩展性,UEFI 已经被广泛应用于现代计算
机系统中,以取代传统的基本输入/输出系统(Basic Input/Output System,简称 BIOS)[2].然而,随着 UEFI 的广泛应
用,其安全性也面临着越来越多的威胁.根据 CVE 官方网站的数据[3],仅在 2022 年,就有 134 个与 UEFI 相关的
漏洞被披露.这些漏洞中,71%的 CVSS 评分[4]高于 7.0 分,属于高危漏洞.这些漏洞可能导致绕过操作系统安全措
施的控制权劫持、任意位置的敏感数据泄露、恶意程序(如 Bootkit[5])的植入等一系列高危的安全威胁.因此,
针对 UEFI 的漏洞挖掘显得尤为重要,通过主动发现这些问题并及时修补,可以有效地预防这些安全威胁.
目前对于 UEFI 的漏洞挖掘主要有两种方法,分别为动态模糊测试和静态污点分析.在动态模糊测试方法
中,考虑到固件通常无法直接运行,因此一般采用模拟执行的方式来运行 UEFI 固件.然后再利用动态插桩等方
法收集测试过程产生的覆盖率信息以及程序状态来引导模糊测试器生成有效的测试用例对 UEFI 进行测试.而
现有的静态污点分析方法不具备通用性,在现有的代表性工作中仅针对 SMM 提权漏洞,这是因为该类型的漏
洞在具有保护的 SMRAM 区域中对外部函数出现逃逸引用时就会发生,因此只需要检测其中的数据流及控制
流依赖即可发现该漏洞,而其他类型的漏洞一般不具有该特征.
考虑到 UEFI 漏洞挖掘通用性的需求,本文利用动态模糊测试方法对 UEFI 进行漏洞挖掘.由于在实际应用
场景中,UEFI 的符号信息通常难以获得,这些符号信息具体包括:(1)基础服务信息,即全局服务函数以及协议的
符号信息(2)函数签名信息,即函数名以及对应的参数数据结构.缺失符号信息导致对 UEFI 进行模糊测试时会
出现如下的问题:UEFI 功能难以正确模拟、攻击平面难以定位以及变异过程盲目,最终导致模糊测试的覆盖率
低下甚至无法测试.例如,当缺失基础服务信息时,会导致模拟器无法模拟全局服务函数和协议,从而使得 UEFI
在模拟时,无法正常使用这些基础服务,最终使得运行到相关代码时导致 UEFI 模拟失败;此外,一部分的服务函
数还与外部输入有关,定位攻击平面需要确定这些服务函数和协议的位置.而缺失函数签名信息时,由于 UEFI
大量使用复杂数据结构,例如多层嵌套结构体在没有符号信息的辅助下进行模拟,可能会因为指针解析、校验
和检查错误等原因产生崩溃,使得 UEFI 模糊测试的变异过程变得盲目,从而导致测试时 UEFI 代码覆盖率有限.
针对上述问题,本文提出了一种逆向分析 UEFI 的方法,即通过对二进制固件反汇编代码的分析来还原上述
两种符号信息:使用服务函数访问模式逆向分析的方法还原基础服务信息;使用断言匹配、二进制代码比对和
函数调用关系的逆向分析方法还原函数签名信息.最后,本工作在逆向分析方法的基础上,设计并实现了一个
UEFI 的模糊测试原型系统 ReUEFuzzer.为了验证其有效性,本工作在随机选取 EDK2[6] (EFI Development Kit 2)
中的 EFI 进行基础服务信息还原时,在随机选择的样本中含有 617 个全局服务函数,其中 595 个成功还原,还原
成功率为 96.43%.在随机选取固件解包得到的 EFI 进行函数签名信息还原时,在随机选择的样本中含有 1645 个
函数,其中 1315 个成功还原出相应的符号,数据结构等信息,还原成功率为 79.94%.这表明本文提出的逆向分析
方法能够较好地还原 UEFI 的符号信息.之后,我们进行了对比实验,将 ReUEFuzzer 与缺失符号信息的模糊测试
器进行对比.实验结果表明,在第三方测试场景下, ReUEFuzzer 能够更加有效地测试复杂函数并且提高代码覆
盖率.此外,利用 ReUEFuzzer,我们还发现了一个零日漏洞,并被 SuperMicro[7]公司的团队确认,同时将其上报给
了国家信息安全漏洞共享平台 [8]以及公共漏洞和披露系统[3].

1

背景

1.1

UEFI漏洞挖掘
UEFI[1] 是一种在计算机硬件和操作系统之间提供软件接口的系统规范,其起源于 Intel[9] 公司的 EFI(可扩

展 固 件 接 口 ,Extensible Firmware Interface) 项 目 .UEFI 旨 在 取 代 传 统 的 BIOS( 基 本 输 入 输 出 系 统 ,Basic
Input/Output System),为现代计算机提供更高效、灵活和安全的启动和运行环境.目前,关于 UEFI 漏洞挖掘的工
作较少,主要有两种技术路线,分别采用动态模糊测试方法和静态污点分析方法.
Zhenkun Yang 等人[10]提出一种动态模糊测试方法,该方法基于 simics 仿真器[11]对 UEFI 固件代码进行灰盒
模糊测试.但是目前该方法仅适用于厂商内部对 UEFI 进行测试,因为该方法需要测试人员拥有关于待测 UEFI
固件的源码,文档等信息用来定位测试的关键位置,并且具备编译固件的能力,使其能够在 simics 仿真器上运行.

因此,在无法获取源码和文档的第三方安全测试的场景下,该方法无法有效地应用. Jiawei Yin 等人[12]提出一种
静态污点分析方法.他们发现尽管已经部署了硬件隔离和漏洞缓解机制,攻击者仍然可以利用 SMI 处理程序漏
洞绕过现有的保护机制,并实现 SMM 提权攻击.他们指出 SMI 处理程序中存在逃逸引用,可能引入 SMM 权限
提升漏洞,并提出一种静态分析框架 SPENDER 用于检测 SMM 权限提升漏洞.然而,该方法仅仅针对 SMM 权限
提升漏洞,难以进行扩展.因此,目前对于第三方安全测试的场景而言,仍然缺失一种全面有效的动态分析方法
来进行 UEFI 固件的模糊测试.为此,需要进一步研究和开发基于仿真器的动态分析方法,以在不需要源码和文
档等信息的情况下,对 UEFI 固件进行全面有效的模糊测试.
1.2

基于模拟执行的模糊测试
模糊测试 [13–17] 是一种重要的软件测试技术,通过为目标程序提供随机输入,并监控程序异常行为(例如崩

溃)来发现程序中的错误.由于固件和普通程序不同,固件通常无法在测试环境下直接执行,因此在固件的模糊
测试中,广泛使用模拟执行技术来进行辅助.目前,基于模拟执行的模糊测试工作已经有一些具有代表性的成
果.
QEMU[18] 是一种支持多种指令集和系统模拟的仿真平台,在固件安全性分析的仿真平台设计中已被广泛
应用. Chen 提出的 Firmadyne[19]基于 QEMU 模拟器,并且首次对固件镜像进行了大规模的动态分析.在后续的
工作中, firmAE[20] 优化了 Firmadyne 的模拟过程,而 FirmAFL[21]在 Firmadyne 的基础上,还结合了 QEMU 的全
系统仿真模式和用户仿真模式,利用用户模式开销较低的特性来提升模糊测试的吞吐量. 这些工作主要针对
具有操作系统抽象的嵌入式固件,但是无法测试不具有操作系统抽象的 UEFI 固件.而 QEMU STM32[22]项目,将
QEMU 的仿真对象扩展到了 STM32 芯片上,并证明当有完整的硬件文档时,通过对硬件的仿真可以完全对不具
有操作系统抽象的固件镜像进行仿真.它也可以和模糊测试器相结合来进行模糊测试.但是会依赖在进行 UEFI
第三方测试时无法获取的文档,因此无法将该方法应用于 UEFI 第三方测试.
此外,一些工作通过真实硬件与模拟器结合的方法来解决模拟执行的问题.该方法是由 Avatar[23] 首先提出
的. Avatar 旨在通过提供更好的硬件组件支持来实现嵌入式固件的动态程序分析.它通过构建一个包括处理器
仿真器和真实硬件的混合执行环境,使得 Avatar 可以利用仿真器执行和分析指令,同时将 I/O 操作通道传导到
物理硬件上. PROSPECT[24]在 QEMU 中创建了一个虚拟字符设备,用于拦截内核中与字符设备进行通信的系
统调用,并将其转发到目标嵌入式系统上的正确字符设备,从而在模拟器中正确处理与字符设备交互的操作.该
系统可在目标设备上执行,并将执行结果反馈给分析系统.在随后的扩展工作中[25], Kammerstetter 改进了系统
的可伸缩性,通过缓存预期的外设行为,并在外设行为偏离预期时重置缓存,从而近似固件状态.这些工作能够
更好地模拟固件,但是需要依赖真实硬件,而且只能对于具有操作系统抽象的固件镜像有效.
与上述测试对象相比 UEFI 本身结构复杂,没有操作系统抽象,使用特有的全局变量共享方式和协议机制来
进行模块间通信.同时在第三方测试场景下也无法使用源码和文档.因此,UEFI 难以使用上述模拟方法进行模
糊测试.

2

UEFI 启发式逆向分析方法设计

2.1

逆向分析方法总体架构
为了还原基础服务信息以及函数签名信息,本文提出了如图 1 所示的逆向分析方法总体架构,根据他们各

自的特点分别使用不同的方法来还原出这两种信息.

逆向分析

服务函数访问模式

① 断言匹配
逆向分析

② 二进制代码相似性对比

按优先级
整合

③ 函数调用关系
UEFI固件镜像
辅助分析UEFI固件镜像

图1

逆向分析方法总体框架

针对基础服务信息的还原,本文提出一种基于服务函数访问模式的逆向分析方法.我们发现在 UEFI 中的一
些服务函数具有固定的访问模式,通过访问模式可以还原出这些服务函数相关的符号信息,并且还可以利用这
些函数还原协议和 GUID 等信息.
而针对函数签名信息的还原,本文提出三种方法:第一种方法基于断言匹配,我们发现 UEFI 固件中普遍存
在一些断言信息,通过这些信息与 UEFI 标准中函数的特点进行匹配,从而能够推断上下文,逆向分析出函数名
和数据结构.第二种方法基于二进制代码相似性比对,我们利用 UEFI 固件与现有的开源实现之间的相似性,对
比 UEFI 固件和开源实现,推断出固件内部复杂的数据结构信息.第三种方法基于函数调用关系,我们分析固件
的函数调用图以查看传递的参数信息,找出未被前面方法覆盖的函数签名信息,并验证已经还原出的函数名与
数据结构,从而使得对 UEFI 固件的逆向分析结果更加全面.并且为了整合不同方法得到的结果,我们通过一种
基于优先级的整合策略对上述几种方法推断得到的函数签名信息进行综合得到最终的推断结果.
2.2

基础服务信息的识别
本文提出一种基于服务函数访问模式的逆向分析方法,用于还原 UEFI 固件中关于全局服务表的全局变

量、全局服务函数、以及协议的 GUID 等基础服务信息.根据 UEFI 规范 UEFI 中要设置大量的服务函数,这些
函数以函数指针的形式存放在一些重要的全局变量指向的表结构中,例如 gST(全局系统表)、gBS(启动服务表)
和 gRT(运行时服务表).UEFI 规范规定了系统表传递给各模块的函数原型以及各模块使用系统表中各函数的
调用方法,因此 UEFI 模块对于全局变量、数据结构具有固定的访问模式,这些访问模式在汇编层面表现为易于
识别的特征,并且这些访问模式在不同的指令集下都是类似的.因此,我们可以通过分析这些访问模式来还原基
础服务信息.
如图 2 所示,我们总结了四种访问模式,用于还原 UEFI 固件中的全局服务函数、存放相关指针的全局变量
等信息.我们依次使用这些访问模式,逐步对全局服务函数相关的信息进行推断,每一步推断中使用之前推断的
结果与当前访问模式相结合从而推断出更多的信息.下面我们将依次介绍这四种访问模式以及如何利用它们
来推断符号信息.

访问模式①

mov
lea
xor
lea
call

rax, cs:gBS_D80
r8, [rsp+28h+Interface]
edx, edx
rcx, Protocol_GUID
[rax+EFI_BOOT_SERVICES.LocateProtocol]

访问模式②

subFunc:
// SystemTable -> rdx
mov rax, [rdx+0x60]
mov cs:qword_D80, rax

cs:gBS

访问模式③

UEFI固件

ModuleEntryPoint(
EFI_HANDLE ImageHanlde,
EFI_SYSTEM_TABLE *SystemTable){
...
subFunc(ImageHandle, SystemTable)
}

访问模式④

mov rax, cs:gBS_D80
lea rdx, [rdi+8]
mov r8, rcx
mov rcx, rsi
call qword ptr [rax+160h]
gBS_D80->CopyMem

图2

访问模式总览

访 问 模 式 ① : 大 多 数 UEFI 模 块 入 口 点 函 数 的 参 数 为 EFI_HANDLE 类 型 的 ImageHandle 和
EFI_SYSTEM_TABLE*类型的 SystemTable.对于访问模式①,我们在利用该模式进行推断时,首先定位 UEFI 固
件的模块入口点,然后根据模块入口点的参数传递,我们可以推断出 ImageHandle 变量以及 SystemTable 变量.
其中,System Table 是一个十分重要的数据结构,它包含了许多关键信息,例如系统的启动时间戳、内存分配器、
协议栈等.其中,System Table 中的表项存储了全局服务表的指针,包括 gBS(Boot Services)和 gRT(Runtime
Services).这些全局服务表提供了许多基本的系统功能,例如内存分配、文件读写、控制台输出等.
访问模式②:UEFI 模块通过 SystemTable 加上特定偏移来获取全局服务表指针.因此对于访问模式②,我们
通过搜索针对 SystemTable 特定偏移的查表操作,来推断出例如 gBS(Boot Services)和 gRT(Runtime Services)等
全局服务表的指针,以及负责存储它们的全局变量.以 gBS 与 gRT 为例,对于 32 位的 UEFI 固件,gBS 相对于
SystemTable 的偏移为 0x3c、gRT 的偏移为 0x40；对于 64 位的 UEFI 固件,gBS 的偏移为 0x58,gRT 的偏移为
0x60.因此我们可以在固件代码中全局匹配对于这种含有特定偏移的移动操作,通过这种模式能够匹配到全局
变量和全局服务表的映射.从而还原出有关全局服务表的符号信息.
访问模式③:在模块使用全局服务函数时,使用全局服务表加上偏移的方式来计算并读取对应的函数指针,
对于不同 UEFI 固件中的同一种服务函数这个偏移是不变的.因为 UEFI 标准规定了服务表的结构和其中所有
的函数的偏移.因此对于访问模式③,我们通过搜索汇编代码中使用全局服务表与偏移来读取内存的代码块,并
根据其中使用的全局服务表的指针以及使用的偏移大小来找到该调用对应的服务函数.
访 问 模 式 ④ : 服 务 函 数 中 的 部 分 函 数 传 递 的 参 数 包 含 GUID, 这 些 函 数 包 括 LocateProtocol 、
InstallProtocolInterface、InstallMultipleProtocolInterfaces.以 LocateProtocol 为例,该函数是用来定位已经被安装
的协议,协议是一种通过预定义的接口规范来提供服务的机制.UEFI 规范定义了一系列的协议,包括但不限于
Boot 服务、Driver Binding 协议、Device Path 协议等.每个协议都有自己的 GUID 用于标识和访问,同时也有
一系列预定义的接口函数来实现相应的服务.UEFI 中的协议机制为操作系统和硬件提供了一种通用的接口,使
得操作系统可以通过协议来访问硬件设备的服务,从而实现更加灵活的硬件控制和驱动.协议和对应的 GUID
会以 LocateProtocol 参数的形式传入到该函数中,因此我们可以通过该函数的参数来推断得到协议以及它对应
的 GUID 的信息.
2.3

函数签名信息的识别
在逆向还原 UEFI 中的函数签名信息时,我们主要依赖 UEFI 固件与 UEFI 开源实现之间的相似性.这是因

为 UEFI 必须遵循 UEFI 规范[26],而开源实现则是 UEFI 规范的参考实现,因此,大多数厂商都会在开源实现的基
础上进行二次开发.我们调查了总市场占比 87%的主流厂商的 UEFI 固件,发现都存在这一现象.基于此,我们提
出了三种方法来还原函数签名信息,分别为基于断言匹配的逆向分析方法、基于二进制代码相似性匹配的逆向
分析方法和基于函数调用关系的逆向分析方法.

2.3.1 基于断言匹配的逆向分析
我们提出了一种基于断言匹配的方法,用于还原 UEFI 模块中的函数签名信息.该方法基于如下的发现,即
UEFI 固件中通常会保留一些断言信息.断言[27] 是一种放在程序中的一阶逻辑,用于标识验证程序开发者预期的
结果.断言的作用是验证程序的假设和前提条件是否成立,以及检查程序的执行结果是否符合预期.通过添加断
言,开发人员可以减少程序中的错误和漏洞,提高程序的可靠性和安全性.这些断言通常采用类似于如图 3 中
sub_2F38A808 所示的形式,其中包含断言信息和触发该断言的源代码文件路径信息. 由于断言信息通常与函
数功能高度相关,某些断言信息只会在具有相应功能的函数中被触发,因此断言可以与 UEFI 标准中的函数特征
进行匹配.这种映射关系使得我们能够推断上下文,并对 UEFI 中的数据结构和符号信息进行逆向分析.

①
反编译

UEFI解包后得到
的efi文件

__int64 __fastcall sub_2F38AF60(__int64 a1){
sub_2F38A808(
"/usr1/code/.../Third_Party/open_source/"
"Edk2/MdePkg/Library/BaseLib/String.c",
173i64,
"((UINTN) String & 0x00000001) == 0")
...
}

②
相互匹配

③
推断得到

sub_2F38AF60为
StrLen函数

UINTN
EFIAPI
StrLen (
IN
CONST CHAR16
*String
)
{
UINTN
Length;

现有实现源码
String.c

ASSERT (String != NULL);
ASSERT (((UINTN) String & BIT0) == 0);
...
return Length;
}

图3

根据断言信息进行匹配推断

为了实现这种方法,我们使用了一种自动化的流程来识别以及提取固件中的断言信息.从这些信息中,我们
首先提取出该断言所在源文件的文件路径信息.然后考虑到编译过程中语义信息的丢失,我们使用字符串相似
性匹配算法在源文件中查找具有相似断言信息的函数,并使用这些符号信息来恢复 UEFI 固件中对应函数的签
名信息.以图 3 中的情况为例,我们首先会识别到断言函数,即图中的 ASSERT,然后根据断言函数参数中的字符
串提取出断言所在的文件路径,即 MdePkg/Library/BaseLib/String.c.然后利用字符串相似性匹配算法在开源实
现对应的源文件中查找具有((UINTN) String & 0x00000001)==0 的断言内容,在成功匹配后则可以得到具有相
似断言的函数为 StrLen,因此可以将 sub_2F38AF60 与 StrLen 函数相互匹配.
需要注意的是,由于编译器的内联优化,有些函数可能会为了避免函数调用而被直接嵌入到上级函数中.在
通过断言信息匹配函数时,如果编译器优化内联了目标函数,就无法通过断言信息将现有实现中的函数与 UEFI
固件镜像中对应断言所在的函数进行匹配.这种情况下,与 UEFI 固件镜像中函数相匹配的将是现有实现中函数
的上层函数.而在这种无法匹配的情况下,UEFI 固件镜像与现有实现中的函数参数数量等基础特征通常无法对
应.因此,我们使用一种启发式的方法来缓解编译器内联优化导致的错配问题,即当函数参数数量等基础特征不
匹配时,我们将这个存在断言的函数与现有实现中的上一级函数进行配对.
2.3.2 基于二进制代码相似性比对的逆向分析
在我们的研究中,我们发现大多数 UEFI 固件都是基于符合 UEFI 规范的成熟实现进行开发的,例如 EDK2.
这些固件会在其基础上进行二次开发,以满足一些特殊的硬件要求和厂商的定制功能.由于这些固件中的大部
分代码都存在复用开源代码的情况,而只有小部分是定制化的实现,因此通过对比开源实现和定制化固件的异

同,我们可以推断出其中复杂的数据结构与符号信息.为了实现这一点,我们利用二进制代码相似性比对技术
(BinDiff)[28–31] 来比较开源实现以及 UEFI 固件样本之间的异同,其流程图如图 4 所示.

UEFI规范

BinDiff

还原UEFI固件④

匹
配
②
生
成

不同架构/不同版本的
开源实现

编译
生成

UEFI固件

①

③
挑选
...

置信度相
似度最高
的结果

图4

不同架构/不同版本的
UEFI实现源代码

基于二进制代码相似性比对技术进行逆向还原的流程

二进制代码相似性比对是在二进制代码分析领域中,检测二进制代码相似性和差异性的强有力技术.它可
以比较两个二进制文件,并确定从一个文件到另一个文件所做的更改.对于 UEFI 而言,可以通过比较 UEFI 固件
解包后得到的 EFI 文件,来比较不同的 UEFI 固件镜像,以识别 UEFI 固件的差异和相似之处.通过使用二进制代
码相似性比对技术,我们可以有效地对比开源实现和定制固件中的常见数据结构和控制流,从而恢复定制固件
中的函数签名信息.
为了尽可能消除不同架构以及不同版本的开源实现,对二进制代码相似性比对结果的影响,在本方法中首
先收集不同架构/不同版本的开源实现.然后使用二进制代码相似性比对技术将这些开源实现分别与 UEFI 固
件进行匹配,该匹配过程会生成一系列的匹配结果,包括函数名称,匹配算法,相似度,置信度等.我们在其中挑选
置信度最高的匹配结果来推断 UEFI 固件中的函数签名信息.置信度是指在二进制文件比较过程中,两个二进制
文件相似度的可靠程度,置信度的计算基于多种因素,包括二进制文件的大小、结构、代码重用情况等. 在使用
二进制代码相似性比对时,高置信度通常代表更低的误报率.
2.3.3 基于函数调用关系的逆向分析
在我们之前的设计中,我们采用两种不同的方法来推断 UEFI 固件样本中存在的函数签名信息.尽管这些方
法能够推断出 UEFI 固件样本中的函数以及内部数据结构信息,但是它们可能无法充分推断供应商定制化过程
中开发的自定义组件.这是因为这些组件可能不存在于开源实现中,并且无法被上述提到的方法进行推断.
为了缓解这个问题,我们提出了一种方法,利用函数之间的关系信息(例如调用和被调用函数之间传递的参
数)来推断这些未被覆盖的部分,从而使得还原的信息更加完善.我们的方法使用固件模块的函数调用图,以识
别函数依赖关系并揭示出前述方法未能记录的函数和数据结构.我们还会分析函数之间传递的参数,以推断每
个函数使用的数据类型和格式.通过这些信息与前述方法获得的结果相结合,我们能够更加全面的推断该固件
的符号信息.

2.3.4 优先级整合
在前文中,我们介绍了三种不同的逆向分析方法,以获取函数签名信息.然而,这些分析方法本身是相对独
立的,它们从不同的角度对 UEFI 固件进行分析,因此,它们得到的结果可能会相互冲突.为了解决这个问题,我们
还需要一个策略来整合它们的结果.
我们提出的这些针对 UEFI 的逆向分析方法具有不同的可信度,在这些方法中推断过程越复杂,就需要依赖
越多的假设,只有该方法所依赖的假设全部成立的时候,才能够完全采信该方法推断出来的结果.因此,使用假
设越少的方法通常是可信度更高的.
直接基于断言匹配的方法可信度最高,因为它只是简单地进行字符串的匹配,然后推断出上下文的信息.其
次是基于二进制代码相似性比对的方法,因为该方法受到二进制代码相似性比对技术的局限性,无法保证匹配
成功的函数和数据结构完全可信,所以它的可信度不及基于断言匹配的逆向分析方法.最后是基于函数调用关
系的方法,它需要依赖前面两种方法推断得到的结论然后在这个基础上进行进一步的推断,所以可以认为它的
可信度是最低的.
因此,在整合过程中,我们只需要使用可信度作为优先级,首先先采信可信度较高的方法得到的结果.如果
后续方法得到的结果与可信度较高的方法得到的结果冲突,则以可信度较高的结果为准.通过这种方式,我们可
以整合不同方法得到的结果.
2.4

逆向分析方法实现
本文采用静态反编译软件 IDA Pro[32]作为实现上述提到的四种 UEFI 逆向分析方法的工具.为了系统地介

绍这些方法的实现过程,本节将从准备工作和具体实现两个方面出来进行论述.
2.4.1 获取解析固件与开源实现
我们从主流的制造商官网上下载 UEFI 固件镜像作为实验所需的样本,这些制造商包括 Intel[9]、Lenovo[33] 、
SuperMicro[7]等.通常,这些制造商在其网站上提供的固件镜像有两种格式,一种是原始的 UEFI 固件镜像,即.bin
格式,另一种是用于 UEFI 胶囊式固件更新的更新包,即.CAP 格式.这两种格式的镜像均包含了 UEFI 中各个模
块的实现,并且可以在一定程度上相互转化.
由于我们需要单独分析 UEFI 内部的子模块，因此需要对 UEFI 的内部模块进行解析.我们使用开源工具
UEFITool[34]来对 UEFI 固件进行格式分析, UEFITool 是一款开源的 UEFI 固件查看器和编辑器,该工具会将
UEFI 固件中的内容解析为树结构.该树结构包含各个模块的二进制文件以及该二进制文件的简单信息介绍.我
们的分析主要使用位于 BIOS region 文件夹下的文件,该文件夹下包含 UEFI 的各种功能模块.
根据我们的调查,各大主流厂商都广泛使用基于 UEFI 标准的开源固件开发框架 EDK2 进行开发 , EDK2
提供了一个完整的开发环境,用于开发 UEFI 固件和 UEFI 应用程序.因此,我们选择 EDK2 作为基于二进制代码
相似性比对方法的比较对象.
由于许多产品需要稳定性,因此大部分 UEFI 固件镜像与版本较早的 EDK2 实现更加接近.基于此我们选择
了较早版本的 EDK2 源码,而不是最新版本.我们使用 Ubuntu 18.04 系统在 x86-64 架构下使用 GCC5 编译了
edk2-stable202111、edk2-stable202111-rc1、edk2-stable202105 和 edk2-stable202008 这四个版本的 EDK2 源代
码,得到了不同版本的各个模块对应的 EFI 文件.在编译这些模块的时候,我们保留了其符号信息,以便指导后续
的逆向分析步骤.
2.4.2 基于 IDA Pro 的逆向分析
本文基于 IDA Pro[32]来实现本章中详述的针对 UEFI 固件的逆向分析方法.IDA Pro 是一个多平台、多处理
器的反汇编程序,它可以将机器可执行代码转化为汇编语言以及源代码,用于调试和逆向工程.
在还原基础服务信息时,我们采用了基于服务函数访问模式的方法,并使用 IDAPython[35] 进行实现.我们利
用 IDAPython 按照 4 个访问模式依次对固件中的代码以及数据进行匹配,然后根据匹配结果逐步还原基础服务
相关的信息,并对相关变量的数据类型进行修改.在对固件中的数据类型进行修改时,在 x86-64 架构下,我们依
赖于 IDA 自带的类型库 uefi.til 以及 uefi64.til,在 ARM 架构下,我们将类型结构写入 C 语言格式的头文件中,然

后使用 IDA 的解析 C header 的功能对其进行解析来自动生成对应的数据类型以及结构体信息.
在还原函数签名信息时,我们采用了三种方法，这三种方法也使用 IDAPython 脚本进行实现.其中,对于基
于字符串匹配方法,我们首先进行一些预处理的工作.我们使用 IDAPython 脚本自动化地批量处理对 UEFI 固件
解包后得到的 EFI 文件,获取 EFI 文件中的所有字符串以及其地址,并且利用同一函数下被使用的字符串的地
址紧密排布的特点对它们进行合并分组.然后提取每组字符串中记录的源文件信息,以搜索 EDK2 中对应的源
文件,并在源文件中对该组的其他字符串进行相似度匹配,最终得到源文件中使用该组字符串函数的函数名和
其他参数.最后,使用 IDAPython 脚本自动将 EFI 文件中的函数重命名并修改参数类型.
对于基于二进制代码相似性比对的方法,我们首先使用 IDA 加载不同版本开源实现中的 EFI 文件得到 IDA
的数据库文件.然后利用 IDA 的 BinDiff 插件[36],分别将解包得到的 EFI 文件与不同版本开源实现中的对应文
件进行匹配,并从匹配输出挑选出置信度最高的作为最终结果,同时从具有 DEBUG 信息的开源实现中导入对
应的符号信息.
对基于函数调用关系的方法,我们在使用 IDA 分析 EFI 文件时,遍历各个函数,将每个函数中的子调用的参
数类型改为它所使用的局部变量参数的数据类型.
最后,本文将顺序执行这几种方法的实现,并且规定后面的方法不能覆盖前面方法得到的推断.其结果将会
自动保存在 IDA 的数据库中.

3

基于启发式逆向分析的模糊测试器设计与实现
基于前 文提到 的启 发式 逆向 分析方 法,我们 进一 步设计 并 实现了 一个 UEFI 的模糊 测 试器原 型系 统

ReUEFuzzer.该原型系统的总体架构如图 5 所示.
由于在 UEFI 中存在许多无法通过 EFI 模块入口执行到达的跨 EFI 模块的函数调用,我们采用直接模拟该
函数的方法来测试该函数并发现其中可能存在的缺陷.因此,在 ReUEFuzzer 中我们针对单个函数进行模糊测
试.在设 计上我 们参考 Zhenkun Yang 等 人 [10] 的 设计思 路,并使用 模拟器 来替代 simics 仿真器 进行实 现.
ReUEFuzzer 由三个模块组成,首先,使用逆向分析模块还原 UEFI 固件的符号信息,来辅助模拟执行模块和模糊
测试模块.之后,使用模拟执行模块对 UEFI 固件进行模拟,并利用模拟器的功能对待测位置进行拦截.最后,使用
模糊测试模块来生成测试样例,并在模拟器中将测试样例填入到合适位置以进行测试.引入符号信息使得模糊
测试器可以生成结构化的测试样例.本工作在原始的 qiling 模拟器[37] 以及 AFL++[38] 上新增了大约 600 行 Python
代码来实现针对任意函数测试的功能以及结构化参数生成的功能.

逆向

UEFI固件

逆向分析模块

模拟

符号信息

共享信息

模拟执行模块

模糊测试模块
辅助模拟执行

图5

ReUEFuzzer 总体架构

3.1

逆向分析模块
本模块旨在通过第二章中介绍的四种启发式逆向分析方法,分别逆向还原出基础服务信息和函数签名信

息.这些获得的符号信息可以为模拟执行模块提供全局服务函数的位置,调用的协议的 GUID 等信息,使其能够
根据这些信息对特定的函数进行模拟实现,避免单一模块或者函数模拟时因缺失这些跨模块的信息而导致模
拟失败的问题.此外,该模块还可以提供一些数据结构信息,使得模拟执行模块能够对结构中的字段进行取值约
束,避免非法取值导致指针访问非法地址等问题.我们将利用逆向分析模块提供的符号信息,使用 Python 编写结
构体约束,传递给模糊测试模块生成满足约束的输出,同时,我们将确保模拟执行模块能够正确解析结构体以进
行模拟.
3.2

模拟执行模块
本工作中,我们使用 qiling 模拟器实现该模块.qiling 模拟器[37]是一个基于 unicorn[39]的轻量级二进制仿真

框架,能够对 UEFI 进行模拟.在本工作中,我们通过修改 qiling 模拟器的源代码,修改 EFI 文件的加载逻辑,使得
模拟器初始调用从默认调用 module_entry 函数转化为可以调用任意函数,这可以让后续测试得以从任意函数
开始.同时,我们提前在模拟器内存中初始化全局系统表中包含的函数以及协议结构用来替换模拟执行时对这
些函数以及协议的调用.并且,我们对存放全局系统表、全局服务表的全局变量进行拦截,这些全局变量的位置
由逆向分析模块提供,使得它们有关的函数调用在仅模拟单个模块时也能够被正确处理.同时在模拟器中对输
入的测试样例进行处理,通过递归解析约束,分解结构体的树形结构,使其能够正确解析结构化的参数,将输入
参数按约束要求布置到模拟器的内存中,使得在执行过程中结构体能够被正确处理.
3.3

模糊测试模块
本工作中,我们使用 AFL++[38] 来实现模糊测试模块.AFL++是一个基于 AFL[40] 框架的改进版模糊测试框

架,综合了大部分已有的改进方案.考虑到我们使用的模拟器是基于 unicorn 的 qiling 模拟器,因此,我们采用了
AFL++的 unicorn 模式,以兼容 qiling 模拟器,并连接模拟器和模糊测试器.该模式使用动态二进制插桩技术来收
集覆盖率信息,并引导模糊测试器进行高效的变异.对于模糊测试器生成的测试用例,我们可以通过回调函数将
其传回到模拟器中,并通过解析测试用例的值,来布置内存以及修改寄存器.同时, 为了缓解不真实输入的生成,
我们还引入了逆向分析模块编写的结构体约束,以生成满足结构体约束条件的测试用例,从而使测试更具针对
性,并且在实际应用过程中主要测试攻击平面附近的函数,以减少输入被限制的情况发生.

4

实验
在本章中,我们首先使用由 4 个不同厂商的 UEFI 固件并解包为 525 个 EFI 文件,用这些 EFI 文件和 EDK2

来构造测试数据集.接着,我们使用本文提出的逆向分析方法来还原这些 UEFI 固件中的符号信息.并与人工逆
向的方式比较,来说明了该方法的有效性.然后,我们将 ReUEFuzzer 于缺失符号信息的普通模糊测试器进行对
比,用来说明 ReUEFuzzer 可以更有效地测试 UEFI 固件.最后,我们利用 ReUEFuzzer 对 EFI 文件进行批量测试,
并且发现了一个零日漏洞,已经报告给厂商 SuperMicro 进行处理,同时还提供了修复建议以供厂商参考.
表1
软/硬件

4.1

实验环境
参数

操作系统

64 位 Ubuntu 20.04

CPU

Inter(R) Core(TM) i7-10700 CPU @2.90GHz

内核版本

linux 5.15.0-71-generic

逆向还原能力评估
为了对还原基础服务信息的能力进行评估,我们直接针对 EDK2 编译得到的 EFI 二进制文件进行逆向还

原.因为目前的逆向分析工作中暂无对本文工作中重点关注的符号信息针对性还原的方法,因此我们评估了该

方法能够从全局系统表、全局服务表等有关的服务函数中还原的比例,并使用该比例作为评估的指标.本实验
中我们使用的 EDK2 版本为 edk2-stable202111-rc1,我们随机选取其中 10 个 EFI 二进制文件进行分析对比.还原
的比例如表 2 所示,具体结果如下.
表2

逆向分析方法对基础服务信息的还原情况

EFI

还原的服务函数

总共的服务函数

还原比例

ArpDxe.efi

54

56

96%

BdsDxe.efi

183

188

97%

CapsuleRuntimeDxe.efi

17

22

77%

ConPlatformDxe.efi

44

49

90%

Dhcp4Dxe.efi

85

87

98%

DpcDxe.efi

7

7

100%

EbcDxe.efi

19

19

100%

SmbiosDxe.efi

13

13

100%

TcpDxe.efi

98

100

98%

UdpDxe.efi

75

76

99%

从表 2 中可以看出,大多数 EFI 二进制文件中的全局服务函数还原比例均超过 90%,在 617 个全局服务函
数中有 595 个被成功还原,总的还原成功率为 96.43%.这表明本工作可以有效地还原出基础服务信息.
对于还原函数签名信息能力的评估,我们选取了主流的个人计算机和服务器厂商 Intel, Msi, SuperMicro,
Bull 的固件进行逆向分析.我们对这些固件进行解包,并随机选取其中的 11 个 EFI 二进制文件,使用逆向分析方
法的实现对它们进行还原,从而得到这些 EFI 的函数签名信息.与人工分析的结果进行比较后,我们将被还原的
函数数目以及还原比例列在表 3 中.
从表 3 可以看出,我们的方法可以成功还原绝大部分函数签名信息,包括其中的函数名以及参数数据结构.
在 11 个 EFI 模块中,有 6 个模块的还原比例在 80%以上.在 1645 个函数中有 1315 个函数签名信息被成功还原,
还原成功率为 79.94%.并且在人工分析比对过程中,我们发现没有被还原的函数中还存在一部分功能简单的函
数(例如直接返回常数),它们虽然没有被自动还原出来,但是通过人工分析可以比较容易地了解它们的内部逻
辑,因此也无需还原这些功能简单的函数.
表3

逆向分析方法对函数签名信息的还原情况

厂商

UEFI

Intel

PA0050.cap

Msi

E17H3IMS.109

SuperMicroBIOS_X12DPG-QR-1C55_20230202_1.4b_STDsp.bin

Bull

BIOS_SKD080.18.02.003.sign

EFI

还原函数数目

总函数数目

还原比例

Dxelpl.efi

56

79

71%

PeiCore.efi

95

125

76%

PeiCore.efi

88

117

75%

DxeCore.efi

235

285

82%

English.efi

5

8

60%

TcpDxe.efi

280

302

93%

TcpDxe.efi

172

202

85%

Udp4Dxe.efi

132

158

84%

CapsuleRuntime.efi

63

155

41%

ConPlatform.efi

68

73

93%

Dhcp4Dxe.efi

121

141

86%

4.2

ReUEFuzzer测试能力评估
本节旨在通过与缺失符号信息的普通模糊测试器进行对比的方式,来评估使用符号信息辅助的

ReUEFuzzer 在 UEFI 模糊测试中的测试能力.我们在 BIOS_SKD080.18.02.003.sign 固件中随机选取 10 个能够
被我们的逆向方法进行分析的函数进行模糊测试.在测试时,我们将模糊测试器产生的输出当作函数参数传入
函数,并对每个函数进行两次测试,由于该类模糊测试相关研究没有开源的工具发布,我们将自己实现的模糊测
试工具移除了符号信息作为对比基准,下文中称其为普通模糊测试器,以证明符号信息对于提升代码覆盖率的
有效性.两次测试分别测试 30 分钟,对两种模糊测试器测试的可行性和覆盖率进行比较.其中在使用符号信息
时需要根据 ReUEFuzzer 的逆向分析模块得到的信息编写约束文件,用来引导结构化的模糊测试器对函数进行
测试,对相同的数据结构而言这些约束文件可以复用.在本实验中,我们编写了大约 400 行 Python 代码用来生成
不同的约束.
表4

覆盖路径对比结果

模块名

函数名

推测函数名

TcpDxe.efi

sub_1348

TcpDxe.efi
TcpDxe.efi

覆盖路径
普通模糊测试器

ReUEFuzzer

TcpFlushPcb

0

2

sub_4A34

SockConnClosed

0

3

sub_586C

TcpSendAck

0

0

Udp4Dxe.efi

sub_2270

Udp4Transmit

1

3

Udp4Dxe.efi

sub_1A9C

Udp4CreateService

0

0

CapsuleRuntime.efi

sub_8C48

StringToBits

1

34

CapsuleRuntime.efi

sub_3D40

CompareGuid

1

1

ConPlatform.efi

sub_1068

IsGopSibling

0

1

Dhcp4Dxe.efi

sub_1C94

DhcpHandleSelect

0

4

Dhcp4Dxe.efi

sub_107C

DhcpComputeLease

6

6

实验结果表明,在使用普通模糊测试器进行测试时,10 个函数中有 6 个函数无法进行测试,而 ReUEFuzzer
只有 2 个函数无法进行测试.在都能进行模糊测试的 4 个函数中,相同时间内 ReUEFuzzer 的路径覆盖率也比普
通模糊测试器的路径覆盖率持平或有较大提升,验证了 ReUEFuzzer 能够在单位时间 内覆盖更多路径,即
ReUEFuzzer 的时间效率比普通模糊测试器更高.这说明基于启发式逆向方法的 UEFI 模糊测试器原型系统
ReUEFuzzer 相较于普通模糊测试器能够更加有效地对 UEFI 进行测试.
进一步地,我们分析了普通的模糊测试器无法测试的主要原因:因为参数结构体中的二级指针无法被正确
解析以及全局服务函数无法使用导致无法进行测试.例如 TcpDxe.efi-sub_1348、Dhcp4Dxe.efi-sub_1C94 无法成
功解析参数结构体中的二级指针而在运行时会发生崩溃;TcpDxe.efi-sub_4A34、ConPlatform.efi-sub_1068 因为
无法使用 bootService 表中的函数而导致运行时发生崩溃.对于 TcpDxe.efi-sub_586C 因为子调用中的数据被解
析成结构体,导致出现非法内存访问从而使得在两种情况下都无法进行模糊测试.对于 Udp4Dxe.efi-sub_1A9C
因为局部变量被强制类型转化解析为结构体而导致出现非法内存访问的情况.
最后我们利用 ReUEFuzzer 对一些 UEFI 容易产生漏洞的关键位置进行测试,例如 UEFI Variable、外设 IO、
SMM 等.我们使用批量测试的方法针对 Bull 公司的 BIOS_SKD080.18.02.003.sign 固件和 SuperMicro 公司的
BIOS_X12DPG-QR-1C55_20230202_1.4b_STDsp.bin 固件中的 371 个 EFI 文件进行批量测试,测试中出现了
1408 个 crashes,由于 unicorn 动态插桩的机制,在出现控制流劫持后会将跳转地址对应的内存解析为代码块,从
而不同的跳转地址会被识别为不同的分支,这导致了 crashes 的冗余.经过人工分析后,我们发现这些 crashes 实
际源于两处可疑漏洞,并分别向对应的公司进行提交,其中一个未知的 DXE 栈溢出漏洞被 SuperMicro 公司确
认,该漏洞对应的漏洞编号为 CNVD-2023-58048, CVE-2023-34853.

5

总结与展望
本文提出了一种启发式的 UEFI 逆向分析方法,旨在还原 UEFI 固件中的基础服务信息和函数签名信息,并

利用该方法设计了一个基于启发式逆向的模糊测试器原型系统 ReUEFuzzer,从而更有效地对 UEFI 进行第三方
安全测试.在由 EDK2 以及来自 4 个不同厂商的、解包为 525 个 EFI 文件的 UEFI 固件构成的测试数据集上,
测试逆向分析方法的有效性,结果发现:该方法可以成功地还原 UEFI 的符号信息,其中基础服务信息识别成功
率为 96.4%,函数签名信息识别成功率为 79.94%.并且进一步地使用能够被还原符号信息的函数进行对比实验,
结果表明:相比缺失符号信息的普通模糊测试器,ReUEFuzzer 能够测试更多的函数,并且有显著的覆盖率提升.
同时,本工作还发现了一个零日漏洞,漏洞编号为 CNVD-2023-58048, CVE-2023-34853.这些结果表明,本文提出
的方法和系统在 UEFI 漏洞检测方面具有有效性和实用性,可以为 UEFI 安全提供一定的保障.
下一阶段,我们计划针对需要人工构造引导模糊测试器的模型的部分进行优化.使用动态分析技术,例如符
号执行对待执行函数片段尝试求解,利用其收集的约束信息,自动推断生成模型约束,从而减少人工参与,节约
人工成本.并且进一步地考虑 UEFI 中各模块间的相互引用关系,从单一模块测试进行扩展,以达成跨模块测试
的效果,从而发现更为复杂的缺陷.
References:
[1]

UEFI - wikipedia. 2023. https://en.wikipedia.org/wiki/UEFI#.

[2]

BIOS. Wikipedia, 2023.

[3]

CVE - cve. 2023. https://cve.mitre.org/index.html.

[4]

Common vulnerability scoring system. Wikipedia, 2023.

[5]

Rootkit. Wikipedia, 2023.

[6]

Tianocore/edk2: edk ii. 2023. https://github.com/tianocore/edk2.

[7]

Supermicro data center server, blade, data storage, ai system. 2023. https://www.supermicro.com/en/home.

[9]

Intel | data center solutions, iot, and pc innovation. 2023. https://www.intel.com/content/www/us/en/homepage.html.

[10]

Yang Z, Viktorov Y, Yang J, Yao J, Zimmer V. UEFI firmware fuzzing with simics virtual platform. 2020 57th ACM/IEEE
Design Automation Conference (DAC). San Francisco, CA, USA: IEEE, 2020: 1–6.

[11]

Simics. Wikipedia, 2023.

[12]

Yin J, Li M, Wu W, Sun D, Zhou J, Huo W, Xue J. Finding smm privilege-escalation vulnerabilities in uefi firmware with
protocol-centric static analysis. 2022 IEEE Symposium on Security and Privacy (SP). San Francisco, CA, USA: IEEE, 2022:
1623–1637.

[13]

Manes VJM, Han H, Han C, Cha SK, Egele M, Schwartz EJ, Woo M. The art, science, and engineering of fuzzing: a survey. arXiv,

[14]

Rawat S, Jain V, Kumar A, Cojocar L, Giuffrida C, Bos H. VUzzer: application-aware evolutionary fuzzing. Proceedings 2017

[15]

Pham V-T, Böhme M, Santosa AE, Căciulescu AR, Roychoudhury A. Smart greybox fuzzing. arXiv, 2018.

[16]

Nccgroup/triforcelinuxsyscallfuzzer:

2019.
Network and Distributed System Security Symposium. San Diego, CA: Internet Society, 2017.
a

linux

system

call

fuzzer

using

llvm

17.0.0git

triforceafl.

2023.

documentation.

2023.

https://github.com/nccgroup/TriforceLinuxSyscallFuzzer.
[17]

LibFuzzer

–

a

library

for

coverage-guided

fuzz

testing.

—

https://llvm.org/docs/LibFuzzer.html.
[18]

QEMU. 2023. https://www.qemu.org/.

[19]

Chen DD, Egele M, Woo M, Brumley D. Towards automated dynamic analysis for linux-based embedded firmware. Proceedings
2016 Network and Distributed System Security Symposium. San Diego, CA: Internet Society, 2016.

[20]

Kim M, Kim D, Kim E, Kim S, Jang Y, Kim Y. FirmAE: towards large-scale emulation of iot firmware for dynamic analysis.
Annual Computer Security Applications Conference. Austin USA: ACM, 2020: 733–745.

[21]

Zheng Y, Davanian A, Yin H, Song C, Zhu H, Sun L. FIRM-afl: high-throughput greybox fuzzing of iot firmware via augmented
process emulation.

[22]

Features | qemu stm32. 2023. http://beckus.github.io/qemu_stm32/.

[23]

Zaddach J, Bruno L, Francillon A, Balzarotti D. Avatar: a framework to support dynamic security analysis of embedded systems’
firmwares. Proceedings 2014 Network and Distributed System Security Symposium. San Diego, CA: Internet Society, 2014.

[24]

Kammerstetter M, Platzer C, Kastner W. Prospect: peripheral proxying supported embedded code testing. Proceedings of the 9th
ACM symposium on Information, computer and communications security. Kyoto Japan: ACM, 2014: 329–340.

[25]

Kammerstetter M, Burian D, Kastner W. Embedded security testing with peripheral device caching and runtime program state
approximation. 2016.

[26]

Unified extensible firmware interface (uefi) specification, release 2.10.

[27]

Assertion (software development). Wikipedia, 2023.

[28]

Haq IU, Caballero J. A survey of binary code similarity. arXiv, 2019.

[29]

Dullien T, Bochum R-U, Rolles R. Graph-based comparison of executable objects.

[30]

David Y, Partush N, Yahav E. Statistical similarity of binaries. Proceedings of the 37th ACM SIGPLAN Conference on
Programming Language Design and Implementation. Santa Barbara CA USA: ACM, 2016: 266–280.

[31]

Chandramohan M, Xue Y, Xu Z, Liu Y, Cho CY, Tan HBK. BinGo: cross-architecture cross-os binary search. Proceedings of the
2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering. Seattle WA USA: ACM, 2016:
678–689.

[32]

Hex rays - state-of-the-art binary code analysis solutions. 2023. https://hex-rays.com/ida-pro/.

[33]

Lenovo official us site | laptops, pcs, tablets & data center | lenovo us. 2023. https://www.lenovo.com/us/en/.

[34]

LongSoft/uefitool: uefi firmware image viewer and editor. 2023. https://github.com/LongSoft/UEFITool.

[35]

Idapython/src: idapython project for hex-ray’s ida pro. 2023. https://github.com/idapython/src.

[36]

Zynamics.com - software. 2023. https://www.zynamics.com/software.html.

[37]

Qilingframework/qiling: a true instrumentable binary emulation framework. 2023. https://github.com/qilingframework/qiling.

[38]

AFLplusplus/aflplusplus: the fuzzer afl++ is afl with community patches, qemu 5.1 upgrade, collision-free coverage, enhanced
laf-intel

&

redqueen,

aflfast++

power

schedules,

mopt

mutators,

unicorn_mode,

and

a

lot

more!

2023.

https://github.com/AFLplusplus/AFLplusplus.
[39]

Unicorn-engine/unicorn: unicorn cpu emulator framework (arm, aarch64, m68k, mips, sparc, powerpc, riscv, s390x, tricore, x86).
2023. https://github.com/unicorn-engine/unicorn.

[40]

Zalewski M. American fuzzy lop. 2023. https://lcamtuf.coredump.cx/afl/.

附中文参考文献:
[8]

国家信息安全漏洞共享平台. 2023. https://www.cnvd.org.cn/.


软件学报 ISSN 1000-9825, CODEN RUXUEW
Journal of Software,2024,35(3):1515−1533 [doi: 10.13328/j.cnki.jos.006826]
©中国科学院软件研究所版权所有.

E-mail: jos@iscas.ac.cn
http://www.jos.org.cn
Tel: +86-10-62562563

*

WiFense: 从衍射模型到边界监控
刘兆鹏 1,2, 李晟洁 1,2, 张 越 1,2, 曾有为 1,2, 张大庆 1,2
1

(北京大学 信息科学技术学院 , 北京 100871)

2

(高可信软件技术教育部重点实验室 (北京大学), 北京 100871)

通信作者: 张大庆, E-mail: dqzhang@sei.pku.edu.cn

摘

要: 近年来, 随着普适计算概念的深入人心, 智能感知技术已成为研究者们关注的焦点, 且基于 WiFi 的非接触

式感知因其优秀的普适性、低廉的部署成本以及良好的用户体验越来越受到学术界和工业界的青睐. 典型的
WiFi 非接触式感知工作有手势识别、呼吸检测、入侵检测、行为识别等, 这些工作若实际部署, 需首先避免其他
无关区域中无关行为的干扰, 因此需要判断目标是否进入到特定的感知区域中. 这意味着系统应具备精准判断目
标在界线哪一侧的能力, 然而现有工作未能找到一个可以对某个自由设定的边界进行精确监控的方法, 这阻碍了
WiFi 感知应用的实际落地. 基于这一关键问题, 从电磁波衍射的物理本质出发, 结合菲涅尔衍射模型 (Fresnel
diffraction model), 找到一种目标穿越 link (收发设备天线的连线) 时的信号特征 (Rayleigh distribution in Fresnel
diffraction model, RFD), 并揭示该信号特征与人体活动之间的数学关系; 之后以 link 作为边界, 结合天线间距带来
的波形时延以及 AGC (automatic gain control) 在 link 被遮挡时的特征, 通过越线检测实现对边界的监控. 在此基础
上, 还实现了两个实际应用, 即入侵检测系统和居家状态监测系统, 前者的精确率超过 89%、召回率超过 91%, 后
者的准确率超过 89%. 在验证所提边界监控算法的可用性和鲁棒性的同时, 也展示了所提方法与其他 WiFi 感知技
术相结合的巨大潜力, 为 WiFi 感知技术的实际部署提供了思考方向.
关键词: 非接触式感知; WiFi; 信道状态信息 (CSI); link 穿越检测; 边界监控
中图法分类号: TP393
中文引用格式: 刘兆鹏, 李晟洁, 张越, 曾有为, 张大庆. WiFense: 从衍射模型到边界监控. 软件学报, 2024, 35(3): 1515–1533. http://
www.jos.org.cn/1000-9825/6826.htm
英文引用格式: Liu ZP, Li SJ, Zhang Y, Zeng YW, Zhang DQ. WiFense: From Diffraction Model to Boundary Monitoring. Ruan Jian
Xue Bao/Journal of Software, 2024, 35(3): 1515–1533 (in Chinese). http://www.jos.org.cn/1000-9825/6826.htm

WiFense: From Diffraction Model to Boundary Monitoring
LIU Zhao-Peng1,2, LI Sheng-Jie1,2, ZHANG Yue1,2, ZENG You-Wei1,2, ZHANG Da-Qing1,2
1

(School of Electronics Engineering and Computer Science, Peking University, Beijing 100871, China)

2

(Key Lab of High Confidence Software Technologies (Peking University), Ministry of Education, Beijing 100871, China)

Abstract: Recently, with the popularity of ubiquitous computing, intelligent sensing technology has become the focus of researchers, and
non-contact sensing based on WiFi is more and more popular in academia and industry because of its excellent generality, low deployment
cost, and great user experience. The typical non-contact sensing work based on WiFi includes gesture recognition, breath detection,
intrusion detection, behavior recognition, etc. For real-life deployment of these works, one of the major challenges is to avoid the
interference of irrelevant behaviors in other irrelevant areas, so it is necessary to judge whether the target is in a specific sensing area or
not, which means that the system should be able to determine exactly which side of the boundary line the target is on. However, the
existing work cannot find a way to accurately monitor a freely set boundary, which hinders the actual implementation of WiFi-based

*

基金项目: 2020 年度中日韩 A3 前瞻计划 (62061146001); 北大-英特尔合作项目
收稿时间: 2021-09-24; 修改时间: 2022-05-26; 采用时间: 2022-11-06; jos 在线出版时间: 2023-05-17
CNKI 网络首发时间: 2023-05-19

软件学报 2024 年第 35 卷第 3 期

1516

sensing applications. In order to solve this problem, based on the physical essence of electromagnetic wave diffraction and the Fresnel
diffraction model, this study finds a signal feature, namely Rayleigh distribution in Fresnel diffraction model (RFD), when the target passes
through the link (the line between the WiFi receiver and transmitter antennas) and reveals the mathematical relationship between the signal
feature and human activity. Then, the study realizes a boundary monitoring algorithm through line crossing detection by using the link as
the boundary and considering the waveform delay caused by antenna spacing and the features of automatic gain control (AGC) when the
link is blocked. On this basis, the study also implements two practical applications, that is, intrusion detection system and home state
detection system. The intrusion detection system achieves a precision of more than 89% and a recall rate of more than 91%, while the
home state detection system achieves an accuracy of more than 89%. While verifying the availability and robustness of the boundary
monitoring algorithm, the study also shows the great potential of combining the proposed method with other WiFi-based sensing
technologies and provides a direction for the actual deployment of WiFi-based sensing technologies.
Key words: non-contact sensing; WiFi; channel state information (CSI); crossing link detection; boundary monitoring

近年来, 随着普适计算概念的兴起和智能感知技术的发展, 以“人为中心的计算”理念使得计算机技术在智能
家居、智能看护、室内安防等场景中的应用受到了广泛的关注. 在这些应用场景中, 人们希望通过一定的方式, 获
取目标的活动状态, 进而提供更具体、更有针对性的服务. 例如在智能养老系统中, 可以通过实时监测老年人的居
家活动状态来评估其健康状况以及防止意外发生; 而在智能家居系统中, 可以根据目标所在位置而启用一些位置
相关的服务, 如自动开光灯等. 这些系统的典型技术手段一般是基于接触式的可穿戴传感器, 如要求使用者穿戴相
应的加速度传感器、GPS 定位装置等, 从而实现对其日常活动和位置的感知. 然而这种接触式的感知方式要求使
用者时刻携带相应的传感器, 且需要定期充电和维护, 给日常生活带来了负担, 因此非接触式感知应运而生. 所谓
非接触式感知, 即指在目标不接触任何传感器的情况下实现对目标的感知. 与接触式感知方式相比, 非接触式感知
具有便捷、舒适、无需目标配合的优点, 与“以人为中心的计算”理念更为契合, 也更容易被使用者接受. 典型的非
接触式感知手段主要包括基于雷达信号、基于 WiFi 信号、基于声波和基于红外的感知等. 但雷达设备, 如
FMCW、UWB 等的成本较高、且需要在环境中部署专门的设备; 而声波由于衰减较快的问题, 可感知范围较小,
无法适应实际应用场景; 基于红外线的感知 [1,2]则易受环境温度和光照的影响. 相比之下, 随着 WiFi 技术的发展,
WiFi 信号作为通信媒介已经无处不在, 基于 WiFi 的非接触感知因可复用现有的 WiFi 设备而成本低廉, 而 WiFi
信号较大的传播范围也使基于 WiFi 的感知具有感知范围较广的优势, 同时 WiFi 信号丰富的感知特征也使其具有
很强的感知潜力. 由于这些优点, 基于 WiFi 的非接触感知逐渐受到了研究者的广泛关注, 基于 WiFi 信号的感知系
统也大量涌现出来.
基于 WiFi 的非接触感知的基本原理是利用目标对环境中 WiFi 信号的影响来推测其所处的状态或当前从事
的行为. 如图 1 所示, WiFi 接收端接收到的信号实际上是多条传播路径的叠加信号 [3], 而当人活动时, 其引起的衍
射、反射等信号会发生变化, 从而使得接收端接收到的信号随之发生变化. 通过寻找人的活动与信号变化之间的
规律, 即可从接收信号反推出人的活动状态和行为.
天花板反射
天花板
人体反射
人体衍射
直接路径
发射端
地面

家具反射

图1

地面反射

接收端

人对信号传播的影响

典型的 WiFi 非接触式感知工作有手势识别、呼吸检测、定位追踪、入侵检测、行为识别等 [4], 这些工作若
要实际部署, 需首先避免其他无关区域中无关行为的干扰, 这就要求判断目标是否进入到特定的感知区域中. 例
如, 目标在电视附近时应开启手势识别应用来控制电视、目标躺在床上应进行呼吸检测或睡眠监测等, 而当目标
在其他区域时, 这些功能不应开启, 否则可能产生无意义的结果; 再比如入侵检测, 则需要知道目标已进入某个特

刘兆鹏 等: WiFense: 从衍射模型到边界监控

1517

定区域, 才能及时地给出警报; 而对于行为识别, 在识别睡觉、吃饭等语义层次较高的行为时, 除了识别目标当前
的动作, 往往还需要获取目标所处的区域位置这一重要的上下文信息, 例如人躺在地板上 (摔倒) 和人躺在床上
(休息) 显然对应的行为不同. 因此, 不管是手势识别、呼吸监测、入侵监测等动作识别应用还是高层次的行为识
别应用, 确定出感知目标的区域位置信息都尤为重要.
针对这样的需求, 最直接的解决方法是对目标进行定位或追踪, 使系统实时掌握目标的具体位置, 从而得到其
区域信息. 但不幸的是, 诸如 LiFS、IndoTrack 等工作的定位误差可达 1–2 m, 这种水平的精度是无法满足需求的,
比如当目标站在卧室外时, 很有可能因为误差导致系统判断其在卧室内, 得到错误的位置信息. 另一方面, 一些工
作采取确定边界的思路来解决此问题, 如 Wiborder 通过对区域界线两侧信号特征的区分, 实现了准确的越线检测,
得到了目标的区域信息, 但由于其需要借助墙壁, 在部署上有一定的局限性, 不能完全按照应用需要来确定区域边
界. 与此类似, 一些研究者想到直接使用收发设备天线连线 (下文称 link, 如图 2 所示) 作为区域边界, 并做了一些
探索, 例如 WiVit 提出当目标穿越 link 时多普勒速度会出现“负-零-正”的变化规律, 并以此判断是否有目标穿越边
界到达另一区域, 但这种规律并不是目标穿越 link 所特有的, 有很多情况与之混淆; 有研究者提出当目标穿越
link 时, RSSI 会出现大幅下降 [5], 然而这种特征也不是唯一的, 且每次穿越点、穿越角度的差异又导致特征具有多
样性, 因此也难以实现越线检测, 无法给我们提供准确的区域位置信息.
Link
TX

RX

图2

何为“link”

考虑目标在 link 上的情况, 根据电磁波的传播规律, 此时目标引入了衍射效应, 用传统的反射模型对信号进行
建模将不再合适. 对于只存在反射的情况, 菲涅尔区模型 [6]很好地解释了目标活动与信号波动之间的关系, 然而当
衍射存在、特别是衍射信号占主导时, 需要引入新的手段对信号进行刻画. 基于这样的思考, 在继承以 link 为边界
的思路的同时, 不同于已有工作, 本文从衍射模型出发, 建立了人穿越 link 时造成的衍射效应和 WiFi 信号之间的
理论关系, 并提取了信号特征——RFD (Rayleigh distribution in Fresnel diffraction model), 它刻画了目标在人遮挡
link 时衍射效应下的动态信号能量. 在目标穿越 link 时, RFD 会呈现出稳定可复现的“M”型波形, 通过在信号上对
目标穿越 link 的过程进行建模, 我们揭示了该波形的形成原因, 并通过检测该波形的出现实现了对目标的越线检
测. 另外, 由于目标处于 link 附近时才会产生较强的衍射效应, 因此为了保证衍射模型的适用性并增加系统的鲁棒
性, 我们使用了网卡中的自动增益控制 (automatic gain control, AGC) 参数来对目标是否遮挡 link 进行判断. AGC
的值取决于接收信号的能量大小, 即当 AGC 显著上升时, 说明 link 被目标遮挡, 此时目标引起的衍射效应占主导,
从而可以进一步利用衍射特征 RFD 来进行越线检测.
本文第 1 节回顾相关工作. 第 2 节阐述菲涅尔模型下目标穿越 link 的特征. 第 3 节具体介绍基于 link 穿越检
测的边界监控方法. 第 4 节将实现两个应用, 并评估系统效果. 第 5 节总结全文内容.

1 相关工作
近几年中, 研究人员提出了各种各样的非接触式感知技术, 例如现在流行的基于红外线的感知技术 [1,2]、基于
雷达的感知技术 [7−12]、基于声波的感知技术 [13−15]等. 然而, 红外线的感知易受环境温度和光照的影响; 基于雷达的
感知技术利用目标活动时在雷达信号上产生的多普勒效应等实现对目标的感知, 但其使用了专用设备, 部署难、
成本高, 且较大的信号功率有可能干扰其他通讯系统的正常使用; 基于声波的感知技术, 通过分析声波的变化特征
来判断目标的活动情况, 但声波信号的传播衰减严重, 感知范围往往较小, 无法覆盖正常的室内家居环境. 因为现
有技术的不足, 研究人员开始尝试寻求新的、低成本、范围广的技术来实现对目标的非接触式感知.
相比于其他技术, 作为通信媒介的 WiFi 信号已经遍布在我们生活中的每个角落, 因此, 复用生活中无处不在
的 WiFi 信号进行非接触式感知引起了研究者的广泛关注. 接受信号强度 RSS (received signal strength) 及信道状
态信息 (channel state information, CSI) 是最主要的两种可用于感知的基本信号信息, 后者在正交频分复用

软件学报 2024 年第 35 卷第 3 期

1518

(orthogonal frequency division multiplexing, OFDM) 子载波的层次上揭示了比前者更加细粒度的多径传播特性, 同
时结合 WiFi 信号的 MIMO (multiple input and multiple output) 特性, 使更精细的 WiFi 感知成为可能.
典型的 WiFi 非接触式感知工作有手势识别 [16−18]、呼吸检测 [19−23]、定位追踪 [24−30]、入侵检测 [31,32]、行为识
别 [33−38]等. 这些工作一部分是需要目标位置作为先验信息的, 比如手势识别和呼吸检测, 需要知道人已进入检测区
域, 同时也需要排除所关注区域之外的活动带来的干扰; 而另一部分, 如入侵检测、行为识别等, 目标位置本身就
是一个重要信息, 准确的位置判断是相关功能可以运行的基础. 因此, 检测目标是否进入某特定区域是一个重要的
研究课题. 具体来讲, 就是对区域边界进行监控, 当人在边界内外时给出准确的判断.
而在诸多基于 WiFi 信号的感知工作中, 与越线检测边界监护相关的主要有两类, 第 1 类是目标的定位追踪技
术, 如 LiFS[28]、IndoTrack[26]等; 理想情况下, 如果能够获知目标在空间中的准确定位, 那么进行边界监控将十分简
单, 然而这类工作的定位误差可以达 1–2 m, 这对于精确的边界监控是不能接受的, 因此依赖当前的定位方法无法
准确地判断目标是否跨越某个界线. 第 2 类工作则是确定感知边界的技术, 这种想法更为直接, 具有代表性的方案
有 3 种. 第 1 种是基于 RSSI 的方法, 当目标穿越 link 时 RSSI 的值会出现明显下降 [5], 然而这种特征并不只在目标
穿越 link 时出现, 并且其波动变化规律也不稳定, 难以作为一个稳定有效的越线检测特征; 第 2 种是 WiVit[38]提到
的将多普勒速度的“负-零-正”的过程作为穿越 link 的判断依据, 但在很多非穿越 link 的情况下也会产生相同的多
普勒速度特征, 例如目标先接近 link 再远离 link, 造成大量的混淆; 第 3 种则是 Wiborder[32], 该工作借助墙壁实现
了精准的越线检测技术, 但该方法确定边界时依赖于空间中墙壁的结构, 并不能按照应用的需要自由的确定感知
边界.
上述工作虽然都在基于 WiFi 的边界监控问题上提供了一些思考角度, 但都没有找到一种普遍适用的边界监
控特征, 无法为各类感知应用确定出精确、任意的感知边界. 与他们相比, 本文通过分析人在穿越 link 时的信号特
征, 将其与菲涅尔衍射模型 [21,39]相联系, 提取出了 RFD 这一稳定有效的指标, 从模型的角度揭示了目标穿越 link
和 CSI 变化之间的理论关联, 实现了准确的边界确定, 进而达到边界监控的目的.

2 从菲涅尔衍射模型到边界监控特征 RFD
为了利用 WiFi 信号来检测目标穿越 link 的事件, 我们从 CSI 中提取了有效的边界监控指标 RFD, 通过识别
RFD 在目标穿越 link 时呈现的“M”型模式, 可以实现精准的边界穿越检测. 不仅如此, 通过与菲涅尔衍射模型相结
合, 我们揭示了 RFD“M”型模式背后的物理意义, 建立了 RFD 与感知目标活动之间的数学关系. 本节将首先简单
介绍一下 CSI, 之后将具体介绍如何从 CSI 提取 RFD, 最后将利用菲涅尔衍射模式揭示出 RFD 与感知目标活动之
间的关系.
2.1 CSI 简介
CSI 的全称为信道状态信息, 是 WiFi 通信中被用来描述通信信道质量的信息. 它反映了将信号从发射端到接
收端经历的变化. 在最简单的只有一条传播路径的情况下, CSI 的数学表达如公式 (1) 所示. 其中 A 代表振幅的衰
减, 代表信号沿该路径从发射端传播到接收端所用的时间, 也称传输时延; f 为信号传输的载波频率, e−2π f τ(t0 +t) 代
表相应时刻由于传输时延造成的相位偏移.

(
)
H f, t0 + t = Ae− j2π f τ(t0 +t)

(1)

而室内环境往往存在多径传播, 即接收器接收到的 WiFi 信号是来自所有直射和反射路径的信号的叠加. 因
此, CSI 可以表示为公式 (2). 其中, L 为传播路径的个数, αl (t0 + t) 和 τl (t0 + t) 分别为第 l 条传播路径在 (t0 + t) 时刻
的振幅衰减和传播时延.
H ( f, t0 + t) =

L
∑

αl (t0 + t)e− j2π f τl (t0 +t)

(2)

l=1

在这些叠加的传播信号中, 我们可以进一步将他们分为静态信号和动态信号, 其中静态信号是由环境中的静
态物体反射产生, 例如天花板、地面、家具等, 这部分信号不随时间发生变化. 动态信号则是由环境中活动的人体

刘兆鹏 等: WiFense: 从衍射模型到边界监控

1519

反射的信号, 它们会随人体目标的运动而发生变化 [3]. 基于此, CSI 可以进一步表达为两部分的叠加, 如公式 (3), 其
中 S 是静态路径的集合, D 是动态路经的集合.
S
D
∑
∑
H ( f, t0 + t) =
α s e− j2π f τs +
αd (t0 + t)e− j2π f τd (t0 +t)
s=1

(3)

d=1

由于静态路径并不随时间变化, 可以进一步将静态路径合并为一个复数常量, 对公式 (3) 进行简化后得到公式 (4).
其中, αd (t0 + t) 和 ϕd (t0 + t) 分别代表 t0 + t 时刻第 d 条动态信号的振幅衰减和相位偏移. 当人在环境中移动时, 这两个量
会随着人的移动发生相应改变, 从而改变 CSI 的整体值. 也就是说, CSI 的变化反映了室内多径环境中人的活动情况.
∑D
H (t0 + t) = As e jϕs +
αd (t0 + t) e jϕd (t0 +t) )
(4)
d

以上介绍的是理想感知场景中的 CSI, 但在商用 WiFi 设备上, 由于发射端和接收端时钟的不同步, 为每个
CSI 采样带来了一个时变的相位误差 θoﬀset , 因此我们实际获得的 CSI 为公式 (5).
∑D
H (t0 + t) = e jθoﬀset (As e jϕs +
αd (t0 + t) e jϕd (t0 +t) )

(5)

d

2.2 RFD 衍射特征的提取与解释
2.2.1

RFD 的提取

由于随机相位误差 θoﬀset 会使 CSI 的相位出现显著的偏差, 导致 CSI 的变化与感知目标的活动不再具有清晰
的映射关系. 因此, 需要先消除相位误差的影响, 拿到可以反映人活动的 CSI. 由于同一 WiFi 网卡的不同天线 (例
如 Intel 5300 WiFi 网卡有 3 根天线) 共用同一个射频振荡器, 所以每个时刻不同天线的相位误差是相同的. 因此,
为了相除这一误差, 我们将两根接收天线得到的 CSI 数据进行了共轭相乘 [26]. 共轭相乘后的 CSI 可以看作由 4 部
分组成, 如公式 (6), 第 1 部分为两根天线静态 CSI 的共轭乘积, 第 2 个部分为第 1 根天线的静态 CSI 和第 2 根天
线的动态 CSI 的共轭乘积, 第 3 个部分为第 1 根天线的动态 CSI 和第 2 根天线的静态 CSI 的共轭乘积, 第 4 部分
为两根天线动态 CSI 的共轭乘积.



D1
D2
∑
∑

 

∗
jϕ s1
jϕd1 (t0 +t) 
− jϕ s2
− jϕd2 (t0 +t) 




 As2 e
H1 ( f, t0 + t) ∗ H2 ( f, t0 + t) = A s1 e +
αd1 (t0 + t) e
+
αd2 (t0 + t) e

d1 =1

d2 =1

D2
∑

D1
∑
= As1 As2 e j(ϕs1 −ϕs2 ) + As1
αd2 (t0 + t) e− j(ϕd2 (t0 +t)+ϕs1 ) + As2
αd1 (t0 + t) e j(ϕd1 (t0 +t)−ϕs2 )
{z
}
|
d =1
d =1
1
| 2
{z
} | 1
{z
}
2

+

D1
∑
d1 =1

|

αd1 (t0 + t) e jϕd1 (t0 +t)

D2
∑

3

αd2 (t0 + t) e jϕd2 (t0 +t)

d2 =1

{z

(6)

}

4

由于第 4 部分的值为两根天线动态路径部分的相乘而得, 而动态路径的能量相比于静态路径的能量要小很
多, 所以这一部分相对于前 3 个部分的大小可以忽略不计, 因此可进一步将共轭乘积 CSI 简化为公式 (7).
D
D
∑
∑
H1 ( f, t0 + t) ∗ H2∗ ( f, t0 + t) = As1 As2 e j(ϕs1 −ϕs2 ) + As1
αd (t0 ) e− j(ϕd (t0 +t)+ϕs1 ) + As2
αd (t0 ) e j(ϕd (t0 +t)−ϕs2 )
d=1

(7)

d=1

假设, 在一个很短的时间窗口中, 共有 M 个 CSI 共轭乘积的采样, 可以将其表示为一个向量如下: S (t0 ) =
[H 1 (t0 ) ∗H2∗ (t0 ) , H1 (t0 + ∆t) ∗H2∗ (t0 + ∆t) , . . . , H1 (t0 + M∆t) ∗H2∗ (t0 + M∆t) ] . 其中, [0, ∆t, . . . , M∆t] 是相对于时 t0 时刻的

CSI 采样间隔, ∆t 为相邻采样的间隔时间. 之后, 通过减去向量 S (t0 ) 的均值, 我们可以去除其常量部分并得到一个
零均值的向量 x (t0 + k∆t) :
x (t0 + k∆t) ∈ S (t0 ) − S (t0 )

继续经过一系列推导 [32], 可以证明 |x (t0 + k∆t)| 符合瑞利分布, 且其分布参数可被表示为:
√
∑D
σ (t0 ) = (A2s1 + A2s2 )
α2d (t0 + t)/2
d

(8)

(9)

软件学报 2024 年第 35 卷第 3 期

1520

根据公式 (9), 从信号角度来讲, σ (t0 ) 刻画了动态信号的能量. 基于这样的统计学特征, 结合一个时间窗口内
的 CSI 采样, 我们可以通过瑞利分布的最大似然估计计算出某时刻信号动态能量的值 [3,32], 如公式 (10).
v
t
M
1 ∑N ∑
2
x f (t0 + k∆t)
σ (t0 ) =
f
=1
2M × N
k=1

(10)

对于某次目标穿越 link 的事件, 我们将通过上述方法计算出的信号动态能量, 记为 RFD. 根据真实实验数据,
RFD 序列的波形如图 3 所示. 有趣的是, 在人的穿越过程中对应的 RFD 序列呈现“M”形, 而重复实验后发现, 这一
特征始终保持. 结合该特征的物理意义, 表明在目标穿越 link 的过程中, 动态信号的能量会经历先升高然后再降
低, 之后又会升高再降低的过程. 图 3 是多次实验中的一个例子, 其中横轴为时间轴 (s), 纵轴为瑞利分布参数, 可
见整体呈现出“M”的变化趋势.
350
300

时间 (s)

250
200
150
100
50
0

0

图3
2.2.2

1

2

3

4
5
6
瑞利分布参数

7

8

9

目标穿越 link 时的真实 RFD 序列

RFD 的解释

唯一稳定的“M”型波形是一种巧合吗？这样的疑问使我们开始思考现象背后的物理意义. 当目标穿越 link 时,
其处于衍射区, 大部分信号能量以衍射的方式传播 [21]. 根据文献 [21,39,40], 一个柱形目标对电磁波的衍射效应可
以分别对前后两个表面进行建模, 如图 4 所示.
hfront
TX

RX
hback

d1

图4

d2

圆柱目标在衍射区中

若 hfront 表示目标前表面到 link 的距离, hback 表示目标后表面到 link 的距离, d1 、 d2 分别为目标中心到信号
发射端和接收端的距离, λ 为电磁波的波长, 公式 (11) 即为整体衍射效应, 此时可以理解为信号的理论 CSI. 其中,
√
√
2(d1 + d2 )
2(d1 + d2 )
, vfront = hfront
.
vback = hback
λd1 d2
λd1 d2
(
)
(
)
∫
∫
1 + j vback
− jπz2
1+ j ∞
− jπz2
F=
exp
dz +
exp
dz
(11)
2
2
2
2
−∞
vfront
而人体躯干可以近似为一个直径为 20–40 cm 的圆柱体, 因此物理学中菲涅尔衍射模型恰好可以来刻画人穿
越 link 过程的衍射效应. 基于菲涅尔衍射模型, 我们可以对目标穿越 link 这一过程进行模拟, 根据模型计算出相应
的理论 CSI 序列. 基于理论 CSI 序列, 我们可以根据第 2.2.1 节中 RFD 提取方式计算出目标穿越 link 过程中的理
论 RFD 序列. 以 d1 = d2 = 1 m 为例, 并以一个直径为 30 cm 的圆柱作为理论目标, 代入衍射模型计算 RFD 序列, 得
到如图 5 所示, 理论计算出的 RFD 序列确实呈现“M”形. 其中, 横轴为目标前表面与收发设备所在直线的相对位
置, 单位为 cm.

刘兆鹏 等: WiFense: 从衍射模型到边界监控

−100

图5

−50

1521

0

50

100

150

目标穿越 link 时的理论 RFD 序列与目标位置的对应关系图

由此说明, RFD 的“M”型波形并非一种巧合, 这实际上是电磁波衍射效应在信号特征上的展现. 可以看到,
RFD 序列的峰谷分别对应了目标靠近 link、遮挡 link、远离 link 的 3 个阶段, 从信号意义上来讲, 这一“M”形的过
程是符合直觉的: 首先, 当目标刚进入衍射区时, 即前表面到达左边第 1 根虚线处, 由于目标距离能量最集中的
link 区域很近, 引起的动态信号能量较大; 而随着目标逐渐遮挡了 link, 到达第 2 根虚线处, 一大部分能量被目标吸
收、阻挡或者逸散, 动态信号能量随之降低; 之后的过程与上述对称, 故而呈现出了“M”形的能量变化模式.
2.3 实验验证
2.3.1

RFD vs 常用指标

在之前的 WiFi 感知工作中, RSS、CSI 振幅、CSI 振幅商和 CSI 相位差 [41]是最常被利用的信号指标. 在这部
分, 我们将 RFD 与这些特征在目标穿越 link 时的表现进行对比. 具体来说, 为了验证指标在穿越 link 时呈现特征
的稳定性和可复现性, 我们将同时对比不同天线 (对) 在同次实验的指标特征, 以及同天线 (对) 在不同次实验的指
标特征, 如图 6、图 7 所示, 其中横轴为时间轴 (s), 纵轴是相应特征. 具体来讲, 如图 6(a) 是一次实验中不同天线
的 RSS 对比, 而图 7(a) 是两次实验中同一天线的 RSS 对比.
60

40

相应特征

相应特征

50
30
20
10
0

0

0

1

2

3

4

0

1

2

3

4

5

6

7

8

9

10

5
6
时间 (s)
(b) CSI 振幅

7

8

9

10

30

40

相应特征

相应特征

20
0

0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0

50
30
20
10
0

40

0

0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0
时间 (s)
(a) RSS

图6

20
10
0

不同天线 (对) 在同次实验的指标对比

软件学报 2024 年第 35 卷第 3 期

1522

相应特征

1.0
0.5
0

相应特征

4

0

1

2

3

4

5

6

7

8

9

2
0
−2
−4

10

20

4

15

2

相应特征

相应特征

1.5

10
5
0

1

2

3

4

5
6
7
时间 (s)
(c) CSI 振幅商

图6

8

9

1

2

3

0

1

2

3

4

5

6

7

8

4
5
时间 (s)
(d) CSI 相位差

6

7

8

0
−2
−4

0

0

10

不同天线 (对) 在同次实验的指标对比 (续)

50

30

45
25

40

20
相应特征

相应特征

35
30
25
20

15
10

15
10

5

5
0

5

0

10
15
时间轴 (s)

0

20

0

2

4

6

10

4

9

3

8

16

2
相应特征

7
相应特征

14

(b) CSI 振幅

(a) RSS

6
5
4
3

1
0
−1
−2

2

−3

1
0

8
10 12
时间轴 (s)

0

2

4

6

8
10 12
时间轴 (s)

14

16

18

−4

0

2

4

6

(c) CSI 振幅商

图7

8 10 12
时间轴 (s)

14

16

18

20

(d) CSI 相位差

同天线 (对) 在不同次实验的指标对比

如图 6 和图 7 可以看到, 对于 RSS、CSI 振幅、CSI 振幅商和 CSI 相位差, 同次实验中这些信号特征在不同
天线 (对) 上的表现是不一致的, 缺乏一个统一的可判别特征; 而在不同次实验中, 这些指标在同一天线 (对) 上
的波形也并不统一. 这表明, 上述指标特征不具有良好的稳定性和可复现性, 无法与目标穿越 link 这一事件建立
起一一映射的关系, 这使得他们难以被用来进行目标穿越 link 的检测. 究其原因, RSS、CSI 振幅、CSI 振幅商

刘兆鹏 等: WiFense: 从衍射模型到边界监控

1523

和 CSI 相位差信号都是在多径叠加信号之上 (动态信号和静态信号的混合) 提取出的信号, 所以这些信号的波
形变化不仅会受到动态路径的影响, 还会受到静态多径的影响. 目标在穿越 link 时, 静态路径会由于 LOS 被遮
挡而发生较大改变, 而人沿不同角度、不同位置穿越 link 时, 静态路径的变化都不相同, 所以导致了这些信号的
波形变化多种多样, 难以稳定. 与这些信号特征不同的是, RFD 刻画的是动态信号能量变化, 不会受到静态多径
的影响, 因此在人穿越 link 过程中会表现出稳定的“M”形, 也就是说 RFD 这一特征可以被用来有效检测目标穿
越 link 这一事件.
2.3.2

RFD 特征的实证研究

在实际应用中, 收发端之间的距离、目标穿越 link 的角度及位置等都会对信号产生不同的影响. 例如, 目标
从 link 中垂线上穿越, 和目标以各个角度穿越, 各种情况下的 RFD 特征是否一致? 由于在真实场景下, 我们无法
规定目标的行走路线, 因此这里, 我们将对多种不同情况下的 RFD 特征进行验证.
为了方便进一步理解, 我们将目标穿越路线与 link 所成角度定义为穿越角度, 将穿越路线与 link 的交点定义
为穿越点. 我们分别采集了实际室内场景下常见的不同 link 长度、不同穿越角度、不同穿越点及不同体型目标穿
越时的 CSI 数据, 验证 RFD 特征的普遍性. 我们只关注 RFD 序列的波形趋势特征, 而不关注其具体的绝对大小.
● 不同 link 长度. 我们选取了 1.5 m、3 m、5 m 和 10 m 这 4 种 link 长度进行实验, 得到 RFD 波形如图 8 所
示. 其中, 横轴为时间轴 (s), 纵轴为计算出的 RFD (数学意义为瑞利分布的参数, 无单位), 适用于下文所有波形图.
可以看到, 前 3 种情况下, RFD 保持了“M”波形, 但 10 m 的情况下, 波形不符合预期. 这可能是因为, 当 link 长度过
长时, 信号在空间中的传播更加分散, link 不再占据绝大部分信号能量, 也就是说当目标穿越 link 时衍射效应不再
占主导, 我们的理论将失效. 但幸运的是, 我们的使用场景往往是室内环境, 在正常大小的房间中, 1.5–5 m 的 link
350

300

300

250

250

计算出的 RFD

计算出的 RFD

长度已经满足需要.

200
150
100

150
100
50

50
0

200

2

0

4

6
时间 (s)

8

10

0

12

0

1

2

3

350

300

300

250

250
200
150
100

7

8

200
150
100
50

50
0

6

(b) 3 m

计算出的 RFD

计算出的 RFD

(a) 1.5 m

4
5
时间 (s)

0

1

2

3

4
5
时间 (s)

6

7

8

0

0

(c) 5 m

2

4

6
时间 (s)
(d) 10 m

图8

不同 link 长度

8

10

12

软件学报 2024 年第 35 卷第 3 期

1524

350

400

300

350

250

300

计算出的 RFD

计算出的 RFD

● 不同穿越角度. 我们选取了 90° (即垂直穿越)、60°、30°、15°这 4 种穿越角度进行实验, 得到 RFD 波形如
图 9 所示. 可以看到, 各个角度下, RFD 序列都呈现出“M”形. 从物理意义上理解, 无论穿越角度如何, 目标都经历
了进入衍射区、遮挡 link、离开衍射区等阶段, 对信号的影响本质上是一样的, 因此呈现出一致的波形. 而在角度
很小时, “M”的波谷变宽, 这是因为这种情况下人几乎平行于 link 穿了过去, 对 link 的遮挡时间变长, 不过这并不
影响序列的整体特征, 不会对判断造成影响.

200
150
100
50
0

250
200
150
100

1

0

2

3

4
5
时间 (s)

6

7

8

50

9

0

1

2

3

4
5
时间 (s)

450

400

400

350

350

300
250
200
150

200
150
100
50

1

2

3

4

9

250

50
0

8

300

100
0

7

(b) 60°

450

计算出的 RFD

计算出的 RFD

(a) 90°

6

5
6
时间 (s)

7

8

9

0

10

0

1

2

3

4

5
6
时间 (s)

(c) 30°

7

8

9

(d) 15°

图9

不同穿越角度

350

300

300

250

250

计算出的 RFD

计算出的 RFD

● 不同穿越点. 我们选取了 link 中点、四分之一等分点、四分之三等分点、靠近 TX 和靠近 RX 这 5 种情况
进行验证, 得到 RFD 波形如图 10 所示, 都符合“M”形. 与不同穿越角度类似, 不同的穿越点并不影响目标行进过
程中的 3 个阶段, 因此“M”波形都得到了保持.

200
150
100

150
100
50

50
0

200

0

1

2

3

4
5
时间 (s)

6

7

8

9

0

0

(a) 中点

1

2

3

4

5
6
时间 (s)

7

(b) 四分之一等分点

图 10

不同穿越点

8

9

10

刘兆鹏 等: WiFense: 从衍射模型到边界监控

1525

400

500

350

450
400
计算出的 RFD

计算出的 RFD

300
250
200
150
100

300
250
200
150
100

50
0

350

50
1

0

2

3

4
5
时间 (s)

6

7

0

8

0

1

2

3

4
5
6
时间 (s)

(c) 四分之三等分点

7

8

9

(d) 靠近 TX

500
450
计算出的 RFD

400
350
300
250
200
150
100
50
0

0

1

2

3

4

5
6
时间 (s)

7

8

9

10

(e) 靠近 RX

图 10

不同穿越点 (续)

● 不同体型的目标. 我们让两个不同体型的实验者分别进行实验, 得到结果如图 11 所示, RFD 皆呈现出“M”
型波形. 不同体型目标的躯体横截面积不同, 因此对信号的衍射、遮挡、吸收等效应略有不同, 但这并不影响其穿
越 link 时信号的整体变化趋势. 对于正常人来讲, 躯体的横截面积差异不会太大, 直径一般在 20–30 cm, 这种尺寸
300

350

250

300
计算出的 RFD

计算出的 RFD

的障碍物对信号带来的遮挡、衍射效应是明显的, 我们的模型依然适用.

200
150
100
50
0

250
200
150
100
50

0

1

2

3

4
5
6
时间 (s)
(a) 185 cm, 90 kg

7

图 11

8

0

0

1

2

3

4
5
时间 (s)
(b) 164 cm, 50 kg

6

7

8

不同体型的目标

观察上述波形图还可以发现, 在大多数情况下 “M”的第 1 个峰会低于第 2 个峰, 这是可能由于人的正面没有
背面平整, 相比于离开 link 时, 人靠近 link 时有更多的信号被人体吸收和损耗, 使得回到接收端的信号能量变小.
但这不影响“M”波形的整体形态, 不会对 RFD 特征的正常使用造成影响. 综上所述, 在各种常见情况下, 虽然

软件学报 2024 年第 35 卷第 3 期

1526

RFD 的波形略有差异, 但整体上仍符合“M”形, 表现出了较好的鲁棒性.
RFD 特征本质上是由信号的衍射效应引起的, 主要出现在第一菲涅尔区之内, 且之前大量的工作对这种衍射
现象进行了探索, 具体为菲涅尔衍射模型 (Fresnel diffraction model)[21,39]. 本文就是以此模型为基础, 考虑了 WiFi
信号的传播特性, 对物体在穿越 link 时的波动规律进行更加细粒度的建模和推导, 并通过真实的实验进行了验证,
揭示了在人穿越 WiFi link 时信号的波动规律与背后的物理原理. 借助这一特征, 我们可以实现准确的越线检测,
从而达到边界监控的目的. 在第 3 节, 将详述以此特征为主要手段的边界监控方法.

3 基于 link 穿越检测的边界监控方法
通过第 2 节的推导和验证, 我们得到了一个稳定的目标穿越 link 特征——RFD 序列的“M”型波形. 但需要注
意的是, 这样的特征本质上是电磁波的衍射效应造成的. 当目标在远离 link 的区域, 信号以反射为主, 衍射效应很
弱, 这时不可能发生目标穿越 link 的事件, 那 RFD 特征也就没有意义. 所以 RFD 可用于检测目标是否穿越 link 的
前提是需要判断出目标是否进入到衍射区域中.
3.1 目标遮挡 link 检测
目标遮挡 link, 是目标进入衍射区域的充分条件, 同时也是目标穿越 link 的必要条件. 如何判断 link 是否被目
标所遮挡呢? 从直观上来讲, 当目标遮挡了 link, 大量信号能量会被吸收或者逸散, 接收端收到的信号能量会相对
降低. 而在通信中, 为了保证这种情况下信号能量不至于因太低而无法解析其通信数据, 会在接收端硬件中加入
AGC (自动增益控制) 模块, 将信号能量调节至可接受的范围. 而当 AGC 参数越大时, 意味着信号衰减越强. 更进
一步, 当 AGC 参数突然升高时, 是否意味着 link 被遮挡？我们正是利用了 AGC 在人遮挡 link 时会发生骤升这一
特性实现了对人是否进入衍射区域的判定. 下面将结合实际数据验证 AGC 作为特征用于判定目标是否进入衍射
区域的有效性.
为了保证实验的严谨性, 我们设计了如下实验: 人在远离 link 的区域活动一会, 之后走近并穿过 link, 然后重
复这一过程回到起点, 得到数据如图 12 所示, 其中纵轴为 AGC 参数, 横轴为时间轴 (s). 可以看到, 人在远离 link
的区域时, AGC 几乎没有太大变动; 而当人遮挡 link 时, AGC 的值较人远离 link 时显著升高; 因此, 若人由远离
link 处走到 link 上, AGC 的值会显著升高, 反之则降低, 这在人不遮挡 link 时是不会发生的. 综上所述, AGC 的骤
升可以作为判断目标遮挡 link 的依据, 而这是目标进入衍射区域的充分条件, 同时也是目标穿越 link 的必要条件.
因此当其发生时, 系统可以进一步利用 RFD 特征判断目标是否穿越了 link.

AGC 参数

60

遮挡 link

50
Link 外活动

Link 外活动

Link 外活动

40
30

0

5

图 12

10
15
时间 (s)

20

25

目标活动与 AGC 的变化

3.2 目标穿越 link 检测
当我们判断出 link 已被目标遮挡时, 即目标从 link 的某一侧到达 link 附近时, 如图 13 所示, 接下来目标的行
为无外乎如图 13 的 3 种情况: (1) 目标停留在 link 上; (2) 目标离开 link, 返回之前来时所在的一侧; (3) 目标离开
link, 去往另一侧, 即所谓的穿越 link. 对于第 1 种情况, 实际上是很容易剔除的, 因为目标若停留在 link 上, AGC
的值会保持较高的水平. 对于后两种情况, 我们期望可以通过 RFD 特征进行区分, 这样就可以准确地检测出目标
穿越 link 这一事件. 然而不幸的是, 在实际数据中我们发现, 第 2 种情况和第 1 种情况的 RFD 波形相似度极高, 都
呈现出“M”形. 这也是容易理解的, 如果将目标行为分为遮挡 link 前、遮挡 link 和遮挡 link 后 3 部分的话, 情况 2、

刘兆鹏 等: WiFense: 从衍射模型到边界监控

1527

情况 3 的前两部分是完全一致的, 第 3 部分的区别仅在于目标离开的方向不同, 但这在信号衍射效应上并不具有
区分性, 因此两种情况在 RFD 特征上的表现是一致的. 故而仅用这一个特征, 难以准确检测目标穿越 link.

(a) 情况 1

(b) 情况 2

图 13

(c) 情况 3

3 种情况示意图

幸运的是, WiFi 设备往往都具有 3 根或更多天线, 且天线间有一定的间距, 这意味着, 利用不同天线对在同一
次穿越的“M”的不同时序关系, 将有可能对情况 2 和情况 3 进行进一步区分. 为了验证这一想法, 我们针对情况 2
和情况 3 进行了实验. 图 14 分别为情况 2 (图 14(a)) 和情况 3 (图 14(b)) 不同天线对的 RFD 变化情况. 可以看到,
若目标真正穿越 link 时, 不同天线对计算出的“M”具有一定的时序关系, 具体表现为两个“M”各自的 3 个锚点有一
致的先后顺序, 更靠近目标来时方向的天线对的“M”更靠前; 而若目标到达 link 后未穿越而返回, 锚点的先后顺序
不一致. 基于这种观察, 我们就成功区分了情况 2 和情况 3, 并且更进一步地, 通过“M”地先后顺序与天线的摆放顺
序, 真实的检测出穿越时间并识别出方向.
400

350

350

300
250

250

RFD 指标

RFD 指标

300

200
150

150
100

100

50

50
0

200

0

1

2

3

4

5
6
时间 (s)
(a) 情况 2

图 14

7

8

9

10

0

0

1

2

3

4

5
6
时间 (s)
(b) 情况 3

7

8

9

10

情况 2、情况 3 的不同天线对的 RFD 序列

综上, 我们可以首先利用 AGC 来判断目标是否遮挡 link; 之后, 通过对不同天线对的 RFD 序列时序关系进行
判断就可以检测越线行为的发生, 从而实现对特定边界的监控. 第 4 节, 我们将基于此方法实现两个实际应用, 以
展示本方法的可用性和有效性.

4 应

用

在本节, 我们将利用商用 WiFi 设备实现两种基于边界监控的应用, 以验证本文方法的可用性第 1 个应用是入
侵检测系统, 为本文边界监控方法的直接应用; 第 2 个则是以多个边界的监控为基础的目标状态检测系统, 可以实
时检测目标所在区域及当前活动情况.
4.1 入侵检测
在室内场景的感知工作中, 入侵检测一直是最受关注、使用最广的应用, 可以作为室内安防的一种解决方案. 无
论在居家环境还是办公室环境下, 入侵检测都有很强的需求. 具体来讲, 入侵检测即检测是否有进入到关注区域, 而
所谓关注区域, 可以是某个房间, 也可以是房间的某个部分. 例如在某个存有危险物品的房间之中, 堆放危险物品的
区域禁止无关人员进入, 这时就需要对区域边界进行监控, 一旦出现越线事件, 及时提示和警报; 再如对于在一个居

软件学报 2024 年第 35 卷第 3 期

1528

住有认知障碍老年人的家庭中, 当老人接近一些危险位置 (如窗边、灶台) 时给出警报. 本节将针对这一需求实现入
侵检测系统, 具体实验环境为一个房间, 房间的某一端为关键区域, 有人进入这部分区域时系统需要给出警报或提示.
4.1.1

系统设计

基于 RFD, 我们搭建了一个实时的入侵检测系统, 该系统利用两台装有 Intel 5300 网卡的 miniPC 作为发送和
接收设备, 其中发送设备配有一根天线, 接收设备配有 3 根天线, 且天线依次间距 20 cm 左右. 实验中, 收发设备工
作在 5 GHz 频段、20 MHz 带宽上, 采样率为 2 000 数据包/s. 根据第 3 节的边界监控方法, 对于入侵检测系统, 其
设计如图 15 所示.
CSI 流

是

获取 AGC, 判断
是否遮挡 link

计算瑞利分布,
判断是否穿越
link

是
越线警告

否
否

图 15
4.1.2

无穿越 link 发生

入侵检测系统流程图

系统效果

为了充分验证入侵检测的系统性能, 我们在两个典型的多径环境中布置了系统: 一个空房间和一个办公室. 具
体房间分别如图 16 和图 17 所示, 其中空房间是多径环境较简单的情况, 而办公室是多径环境较丰富的情况, 同时
也更贴近实际场景. 对于每种环境, 3 名实验者 (2 男 1 女) 分别进行了 25 次越线行为和 25 次非越线行为, 最终共
形成 300 组数据. 总体来说, 系统的精确率 (precision) 为 89.0%, 召回率 (recall) 为 91.3%. 下面, 将从不同角度具体
评估各因素给系统效果带来的影响.

图 16

空房间

图 17

办公室

● 不同人的影响. 由于不同人的体态、行走方式等存在差异, 我们比较了不同体型的 3 名实验者 (2 男 1 女)
对系统效果的影响, 如图 18 所示, 结果表明本系统对于不同人的检测效果具有较好的一致性.
● 不同行走速度的影响. 由于人的行走速度可快可慢, 为了充分证明系统的鲁棒性, 我们还考虑了不同行走速
度的影响. 根据文献 [42,43], 一般人的行走速度在 1–1.5 m/s 之间, 因此我们定义 3 种速度: 慢速 (0.8–1 m/s)、中速
(1–1.5 m/s) 和快速 (1.5–2 m/s). 我们让一名实验者以 3 种速度进行了多次越线行为和非越线行为, 检测结果如图 19
所示. 在慢速和中速下, 系统效果较好且稳定; 而在快速时, 系统的精确率依然较好, 但召回率下降至 84%. 这是因
为当人走的较快时, 天线间距造成的波形时延会变短, 从而使算法的判断难度增加, 漏判了一些越界事件, 导致召
回率变差. 但结合实际情况, 人在室内的走动速度一般不会很快, 类似漏判发生的可能性较小. 因此, 本系统在正常
速度下的效果是较好的.

刘兆鹏 等: WiFense: 从衍射模型到边界监控

90.0% 90.0%

88.5%

召回率
92.0%

88.5%

92.0%

指标

指标

精确率
1.0
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0.0

1529

实验者 1

图 18

实验者 2

1.0
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0.0

92.0% 92.0%

慢速

实验者 3

图 19

不同人的影响

精确率 召回率
91.7% 88.0%
91.3%
84.0%

中速

快速

不同行走速度的影响

● 不同环境的影响. 我们对比了系统在 2 种环境下的测试结果, 如图 20 所示, 发现系统效果差异较小. 这表明,

指标

本文提出的基于衍射特征的边界监控算法不依赖于环境, 具有较好的可迁移性.
1.0
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0.0

89.6%

精确率
92.0%

环境 1

图 20

召回率
88.3% 90.7%

环境 2

不同环境的影响

综上所述, 我们设计的基于边界监控的入侵检测系统具有较好的效果, 且对于不同人、不同速度及不同环境
具有良好的普适性.
4.2 居家状态监测
在室内环境中, 人的行为往往与家具的种类十分相关, 如在沙发处往往是在休息, 在书桌处往往是学习工作,
因此一个房间可由家具的摆放分为不同的功能区域. 例如一个配有书桌和沙发的办公室, 可以分为工作区和休息
区; 而一个既有沙发电视也有餐桌的客厅, 可以被分为休闲区和就餐区. 如果可以获知目标在哪个功能区, 再结合
目标的动静状态, 即可对目标的行为有精细的判断, 我们称目标的 (位置, 动/静) 二元组为目标的居家状态信息. 然
而已有工作往往定位误差较大, 或者只能实现对目标的房间级定位, 难以准确获知目标在哪个具体的功能区. 而对
于我们的边界监控技术, 如果能够做到同时监控多个功能区的边界, 就可以实现对目标所在区域的判断. 这样, 将
对识别目标的行为状态有很大的帮助.
根据这一思路, 我们采用一发多收的部署方案, 在需要的边界上各设置一条 link, 同时基于本文中的算法, 即
可实现对目标所在功能区的判断. 进一步, 辅以已有的动静检测技术, 就可以判断出目标在什么位置静止, 或是在
什么位置活动, 实现对目标活动状态检测 (位置与动静). 进一步, 有了目标的活动状态, 再结合上下文, 就可以对目
标的行为进行合理的推断: 如当目标在某段时间内一直在灶台附近活动, 可以推断其在做饭; 而当目标在沙发附近
静止, 则可以推断其在休息.
4.2.1

系统设计

本系统所用设备与入侵检测系统一致, 具体的系统设计如后文图 21 所示.
4.2.2

系统效果

为验证本系统的实际效果, 我们在一个实际环境中进行了测试, 布局如图 22 所示. 可以看到, 这是一个包含休
息区、就餐区及做餐区的大房间, 休息区有电视、沙发等, 就餐区有餐桌、水壶等, 做餐区则有相关家具; 各个区
域间并没有墙壁作为分界线, 已有的房间级定位技术并不能判断目标的具体所在区域. 按照如图 22 方式部署发射
端和接收端, 即可利用 link 将整个空间分为 3 个部分, 分别对应不同的功能区; 再利用本文的方法对 3 条边界进行
监控, 即可捕捉目标的越线行为, 从而获知目标的位置. 进一步, 利用已有的动静检测技术, 还可以获知目标的动静

软件学报 2024 年第 35 卷第 3 期

1530

状态, 再与目标位置结合, 得到目标居家状态信息.
我们让两名实验者 (1 男 1 女) 在如图 22 的环境中活动, 各积累了 12 h 的数据, 其中包含 100 次以上的区域
切换事件 (越线事件). 对于某一次区域切换事件, 若系统正确识别, 则认为是一次正确的检测; 对未检测出、检测
错误以及未越线但检测出越线这 3 种情况, 都视为错误的检测. 最终, 系统的准确率达到了 89.2%, 其中两名实验
者的实验结果基本一致. 这说明, 系统可以对目标所在位置有较准确的判断. 而对于动静检测 [38,44], 现有技术已经
比较成熟, 识别准确率很高, 在此不再单独进行验证.
Link1 边界监控
Link2 边界监控

多设备 CSI 流

…

位置信息

Linkn 边界监控

动静检测

图 21

居家状态信息

动静信息

居家状态监测系统流程图
RX1

休息区
就餐区

RX2

TX
做餐区

图 22

测试环境

拿到准确的位置信息和动静信息后, 系统就可以得到目标的居家状态信息. 在某一时刻, 当系统识别的目标
(位置, 动/静) 二元组与目标的实际状态相同时, 我们就认为这一时刻的居家状态信息是正确的. 统计数据的全部
时刻, 得到居家状态信息的准确率为 92%. 误差的主要来源为, 位置信息的偶尔错误, 致使居家信息在某段时间出
错. 但只要目标再次进行区域切换, 系统就会捕获新的位置信息, 系统重新回到正确的结果. 总体来看, 居家状态监
测系统达到了很高的准确率, 为更精细的上层应用提供了良好的数据基础.
4.2.3

可行性分析: 从居家状态信息到行为推断

得到目标的居家状态信息后, 是否可以对目标的具体行为进行进一步的推断? 实际上, 除了当前时刻的位置
信息和动静信息, 过去一段时间的信息、即上下文也可以给我们带来很多信息. 比如, 当目标长时间处于休息区,
且大部分时间是静止的, 则可推断其应坐在沙发上休息; 而若目标长时间在某一区域处于活动状态, 则其很可能在
做家务; 同理, 若目标在一段时间中来回穿梭于各个区域, 那么其很可能在踱步. 例如, 在 15–17 点, 如果系统显示
目标主要位于休息区, 并且只进行了零星活动, 因此可以推断他应该是在沙发上休息; 而在这个过程中, 若他去过
一次餐区且停留时间较短, 结合餐区的功能属性, 可以推断其应该是去喝水或者取用食物. 再如, 在 11–12 点, 目标
长时间位于做餐区, 并且始终在活动, 则可推断其在做饭. 按照这样的思路, 对其一天的行为进行合理推断, 即可大
概获得目标的生活轨迹.
诚然, 一个人的活动是很多样的, 难以完全罗列. 但是目标行为与所处位置的强相关性, 使得居家状态信息对

刘兆鹏 等: WiFense: 从衍射模型到边界监控

1531

目标行为推断有着巨大的意义. 未来, 结合对更广泛数据的挖掘、分析, 以及更精细感知手段的加入 (如将动静检
测置换为活动幅度检测等), 就可以实现更精细的行为识别. 我们相信, 在本文算法的基础上, 这将成为一个有趣的
研究方向.

5 总

结

在非接触式的智能感知系统中, 目标的位置是十分重要的上下文信息, 同时也是很多应用正常运行的基础, 因
此对特定边界的准确监控是本领域中一个重要的课题. 随着 WiFi 技术的兴起, 基于 WiFi 信号的感知技术受到了
研究人员的广泛关注, 越来越多的 WiFi 感知工作涌现而出. 然而, 诸如手势识别、呼吸监测、动静检测等应用, 通
常只关注于功能本身的实现, 缺少对实用性的考虑. 在实际场景中, 往往需要先对目标所在区域位置进行判断, 当
其进入到关注的区域时, 再运行更细化的感知功能. 而已有的 WiFi 感知工作, 一部分没能找到一个可以对特定边
界进行精确监控的方法, 另一部分则需要墙壁等的辅助才能实现. 不同于现有工作, 本文从衍射的物理本质出发,
建立了目标运动与信号衍射的关系, 找到了目标穿越 link (收发设备天线的连线) 时的信号特征; 基于这种特征, 我
们以 link 作为边界界线, 结合天线间距带来的波形时延以及 AGC 在 link 被遮挡时的特征, 设计了一个可自由确
定界线的边界监控方法. 在此基础上, 本文还实现了两个实际应用, 即入侵检测系统和居家状态监测系统, 在验证
了算法的可用性和鲁棒性的同时, 展示了算法与其他感知技术相结合的巨大潜力.
未来, 在本文基于 WiFi 信号的边界监控技术的基础上, 可以继续拓展更多的应用, 形成更全面的非接触式感
知系统. 例如第 4 节中的第 2 个应用, 我们将已有的动静检测技术与本文技术相结合, 就可以判断出目标在什么位
置静止, 或是在什么位置活动, 从而可以进一步推断其具体行为, 如做饭、沙发上休息等. 继续拓展, 本文的算法还
可以与手势识别结合、与呼吸检测结合、与行为识别结合等, 即将目标的位置和与位置相关的服务有机结合在一
起, 形成一整套感知系统, 这将产生强大的感知能力, 并且推动智能感知技术的发展和商用落地. 当然, 这在实际中
必然充满着挑战, 但却是极有价值的努力方向, 值得继续深入探讨和研究.
References:
[1]

Klakegg S, Goncalves J, Luo C, Visuri A, Popov A, van Berkel N, Sarsenbayeva Z, Kostakos V, Hosio S, Savage S, Bykov A, Meglinski I,
Ferreira D. Assisted medication management in elderly care using miniaturised near-infrared spectroscopy. Proc. of the ACM on
Interactive, Mobile, Wearable and Ubiquitous Technologies, 2018, 2(2): 69. [doi: 10.1145/3214272]

[2]

Yu ZY, Yuan LW, Luo W, Feng LY, Lv GN. Spatio-temporal constrained human trajectory generation from the PIR motion detector
sensor network data: A geometric algebra approach. Sensors, 2016, 16(1): 43. [doi: 10.3390/s16010043]

[3]

Li SJ. Wi-Fi based technologies for continuous monitoring of daily life status [Ph.D. Thesis]. Beijing: Beijing University, 2021
(in Chinese with English abstract).

[4]

Wang YX, Li SJ, Wang H, Ma JY, Wang YS, Zhang DQ. Survey on Wi-Fi based contactless activity recognition. Journal of Zhejiang
University (Engineering Science), 2017, 51(4): 648–654, 690 (in Chinese with English abstract). [doi: 10.3785/j.issn.1008-973X.2017.
04.002]

[5]

Woyach K, Puccinelli D, Haenggi M. Sensorless sensing in wireless networks: Implementation and measurements. In: Proc. of the 4th Int’l
Symp. on Modeling and Optimization in Mobile, Ad Hoc and Wireless Networks. Boston: IEEE, 2006. 1–8. [doi: 10.1109/WIOPT.2006.
1666495]

[6]

Zhang DQ, Wang H, Wu D. Millimeter-scale Wi-Fi contactless sensing: From pattern to model. Communications of the CCF, 2018,
14(1):18–25 (in Chinese with English abstract).

[7]

Adib F, Hsu CY, Mao HZ, Katabi D, Durand F. Capturing the human figure through a wall. ACM Trans. on Graphics, 2015, 34(6): 1–13.
[doi: 10.1145/2816795.2818072]

[8]

Adib F, Kabelac Z, Katabi D, Miller RC. 3D tracking via body radio reflections. In: Proc. of the 11th USENIX Conf. on Networked
Systems Design and Implementation. Seattle: USENIX Association, 2014. 317–329.

[9]

Anderson B, Shi MQ, Tan VYF, Wang Y. Mobile gait analysis using foot-mounted UWB sensors. Proc. of the ACM on Interactive,
Mobile, Wearable and Ubiquitous Technologies, 2019, 3(3): 73. [doi: 10.1145/3351231]

[10]

Fairchild DP, Narayanan RM. Multistatic micro-Doppler radar for determining target orientation and activity classification. IEEE Trans.
on Aerospace and Electronic Systems, 2016, 52(1): 512–521. [doi: 10.1109/TAES.2015.130595]

[11]

Fogle OR, Rigling BD. Micro-range/micro-Doppler decomposition of human radar signatures. IEEE Trans. on Aerospace and Electronic

1532

软件学报 2024 年第 35 卷第 3 期

Systems, 2012, 48(4): 3058–3072. [doi: 10.1109/TAES.2012.6324677]

[12]

Gezici S, Tian Z, Giannakis G B, Kobayashi H, Molisch AF, Poor HV, Sahinoglu Z. Localization via ultra-wideband radios: A look at
positioning aspects for future sensor networks. IEEE Signal Processing Magazine, 2005, 22(4): 70–84. [doi: 10.1109/MSP.2005.1458289]

[13]

Dissanayake T, Maekawa T, Amagata D, Hara T. Detecting door events using a smartphone via active sound sensing. Proc. of the ACM
on Interactive, Mobile, Wearable and Ubiquitous Technologies, 2018, 2(4): 160. [doi: 10.1145/3287038]

[14]

Wang W, Liu AX, Sun K. Device-free gesture tracking using acoustic signals. In: Proc. of the 22nd Annual Int ’l Conf. on Mobile
Computing and Networking. New York: ACM, 2016. 82–94. [doi: 10.1145/2973750.2973764]

[15]

Xu W, Yu ZW, Wang Z, Guo B, Han Q. AcousticID: Gait-based human identification using acoustic signal. Proc. of the ACM on
Interactive, Mobile, Wearable and Ubiquitous Technologies, 2019, 3(3): 115. [doi: 10.1145/3351273]

[16]

Abdelnasser H, Youssef M, Harras KA. WiGest: A ubiquitous WiFi-based gesture recognition system. In: Proc. of the 2015 IEEE Conf.
on Computer Communications. Hong Kong: IEEE, 2015. 1472–1480. [doi: 10.1109/INFOCOM.2015.7218525]

[17]

Niu K, Zhang FS, Jiang YH, Chang ZX, Wang LY, Zhang DQ. A contactless Morse code text input system using COTS WiFi devices. In:
Proc. of the 2019 ACM Int ’l Joint Conf. on Pervasive and Ubiquitous Computing and Proc. of the 2019 ACM Int ’l Symposium on
Wearable Computers. London: ACM, 2019. 328–331. [doi: 10.1145/3341162.3343850]

[18]

Gao RY, Zhang M, Zhang J, Li Y, Yi EZ, Wu D, Wang LY. Towards position-independent sensing for gesture recognition with Wi-Fi.
Proc. of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies, 2021, 5(2): 61. [doi: 10.1145/3463504]

[19]

Liu J, Wang Y, Chen YY, Yang J, Chen X, Cheng J. Tracking vital signs during sleep leveraging off-the-shelf WiFi. In: Proc. of the 16th
ACM Int’l Symp. on Mobile Ad Hoc Networking and Computing. Hangzhou: ACM, 2015. 267–276. [doi: 10.1145/2746285.2746303]

[20]

Zeng YW, Wu D, Xiong J, Yi EZ, Gao RY, Zhang DQ. FarSense: Pushing the range limit of WiFi-based respiration sensing with CSI
ratio of two antennas. Proc. of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies, 2019, 3(3): 121. [doi: 10.1145/
3351279]

[21]

Zhang FS, Zhang DQ, Xiong J, Wang H, Niu K, Jin BH, Wang YX. From Fresnel diffraction model to fine-grained human respiration
sensing with commodity Wi-Fi devices. Proc. of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies, 2018, 2(1): 53.
[doi: 10.1145/3191785]

[22]

Zeng YW, Wu D, Gao RY, Gu T, Zhang DQ. FullBreathe: Full human respiration detection exploiting complementarity of CSI phase and
amplitude of WiFi signals. Proc. of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies, 2018, 2(3): 148. [doi: 10.
1145/3264958]

[23]

Zeng YW, Wu D, Xiong J, Liu JY, Liu ZP, Zhang DQ. MultiSense: Enabling multi-person respiration sensing with commodity WiFi.
Proc. of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies, 2020, 4(3): 102. [doi: 10.1145/3411816]

[24]

Kotaru M, Joshi K, Bharadia D, Katti S. SpotFi: Decimeter level localization using WiFi. In: Proc. of the 2015 ACM Conf. on Special
Interest Group on Data Communication. London: ACM, 2015. 269–282. [doi: 10.1145/2785956.2787487]

[25]

Li X, Li SJ, Zhang DQ, Xiong J, Wang YS, Mei H. Dynamic-MUSIC: Accurate device-free indoor localization. In: Proc. of 2016 ACM
Int’l Joint Conf. on Pervasive and Ubiquitous Computing. Heidelberg: ACM, 2016. 196–207. [doi: 10.1145/2971648.2971665]

[26]

Li X, Zhang DQ, Lv Q, Xiong J, Li SJ, Zhang Y, Mei H. IndoTrack: Device-free indoor human tracking with commodity Wi-Fi. Proc. of
the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies, 2017, 1(3): 72. [doi: 10.1145/3130940]

[27]

Qian K, Wu CS, Yang Z, Liu YH, Jamieson K. Widar: Decimeter-level passive tracking via velocity monitoring with commodity Wi-Fi.
In: Proc. of the 18th ACM Int’l Symp. on Mobile Ad Hoc Networking and Computing. Chennai: ACM, 2017. 6. [doi: 10.1145/3084041.
3084067]

[28]

Wang J, Jiang HB, Xiong J, Jamieson K, Chen XJ, Fang DY, Xie BB. LiFS: Low human-effort, device-free localization with fine-grained
subcarrier information. In: Proc. of the 22nd Annual Int’l Conf. on Mobile Computing and Networking. New York City: ACM, 2016.
243–256. [doi: 10.1145/2973750.2973776]

[29]

Xiao J, Wu KS, Yi YW, Wang L, Ni LM. Pilot: Passive device-free indoor localization using channel state information. In: Proc. of the
33rd IEEE Int’l Conf. on Distributed Computing Systems. Philadelphia: IEEE, 2013. 236–245. [doi: 10.1109/ICDCS.2013.49]

[30]

Youssef M, Mah M, Agrawala A. Challenges: Device-free passive localization for wireless environments. In: Proc. of the 13th Annual
ACM Int’l Conf. on Mobile Computing and Networking. Montréal: ACM, 2007. 222–229. [doi: 10.1145/1287853.1287880]

[31]

Li SJ, Li X, Niu K, Wang H, Zhang Y, Zhang DQ. AR-alarm: An adaptive and robust intrusion detection system leveraging CSI from
commodity Wi-Fi. In: Proc. of the 15th Int’l Conf. on Smart Homes and Health Telematics. Paris: Springer, 2017. 211–223. [doi: 10.1007/
978-3-319-66188-9_18]

[32]

Li SJ, Liu ZP, Zhang Y, Lv Q, Niu XP, Wang LY. WiBorder: Precise Wi-Fi based boundary sensing via through-wall discrimination.
Proc. of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies, 2020, 4(3): 89. [doi: 10.1145/3411834]

[33]

Zhang DQ, Wang H, Wu D. Toward centimeter-scale human activity sensing with Wi-Fi signals. Computer, 2017, 50(1): 48–57. [doi: 10.
1109/MC.2017.7]

刘兆鹏 等: WiFense: 从衍射模型到边界监控

[34]

1533

Wang H, Zhang DQ, Wang YS, Ma JY, Wang YX, Li SJ. RT-Fall: A real-time and contactless fall detection system with commodity
WiFi devices. IEEE Trans. on Mobile Computing, 2017, 16(2): 511–526. [doi: 10.1109/TMC.2016.2557795]

[35]

Li SJ, Li X, Lv Q, Tian GY, Zhang DQ. WiFit: Ubiquitous bodyweight exercise monitoring with commodity Wi-Fi devices. In: Proc. of
the 2018 IEEE SmartWorld, Ubiquitous Intelligence & Computing, Advanced & Trusted Computing, Scalable Computing &
Communications, Cloud & Big Data Computing, Internet of People and Smart City Innovation. Guangzhou: IEEE, 2018. 530–537. [doi:
10.1109/SmartWorld.2018.00114]

[36]

Wang W, Liu AX, Shahzad M, Ling K, Lu SL. Understanding and modeling of Wi-Fi signal based human activity recognition. In: Proc.
of the 21st Annual Int’l Conf. on Mobile Computing and Networking. Paris: ACM, 2015. 65–76. [doi: 10.1145/2789168.2790093]

[37]

Wang Y, Liu J, Chen YY, Gruteser M, Yang J, Liu HB. E-eyes: Device-free location-oriented activity identification using fine-grained
WiFi signatures. In: Proc. of the 20th Annual Int’l Conf. on Mobile Computing and Networking. Maui: ACM, 2014. 617–628. [doi: 10.
1145/2639108.2639143]

[38]

Li X, Zhang DQ, Xiong J, Zhang Y, Li SJ, Wang YS, Mei H. Training-free human vitality monitoring using commodity Wi-Fi devices.
Proc. of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies, 2018, 2(3): 121. [doi: 10.1145/3264931]

[39]
[40]

Molisch AF. 2005. Wireless Communications. Hoboken: John Wiley and Sons.
Zhang FS, Niu K, Xiong J, Jin BH, Gu T, Jiang YH. Towards a diffraction-based sensing approach on human activity recognition. Proc.
of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies, 2019, 3(1): 33. [doi: 10.1145/3314420]

[41]

Zeng YW, Wu D, Xiong J, Zhang DQ. Boosting WiFi sensing performance via CSI ratio. IEEE Pervasive Computing, 2021, 20(1):
62–70. [doi: 10.1109/MPRV.2020.3041024]

[42]
[43]

Fritz S, Lusardi M. White paper: “Walking speed: The sixth vital sign”. Journal of Geriatric Physical Therapy, 2009, 32(2): 46–49.
Bohannon RW. Comfortable and maximum walking speed of adults aged 20 –79 years: Reference values and determinants. Age and
Ageing, 1997, 26(1): 15–19. [doi: 10.1093/ageing/26.1.15]

[44]

Zhang F, Wu CS, Wang BB, Lai HQ, Han Y, Liu KJR. WiDetect: Robust motion detection with a statistical electromagnetic model. Proc.
of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies, 2019, 3(3): 122. [doi: 10.1145/3351280]

附中文参考文献:
[3] 李晟洁. 基于商用Wi-Fi的日常起居状态连续监测技术研究 [博士学位论文]. 北京: 北京大学, 2021.
[4] 王钰翔, 李晟洁, 王皓, 马钧轶, 王亚沙, 张大庆. 基于Wi-Fi的非接触式行为识别研究综述. 浙江大学学报(工学版), 2017, 51(4):
648–654, 690. [doi: 10.3785/j.issn.1008-973X.2017.04.002]

[6] 张大庆, 王皓, 吴丹. 毫米级的Wi-Fi无接触感知: 从模式到模型. 中国计算机学会通讯, 2018, 14(1): 18–25.
刘兆鹏(1996－), 男, 硕士, 主要研究领域为普适

曾有为(1995－), 男, 博士, 主要研究领域为普适

计算, 无线感知.

计算, 无线感知.

李晟洁(1994－), 女, 博士, 主要研究领域为普适

张大庆(1964－), 男, 博士, 教授, 博士生导师,

计算, 无线感知.

CCF 杰出会员, 主要研究领域为普适计算, 无线
感知, 大数据分析.

张越(1994－), 男, 硕士, 主要研究领域为普适计
算, 无线感知.


66 软件学报 ISSN 1000-9825, CODEN RUXUEW
Journal of Software, [doi: 10.13328/j.cnki.jos.007013]
©中国科学院软件研究所版权所有.

E-mail: jos@iscas.ac.cn
http://www.jos.org.cn
Tel: +86-10-62562563

一种分组并行的轻量化实时微观三维形貌重建方法

∗

闫涛 1,2,3, 高浩轩 1,2, 张江峰 1,2, 钱宇华 1, 张临垣 4
1

(山西大学 大数据科学与产业研究院,山西 太原

2

(山西大学 计算机与信息技术学院,山西 太原

3

(哈尔滨工业大学 重庆研究院,重庆 401151)

4

(北京中钞钞券设计制版有限公司,北京 100070)

030006)
030006)

通讯作者: 钱宇华, E-mail: jinchengqyh@sxu.edu.cn

摘 要:

微观三维形貌重建作为精密制造领域生产制造的关键环节,其重建过程依赖于高分辨率稠密图像的

采集.而面对复杂应用场景的高时效性需求,高分辨率稠密图像的输入会导致运算量与计算复杂度呈几何倍增
加,无法实现高效率低延时的实时微观三维形貌重建.针对上述现状,本文提出一种分组并行的轻量级实时微观
三维形貌重建方法 GPLWS-Net, GPLWS-Net 以 U 型网络为基础构造轻量化主干网络,以并行分组式查询加速三
维形貌重建过程,并针对神经网络结构进行重参数化设计避免重建微观结构的精度损失.另外,为弥补现有微观
三维重建数据集的缺失,本文公开了一组多聚焦微观三维重建数据集(Micro 3D),其标签数据利用多模态数据融
合的方式获取场景高精度的三维结构.结果表明,本文提出的 GPLWS-Net 网络不仅可以保证重建精度,而且在三
组公开数据集中相比于其他五类深度学习方法平均耗时降低 39.15%,在 Micro 3D 数据集中平均耗时降低
50.55%,能够实现复杂微观场景的实时三维形貌重建.
关键词: 微观三维形貌重建;轻量化神经网络;分组并行
中图法分类号: TP391
中 文 引 用 格 式 : 闫 涛 ,高 浩 轩 , 张 江 峰 , 钱 宇 华 ,张 临 垣 .一 种 分 组 并 行 的 轻 量 化 实 时 微 观 三 维 形 貌 重 建 方 法 .软 件 学 报 .
http://www.jos.org.cn/1000-9825/7013.htm
英文引用格式: Yan T, Gao HX, Zhang JF, Qian YH, Zhang LY. A grouping parallel lightweight real-time microscopic 3D shape
reconstruction method. Ruan Jian Xue Bao/Journal of Software (in Chinese). http://www.jos.org.cn/1000-9825/7013.htm

A grouping parallel lightweight real-time microscopic 3D shape reconstruction method
YAN Tao1,2,3,

GAO Hao-Xuan1,2,

ZHANG Jiang-Feng1,2,

QIAN Yu-Hua1,

ZHANG Lin-Yuan4

1

(Institute of Big Data Science and Industry, Shanxi University, Taiyuan 030006, China)

2

(School of Computer and Information Technology, Shanxi University, Taiyuan 030006, China)

3

(Chongqing Research Institute of Harbin Institute of Technology, Harbin Institute of Technology, Chongqing 401151, China)

4

(Beijing Zhongchao Banknote Designing and Plate-making Co., Ltd., Beijing 100070, China)

Abstract: Microscopic three-dimensional (3D) shape reconstruction is a crucial step in the field of precision manufacturing. The
reconstruction process relies on the acquisition of high-resolution and dense images. However, in the face of high efficiency requirements
in complex application scenarios, inputting high-resolution dense images will result in geometrically increased computation and
complexity, making it difficult to achieve efficient and low-latency real-time microscopic 3D shape reconstruction. In response to this
situation, this paper proposes a grouping parallelism lightweight real-time microscopic 3D shape reconstruction method GPLWS-Net. The
∗

基 金 项 目 : 国 家 自 然 科 学 基 金 重 点 项 目 (62136005); 科 技 创 新 2030- 重 大 项 目 (2021ZD0112400); 国 家 自 然 科 学 基 金

(62006146); 中央引导地方科技发展资金项目(YDZJSX20231C001, YDZJSX20231B001)
收稿时间: 2023-05-14; 修改时间: 2023-07-07; 采用时间: 2023-08-24; jos 在线出版时间: 2023-09-11

闫涛 等:一种分组并行的轻量化实时微观三维形貌重建方法

2205

GPLWS-Net constructs a lightweight backbone network based on a U-shaped network and accelerates the 3D shape reconstruction process
with parallel group-querying. In addition, the neural network structure is re-parameterized to avoid the accuracy loss of reconstructing the
microstructure. Furthermore, to supplement the lack of existing microscopic 3D reconstruction datasets, this article publicly releases a set
of multi-focus microscopic 3D reconstruction dataset called Micro 3D. The label data uses multi-modal data fusion to obtain a
high-precision 3D structure of the scene. The results show that the GPLWS-Net network can not only guarantee the reconstruction
accuracy, but also reduce the average time of 39.15% in the three groups of public datasets and 50.55% in the Micro 3D dataset compared
with the other five types of deep learning-based methods, which can achieve real-time 3D shape reconstruction of complex microscopic
scenes.
Key words:

microscopic 3D shape reconstruction; lightweight neural network; group parallelism

1 引言
微观三维形貌重建作为三维重建领域的重要分支,广泛应用于精密制造质量控制、新材料结构分析、生物
观测鉴别等领域 [1].现有的微观三维形貌重建方法包括主动光学与被动光学两大类,典型的主动光学方法包括
激光共聚焦与白光干涉等,但这类方法需要昂贵的硬件设备支撑,难以进行大规模工业应用.被动光学以多聚焦
图像三维形貌重建为代表,主要通过微米级光学成像技术从多聚焦图像序列中恢复场景的三维结构,较高的重
建效率与较低的硬件成本使其广受学术与工业界关注[2].
现有的多聚焦图像三维形貌重建主要分为模型设计与数据驱动两大类 [3],模型设计类方法旨在通过设计
聚焦测量算子评价图像序列的聚焦水平,然后选择图像序列中聚焦水平最大值所在帧聚合为场景的深度信息.
因此聚焦测量算子设计的优劣是决定模型类设计方法是否有效的关键,而现有的聚焦测量算子更擅于解决富
纹理场景的重建问题,无法实现弱纹理或低对比度场景的精确重建,其场景偏向性导致模型设计类方法普遍缺
乏良好的场景适应性.数据驱动类方法以基于深度学习的多聚焦图像三维形貌重建为代表,可直接通过多聚焦
图像序列学习得到场景的深度信息 [4] .但现有的深度学习类方法主要围绕宏观场景展开,由于宏观场景通常具
有低分辨率与稀疏采样的特点,加之这类场景的数据规模较小,针对这类深度网络模型的研究通常难以解决微
观场景高分辨率稠密数据产生的计算负担和受限资源条件下网络推理时间增多等问题.
现阶段,构建更深更大的卷积神经网络(CNNs)逐渐成为多聚焦图像三维形貌重建领域的发展趋势 [5].目前
主流的深度网络模型通常有上百层卷积操作和数千个通道进行运算,这些网络的运算量(FLOPs)通常达到数百
万甚至几千万次,从输入图像序列到三维结构的一次推理过程往往需要较长时间.图 1 为五种先进的深度学习
多 聚 焦 图 像 三 维 形 貌 重 建 算 法 FVNet(2022/CVPR)[5] ,DFVNet(2022/CVPR)[5],DDFF(2018/ACCV)[6],
DefocusNet(2020/CVPR)[7]和 AiFDepthNet(2021/ICCV)[8]分别在 128×128×10,256×256×10,512×512×10 与
1024×1024×10 四种不同尺度的输入数据中运算耗时比较.由图 1 可知,上述所有方法的推理耗时均随着输入
数据量的增加而增多,这种高耗时导致其在解决高分辨率稠密数据的微观场景重建问题时会出现推理时间增
大与计算复杂度增加等问题.因此迫切需要从网络模型的轻量化角度探索实时微观三维形貌重建新模型.

图 1 五种典型的深度学习多聚焦图像三维形貌重建算法在不同尺度输入数据中的运算耗时结果
现 有 轻 量 化 网 络 如 MobileNets[9],ShuffleNet[10] 和 GhostNet[11] 等 通 常 利 用 深 度 可 分 离 卷 积 DWConv
(depthwise convolution)或者分组卷积 GConv(group convolution)来降低网络模型的计算复杂度.除上述纯卷积

2206

Journal of Software 软件学报 Vol.32, No.7, July 2021

神 经 网 络 之 外 , 许 多 研 究 也 开 始 设 计 更 快 更 小 的 ViT(vision transformer) 和 多 层 感 知 机 MLP(multilayer
perceptron)架构 [12] 降低网络的计算复杂度.然而现有的轻量化网络大多基于二维图像问题设计,如将其直接扩
展至三维场景,不仅会增加计算负担,而且也无法有效利用多聚焦图像序列间特有的邻域序列关联关系.
综上可知,现有的基于深度网络的多聚焦图像三维形貌重建主要侧重于宏观场景,较小的输入数据量使其
更加关注网络模型设计的有效性.而轻量级网络大多侧重于二维图像问题的设计,并没有对三维数据进行针对
性优化.除此之外,宏观场景数据具有典型的物体遮挡和大景深特性,与微观场景的缓慢过渡与小景深存在一定
的数据鸿沟.因此现有的深度网络设计模式在解决实时微观三维形貌重建问题主要面临如下挑战:
(1) 数据量陡增:已有的基于深度网络模型的多聚焦图像三维重建方法会随着输入图像序列数据量的增加
而导致模型的推理时间显著增大,无法满足实际微观场景中的高时效性需求;
(2) 模型不适用:现有的轻量级网络大多针对二维图像问题设计,而三维数据需要更多的计算资源与时间,
导致模型无法有效兼顾低延时与高精度,且现有轻量级网络无法有效利用多聚焦图像序列间的关联关系;
(3) 宏微观鸿沟:现有深度网络模型大多采用宏观场景中的合成数据集进行训练,加之宏微观数据内蕴结
构的差异性,导致采用这类合成数据训练的网络会出现过拟合,无法准确推断微观场景的三维结构变化.
针对上述挑战,本文提出一种分组并行的轻量化实时微观三维形貌重建网络模型 GPLWS-Net,主要贡献如
下:
(1) 从神经网络各组件时间能耗的角度,重现审视现有微观三维形貌重建网络的性能瓶颈问题,提出轻量
化、低延迟的网络主干;
(2) 从多域并行处理多聚焦特征的角度,设计与多聚焦图像序列三维形貌重建理论相契合的分组并行模
块,并采用结构重参数化进行模型压缩,将原有多卷积层恒等映射为单卷积层,保持三维形貌精度的同时有效降
低网络推理延迟;
(3) 针对微观三维场景数据匮乏的现状,公开了一组微细加工场景的微观三维数据集(Micro 3D).该数据集
标签采用“激光共聚焦+多景深合成+手工微调”等方式生成,弥补了现有微观领域数据集缺乏的不足.
本文第 2 节主要介绍了多聚焦图像三维形貌重建方法与轻量化网络模型的研究进展;第 3 节提出了基于分
组并行的轻量级实时微观三维形貌重建方法 GPLWS-Net;第 4 节与现有深度学习类方法和模型设计类方法在
公用数据集和无标签真实数据集中进行比较分析;最后对本研究进行总结和展望.

2 相关工作
多聚焦图像三维形貌重建通过等间隔调整相机与待测场景之间的焦距,获取可以覆盖场景全部景深范围
的多幅不同焦距的图像序列 { X i }iN=1 ,采用聚焦测量算子 FM(focus measure)评价图像序列中各图像的聚焦水平,
然后将同一区域聚焦水平最大值所在位置聚合为场景的初始深度 DInit ,最后采用迭代修复、正则化等后处理方
法对初始深度图进行精炼得到场景最终的三维形貌重建结果 D [13-14].
=
D P=
( DInit ) , DInit arg max {FM ∗ X i }i =1
N

1≤ i ≤ N

(1)

其中 X i 为图像序列中第 i 幅图像, N 为图像序列总数, P ( ⋅) 为后处理函数.
2.1 多聚焦图像三维形貌重建研究进展
多聚焦图像三维形貌重建主要分为模型设计与数据驱动两大类方法.模型设计类方法主要围绕图像序列
的聚焦评价与深度图精炼两个关键步骤展开,其中聚焦评价旨在通过设计聚焦测量算子评价一幅图像的聚焦
水平,然后延伸至整个图像序列,进而获得场景的初始深度图.这些聚焦测量算子大体可以分为时域和频域两大
类.时域类算子主要侧重局部图像聚焦水平的刻画,代表性方法有环状差分算子 RDF(ring difference Filter)
多方向拉普拉斯算子 MDML(multidirectional modified Laplacian)
信息,典型的频域类算子有非降采样小波变换

[17]

与 Curvelet 变换

[16]

[18]

[15]

,

等;频域类算子更加关注图像的全局聚焦

等.深度图精炼主要通过对初始深度图添加

闫涛 等:一种分组并行的轻量化实时微观三维形貌重建方法

2207

约束条件改善算法的重建效果,如非凸正则优化[19] 、数据保真项[20] 等.然而模型设计类方法在聚焦测量算子的
设计过程中存在一定的场景偏向性,无法保障算法对未知场景的鲁棒性.除此之外,深度图精炼过分依赖于初始
深度图的质量,低质量初始深度图在精炼过程中容易引发错误深度信息蔓延.因此,以深度学习为代表的数据驱
动类方法逐渐引起学者们的关注.
近年来,已有一些研究从深度网络模型构建角度解决多聚焦图像三维形貌重建问题.但这类方法属于典型
的有监督学习,模型的性能依赖于数据集本身.如 Yang 等[5] 提出一种基于差分体积的聚焦和散焦网络 FVNet 和
DFVNet,该网络主要模拟模型设计类方法的聚焦评价过程;Hazirbas 等[6] 提出一种深度卷积神经网络 DDFF,该
网络利用光场和 RGB-D 相机对室内场景进行数据采集,构建了 DDFF-12 数据集,并对场景的聚焦信息和深度信
息进行端到端学习;Wang 等[8] 利用深度图像和全聚焦图像之间的关联关系设计了一个可共享的卷积神经网络
AiFDepthNet,该网络引入一个可以被共享的中间注意力图,用于预测场景深度和全聚焦图像;Maximov 等 [7] 提
出一种利用散焦图像训练的聚焦与散焦对齐网络 DefocusNet.尽管上述网络模型为深度学习类多聚焦图像三
维形貌重建提供一些有益的思路,但在解决实时微观三维形貌重建问题时需要考虑如下问题.首先,随着输入图
像序列分辨率的提升,网络处理数据量的倍增会导致收敛速度变慢;其次,上述网络训练的数据集主要集中在宏
观场景,且多数训练集为合成数据,基于这类数据集设计的网络可能无法有效刻画微观场景中缓慢的深度变化
与噪声干扰等情况.因此,如何针对微观场景特有的数据特点设计轻量化网络模型是解决实时微观三维形貌重
建问题的关键.
2.2 轻量化神经网络相关研究进展
近年来,随着深度神经网络在计算机视觉领域取得巨大成功,越来越多场景提出了智能化应用需求,然而在
实际的资源受限应用场景中通常无法满足神经网络的算力需求.为权衡神经网络的精度与性能,轻量化神经网
络 应 运 而 生 . 如 ShuffleNet[10] 、 SqueezeNet[21] 与 MobileViT[12] 主 要 是 对 网 络 模 型 的 参 数 进 行 优 化 ; 而
MobileNets[9] 、 MobileNeXt[22] 、 GhostNet[11] 、 Xception[23] 和 IGCNets[24] 等 模 型 则 侧 重 于 优 化 FLOPs;
EfficientNet[25] 和 TinyNet[26]在优化 FLOPs 的同时研究了网络的深度、宽度和输入图像分辨率的复合缩放;仅有
少数 网络 如 ShuffleNetV2[27] 、MobileNetV3[28] 、FasterNet[29] 和 MobileOne[30] 等对 网络 推理 时间 进行 优 化,
ShuffleNetV2[27]表明 FLOPs 和网络参数量与网络推理时间并没有呈现很好的相关性,MobileOne[30] 则发现推理
时间与 FLOPs 适度相关,与参数量弱相关.针对轻量化 ViT 的研究主要试图通过减少注意力操作的复杂度实现
网络精度与推理时间的平衡.如 MobileFormer[31] 和 MobileViT[12]针对参数和 FLOPs 进行优化,其表现已经超越
了低 FLOPs 的高效卷积神经网络,尽管这些模型在精度上取得了显著提升,但推理时间并未随之缩短.因此,仅
仅拥有低的 FLOPs 并不能必然导致推理时间的降低.
综上可知,现有的轻量级神经网络大多针对二维图像任务设计,而对于三维数据而言,更高的输入数据量可
能导致网络的计算量成倍增加.因此,从参数量优化和 FLOPs 优化的视角并不能有效降低网络的推理时间和增
加推理精度,需要从网络设计的全链条环节并结合三维数据特有的邻域序列关系重新进行轻量化网络模型的
设计.本文首先从理论上分析多聚焦图像序列子域数据分组并行的可行性,并根据该理论设计了分组并行模块,
可有效提升深度信息的寻找过程;其次摒弃原有二维卷积提取单帧图像局部聚焦特征的操作,转为三维立体卷
积精确跟踪多聚焦图像序列间的差异性,进而充分利用图像序列间的邻域关系实现高可靠性的深度信息判断;
最后采用结构重参数方法将三分支稀疏特征提取矩阵变为单分支密集特征提取矩阵,加速网络三维结构预测.

3 GPLWS-Net:基于分组并行的轻量化实时微观三维形貌重建方法
3.1 多聚焦图像序列的子域数据分组并行理论分析
基于多聚焦图像序列的微观三维形貌重建方法利用光学设备成像过程的有限景深判定聚焦区域与相机间
的相对距离,进而还原待测场景的三维形貌.根据高斯成像公式可得,物距的倒数与像距的倒数之和等于焦距的
倒数:

2208

Journal of Software 软件学报 Vol.32, No.7, July 2021

1 1 1
=
+
f u v

(2)

其中 f 表示镜头焦距,u 表示物体到透镜的距离,v 表示成像到透镜的距离.
上述理论表明多聚焦图像三维形貌重建的本质是从一系列不同聚焦水平的图像序列中挖掘深度信息,其
中单帧图像中的聚焦与散焦变换可以通过全聚焦图像的滤波得到.因此,局部聚焦图像 I L 可以通过全聚焦图像

I 与点扩散函数 h 的卷积得到,
(3)

I L= I ∗ h

其中点扩散函数在光学成像模式中可以简化成如下高斯函数:
=
h ( i, j )

 i 2 +j 2 
exp  −

2pσ h
 2σ h 
1

(4)

其中 σ h 用来刻画一幅图像的模糊水平,研究表明在 I i , j 位置的模糊水平 σ h 与场景深度 u 存在如下关系 [32]:

σ h ( i, j ) =

2
κ f u − uf

A u (u f − f )

(5)

其中, u f 为相机设置的聚焦位置, κ 为相机参数, A 为相机焦距与透镜直径的比值.假定
=
M κ f 2 A ( u f − f ) ,对公
式(5)求导可得:
u − uf
∂σ h ( i, j )
=
=
M
∂u
u

u f
 − 1 u < u f
u

1 − u f u ≥ u
f

u

(6)

图 2 为模糊水平 σ h 对场景深度 u 的一阶导数曲线,可以表明模糊水平 σ h 为连续函数.

图 2 模糊水平 σ h 对场景深度 u 的一阶导数曲线
根据连续函数的最大值定理可知,待测场景某点的深度对应于多聚焦图像序列内该点聚集的最大值,并根
据模糊水平 σ h 的一阶导数曲线可知该点有且仅有一处.而现有三维形貌重建大都从全局视角出发,通过依次遍
历图像序列搜索全局聚焦信息的最大值,未能通过子域划分进行并行检索,导致出现运行效率瓶颈.
3.2 总体框架
本文提出的基于分组并行的轻量化实时微观三维形貌重建方法主要包括以下 4 个关键环节:
(1) 微观数据集生成:鉴于现有研究普遍缺乏微观场景数据集,通过自研的微米级超景深微观数据采集装
置实现微观场景多聚焦图像序列的采集,采用 3D TFT 算法[3] 得到场景初步的三维重建结果,联合亚微米级精度

闫涛 等:一种分组并行的轻量化实时微观三维形貌重建方法

2209

的激光共聚焦显微镜得到场景的三维结构信息.采用 ALI-Net[33] 对上述两类多模态重建结果进行配准,最后通
过人工筛选微调获得高精度的标签数据.
(2) 轻量网络主干:为降低深度神经网络的内存访问成本和计算开销,利用网络结构设计的优化和网络模
型之间的参数融合等原理对已有的 U 型网络架构进行重构,在减少网络运算的内存访问频次的同时保障其性
能表现,从而实现更低的计算开销.
(3) 分组并行加速:为了最大程度提高图像序列的处理速度和效率,使用分组卷积对多聚焦图像序列中的
图像进行特征处理以找出局部极大值,然后再利用更为高效 1×1 卷积进行全局特征处理找到极值点,分组卷积
操作具有的可并行特性可有效提升计算效率,进而减少整个操作的时间成本.
(4) 结构重参数化:由于神经网络中存在大量冗余参数,这些参数可以融合为新参数进而赋予新的结构,以
提高网络的效率.因此本文使用结构重参数化对网络架构从训练到推理进行解耦合,在不影响网络精度的同时
降低推理时间.
本文提出的基于分组并行的轻量化实时微观三维形貌重建方法示意图如图 3 所示.

图 3 基于分组并行的轻量化实时微观三维形貌重建方法示意图
3.3 GPLWS-Net轻量化网络模型
受到 U 型网络的高效率特征融合方式和低成本参数运算的启发,本文以 U-Net 作为网络主干基础.由于相
同聚焦设置下不同尺度的图像序列对于同一聚焦算子的敏感度不同,其关键在于聚焦算子对于像素信息的抽
取会伴随图像尺度的改变发生变化.而 U-Net 网络主干通过多次下采样操作降低特征图分辨率,可获得多聚焦
图像序列不同尺度的特征.U 型网络中收缩模块和扩张模块在特征信息尺寸和神经网络宽度方面保持相互对
称,在收缩模块中逐步缩小特征信息的尺度并增加特征信息的维度,而收缩模块恰好相反,二者间使用跳跃连
接(skip connection)有效耦合图像的表层特征和深层特征.而跳跃连接在神经网络中具有的缺点对于网络的轻
量化设计是不容忽视的:首先跳跃连接会增加网络参数量,导致模型的复杂性和训练难度增加;其次,跳跃连接
需要额外的计算操作处理连接路径,增加了计算资源的需求和时间成本;此外,如果底层特征和上层特征存在冗
余,跳跃连接可能会引入特征冗余.而残差连接通过将前一层的输入添加至后续层,有效缓解了梯度消失问题,
使得整个网络更易于优化和训练.其次残差连接加速了网络优化过程,使得网络更快收敛,有效减少了训练时间
和算力需求.此外,残差连接允许底层特征直接传递至较深层并进行特征融合的操作可有效提升网络性能,增强
对数据细节信息和复杂关系的捕捉能力.综上所述,残差连接不仅可有效改善神经网络的训练优化过程,而且能
够在提高模型性能的同时减少训练时间和计算资源需求,非常有利于轻量化网络模型的构建.因此,本文设计的
网络主干将跳跃连接改为残差连接,既能有效耦合前后特征信息的梯度传递,同时也能降低网络参数量.图 4

2210

Journal of Software 软件学报 Vol.32, No.7, July 2021

为本文构建的 GPLWS-Net 网络结构示意图.

图 4 GPLWS-Net 网络模型结构
3.4 分组并行模块
基于多聚焦图像序列的三维形貌重建方法主要通过光学成像设备对场景进行等间隔扫描,由于场景中各
点深度信息的唯一性可得出聚焦最大值点存在唯一性.分组卷积的子域并行特性完美契合聚焦判定过程中局
部极值点导出全局极值点的特性.本文重新审视分组卷积的固有特性,验证多聚焦图像单峰时序信号中分组卷
积的可用性和高效性.
该模块借鉴深度可分离卷积[34] ,依据聚焦曲线特性进行重新设计,用于贴合三维形貌重建过程.深度可分离
卷积作为常规卷积的流行变体,其核心在于通过拆分空间维度和通道维度的相关性,减少卷积计算所需的参数.
深度可分离卷积由两层卷积代替原有普通卷积层,其中包含对单通道数据进行特征提取的逐深度卷积
(depthwise convolution) 和 对 单 维 度 进 行 特 征 融 合 的 逐 点 卷 积 (pointwise convolution). 对 于 输 入 序 列
I ∈ � C × H ×W ( C , H ,W 分别为通道,图像高和宽),深度卷积对每个输入通道应用单个卷积核 W ∈ � k × k ( k 为卷积核

大小)计算输出的序列 O ∈ � C × H ×W ,然后逐点卷积应用 1×1 的卷积核将深度卷积的输出进行线性组合.与具有
运算量的常规卷积相比,深度可分离卷积的运算量低至 k 2 × c × H × W + H × W × c 2 ,约为常规卷积的 1 k 2 .尽管深
度可分离卷积显著提高了模型的运行速度和减少了计算成本,但仍面临分组过多导致内存访问量过大与单层
特征抽取等问题.
本文将原有的逐通道卷积改为分组卷积平衡精度与效率,可有效契合三维形貌重建过程聚焦最大值的并
行查找过程.此外,本文仍保留 1×1 卷积对全域聚焦最大值进行整合.研究表明引入通道重洗(channel shuffle)[11]
可实现通道组之间的信息流不再受到限制(图 5b),进而有效提升特征的表示能力,但通道重洗操作也会带来相
应的缺点:即需要大量的指针跳转和内存空间.同时通道重洗操作又特别依赖于实现细节,导致实际运行速度不
理想.对于线性计算的卷积层来说,通道重洗操作使得网络无法进一步优化.而本文使用 1×1 卷积加强各通道
间的信息流动(图 5c),可增强网络的特征表达能力.通道重洗仅能互换部分特征区域,而 1×1 卷积可有效选择全
域聚焦信息.

2211

闫涛 等:一种分组并行的轻量化实时微观三维形貌重建方法

图5

(a) 分组卷积

(b) 通道重洗卷积

(c) 逐点卷积

如图 6 所示,在分组并行模块中,首先将输入的特征张量按照原先设定的分组数进行等分;然后针对分组张
量并行抽取特征,得到分组子域内聚集极大值;最后 1×1 卷积层将每组卷积的聚焦结果进行合并,并选择全域
内聚焦最大值.相较于传统卷积操作,分组卷积将输入特征在通道维度进行划分以减少卷积提取区域,可降低模
型训练及推理的时间.

图 6 本文采用的分组并行策略
3.5 结构重参数化
基于神经网络的微观三维形貌重建模型使用更深更宽或多分支的卷积层抽取高维特征矩阵,可有效增加
神经网络模型的特征表达能力.在三维形貌重建过程中,单分支串型结构易于陷入局部最优解,而多分支并行结
构则额外增加内存访问成本.因此,本文在训练时采用多分支结构,推理时将多分支等价转换为单分支结构,通
过网络结构重参数化实现优势互补.具体操作如下:对于卷积核大小为 K 的卷积层而言,输入通道的维度是 Cin ,

2212

Journal of Software 软件学报 Vol.32, No.7, July 2021
2

输出通道的维度是 Cout ,则卷积核的权重矩阵可以表示为 W ′ ∈ � Cin ×Cout × K , 偏置项 b′ ∈ � D .归一化层中包含累积
得到的均值 µ ,标准差 α 和学到的缩放因子 γ 以及偏置项 β .由于卷积层和归一化层均属于线性运算.因此在推
ˆ ( b − µ ) γ / α .神
理时可合并为一个新卷积操作.在新卷积层中,卷积核权重可以表示为 Wˆ = W ′γ / α ,偏置项为 b=
经网络中多分支结构组成的多角度特征空间有利于挖掘三维形貌重建过程的最大聚焦点.

4 实验分析
4.1 实验设置
本文 Micro 3D 数据集共采集 488 组 20 张大小为 256×256 的多景深微观图像序列,并通过聚焦形貌测量
构建初始形貌、激光逐点扫描细化形貌和手工微修场景先验深度等步骤生成标准的三维形貌图作为监督信息,
其中部分数据集形貌如图 7 所示.

图7

Micro 3D 数据集中部分数据

本文使用 382 组多聚焦微观图像序列训练网络模型,训练过程使用图像旋转、场景翻转、伽马变换和区域
裁剪等数据增广方式.多聚焦图像序列作为网络模型的输入,三维形貌图作为网络模型的监督信息,在 106 组数
据进行测试,共进行 720 次迭代训练.在训练过程中使用 Adam 优化器,初始学习率设置为 0.001,批处理大小设置
为 4,其余参数皆为默认参数.本文使用 Pytorch 框架搭建网络模型,采用 TITAN XP GPU 训练并测试 GPLWS-Net
与 其 他 网 络 模 型 的 性 能 . Micro 3D 数 据 集 和 网 络 模 型 GPLWS-Net 已 公 布 在 GitHub:
https://github.com/jiangfeng-Z/Multi-focus_Microscopic_3D_reconstruction_Datasets.
4.2 消融实验
为进一步验证本文 GPLWS-Net 网络模型结构的合理性,消融实验中主要使用公共数据集 4D Light Field[33]
对消融模型进行交叉验证,从模型性能(MSE)、模型参数和延迟进行多角度分析.为避免设备自身算力波动对实
验结果的影响,消融实验在同一环境下重复验证 50 次并计算其均值.以下分别从残差连接与跳跃连接、分组并
行模块与结构重参数化三个模块的有效性展开.
根据 U 型主干网络中相同尺度特征连接方式的不同,分别使用“拼接式”的跳跃连接和“相加式”的残差
连接进行对比,表 1 结果表明:跳跃连接和残差连接两种连接方式并不影响模型的性能,但残差连接的网络延迟
显著优于跳跃连接,由于跳跃连接之后网络变宽与卷积核参数增加,使得运算量激增导致网络预测时间增加.
表1
连接方式
跳跃连接
残差连接

模型参数/个
546321
540273

残差连接与跳跃连接消融实验结果
延迟/s
2.419
0.437

MSE
0.0275
0.0275

分组并行模块的消融实验如表 2 所示,根据 U 型网络收缩模块和扩张模块的连接方式不同可分为“拼接
式”和“相加式”,其中又分别将常规卷积、深度可分离卷积和分组卷积进行对比,因此共得到六组对比实
验.消融模型 1 和消融模型 4 对比可知,相加连接方式不仅有助于提升模型精度,而且可降低网络预测延迟;消
融模型 4 和消融模型 5 对比可知:深度可分离卷积尽管可以降低网络参数量,但由于分组数过多导致内存访
问频繁,进而导致网络预测时长增加;消融模型 5 和消融模型 6 对比可知:分组并行模块仅牺牲部分存储空间,
但可有效降低网络延迟,并保证网络预测精度.因此,本文采用消融模型 6 中分组并行模块和相加连接方式.

2213

闫涛 等:一种分组并行的轻量化实时微观三维形貌重建方法

表2
消融模型
1
2
3
4
5
6

基础模块
常规卷积
深度可分离卷积
分组并行模块
常规卷积
深度可分离卷积
分组并行模块

分组并行模块消融实验结果
模型参数/个
1213872
200400
444048
754020
106344
260853

模型性能 MSE
0.0516
0.0473
0.0534
0.0468
0.0479
0.0372

延迟/s
0.457
0.463
0.399
0.353
0.394
0.350

连接方式
拼接

相加

结构重参数化的消融实验结果如表 3 所示,使用与推理阶段相同配置的普通卷积层代替结构重参数化模
块,结果表明在延迟不发生改变的情况下,使用结构化重参数的模型参数量更低,性能更佳.
表3
卷积方式
结构重参数化
常规卷积

结构重参数化与常规卷积消融实验结果

模型参数/个
540273
540936

MSE
0.0275
0.0339

延迟/s
0.437
0.437

4.3 实验分析
本节根据应用场景的差异分为对比实验、泛化实验和延时实验三类,分别对不同三维形貌重建方法进行
定量对比和定性分析,其中对比实验通过对公共数据集学习验证网络模型的有效性,泛化实验可验证本文数
据集在微观领域提出的必要性,而延时实验则可明确对比各模型在实际应用场景的效率.
本节主要选择 4D Light Filed [33]、DefocusNet[7]、FlyingThing3D[33]、Middlebury[33]和本文的 Micro 3D
数据集进行测试.通过对比先进的模型设计类三维形貌重建模型 RDF [15] 和 RR[19]与基于深度学习的三维形貌
重建模型(DDFF [6] 、DefocusNet[7] 、AiFDepthNet[8] 、FVNet[5] 、DFVNet[5] )评估本文提出数据集 Micro 3D 数
据集和 GPLWS-Net 算法的性能.其中 4D Light Field 数据集中包含了 25 种复杂现实场景,主要用于测试精细
结构与弱纹理及光滑表面等情形;DefocusNet 数据集构建浮点级深度信息,主要验证网络模型对于复杂场景
的拟合情况;FlyingThing3D 数据集共公布 1000 组三维场景数据.
实验中使用均方误差 MSE(mean squared error)、平均绝对误差 MAE(mean absolute error)、均方根误差
RMSE(root mean squared error)、绝对相关系数 AbsRel(absolute relative error)、平方相对误差 SqRel(square
relative error)、颠簸性(bumpiness)和推理时间(secs)指标定量评估 GPLWS-Net 与其他三维形貌重建模型的
性能.
4.3.1 对比实验分析
本节将 GPLWS-Net 与现有的深度学习类三维形貌重建方法进行对比,为确保实验结果的公正客观,本节
在公共数据集 4D Light Field 与 DefocusNet 上进行网络模型的性能分析.具体定量指标对比如表 4 所示,其中
部分数据来自论文 [8,33] .由表 4 可知,本文提出的 GPLWS-Net 模型在两类数据集上的重建精度显著优于其他
算法.(注:性能最优用红色加粗字体标注,次优用蓝色加粗字体标注.)
表4
Methods

Datasets/Metrics

FVNet[5]
DFVNet[5]
DDFF[6]
DefocusNet[7]
AiFDepthNet[8]
GPLWS-Net

MSE
0.0301
0.0317
0.1150
0.0593
0.0472
0.0275

4D Light Field 数据集和 DefocusNet 数据集的定量评价
4D Light Field[33]
RMSE
0.1537
0.1549
0.3310
0.2355
0.2014
0.1442

Bump
2.95
2.69
1.58
2.49

MSE
0.0189
0.0205
0.0440
0.0175
0.0127
0.0106

DefocusNet [7]
MAE
0.1312
0.0637
0.0549
0.0534

AbsRel
0.1400
0.1300
0.3556
0.1386
0.1115
0.1394

4D Light Field 可验证各模型对于精细结构的三维形貌重建.由图 8 所示:DDFF 模型和 DefocusNet 模型仅
能分辨场景的相对深度,无法重建场景内的精细结构;AiFDepthNet 模型无法有效保持深度图的边缘细节,深度
信息容易弥散;DFVNet 模型和 FVNet 模型对于场景内富纹理背景处理不佳.本文提出的 GPLWS-Net 模型在精
细结构的表达和聚焦区域的判断方面表现良好,例如场景一的绳结和场景二的鞋身.

2214

Journal of Software 软件学报 Vol.32, No.7, July 2021

图8

4D Light Field 数据集的定性比较

由表 5 可以看出,本文的 GPLWS-Net 模型在 FlyingThing3D 数据集上与 FVNet、DFVNet、DDFF 和
AiFDepthNet 相比在所有评价指标方面均有较大提升,其中 AiFDepthNet 在各项指标中也得到了第二优的结
果.(注:由于 DefocusNet 未公布其训练模型和网络代码,因此下列表单中未列出该网络的定量评估结果以及预
测图.)
表5
Methods

Dataset/Metrics

FVNet[5]
DFVNet[5]
DDFF[6]
AiFDepthNet[8]
GPLWS-Net

FlyingThing3D 数据集定量评估

MAE
27.442
27.951
17.182
6.838
6.053

RMSE
35.406
36.098
27.077
12.247
11.045

FlyingThings3D

AbsRel
2.388
2.334
1.654
0.666
0.631

SqRel
78.952
79.705
45.132
8.788
7.575

FlyingThing3D 数据集可验证各模型对于复杂遮挡场景的三维形貌重建.由图 9 可知,DDFF 模型仅能分辨
场景的前后关系,无法识别场景的语义关系,且易发生聚焦扭曲;DFVNet、FVNet 和 AiFDepthNet 模型可表达场
景的语义关系,但易受到场景纹理信息的干扰,无法捕获更加丰富的层次信息.而本文的 GPLWS-Net 模型在三
维形貌的边缘保持和遮挡重叠区域有较好的重建效果.

图9

本文 GPLWS-Net 与其他四类网络在 FlyingThing3D 数据集上的深度预测

4.3.2 泛化实验分析
本节使用 FlyingThings3D 数据集训练本文 GPLWS-Net 与其他四种深度学习模型,并在 Middlebury 数据集、
DefocusNet 数据集和 4D Light Field 数据集上进行测试,用以定量分析本文模型的泛化性能.如表 6 所示,除在
DefocusNet 数据集中的 AbsRel 和 SqRel 指标外,本文 GPLWS-Net 模型在三类测试集中的其他指标均能保持最
优性能.
表6
模型
FVNet[5]
DFVNet[5]
DDFF[6]
AiFDepthNet[8]
GPLWS-Net

训练集

测试集

FlyingThings3D

Middlebury

跨不同数据集的定量结果
MAE
11.276
11.407
32.499
3.825
2.539

MSE
177.649
183.267
1480.444
58.570
17.497

RMSE
13.147
13.302
37.544
5.936
4.062

AbsRel
0.360
0.344
1.197
0.165
0.100

SqRel
5.011
4.711
52.169
3.039
0.642

2215

闫涛 等:一种分组并行的轻量化实时微观三维形貌重建方法

FVNet[5]
DFVNet[5]
DDFF[6]
AiFDepthNet[8]
GPLWS-Net
FVNet[5]
DFVNet[5]
DDFF[6]
AiFDepthNet[8]
GPLWS-Net

FlyingThings3D

DefocusNet

FlyingThings3D

4D Light Field

0.271
0.271
89.351
0.183
0.126
1.485
1.352
94.106
0.205
0.021

0.144
0.152
9305.360
0.080
0.053
3.053
2.432
9806.356
0.106
0.042

0.353
0.360
95.214
0.261
0.234
1.704
1.552
95.715
0.313
0.036

0.555
0.529
331.124
0.725
2.011
2.421
1.780
77.899
0.198
0.010

0.198
0.204
3357.152
0.404
1.826
5.676
3.377
7464.454
0.151
0.036

图 10 使用本文公开的 Micro 3D 数据集进行训练, 并在印辊微观场景(不同于本文的 Micro 3D 数据集)验
证其泛化性,由此说明本文数据集的必要性和泛化性.其中 RR、AiFDepthNet 和 FVNet 方法易受离散噪声干扰,
对于场景的微细纹理变化敏感; RDF、DDFF 和 DFVNet 方法都具备对噪声的抗干扰能力,但深度边缘保持不佳;
而本文提出的 Micro 3D 数据集及 GPLWS-Net 模型可有效适应未知的微观场景,具有良好的抗噪性和鲁棒性.

图 10

模型设计与深度学习类方法以及本文的 GPLWS-Net 在三组微观印辊数据集下的泛化性对比.

4.3.3 延时实验分析
本节将设计不同图像分辨率和采样频率的数据验证各模型的效率.图像分辨率中选定 256×256、512×
512、600×800 和 540×960 测试各模型的尺度性能,采样频率则是从 5-100 等间隔验证各模型在不同采样频率
的性能.由表 7 可以看出:相比于其他深度网络模型,本文提出的 GPLWS-Net 模型具有显著的速度优势,未来通
过 GPU 加速可大幅降低运算时间,具备了工业化应用实时三维形貌重建的条件.
表7

GPLWS-Net 与其他网络在不同分辨率下的延时对比

描述
测试数据集

焦点
堆栈

分辨率

DefocusNet

5

256×256

4D Light Field

10

512×512

FlyingThings3D

15

540×960

Micro 3D

10

600×800

模型

时间(s)

降低百分比↓

CPU

FVNet[5]
DFVNet[5]
DDFF[6]
DefocusNet[7]
AiFDepthNet[8]
GPLWS-Net
FVNet[5]
DFVNet[5]
DDFF[6]
DefocusNet[7]
AiFDepthNet[8]
GPLWS-Net
FVNet[5]
DFVNet[5]
DDFF[6]
DefocusNet[7]
AiFDepthNet[8]
GPLWS-Net
FVNet[5]

3.100
3.020
4.491
5.906
4.405
2.617
14.330
13.850
23.489
18.624
11.987
9.498
42.614
42.776
79.165
70.076
64.441
30.440
39.580

15.56%
13.34%
41.73%
55.69%
40.60%
/
33.71%
31.42%
59.57%
46.59%
20.77%
/
28.56%
28.83%
61.55%
56.57%
52.77%
/
55.02%

Intel(R) Xeon(R)
Silver 4210 CPU

2216

Journal of Software 软件学报 Vol.32, No.7, July 2021
DFVNet[5]
DDFF[6]
DefocusNet[7]
AiFDepthNet[8]
GPLWS-Net

5 总

36.980
38.780
34.328
31.567
17.800

51.87%
54.11%
48.15%
43.62%
/

结

微观三维形貌重建作为微纳级显微设备的核心技术,可对精密制造领域的数据建模、产品加工以及质量控
制的全链条环节提供保障.而现有的三维形貌重建方法无法应对微观场景中的高分辨率稠密数据的处理,给实
时微观三维形貌重建带来挑战.本文从多聚焦图像序列特有的聚焦曲线连续性特点出发,分割一维时序景深数
据进行多分支并行,通过网络结构的重参数化保障重建精度,可有效兼顾网络的效率与精度,为微观三维形貌重
建方法的多场景部署应用提供解决思路.除此之外,本文公开的微观三维形貌数据集 Micro 3D,可有效缓解现阶
段微观领域数据集缺失的问题,为设计高效的深度网络提供数据基础.未来研究主要从标签数据的自动标注和
微观三维重建大模型的设计方面展开.
References:
[1]

Huang B, Wang W, Bates M, Zhuang X. Three-dimensional super-resolution imaging by stochastic optical reconstruction

[2]

Nayar S, Nakagawa Y. Shape from focus. IEEE Trans. on Pattern Analysis and Machine Intelligence, 1994,16(8):824–831.

[3]

Yan T, Qian YH, Li FJ, et al. Intelligent microscopic 3D shape reconstruction method based on 3D time-frequency transformation.

microscopy. Science, 2008,319(5864):810–813.

Sci Sin Inform, 2023,53:282-308. (in Chinese with English abstract). [doi: 10.1360/SSI-2021-0386]
[4]

Zhang JF, Yan T, Wang KQ, Qian YH, Wu P. 3D shape reconstruction from multi depth of filed images: datasets and models.

[5]

Yang F, Huang X, Zhou Z. Deep depth from focus with differential focus volume. In: Proc. of the IEEE/CVF Conf. on Computer

Chinese journal of computers, 2023,46(8):1734-1752. (in Chinese with English abstract). [doi: 10.11897/SP.J.1016.2023.01734]
Vision and Pattern Recognition. 2022. 12642-12651.
[6]

Hazirbas C, Soyer SG, Staab MC, et al. Deep depth from focus. In: Proc. of the Asian Conference on Computer Vision. 2018:

[7]

Maximov M, Galim K, Leal-Taixé L. Focus on defocus: bridging the synthetic to real domain gap for depth estimation. In: Proc. of

525-541.
the IEEE/CVF Conf. on Computer Vision and Pattern Recognition. 2020:1071-1080.
[8]

Wang NH, Wang R, Liu YL, et al. Bridging unsupervised and supervised depth from focus via all-in-focus supervision. In: Proc. of
the IEEE/CVF Conf. on Computer Vision and Pattern Recognition. 2021:12621-12631.

[9]

Howard AG, Zhu M, Chen B et al. MobileNets: Efficient convolutional neural networks for mobile vision applications. arXiv
Preprint arXiv:1704.04861, 2017.

[10]

Zhang X, Zhou X, Lin M, Sun J. ShuffleNet: An extremely efficient convolutional neural network for mobile devices. In: Proc. of

[11]

Han K, Wang Y, Tian Q, Guo J, Xu C. GhostNet: More features from cheap operations. In: Proc. of the IEEE/CVF Conf. on

the IEEE/CVF Conf. on Computer Vision and Pattern Recognition. 2018:6848-6856.
Computer Vision and Pattern Recognition. 2020:1577-1586.
[12]

Mehta, S, Mohammad R. MobileViT: Light-weight, general-purpose, and mobile-friendly vision transformer. arXiv Preprint
arXiv:2110.02178, 2021.

[13]

Lee JY, Park RH. Complex-valued disparity: Unified depth model of depth from stereo, depth from focus, and depth from defocus
based on the light field gradient. IEEE Trans. on Pattern Analysis and Machine Intelligence, 2021,43(3):830–841.

[14]

Muhammad M, Choi TS. Sampling for shape from focus in optical microscopy. IEEE Trans. on Pattern Analysis and Machine

[15]

Jeon HG, Surh J, Im S, et al. Ring difference filter for fast and noise robust depth from focus. IEEE Trans. on Image Processing,

Intelligence, 2012,34(3):564–573.
2020,29:1045-1060.
[16]

Yan T, Hu Z, Qian, YH, Qiao ZW, Zhang LY. 3D shape reconstruction from multifocus image fusion using a multidirectional
modified laplacian operator. Pattern Recognition, 2020,98:107065.

闫涛 等:一种分组并行的轻量化实时微观三维形貌重建方法

[17]

2217

Yan T, Wu P, Qian YH, Hu Z, Liu FX. Multiscale fusion and aggregation pcnn for 3D shape recovery. Information Sciences, 2020,
536:277-297.

[18]

Minhas R, Mohammed AA, Wu QM. Shape from focus using fast discrete curvelet transform. Pattern Recognition, 2011,44(4):

[19]

Ali U, Muhammad TM. Robust focus volume regularization in shape from focus. IEEE Trans. on Image Processing, 2021,30:

839-853.
7215-7227.
[20]

Moeller M, Benning M, Schnlieb C, Cremers D. Variational depth from focus reconstruction. IEEE Trans. on Image Processing,

[21]

Hu, J, Li S, Gang S. Squeeze-and-excitation networks. In: Proc. of the IEEE/CVF Conf. on Computer Vision and Pattern

2015,24(12):5369–5378.
Recognition. 2018:7132-7141.
[22]

Daquan Z, Hou Q, Chen Y, et al. Rethinking bottleneck structure for efficient mobile network design. arXiv Preprint
arXiv:2007.02269, 2020.

[23]

F Chollet. Xception: Deep learning with depthwise separable convolutions. In: Proc. of the IEEE Conf. on Computer Vision and
Pattern Recognition. 2017:1800-1807.

[24]

Zhang T, Qi GJ, Xiao B, Wang J. Interleaved group convolutions for deep neural networks. arXiv Preprint arXiv:1707.02725,2017.

[25]

Tan M, Le QV. EfficientNet: Rethinking model scaling for convolutional neural networks. arXiv Preprint arXiv:1905.11946,2019.

[26]

Han K, Wang Y, Zhang Q, Zhang W, Zhang T. Model rubik’s cube: Twisting resolution, depth and width for TinyNets. arXiv
Preprint arXiv:2010.14819,2020.

[27]

Ma N Zhang X, Zheng H, Sun J. ShuffleNet V2: Practical guidelines for efficient CNN architecture design. In: Proc. of the
European Conference on Computer Vision. 2018:122-138.

[28]

Andrew H, Mark S, Grace C, et al. Searching for MobileNetV3. In: Proc. of the IEEE/CVF International Conference on Computer
Vision. 2019:1314-1324.

[29]

Chen J, Kao S, He H, Zhuo W et al. Run, Don’t Walk: chasing higher FLOPS for faster neural networks. In: Proc. of the IEEE/CVF

[30]

Pavan K, Vasu A, Gabriel J, et al. MobileOne: An improved one millisecond mobile backbone. In: Proc. of the IEEE/CVF Conf. on

Conf. on Computer Vision and Pattern Recognition. 2023.
Computer Vision and Pattern Recognition. 2023.
[31]

Chen Y, Dai X, Chen D, Liu M, et al. Mobile-Former: Bridging MobileNet and transformer. In: Proc. of the IEEE/CVF Conf. on
Computer Vision and Pattern Recognition. 2022:5260-5269.

[32]

Pentland AP. A new sense for depth of field. IEEE Trans. on Pattern Analysis and Machine Intelligence, 1987,4:523–531.

[33]

Won C, Jeon H. Learning depth from focus in the wild. In: Proc. of the European Conference on Computer Vision. 2022,1-18.

[34]

Krizhevsky A, Sutskever I, Hinton G. ImageNet classification with deep convolutional neural networks. Communications of the
ACM, 2017,60(624):84-90.

附中文参考文献:
[3]

闫 涛 , 钱 宇 华 , 李 飞 江 , 等 . 三 维 时 频 变 换 视 角 的 智 能 微 观 三 维 形 貌 重 建 方 法 . 中 国 科 学 : 信 息 科 学 ,2023,53:282-308. [doi:
10.1360/SSI-2021-0386]

[4]

张江峰,闫涛,王克琪,钱宇华,吴鹏.多景深图像聚焦信息的三维形貌重建:数据集与模型.计算机学报,2023,46(8):1734-1752. [doi:
10.11897/SP.J.1016.2023.01734]


软件学报 ISSN 1000-9825, CODEN RUXUEW
Journal of Software, [doi: 10.13328/j.cnki.jos.007117]
©中国科学院软件研究所版权所有.

E-mail: jos@iscas.ac.cn
http://www.jos.org.cn
Tel: +86-10-62562563

一种利用动态分析的嵌入式应用托管方案

∗

吴华茂 1, 姜木慧 2, 周亚金 1, 李金库 3
1

(浙江大学 计算机科学与技术学院,浙江 杭州

2

(香港理工大学 电子计算学系,香港)

3

(西安电子科技大学 网络与信息安全学院,陕西 西安 710126)

310027)

通讯作者: 周亚金, E-mail: yajin_zhou@zju.edu.cn

摘 要:

固件托管(Firmware Rehosting)是一种对嵌入式设备的软硬件进行建模和仿真,并在仿真环境中运行和分

析嵌入式设备软件的技术. 现有的基于全系统仿真的固件托管方案只能预防性地修复已知的软硬件依赖问题,而
无法解决未知的问题.为应对这一现状,提出了一种由动态分析辅助的固件托管方案 FirmDep.在托管过程中,该应用
记录被分析的嵌入式设备应用的执行轨迹和系统状态.若目标应用无法被成功托管,FirmDep 对执行轨迹进行信息
提取和系统状态补全,并使用多种执行轨迹分析方法识别和仲裁应用的环境依赖问题.基于 PANDA 和 angr 实现了
FirmDep 的原型系统,并使用 217 个来自真实设备固件的嵌入式 Web 应用对其进行了测试.结果表明 FirmDep 可有
效识别嵌入式设备应用的环境依赖问题,提高固件托管的成功率.
关键词: 嵌入式设备;固件;动态分析;固件托管;录制重放
中图法分类号: TP311
中文引用格式: 吴华茂,姜木慧,周亚金,李金库, 一种利用动态分析的嵌入式应用托管方案.软件学报.
http://www.jos.org.cn/1000-9825/7117.htm
英文引用格式: Wu HM, Jiang MH, Zhou YJ, Li JK. FirmDep: Embedded Application Rehosting Assisted with Dynamic Analysis.
Ruan Jian Xue Bao/Journal of Software (in Chinese). http://www.jos.org.cn/1000-9825/7117.htm

FirmDep: Embedded Application Rehosting Assisted with Dynamic Analysis
WU Hua-Mao1, JIANG Mu-Hui2, ZHOU Ya-Jin1, LI Jin-Ku3
1

(College of Computer Science and Technology, Zhejiang University, Hangzhou 310027, China)

2

(Department of Computing, The Hong Kong Polytechnic University, Hong Kong, China)

3

(School of Cyber Engineering, Xidian University, Xi’an 710126, China)

Abstract:

Through providing a virtual environment modeled from embedded devices, firmware rehosting enables dynamic analysis on

embedded device firmware. Existing full-emulation firmware hosting solutions can only preventatively fix known hardware and software
dependencies but cannot address undetected dependencies during the rehosting process. This paper proposes FirmDep, an embedded
application rehosting solution assisted with dynamic analysis. During the rehosting process, FirmDep records the execution trace and
system state of the embedded application to be analyzed. If FirmDep fails to rehost the application, FirmDep extracts information and
recover system states from the execution trace, then uses several algorithoms to identify and arbitrate the unresolved dependency problems.
We implemented the prototype system of FirmDep based on PANDA and angr, and tested it with embedded Web applications from 217
real-world firmware images. The results show that FirmDep can effectively identify unresolved dependencies of embedded application and
improve the success rate of rehosting.
Key words:

embedded device; firmware; dynamic analysis; firmware rehosting; record and replay

近年来,随着物联网技术的发展,嵌入式设备的种类和数量都在逐步增长.这些设备不仅被嵌入到各类复杂
的系统中,例如工控系统、汽车、飞机等,而且还广泛应用于办公场所、公共区域以及个人家居中.然而,由于研
∗

基金项目:国家重点研发计划(No. 2022YFE0113200),国家自然科学基金重点项目(U21A20464)

收稿时间: 2023-09-10; 修改时间: 2023-10-30; 采用时间: 2023-12-15; jos 在线出版时间: 2024-01-05

吴华茂 等: 一种利用动态分析的嵌入式 Linux 应用托管方案

2205

发周期和成本限制等原因,嵌入式设备的研发方常常未能为它们提供足够安全的软件.根据 Yu 等人对嵌入式设
备固件进行的大规模分析 [1],他们的数据集中仅有 29.7%的二进制文件包含了用于抵御栈溢出攻击的 Stack
Canary,尽管该安全特性早已被 GCC 等编译工具链支持并默认启用.
嵌入式设备,尤其是物联网设备的软件安全问题已引起学术界和工业界的关注.由于架构与性能差异、硬
件交互能力不足以及普遍缺乏软件源码等原因,研究者往往难以直接使用被广泛应用于桌面和服务器系统的
安全分析方案分析这些设备,需要特别的方法或辅助手段 [2] .固件托管(Firmware Rehosting)即是一种用于支持
动态分析的技术.简单来说,此种技术通过为嵌入式设备的固件提供仿真环境来实现在原硬件以外的环境运行
嵌入式设备的软件.在此基础上,研究人员可以使用常见的动态分析手段,例如应用漏洞扫描和模糊测试等,对
固件进行分析.与直接测试真实设备相比,此种方法不需要真实的设备硬件,且在使用模糊测试等方法进行测试
时不会受限于原设备的性能水平.
实现固件托管方案面临的最大挑战之一,在于如何解决嵌入式设备固件的运行环境依赖问题.运行于嵌入
式设备的软件通常被设计为运行于特定的硬件平台环境,并依赖特定的硬件外设.若这些软件未能在与硬件交
互时获取其期望的响应,则将无法正常工作.现有的固件托管方案主要通过两种方式来解决这一问
题:Firmadyne[3] 、FirmAE[4] 等基于全系统仿真的固件托管方案通过尽可能收集并预防性地修复可能存在的硬件
依赖问题(例如 NVRAM 闪存的访问等)来提供一个兼容性尽可能高的虚拟运行环境;Avatar[5]、FIRMCORN[6]
等由硬件辅助的固件托管方案则通过将采集自真实硬件的互动结果反馈到仿真环境中的方式来解决环境依赖
问题.这两类方案各有其优劣性:现有的基于全系统仿真的方案能支持较多嵌入式设备固件,但此类方案只能解
决一些已知的硬件依赖问题.对于未被该方案支持的硬件依赖,则需要用户手动解决.由于嵌入式设备固件往往
只提供二进制文件,且为减少体积去除了符号等便于调试的要素,因此对这些软件的修复通常需要耗费大量精
力,并要求用户具备足够的经验.由硬件辅助的方案可为被分析的嵌入式固件提供更为真实的运行环境,然而,
此类方案的硬件支持有限,通常也需要用户对原始硬件有足够的了解,难以应用到大规模固件分析等场景中.
基于当前研究现状,本文提出了一种名为 FirmDep 的固件托管方案.现有的固件托管方案只对已知的软硬
件问题制定了预防措施,但缺乏发现和处理新问题的机制.为识别嵌入式应用的环境依赖问题,FirmDep 使用了
一种基于录制和回放的分析方法.具体而言,FirmDep 在尝试托管嵌入式应用的同时记录该应用的执行,以获取
执行轨迹及用于还原仿真执行环境系统状态的执行回放.若该嵌入式应用未能被成功托管,FirmDep 将对托管
过程中记录的信息进行提炼和补全,使用执行轨迹分析算法从中提取出导致托管失败的错误根源,并制定对应
的仲裁方案.此后,FirmDep 将应用仲裁方案,再次尝试托管.此过程将持续至目标应用被成功托管或没有更多的
仲裁方案可用. 对于可直接顺利托管或直接修复的嵌入式应用,FirmDep 将为其生成一个虚拟机快照,并在此
后的用户使用过程中禁用对仿真执行环境的记录或干涉,消除对用户的安全分析应用性能的影响。
为实现上述托管方案,需要解决一系列问题,这些问题包括:1)如何有效且高效地记录被托管嵌入式应用的
行为;2)如何从记录的信息中提取应用的行为及其还原系统状态以供分析;3)如何从被托管的执行轨迹中识别
导致托管失败的故障根源,以及 4)如何对被托管应用的环境依赖问题进行仲裁.针对 1),FirmDep 采用了一种按
需记录的方式来记录应用的执行轨迹,并为托管过程生成可用于还原任一时刻系统状态的执行回放.针对
2),FirmDep 可基于执行轨迹生成函数调用记录以实现应用高级语义行为的提取,且可配合执行回放还原执行
轨迹中每个基本块前后的系统状态.针对 3),本文为 FirmDep 实现了四种执行轨迹分析算法以识别目标应用在
不同运行状态的错误根源.针对 4),FirmDep 的仿真执行环境提供了干预客户机执行的能力,除调整网络配置、
执行自定义指令等仲裁方案外,还支持通过改变系统状态来影响目标应用的执行.
本文基于 PANDA 与 angr 实现了适用于 32 位 ARM 平台固件的 FirmDep 原型,并使用了来自 8 个厂商的
217 个设备固件对 FirmDep 的有效性进行了评估.据实验结果,FirmDep 在应用了仲裁措施后,成功托管了 147
个(67.7%)嵌入式 Web 应用,优于基于现有经验的现有方案 FirmAE(托管 113 个,成功率 52.1%)及 Firmadyne(托
管 1 个,成功率 0.4%).在此过程中,FirmDep 识别出了 95 个嵌入式 Web 应用的环境依赖问题,并成功修复了其中
的 81 个,有效提高了托管成功率.为验证 FirmDep 是否适用于安全分析场景,本文对成功托管的嵌入式 Web 应

2206

Journal of Software 软件学报 Vol.32, No.7, July 2021

用使用 w3af 和 RouterSploit 进行了漏洞扫描测试,分别检测出 69 个和 9 个有安全隐患的 Web 应用.此外,对于
未能托管的 Web 应用,本文还分析了托管失败的原因.
本文第 1 节介绍本工作的背景知识,第 2 节介绍 FirmDep 固件托管方案的整体设计,第 3 节和第 4 节分别
介绍 FirmDep 的框架设计及分析算法设计,第 5 节介绍 FirmDep 框架的一些实现细节,第 6 节对 FirmDep 的有
效性进行评估,第 7 节讨论本文提出方案的不足之处和可能的改进空间,最后总结全文.

1 相关工作及研究动机
1.1 相关工作
固件托管是一种围绕某个固件镜像,通过仿真运行所必需的硬件进行来实现固件在虚拟环境中高效运行
的技术.根据 Muench 等人的定义[7],嵌入式设备大致可被分为以下三类: (1) I 型设备,通常配备带有 MMU 的
SoC 封装的处理器,运行通用操作系统(如 Linux),应用通过设备驱动与硬件交互;(2) II 型设备,此类设备通常配
备无 MMU 的 MCU 处理器,运行专用操作系统(如 VxWorks),应用程序与软件交互仍然有一层隔离;(3) III 型设
备,此类设备不具备操作系统.此三类设备由于软件、硬件复杂度不同,因此需要通过不同的方式实现固件托管.
本文主要关注运行于 I 型设备上的嵌入式应用的托管.2016 年,Costin 等人[8] 通过网络爬虫收集了大量嵌入
式设备的固件,通过使用 Qemu 和 chroot 创建较为基础的仿真执行环境,实现了对嵌入式设备包含的 Web 服务
的托管.此种方案通过使用全系统仿真解决了嵌入式设备软件对指令集的依赖问题,但却未解决硬件外设交互
等依赖问题,因此托管成功率并不高.同年,Chen 等人也提出了一种针对嵌入式设备固件的大规模固件托管方
案 Firmadyne[3],该方案通过对关键系统调用及个别 API 函数调用进行拦截,缓解了嵌入式设备软件对网络配置
及 NVRAM 硬件的依赖问题.一些研究在 Firmadyne 的基础上实现了多种分析平台,例如用于对嵌入式设备软
件 进 行 模 糊 测 试的 FirmAFL[9] 、FirmFuzz[10] 平 台 等.为 进 一 步 缓 解 Firmadyne 无 法 解 决 的 软 硬 件 依 赖问
题,FirmAE[4]提出了仲裁式仿真(Arbitrated Emulation)这一概念,即通过干涉嵌入式应用程序与系统、硬件的交
互来解决其软硬件依赖问题.FirmAE 分析了 Firmadyne 托管失败的原因,并通过加入一些额外的修复,进一步提
升了固件托管的成功率.
除了从软件层面采取措施外,一些固件托管方案选择通过优化固件托管过程的硬件仿真来解决嵌入式设
备固件的环境依赖问题. 此类方案可被进一步分为两类:一类方案通过分析嵌入式设备固件和与其交互的物
理外设的联系,对其依赖的硬件外设进行抽象和建模,生成用于替代真实硬件的软件组件.例如,HALucinator[11]
通过劫持嵌入式设备固件对硬件抽象层(Hardware Abstraction Layer)的 API 调用来解决固件的外设依赖问
题;P2IM[12]通过对 MMIO 操作进行建模来仿真通过 MMIO 连接的外设;FirmGuide[13]和 ECMO[14]分别通过为仿
真器生成和替换外设的方式实现 I 型设备 Linux 内核的托管.另一类方案在托管的流程中加入真实硬件的辅助,
当嵌入式设备软件需要与硬件发生交互时,通过采集于硬件设备的响应,并反馈给嵌入式设备软件来解决这些
软件的环境依赖问题.例如,Avatar[5]通过使用 UART 和 JTAG 总线将仿真器无法处理的硬件 I/O 请求转发到真
实硬件上处理,并将结果反馈到仿真器中;Pretender[15] 则基于嵌入式设备固件与物理外设之间的交互记录通过
机器学习为每一个外设训练一个外设模型,并将该模型应用到仿真器中.此类方案并不局限于 II 类设备,例如,
FIRMCORN[6]通过同步 CPU 模拟器与真实硬件上的软件的上下文状态, 实现对 I 型设备嵌入式设备软件的高
效模糊测试.
1.2 研究动机
实现一个有效的固件托管方案的关键在于解决嵌入式设备固件的环境依赖问题.现有的基于全系统仿真
的托管方案有一个共同的不足之处:嵌入式设备固件的运行环境依赖问题需要被固件托管框架的开发者手动
识别和解决.若托管框架中未集成某个依赖问题的仲裁措施,则固件的托管很可能会因此失败.由于嵌入式设备
固件往往难以获得源码,且为了减小体积常常还会去掉符号表等信息,修复运行于嵌入式设备的软件往往很困
难,需要研究者对设备有充分的了解.以常见的 NVRAM 依赖问题为例,为了减少对设备闪存的磨损,设备厂商常

吴华茂 等: 一种利用动态分析的嵌入式 Linux 应用托管方案

2207

常会选择将配置文件信息存放于 NVRAM 闪存中,由于不同设备的 NVRAM 硬件均有区别且通常不提供相关
参数资料,Qemu 等仿真 器很 难对其进行仿 真.现有的全系 统仿真方案对 该问题的 应对 方式是分析用 于与
NVRAM 交互的库文件,并使用库函数劫持来处理嵌入式设备软件对 NVRAM 的访问.若固件托管框架未支持
某个 NVRAM 库提供的 API,用户需要推断该 API 函数的行为并推断出返回值,并通过劫持该 API 函数或者是
改写嵌入式设备软件的二进制文件来解决.毫无疑问,这一过程具有门槛且耗时耗力.
与基于全系统仿真的固件托管方案相比,由硬件辅助的固件托管方案使用来自真实硬件的互动反馈来应
对固件与硬件的交互行为,在根源上解决了固件的硬件依赖问题.然而,当前大多数此类方案都只适用于 II 型和
III 型嵌入式设备的固件托管.像 FIRMCORN 这类方案虽然实现了对 I 型设备的软件托管,但是只适用于用户具
备同一型号硬件设备的情况,且需要对被分析的软件有足够的了解,限制了此类方案的应用.

2 方案设计
本文提出了一种使用研究 I 型设备被托管嵌入式设备应用行为的方法,并基于此方法,实现了一个名为
FirmDep 的嵌入式应用托管方案.针对 1.2 中阐述的托管嵌入式设备固件的挑战,FirmDep 记录被托管应用的行
为,并通过多种执行轨迹分析算法识别并仲裁目标应用被托管过程中遇到的环境依赖问题.
FirmDep 包含两个主要部件:仿真执行环境和执行轨迹分析器.其工作流程如图 1 所示.FirmDep 采用基于
chroot 的固件托管方案,在基于固件文件系统的仿真环境中使用预处理阶段提取出的启动参数运行需要被分析
的嵌入式应用.为识别和解决目标应用潜在的环境依赖问题,仿真执行环境在对嵌入式应用进行托管的同时,对
目标应用的执行进行记录.若目标应用被顺利托管且可被宿主机访问,仿真执行环境将生成一个快照,以便此后
进行模糊测试等安全分析应用;若目标应用未能被成功托管,FirmDep 将其执行轨迹传至执行轨迹分析器,并使
用多种执行轨迹分析算法识别出环境依赖问题并提出仲裁方案.在此过程中,执行轨迹分析器依照算法的需求,
从目标应用的托管过程进行还原和提炼.此后,仿真执行环境将应用仲裁措施,并再次尝试托管目标应用.这一
过程将持续至目标应用被成功托管或无更多的仲裁措施可用.

图1

FirmDep 的工作流程

运行在 I 型设备上的应用通常使用系统调用或动态链接库提供的 API 函数与环境交互,而非直接操作硬
件.因此,它的环境依赖问题通常会表现为异常返回的外部函数调用或系统调用.此外,当被托管的应用遇到环
境依赖问题时,其错误处理机制可能会导致程序进入不同于正常工作路径的执行路径.本文针对可能出现的异
常执行状态,制定了不同的分析算法,用于在执行轨迹中寻找可归咎的函数调用,并将其作为应用托管失败的根
源.
要实现上述的嵌入式应用托管方案,需要解决两个问题:1)如何高效记录嵌入式应用托管过程,并在执行轨
迹分析过程中还原应用运行时的系统状态;2)如何从应用的执行轨迹中识别出其遭遇的环境依赖问题.下文通
过介绍 FirmDep 的部件实现和执行轨迹分析算法,来解答这两个问题.

2208

Journal of Software 软件学报 Vol.32, No.7, July 2021

3 FirmDep 框架设计
3.1 仿真执行
FirmDep 的仿真执行环境需要满足三个功能:1)提供嵌入式应用的仿真执行环境;2)记录目标应用的执行
过程;3)提供应用托管过程中需要的仲裁措施,以缓解环境依赖问题.FirmDep 使用 PANDA 作为嵌入式应用的
仿真执行环境,该框架不仅可用于仿真嵌入式硬件环境,还提供了对客户机系统进行动态插桩的能力.FirmDep
在仿真执行环境的仿真硬件层和运行时层分别加载了额外的组件以获取获取执行轨迹和分析环境依赖问题所
需的其他信息.FirmDep 的仿真执行环境系统结构如图 2 所示.
PANDA[16] 是一个基于 Qemu[17]的全系统动态分析引擎,其为用户提供了两种层次的记录客户机内部执行
的能力.首先,PANDA 提供了录制/重放的功能,此功能起源于 Mozilla 的 rr 调试器项目[18], 该功能允许用户录制
程序在一段时间内的执行及对该回放进行重放,以方便对程序进行调试,其录制的原理是以一个包括完整寄存
器及内存状态的虚拟机快照为起点,在客户机运行过程中,记录录制期间执行过的基本块数量及发生的非决定
性输入,此类输入包括硬件中断、键盘输入和网络包等;重放则是通过加载快照,并精确重放所有非决定性事件
来实现.在重放过程中,若用户对回放进行暂停,即还原了回放记录当前点的系统状态.此外,PANDA 提供了一系
列的事件钩子,用户通过使用现成的插件或者执行编写插件并注册对应的事件钩子来在执行或重放过程中捕
获、记录特定事件或者是获取当前系统状态.

图2

FirmDep 仿真执行环境的系统结构

为了提取嵌入式设备应用的高级语义行为并从中找出导致托管失败的环境依赖问题,仿真执行环境记录
的执行轨迹需要满足两个需求:1)该记录需要包含足够的信息来还原函数调用记录,以便解析目标应用与外界
的互动;2)为了了解目标应用与外界交互的内容和结果,该记录需要提供还原任意点系统状态的能力,以满足对
函数调用参数和返回值进行解析的需求.PANDA 的两种记录客户机执行的方法若单独使用,均不能完全满足这
些要求.首先,PANDA 的录制/重放功能并不直接记录应用的执行,而且受限于其实现原理,重放过程只能按顺序
从头开始进行,无法直接读取任意点的状态;如果使用事件钩子记录托管过程中嵌入式应用每个时刻的完整系
统状态,则会带来巨大的性能开销和空间占用.因此,本文为 PANDA 实现了一个名为 Tracer 的插件,使用结合
了这两种方法的记录机制来记录嵌入式应用的托管过程.具体来说,当 Tracer 检测到与目标应用相匹配的新线
程时,它调 用 PANDA 的录 制 /重放功能开始对 托管过程 进 行录制,生成执行 回放文件.当目标应用运 行结
束,Tracer 播放执行回放文件并注册事件钩子,在目标应用的每个基本块执行前和执行后记录执行指令计数和

吴华茂 等: 一种利用动态分析的嵌入式 Linux 应用托管方案

2209

当前系统寄存器状态,生成基本块粒度的执行轨迹记录.通过配合使用执行轨迹记录和执行回放文件,使用者可
以还原执行轨迹记录中任意点的系统状态.
尽管在托管阶段不需要记录完整的系统状态,但由于应用运行时可能需要执行大量基本块,因此记录完整
执行轨迹仍会为仿真和分析带来很大的性能开销.此外,由于执行回放无法实现乱序读取,在分析阶段从执行回
放中提取信息可能有困难.为了缓解这些问题,FirmDep 使用一种按需记录的机制,在执行轨迹生成阶段先对基
本块进行过滤,并获取一些基本块记录以外的粗粒度信息,以减少对执行回放的需求.为了提高记录的效率和精
度的平衡,FirmDep 默认不生成目标应用所加载的外部链接库的执行轨迹,仅记录应用二进制文件本身的基本
块执行记录和应用的内存布局.这种按需记录方式足以还原目标应用的内部运行及外部 API 函数的调用行为,
大大减少了记录的基本块数量.此外,FirmDep 通过预加载共享链接库拦截常见的 POSIX API 函数和关键系统
调用,直接获取目标应用调用这些函数的返回结果、错误代码(errno)等信息,以减少在执行轨迹分析阶段对这些
信息的获取需求,提高分析效率.
除了具备记录客户机系统运行的能力,PANDA 还提供了对客户机运行的干预能力,FirmDep 利用 PANDA
的这些能力对被托管的嵌入式应用进行仲裁.具体来说,FirmDep 支持在托管目标应用之前调整网络设置和执
行自定义命令.此外,FirmDep 支持一种称为“强制执行”的仲裁措施,以处理某些难以解决的外部 API 调用带
来的问题.在托管目标应用的过程中,当系统状态满足一定条件时(例如即将执行某条指令),Tracer 将修改特定
寄存器或特定内存地址的值,以改变应用的执行路径.
3.2 执行轨迹分析
如果仿真执行环境无法成功托管嵌入式应用,FirmDep 的执行轨迹分析器将根据执行轨迹分析算法的要
求,对仿真执行环境产生的执行轨迹进行处理,并应用执行轨迹分析算法来查找目标应用的环境依赖问题.具体
来说,执行轨迹分析器首先在基本块粒度的执行轨迹上提取函数调用粒度的执行轨迹,以满足用户或者轨迹分
析算法对提取应用高级语义的需求.此外,在仿真执行轨迹中收集的系统状态信息仅包括执行基本块前后的寄
存器状态,无法满足某些执行轨迹分析算法的需求.针对这些需求,执行轨迹分析器通过重放托管过程的执行回
放来为执行轨迹中的每个基本块补全系统状态信息.
3.2.1

提取函数调用记录

为从执行轨迹中提取函数调用记录,执行轨迹分析器对执行轨迹中的基本块进行解析,对于每个基本块,识
别其执行前是否返回自某个函数调用,以及其最终是否进行了函数调用.对于 ARM 等主要使用寄存器传递参
数及返回值的架构而言,通过配合分析基本块代码的行为及基本块执行前后的寄存器状态,即可获得每个函数
调用所传递的参数指针及返回值.
FirmDep 使用 angr[19]完成执行轨迹中基本块的解析. Angr 是一个二进制文件的分析平台,支持对多种硬件
架构、多种格式的二进制文件的加载和反汇编、将二进制文件转换为易于分析的中间语言形式、动态符号执
行及控制流图、反向切片、约束求解等多种静态分析功能.为实现对执行轨迹基本块的解析，执行轨迹分析器
首先使用 angr 的 CLE 加载器加载被分析应用的二进制文件,并根据仿真执行环境记录的内存布局将目标应
用所依赖的动态链接库加载到被分析应用的内存模型中.然后,执行轨迹分析器通过调用 angr 的 PyVEX 模块
将执行轨迹中记录的基本块转换为 VEX IR[19],以确定每个基本块的出口类型.最后,从中提取函数调用记录.
算法 1 展示了针对 32 位 ARM 架构的函数调用记录的提取过程.其中,BBTrace 为基本块粒度的执行轨迹,
其中的每个基本块 BB 由三部分组成:基本块执行前的系统状态 BeforeState、执行后的系统状态 AfterState 及
由基本块的二进制汇编翻译而来的 VEX IR 超级块.BeforeState 和 AfterState 均包含当前的执行指令计数及通
用寄存器(R0-R15)的状态.此外，算法 1 中的 CallTrace 为函数调用粒度的执行轨迹，其中的每一项 CallRecord
也由三部分组成:函数调用发生时的系统状态 CallState、返回时的系统状态 ReturnState 和用于确认函数调用间
包含关系的栈深度值 depth.算法 1 维护了一个影子堆栈 ShadowStack,每当基本块中包含返回或者是调用的指
令,即更新其内容.根据 32 位 ARM 架构的函数调用规范,当程序调用函数 f 时,该基本块执行结束时 PC 寄存器
(R15)值为函数 f 的入口地址,LR 寄存器(R14)值为返回地址.当一个二进制基本块被转换为 VEX IR 超级块类型

2210

Journal of Software 软件学报 Vol.32, No.7, July 2021

时，其 jumpkind 属性指示了该基本块的出口类型。若基本块包含对函数调用的发起，其内容被转换为 VEX IR
后出口类型为 Ijk_Call;若基本块包含对系统调用的发起,基本块结束时 R7 寄存器将用于存放系统调用号,出口
类型为 Ijk_Sys_syscall.根据以上原则,即可判断某个基本块是否包含了函数调用.
算法 1. 从基本块粒度执行轨迹中提取函数调用记录.
Input: BBTrace-基本块粒度的执行轨迹;
Output: CallTrace-函数调用记录.
1.
ShadowStack←∅, CallTrace←∅, Depth←0;
2.
foreach BB∈BBTrace do
3.
if ShadowStack≠∅ then
4.
if BB.BeforeState.PC==ShadowStack[last] then
5.
CallTrace[last].ReturnState←BB.BeforeState;
6.
ShadowStack.pop();
7.
Depth--;
8.
end if
9.
end if
10.
if BB.jumpkind∈{Ijk_Call, Ijk_Sys_syscall} then
11.
Depth++;
12.
NewCallRecord.CallState←BB.AfterState;
13.
NewCallRecord.Depth←Depth;
14.
CallTrace.append(NewCallRecord);
15.
ShadowStack.append(BB.AfterState.LR);
16.
end if
17.
end for

图 3 执行轨迹分析器提取的函数调用记录示例.
得到函数调用记录后,执行轨迹分析器通过解析每个函数调用的目标地址、参数和返回地址来确定目标
应用在运行过程中执行了哪些函数调用以及对应的返回结果.函数调用记录的调用前/后系统状态信息可用于
粗略呈现函数调用的参数和返回值,其包含的回放计数信息则用于在之后的分析过程中控制回放进度、还原仿
真执行环境的完整系统状态.此外,用户可以通过预览函数调用记录中各个函数的执行长度,排除执行轨迹中不
感兴趣的部分,以减少执行轨迹分析所需的时间.图 3 展示了一个嵌入式 Web 应用的函数调用记录,从中可以看

吴华茂 等: 一种利用动态分析的嵌入式 Linux 应用托管方案

2211

到,该应用在退出之前执行了一个 open64()函数调用且该操作失败了.
3.2.2

填补系统状态信息

为了解析函数调用的指针型参数和返回值内容,执行轨迹分析器需能够在函数调用及返回时获取仿真执
行环境对应内存地址的内容.此外,以基本块执行为单位的系统状态取样可能不足以满足某些执行轨迹分析算
法的需求.例如,一些基本块中可能存在条件读或条件写的行为,如果执行轨迹分析算法需要分析污点传播,则
需要获取该基本块执行过程中的具体内存操作和条件指令选择.为满足获取更健全系统状态信息的需求,执行
轨迹分析器提供了为执行轨迹中的基本块填补系统状态信息的能力.
FirmDep 的执行轨迹分析器通过使用模拟执行来补全执行轨迹分析所需的系统状态信息并还原基本块内
部的具体内存操作和条件指令选择.具体来说,对于执行轨迹中的每个基本块,执行轨迹分析器将该基本块执行
前的寄存器状态放入 angr 的 VEX 执行引擎中,并对该基本块进行模拟执行.同时,执行轨迹分析器使用
PANDA 重放目标应用的托管过程,并将回放进度与模拟执行的进度同步.在模拟执行过程中,执行轨迹分析器
记录基本块中的所有条件操作和内存操作.若应用有读取内存的行为,执行轨迹分析器将使用 PANDA 获取指
定内存地址的内容,并将其填充到模拟执行引擎的内存模型中.当执行完基本块后,其对应的内存模型将被用作
下一个基本块的初始内存模型.通过这种方式, FirmDep 为执行轨迹中的每个基本块获取足够用于分析应用行
为的内存状态等信息.
图 4 为一个运行于 FirmDep 的执行轨迹分析算法的运行结果,该算法用于基于执行轨迹生成带有参数解析
的文本格式的函数调用记录.如图所示,目标应用绑定了一系列的信号处理函数,创建了一个套接字并尝试将该
套接字绑定到某个网络地址.该算法通过对 bind()函数调用的 sockaddr 格式参数进行解析,得到了其尝试托管时
尝试绑定的地址及端口.最终,该 bind()函数未能将套接字与该地址绑定,应用输出了一条错误信息并退出.

图4

执行轨迹分析算法示例:带参数解析的函数调用记录生成.

4 执行轨迹分析算法设计
在提取函数调用记录的过程中,执行轨迹分析器将对被托管应用的最终运行状态进行简单的确认.对未能
被成功托管的嵌入式应用而言,其最终运行结果可分为以下几类:1)阻塞(执行轨迹结束于未返回的函数调
用);2)进入循环;3)主动结束运行,以及 4)因段错误等原因被终止运行.针对应用不同的运行状态,FirmDep 使用
不同的分析策略来识别环境依赖问题,并提供不同的仲裁方案.
针对阻塞状态的分析策略.此种策略将该函数最后执行且未返回的函数调用视作是目标应用托管失败的
直接原因.此函数调用的相关信息可通过获取函数调用记录中的最后一项得到.依照最后一个函数调用的情
况,FirmDep 会采取对应的措施:
1)

结束于某个未知语义的外部函数调用:由于在生成初始执行轨迹时默认不记录嵌入式应用程序所加
载的动态链接库的执行,因此当程序异常源于其加载的动态链接库时,记录信息是不足的.在这种情况
下,FirmDep 将会将该外部函数所属的动态链接库的地址范围添加到录制执行轨迹的地址范围内,并

2212

Journal of Software 软件学报 Vol.32, No.7, July 2021
通过回放执行以获得新的执行轨迹;

2)

结束于 select()、poll()等已知语义的函数调用:FirmDep 将根据该函数调用的参数,并给出调整仿真执
行环境参数的仲裁措施.对于 select()、poll()等用于等待请求的函数而言,FirmDep 将通过系统调用记
录获取其关联的套接字文件节点信息,并将该信息提供给用户.

针对进入循环及主动结束运行的分析策略.由于存在错误处理机制,当嵌入式应用程序遭遇错误时,可能
会选择一条不同于正常执行的执行路径.图 5 展示了两种常见的错误处理机制的具体实现.图中的三栏分别为
基于原始二进制生成的伪代码、汇编代码及对应的 VEX IR 中间代码.在图 5(a)所展示的例子中,如果应用执行
listen()获得错误结果,程序将输出一条错误信息并跳转至清理退出的代码.这种机制通常用于处理致命性的错
误.另一种机制是对失败的操作进行重试,直到满足某种条件.以图 5(b)中展示的代码为例,该应用将不断重试执
行 check_network(),直到获得一个正值返回.这两种情况有一个共同点:都包含由函数返回值决定的执行分支.在
图 5 所展示的两个例子中,决定程序分支的临时变量值均来自寄存器 r0 中的值.根据 32 位 ARM 的函数调用规
范,该值正是某个函数调用的返回值.

(a)

(b)

图5

常见的程序错误处理机制及反向污点分析示例.

基于这一观察,本文提出了一种定位故障根源的思路:寻找最后一个影响分支跳转且返回值为错误值的函
数调用.为了定位这样的函数调用,本文进一步提出了一种反向污点分析算法.首先,从执行轨迹的最后一个基
本块开始,寻找一个以分支跳转为结束的基本块.然后,将用于决定该分支跳转目标的临时变量值(如图 5(a)中的
t14 和图 5(b)中的 t17)作为污点传播源.对于每一个基本块,从包含该临时变量值的最后一个指令开始,将污点传
播至所有参与计算现有污点元素的每个临时变量、寄存器及内存地址.由于决定程序运行状态的值是来自于程
序外部,因此在传播过程中,若一个污点元素的值完全由常量计算得来,我们将清除其污点状态以减少假阳性污
点.该污点传播的过程将持续至执行轨迹的起点,或者是当污点传播到某个函数的返回值时.此时,若该函数的
返回值为一个代表错误的值,则将该函数调用作为一个候选的错误根源返回给用户.为判断函数返回值是否代
表错误值,FirmDep 维护了 270 个常见 POSIX 函数的返回值范围.对于未维护的函数,FirmDep 将小于 0 的返回
值视作是代表错误的返回值.在获取一个候选的错误根源后,FirmDep 将去除用于传参的寄存器的污点状态,并
继续进行污点分析直至执行轨迹的开始.算法 2 阐述了基本块内部的污点传播过程.在 VEX IR 中,每条指令都
被 分 解 为 多 条 声 明 (stmt), 而 每 条 声 明 都 具 备 一 个 写 入 目 标 (stmt.target) 以 及 即 将 被 写 入 该 目 标 的 表 达 式
(stmt.expr).反向污点传播的过程可以被简单概括为:若该声明中的写入目标已被污点标记,则将污点传播至该
声明的表达式中.已经被污点标记的寄存器及内存地址的污点状态将延续至下一个被分析的基本块.

吴华茂 等: 一种利用动态分析的嵌入式 Linux 应用托管方案

2213

算法 2. 基本块内部的污点传播
Input: bb – 基本块中包含的 VEX IR 语句序列
tainted_addrs -被污点标记的寄存器序号及内存地址
source_tmp – 污点传播源
Output: tainted_addrs -被污点标记的寄存器序号及内存地址
1.
tainted_tmps←∅,uncovered←[0,1,…,|bb|-1];
2.
tainted_tmps.add(source_tmp);
3.
while |uncovered|≥0 do
4.
last←uncovered.pop_last();
5.
stmt←bb[last];
6.
if stmt.target∈tainted_tmps then
7.
if stmt.expr∩tainted_{tmps, addrs}≠∅ then
8.
for t∈stmt.expr.tmps do
9.
tainted_tmps.add(t);
10.
end for
11.
for a∈stmt.expr.addrs do
12.
tainted_addrs.add(a);
13.
end for
14.
else
15.
tainted_{tmps,addrs}.discard(stmt.target);
16.
end if
17.
else if {stmt.target∈tainted_addrs} then
18.
if stmt.expr 包含至少一个临时变量 then
19.
for t∈stmt.expr.tmps do
20.
tainted_tmps.add(t)
21.
end for
22.
else
23.
tainted_addrs.discard(stmt.target);
24.
end if
25.
end if
26. end while

对于通过此分析策略寻找到的托管失败问题,FirmDep 根据具体情况,制定两种类型的仲裁方案:
(1) 若找到的 API 函数调用为已知的函数调用,且能根据其参数、返回值和 errno 等获知导致问题的原因,
则提出调整仿真环境的对应仲裁方案;
(2) 若找到的函数调用为未知的函数调用,FirmDep 将尝试通过使用 angr 的约束求解器,求解出一个可使
程序选择不同分支的函数返回值,并让仿真执行环境在下一次尝试进行托管时使用强制执行对该问
题进行修复.
图 6 展示了一个使用此算法寻找应用托管失败原因的例子.FirmDep 通过污点分析算法找到一个影响了
程序分支的 bind()函数调用,并给出了通过调整仿真环境网络配置的仲裁方案.
针对段错误的分析策略.当嵌入式应用在托管过程中对无效的指针进行操作时,其会触发段异常并被系
统终止运行.该无效的指针可能是一个在仿真执行环境中不存在的物理地址,或者是由于用于与硬件交互的
API 函数未能读取到正确的值而返回的空指针.FirmDep 通过一种对该指针进行溯源的策略来解决段错误的问
题.具体而言,此分析策略检查最后触发段错误的函数调用是否存在空指针或无效指针参数.若存在,则以该参
数作为污点传播源对执行轨迹进行反向污点分析,并寻找产生该无效指针的函数调用,并呈现给用户.若该函数

2214

Journal of Software 软件学报 Vol.32, No.7, July 2021

调用为某一用于查询 NVRAM 或数据库的函数,则通过为对应的查询项增设预设值来仲裁对应的问题.

图 6 通过反向污点分析算法寻找应用托管失败原因的示例.
基于系统调用记录的分析策略.此分析策略是一种备选策略,通过检查关键系统调用,快速排除一些由套
接字初始化失败引起的问题.一些应用的开发者选择通过一个预定义的设置来初始化网络链接,若模拟执行环
境的网络配置与该预设不匹配,会导致网络服务的初始化失败.为提供网络服务,应用需要创建一个套接字,为
套接字绑定一个地址和端口,若必要的话为套接字设置属性,最后监听用户请求.这一系列操作中的每一步都与
一个特定的系统调用相关. 此算法检查的相关的系统调用包括 sys_socket()、sys_bind()、sys_setsockopt()和
sys_listen(),若有失败的套接字相关系统调用记录,FirmDep 将尝试根据系统调用的返回值,对仿真执行环境的
网络配置进行调整.

5 系统实现
本文为 32 位 ARM 架构实现了 FirmDep 的实验原型系统,其源码可在[21]获取.选择这一架构的原因之一是
该架构被广泛应用于 I 型设备中,另一原因则是在开发原型系统时,PANDA 尚未为 x86 和 ARM 以外的硬件架
构提供重放/回放功能的支持.若 PANDA 或其他的提供重放/回放功能的仿真器支持了新的硬件架构,FirmDep
即可该硬件架构提供支持,只要按照该架构的函数调用规范调整分析算法的实现即可.
5.1

仿真执行环境
仿真执行环境默认模拟一个带有 1G 内存,机器类型为 versatilepb 的设备.选择此机器类型的原因是因为在

进行 FirmDep 原型开发时,只有该机器类型的 ARM 设备的执行轨迹录制/回放功能可以正常工作.考虑到
versatilepb 默认仿真的 ARMv6 架构指令集的处理器可能无法运行一些用于较新 ARM 设备的可执行文
件,FirmDep 为仿真设备指定处理器为 Cortex-A15 架构,并加载一个经过修改的可支持 ARMv7 指令集的用于
versatilepb 设备的 Linux 3.14 版本内核.在进行嵌入式应用托管时,仿真执行环境加载一个基于 buildroot[22]搭建
的初始阶段系统并绑定一个默认的 IP 地址(192.168.1.1).若仲裁措施包括网络配置的调整,则按需进行调整.此
后,初始阶段系统使用 chroot 工具将固件中包含的文件系统指定为根文件系统,并运行固件文件系统中的 shell.
仿真执行环境通过设置环境变量,使运行于其中的应用加载时会加载 API 劫持库.最后,使用固件预处理过程中
识别出的启动指令来初始化嵌入式应用.
当仿真器开始启动,Tracer 首先通过 PANDA syscalls2 插件监听 chroot 函数调用,等待环境初始化完毕.此
后,Tracer 通过 PANDA osi_linux 插件在每次 ASID 发生变化时(意味着发生了任务调度)获取当前线程的信息,
若任务名与目标应用相同,则开始为程序录制执行回放.若用户注册了强制执行的任务，Tracer 在这过程还将监

吴华茂 等: 一种利用动态分析的嵌入式 Linux 应用托管方案

2215

视寄存器状态,并通过 QEMU API 对系统状态进行修改.
获 得 执 行 回 放 记 录 后 ,Tracer 重 放 执 行 回 放 记 录 并 生 成 执 行 轨 迹 .具 体 而 言 ,Tracer 插 件 1)通 过 监 听
do_mmap2()系统调用获取最新的内存映射更新,同时也记录一些网络、文件相关系统调用的行为;2)在每次
ASID 变 化 时 获 取 任 务 名 、 pid 等 信 息 以 确 认 当 前 是 否 在 运 行 目 标 应 用 ;3) 为 PANDA 的
PANDA_CB_BEFORE_BLOCK_EXEC 和 PANDA_CB_AFTER_BLOCK_EXEC 钩子注册回调函数,使回调函数
在基本块被执行前后分别被调用.若当前正在运行目标应用且满足录制条件,调用 QEMU 的 API 获取当前的寄
存器状态信息、正在执行的基本块起始地址和长度和当前回放指令计数.在应用托管过程中，预加载动态链接
库通过设置寄存器状态为特定状态来传递信息,这些信息将在此步骤中被 Tracer 插件获取.
FirmDep 实验原型的仿真执行环境以 2020 年 11 月 30 日前的最新 PANDA 源码版本为基础搭建.早期版本
的 Tracer 插件基于 C 语言实现,随着 PANDA 版本更新提供了 Python 绑定,Tracer 插件改为主要由 Python 实现，
包含约 500 行代码.托管过程中加载的预加载动态链接库通过扩展源自 Firmadyne 项目用于劫持 NVRAM 相关
硬件操作的动态链接库实现,添加约 400 行 C 代码.
5.2

执行轨迹分析器
FirmDep 实验原型的执行轨迹分析器以 2020 年 11 月 30 日前的最新 angr 版本为基础搭建.此部分包含约

4500 行 Python 代码,其中执行轨迹预处理、模拟执行及各类分析算法的实现包含约 1100 行代码;执行轨迹查
看器包含约 300 行代码;常见 API 函数参数及返回值的类型识别、错误识别包含约 2000 行代码;针对已识别通
用问题提出修复方案及强制执行修复相关的部分包含约 800 行代码。

6 实验及结果分析
6.1

嵌入式应用托管测试
为验证本文提出的固件托管方案的有效性,本文建立了一个由真实设备固件组成的实验数据集,并使用该

实验集对 FirmDep 进行评估.FirmDep 实验平台在以下环境实现:处理器:Intel(R) Core i7-8700 4.0Ghz;内
存:32GB DDR4;FirmDep 的实现及其依赖均被打包于以 Ubuntu 18.04LTS 为基础的 Docker 容器中.
研究问题. 在本章节,本文尝试回答如下研究问题:


RQ1. 本文提出的固件托管方案的兼容性如何,与现有方案相比能否有效提高托管成功率?



RQ2. 本文提出的固件托管方案能否有效识别出托管失败的嵌入式应用的环境依赖问题,并给出仲
裁方案?



RQ3. 本文提出的固件托管方案是否适用于安全分析应用?

实验集及固件预处理. 本文实验部分的测试对象为一系列运行于 I 型嵌入式设备的 Web 应用.选择此类
服务的原因包括:1)此类服务主要用于为用户提供一个对设备进行控制的界面,在修改配置的过程中较有可能
需要与外设交互,也因此在托管过程中较为容易遇到环境依赖问题;2)此类服务是嵌入式设备漏洞的重灾区.根
据我们的统计, 60%以上的嵌入式设备 CVE 与 Web 应用有关,这使得它们成为了安全研究的重点研究对象.此
外,Web 应用托管成功率为先前的固件托管方案（包括 Costin 方案、Firmadyne 和 FirmAE）的唯一或主要的
托管成功率统计指标,选择 Web 应用作为托管目标可便于与先前的方案作对比.
本文使用由 Firmadyne 项目提供的网络爬虫从多个设备供应商的网站、FTP 服务器及备档获取了一系列
用于路由器、网络摄像机、网关和其他嵌入式设备的固件镜像.对于每个固件镜像,本文尝试使用 binwalk[23]
提取出其中的文件系统,并通过检查其中的可执行文件,判断固件的处理器架构.然后,通过检查固件中是否包
含使用*httpd、boa、goahead、nginx 等常见 Web 服务应用名称的文件,筛选出其中包含 Web 服务的设备固件.
为了方便与先前的方案作对比,本文参考 Firmadyne 和 FirmAE 两个方案在实验阶段中的品牌选择,选取了来自
ASUS、Netgear、D-Link、TP-Link、Linksys、Belkin、TrendNet、Zyxel 这 8 个嵌入式设备固件厂商的固件,
并使设备型号尽可能覆盖先前 Firmadyne 和 FirmAE 方案提供的数据集.考虑到一个设备会有多个版本的固件,

2216

Journal of Software 软件学报 Vol.32, No.7, July 2021

我们为每个设备型号仅保留一个版本的固件.
在确定需要分析的固件后,本文使用一个分析脚本提取出其中嵌入式 Web 服务的启动参数和依赖的配置
文件等.分析脚本首先在固件文件系统包含的所有有效路径及字符串中寻找包含常见 Web 服务应用子字符串
的字符串,包括*httpd、boa、goahead 和 nginx.若在/etc/init.d 或/etc/rc.d 中找到包含这些字符串的脚本文件,则
将这些文件当作是 Web 服务的初始化脚本,直接通过执行它们来初始化服务.而若字符串是来自某个二进制文
件,则将该其当作是 Web 服务的启动指令,并对其进行进一步分析. 如果候选启动指令中包含任何类似路径的
子串,例如/etc/lighttpd.conf,脚本将检查其是否存在,并在必要的时候提醒用户去获取对应的文件.对于一些配置
文件在文件系统中不存在的情况,将尝试使用 FirmAE 托管该固件,并在等待固件初始化基本完成后从其文件
系统中提取文件.最终,本文筛选出 217 个符合条件的嵌入式固件,并对其中的 Web 服务进行了测试,数据集信息
见[21].
RQ1.

FirmDep 的托管兼容性及与已有方案的对比. 本文使用 Firmadyne、FirmAE 及 FirmDep 分别对实

验数据集的嵌入式 Web 应用进行了测试,此类 Web 应用通常为对应设备的管理界面.本文对“成功托管”的定义
为固件中至少有一个 Web 应用能够监听并响应用户的请求.考虑设备固件中可能存在文件缺失的情况,被托管
的应用可能并不能呈现正常的网页,而是返回 HTTP 404 等错误信息.本文认为在这样的情况下,该 Web 应用依
然是正常响应请求,且具备被分析的价值,因此这样的情况本文也定义为托管成功.各固件托管方案对实验集的
托管测试结果如表 1 所示.
在 217 个嵌入式 Web 应用中,FirmDep 在未经执行轨迹分析的情况下能够直接托管 30.4%的 Web 应用,而
经 过 执 行 轨 迹 分 析 及 应 用 仲 裁 方 案 后 能 够 托 管 67.7% 的 应 用 , 成 功 率 提 高 了 一 倍 有 余 . 与 现 有 的 方 案 相
比,Firmadyne 仅成功托管了 1 个 Web 应用,而 FirmAE 的托管成功率整体而言也并不如 FirmDep 经过修复后的
方案,仅对于 TP-Link 的 Web 应用取得了比 FirmDep 更好的托管成功率.CostinFA 方案由于未放出源代码,无法
用于对比实验,但是由于其采用了与 FirmDep 相同的基于 chroot 的托管方案,因此可认为其固件托管成功率应
接近或劣于未进行执行轨迹分析和仲裁的 FirmDep.需要说明的是 FirmDep 并未包含 FirmAE 在 Firmadyne 基
础上添加的仲裁措施,这体现了 FirmDep 的固件托管方案配合半自动执行轨迹分析,其效果可媲美或胜于当前
完全基于人工经验分析的固件托管方案.
表1
品牌

固件数

ASUS
Belkin
D-Link
Linksys
Netgear
TP-Link
TrendNet
Zyxel
总和

45
6
10
13
65
43
15
20
217

FirmDep 及现有固件托管方案对实验集的托管测试结果
成功托管 Web 服务的固件数量
Firmadyne

FirmAE

FirmDep(仲裁前)

FirmDep(仲裁后)

0(0%)
0(0%)
0(0%)
0(0%)
0(0%)
1(2.3%)
0(0%)
0(0%)
1(0.4%)

24(53.3%)
0(0%)
6(60%)
9(69.2%)
32(49.2%)
26(60.5%)
15(100%)
1(5%)
113(52.1%)

1(2.2%)
1(16.7%)
9(90%)
0(0%)
16(24.6%)
23(53.5%)
1(6.7%)
15(75%)
66(30.4%)

29(64.4%)
6(100%)
9(90%)
9(69.2%)
43(66.2%)
23(53.5%)
13(86.7%)
15(75%)
147(67.7%)

RQ2. FirmDep 对嵌入式应用的环境依赖识别和仲裁能力. 通过使用本文提出的启发式算法,FirmDep 识
别出了 95 个未能直接托管成功的 Web 应用的环境依赖问题,并通过应用仲裁方案,修复了 81 个问题.表 2 展现
了 FirmDep 在尝试托管实验数据集固件中的 Web 应用时识别出的部分环境依赖问题及对应的仲裁方案.
总体而言,FirmDep 识别出的环境依赖问题包含以下几类:
（1）与文件相关的问题.在 FirmDep 识别出的环境依赖问题中,此类问题是最常见的.具体表现为尝试使
用 open()、fopen()及 open64()等文件操作 API 以写入模式打开一个.pid 或者是.log 文件.经过对固件文件系统
进行分析,我们发现这通常是因为这些文件所位于的路径是无效的符号链接,这些符号链接往往是指向/tmp 或
/tmp 路径下的目标,这意味着这些符号链接只有在完成了系统初始化过程后才会变得可用.对于此类问题,本文

吴华茂 等: 一种利用动态分析的嵌入式 Linux 应用托管方案

2217

为 FirmDep 制定的仲裁方案为在进行应用托管前为上述的符号链接创建有效的指向目标.
表2
厂商

ASUS

应用路径

/usr/sbin/httpds

/usr/sbin/minhttpd
Belkin

Linksys

FirmDep 识别出的部分环境依赖问题及对应的仲裁方案

/bin/httpd
/usr/local/bin/thttpd
/usr/sbin/lighttpd

/usr/sbin/lighttpd

Netgear

/usr/sbin/httpd

环境依赖问题

仲裁方案

mssl_init(...)=0

强制执行,修改返回值为 1

open("/var/run/httpd.pid","w")=-1

修复路径/var/run

fopen("/var/run/httpd.pid","w")=0

修复路径/var/run

nvram_get("x_Setting")=0

无(未能修复)

bind(...,<"AF_INET","192.168.2.1","8080">,...)=-1

调整 IP 地址及端口映射

bind(...,<"AF_INET","192.168.0.1","80">,...)=-1

调整 IP 地址

fopen("/var/run/httpd.pid","w")=0

修复路径/var/run

fopen("/var/run/thttpd.pid","w")=0

修复路径/var/run

fopen("/var/run/lighttpd.pid","w")=0

修复路径/var/run

fopen("/var/run/httpd.pid","w")=0

修复路径/var/run

open64("/var/run/httpd.pid","w")=0

修复路径/var/run

open64("/var/log/lighttpd/error.log","w")=0

修复路径/var/log/lighttpd

open("/etc/lighttpd/certs/server.pem")=-1

执行命令生成证书文件

fopen("/var/run/httpd.pid","w")=0

修复路径/var/run

agApi_fwGetNextTriggerConf(...)=0

强制执行,修改返回值为 1

bind(...,<"AF_INET","192.168.0.1","80">,...)=-1

调整 IP 地址

isLanSubnet(...)=0

强制执行,修改返回值为 1

（2）与套接字相关的问题.FirmDep 发现了一些失败的套接字初始化操作.通过 errno 信息,FirmDep 识别
出对应的嵌入式 Web 服务尝试为一个套接字绑定一个无效的 IP.FirmDep 通过对 bind()传递的 sockaddr 类型指
针进行解析,FirmDep 识别出了这些函数调用所绑定的目标地址及端口.对于此类的问题,制定的仲裁方案为调
整仿真环境的 IP 地址及端口映射设置,以满足嵌入式 Web 服务的需求.
（3）无效的函数返回值.在尝试托管部分来自 ASUS 固件的嵌入式 Web 服务的过程中,一些 Web 服务由
于无效的 strlen()及 memset()函数调用而触发了段异常错误.造成段异常错误的原因是这些函数调用尝试操作
空指针.通过使用反向污点分析追踪空指针的来源,FirmDep 定位到一个用于从 NVRAM 硬件中读取数据的
nvram_get()函数调用.然而,由于这些 Web 服务未能加载 NVRAM 模拟库,未能为这些服务提供有效的仲裁措施.
（4）非 POSIX 函数导致的问题.通过使用反向污点分析算法,FirmDep 识别出了一些导致程序进入异常执
行路径的函数调用.其中,mssl_init()函数的异常返回值影响了几乎所有的来自 ASUS 的 Web 应用的运
行,agApi_fwGetNextTriggerConf()及 isLanSubnet()的返回值影响了一定数量的来自 Netgear 的 Web 应用的运
行.FirmDep 通过使用约束求解为这些函数生成了可改变执行路径的返回值,并通过使用强制执行对这些问题
进行了修复.
RQ3. 对嵌入式 Web 应用的漏洞扫描测试. 为验证 FirmDep 是否适用于安全分析场景,本文对托管成功的
嵌入式 Web 应用进行了漏洞扫描测试,扫描结果如表 3 所示.
首先,本文使用了加载 full_audit 配置文件的 w3af[24]对被托管的 Web 服务进行扫描,以检查其是否包含网
页漏洞.根据 w3af 的扫描结果,被托管的 147 个嵌入式 Web 应用中有 69 个应用被检测为可能受到漏洞影响,其
中分别有 4 个应用受到高等级漏洞影响、66 个应用受中等级漏洞影响,及 25 个应用受低等级漏洞影响.此外,
有 2 个应用可能包 含缓冲 区溢出漏 洞,3 个应用可能 未 适当配置 虚拟主 机相关的 设 置.此外,本文 还使用
RouterSploit[25]对被托管的 Web 应用进行测试,以检测这些服务是否受到已知漏洞的影响.最终,发现共 9 个 Web

2218

Journal of Software 软件学报 Vol.32, No.7, July 2021

应用受到已知漏洞的影响.此实验结果表明了 FirmDep 应用于安全分析应用的可行性.
表3

对嵌入式 Web 应用的漏洞扫描结果
w3af 扫描结果

危险等级
High

Medium

Low

Info

影响的 Web 应用数目

说明
HTTP Basic authentication

4

Guessable credentials

4

Click-Jacking

66

Secure content over insecure channel

12

Unhandled error in web application

4

Cookie without HttpOnly

4

Private IP disclosure

5

Virtual host identified

11

Path disclosure

6

Potential buffer overflow

3

Potential virtual host misconfiguration

2
69

总数
RouterSploit 扫描结果
漏洞

影响的 Web 应用数目

eseries_themoon_rce

9

uc_httpd_path_traversal

2

总数

9

6.2 未托管案例分析
在 4.1 的固件托管过程中,实验集中的一些嵌入式 Web 应用未能被直接托管,且 FirmDep 未能为它们提供
可用的仲裁方案.未托管的案例可分为以下的情况:
（1）未生成执行轨迹.在实验集的 217 个嵌入式 Web 应用中, FirmDep 未能为 28 个应用它们生成执行轨
迹,原因是在整个托管过程中 FirmDep 都未能识别到这些程序的执行.应用未执行的情况主要归类为下面两者
之一:1)exec error 或 illegal instruction,造成之类问题的原因主要是仿真器对嵌入式应用二进制文件中的指令未
提供完整支持或由于未正确解压导致二进制文件完整性受到破坏;2)缺少动态链接库,造成此类问题的原因主
要是未正确解压或固件未包含完整文件系统导致文件系统不完整.
（2）未 能 完 成 执 行 轨 迹 的 预 处 理 .一 些 未 托 管 成 功 的 Web 应 用 ,尽 管 它 们 的 执 行 轨 迹 被 生 成 了 ,但
FirmDep 未能完成执行轨迹的预处理.造成这种情况的原因包括:1)Web 应用的可执行文件未能被 angr 加载,以
及 2)在判定基本块的有效性时出现了假阴性,导致预处理后的执行轨迹残缺.
（3）未从执行轨迹中识别出环境依赖问题.首先,本文所提出的几种基于错误处理分析的启发式算法并
不能覆盖到所有真实世界的错误处理情况.例如,一些函数在调用时会传递一个用于存放结果的指针,而不通过
函数的返回值来传递结果.此外,一些应用可能对错误有较高的容忍度,使得它们在遭遇错误后仍然能回到正常
的执行路径.例如,通过对执行轨迹的人工分析,我们发现一个来自未托管成功的 D-Link 的 Web 应用实际上已
调用了 select()函数并在后台运行.在此之前,应用尝试将对应的套接字绑定到名为 br0 的网络接口上,但由于该
网络接口不存在于仿真环境中,因此这些操作都失败了.通过调整仿真环境的网络配置,该 Web 服务可被成功托
管.由于该环境依赖问题并非通过本文提出的启发式算法分析得到,因此此案例不计入托管成功的案例中.再例
如,本文在分析一个来自 TP-Link 的 Web 应用时发现了一个未影响到任何执行分支的负值函数调用返回值,不
排除是因为开发人员遗漏了对该函数调用的错误处理.除由于执行轨迹分析算法不足导致的分析失败以外,若
干个 Web 应用由于其二进制文件缺乏符号表,难以从其执行轨迹推断出程序的高级语义行为,故无法提出仲裁

吴华茂 等: 一种利用动态分析的嵌入式 Linux 应用托管方案

2219

方案.
（4）未能为已发现的环境依赖问题提供有效的仲裁方案.除 6.1 中说明的部分未能修复的 nvram_get()函
数调用外,在实验中部分 Web 应用会尝试访问不存在的设备节点或文件.例如,通过反向污点分析,FirmDep 指出
一个来自 Netgear 的 Web 应用可能是因为通过 popen()打开/tmp/jffs2ready 这一管道文件失败而导致托管失败.
由于缺乏对该设备的了解,未能为该问题制定有效的仲裁方案.

7 讨

论

对其他处理器架构的支持. FirmDep 的实现并未包含太多架构相关的内容,可很容易添加对其他架构处理
器的支持.事实上,在进行 FirmDep 原型系统开发的时候,PANDA 已可提供 MIPS 架构及 ARM64 架构客户机的
较好动态插桩及信息采集支持,仅缺失对这些架构的录制/回放功能支持.FirmDep 使用录制/回放机制的动机并
非主要是为分析效率,而是为了托管过程与信息提取过程应用执行的一致性.若可保证托管过程和分析过程的
一致性,亦可使用其他可获取运行轨迹的仿真器,如 Valgrind 或用户态 QEMU 等替代 PANDA,以实现其他处理
器架构的支持.此外,尽管 FirmDep 原型系统仅为 32 位 ARM 函数调用规范(AAPCS)实现了反向污点分析算法,
由于算法的大部分实现是基于 VEX IR 的分析,对于其他的处理器架构,仅需对算法进行少量调整即可.
FirmDep 方案的可能改进和局限性. 经过第 6.1 节的固件托管实验及第 6.2 节对未成功托管的固件的分
析,我们将本文提出的固件托管方案的不足分为两类:(1)通过扩展本文实现的原型可解决的问题,以及(2)本文
提出的方案难以解决的问题.首先,通过扩展 binwalk 和 PANDA/Qemu,可支持对更多设备固件的支持、提高运
行二进制应用的兼容性;通过为 FirmDep 添加更多的执行轨迹分析算法和总裁措施,可以识别修复更多本来无
法托管的应用;本文所实现的 FirmDep 原型系统并未充分利用 PANDA 所提供的对客户机系统的仲裁机能.例
如,PANDA 可通过劫持系统调用来伪造文件系统中的文件,可利用这一技能来模拟一些硬件设备节点.再比
如,FirmDep 原型只使用了修改函数返回值这一强制执行手段,通过修改函数调用参数或者跳过某些函数调用,
可支持修复更多的问题.
至于(2),在实验过程中,我们意识到一些嵌入式应用的环境依赖问题是难以使用本文提出的固件托管方案
来识别和缓解的.首先,我们观察到一些应用的行为与配置文件内容高度相关(例如部分基于 lighttpd 二次开发
的嵌入式 Web 应用).本文提出的执行轨迹分析方法是基于基本块粒度的,因此为此类应用记录的执行轨迹往往
非常庞大,其中很大一部分是对配置文件的文本解析.这带来了两个挑战:1)本文提出的方案缺乏对字符串处理
这一行为的感知,难以将应用的行为与配置文件中的特定内容建立关联;2)此类应用的单次运行可产生包含几
十万乃至几百万个基本块的执行轨迹.本文提出的分析方案的模拟执行部分由于需要为每个基本块保留内存
模型,模拟执行的速度会逐渐减慢且占用大量内存,在这种情况下单次的模拟执行将耗费大量时间,使得本文提
出的方案失去实用价值. FirmDep 在应对一些依赖外部字符串(例如启动参数)的嵌入式应用也面临相同的问
题.在分析本文第六章实验中属于这种情况的嵌入式应用时,我们均尽可能裁剪了执行轨迹中包含配置文件解
析的部分.而对于缺乏符号信息、执行轨迹非常庞大无法裁剪的此类嵌入式应用,我们均标记为托管失败.除此
之外,本文提出的方案难以应对嵌入式应用的环境依赖信息不包含在设备固件中的情况.此类情况包括应用依
赖特殊的设备节点或文件中的内容、依赖特定较复杂的文本 NVRAM 值等.此类情况对于其他的基于全系统仿
真的固件托管方案而言是共同的难题,通常需要获取原始的硬件设备并进行逆向提取解决.
FirmDep 的定位. 本文所提出的 FirmDep 固件托管方案与现有的全自动化固件托管方案是互补的关系,
而非 相 互替 代 的关 系.当 不存 在未 解 决的 环 境依 赖 问题 时,FirmAE 等 全自 动 化固 件 托 管方 案 在使 用 上比
FirmDep 更简易.另外,在本文的实验过程中,由于 FirmAE 框架包含了一些已知环境依赖问题的针对性修复，
FirmAE 可支持托管一些未被 FirmDep 识别出环境依赖问题的服务.用户可通过为 FirmDep 集成 FirmAE 等基
于现有经验的固件托管框架所包含的仲裁措施来提高初始固件托管成功率,亦可通过为 FirmAE 等框架添加部
分由 FirmDep 生成的仲裁措施来实现更高的固件托管成功率.此外,本文认为 FirmDep 的用途并不局限于解决
嵌入式应用托管环境依赖问题,通过新增执行轨迹分析算法,FirmDep 也可用于其他的应用场景,例如恶意软件

2220

Journal of Software 软件学报 Vol.32, No.7, July 2021

行为分析等.

8

结

语

本文针对现有嵌入式设备固件托管方案无法识别和解决被托管应用未知环境依赖问题的缺陷,提出了一
种 I 型嵌入式设备应用托管的固件托管方案 FirmDep. FirmDep 通过使用按需记录及 PANDA 的录制/重放功能,
实现对应用托管过程的高效记录.当嵌入式应用无法被成功托管时, FirmDep 可对执行轨迹进行信息提取及系
统状态补全,并使用多种执行轨迹分析算法识别和仲裁目标应用的环境依赖问题.实验结果表明,本文提出的固
件托管方案可实现高于现有全自动固件托管方案的托管成功率,能有效识别和解决嵌入式设备应用在运行过
程中的环境依赖问题,且可用于嵌入式设备固件的漏洞挖掘.
References:
[1]

Yu R, Nin F, Zhang Y, Huang S, Kaliyar P, Zakto S, Conti M, Portokalidis G, Xu J. Building embedded systems like it’s 1996. In:
Proc. of the 2022 Network and Distributed System Security Symp. San Diego, CA, USA: Internet Society, 2022. 1-18. [doi:
10.14722/ndss.2022.24031]

[2]

Yu YC, Chen ZN, Gan ST, Qin XJ. Research on the technologies of security analysis technologies on the embedded device firmware.
Ji

Shuan

Ji

Xue

Bao/Chinese

Journal

of

Computers,

2021,44(05):859-881,(in

Chinese

with

English

abstract).

http://cjc.ict.ac.cn/online/onlinepaper/yyc-202159181446.pdf [doi: 10.11897/SP.J.1016.2021.00859]
[3]

Chen DD, Egele M, Woo M, Brumley D. Towards automated dynamic analysis for linux-based embedded firmware. In: Proc. of the

[4]

Kim M, Kim D, Kim E, Kim S, Jang Y, Kim Y. FirmAE: towards large-scale emulation of iot firmware for dynamic analysis. In:

2016 Network and Distributed System Security Symp. San Diego, CA: Internet Society, 2016. 1-16.[doi: 10.14722/ndss.2016.23415]
Proc. of the 36th Annual Computer Security Applications Conf. Austin USA: ACM, 2020: 733–745. [doi: 10.1145/3427228.
3427294]
[5]

Zaddach J, Bruno L, Francillon A, Balzarotti D. Avatar: a framework to support dynamic security analysis of embedded systems’
firmwares. In: Proc. of the 2014 Network and Distributed System Security Symp. San Diego, CA: Internet Society, 2014. 1-16. [doi:
10.14722/ndss.2014.23229]

[6]

Gui ZJ, Shu H, Kang F, Xiong X. FIRMCORN: vulnerability-oriented fuzzing of iot firmware via optimized virtual execution. IEEE
Access, 2020, 8: 29826–29841. [doi: 10.1109/ACCESS.2020.2973043]

[7]

Muench M, Stijohann J, Kargl F, Francillon A, Balzarotti D. What you corrupt is not what you crash: challenges in fuzzing
embedded devices. In: Proc. of the 2018 Network and Distributed System Security Symp. San Diego, CA: Internet Society, 2018.
1-15. [doi: 10.14722/ndss.2018.23166]

[8]

Costin A, Zarras A, Francillon A. Automated dynamic firmware analysis at scale: a case study on embedded web interfaces. In: Proc.
of the 11th ACM on Asia Conf. on Computer and Comm. Security. Xi’an China: ACM, 2016: 437–448. [doi: 10.1145/2897845.
2897900]

[9]

Zheng Y, Davanian A, Yin H, Song C, Zhu H, Sun L. FIRM-afl: high-throughput greybox fuzzing of iot firmware via augmented
process emulation. In: Proc. of the 28th USENIX Conf. on Security Symp. USA: USENIX Association, 2019: 1099–1114.

[10] Srivastava P, Peng H, Li J, Okhravi H, Shrobe H, Payer M. FirmFuzz: automated iot firmware introspection and analysis. In: Proc. of
the 2nd Intl. ACM Workshop on Security and Privacy for the Internet-of-Things. London United Kingdom: ACM, 2019: 15–21. [doi:
10.1145/3338507.3358616]
[11] Clements AA, Gustafson E, Scharnowski T, Grosen P, Fritz D, Kruegel C, Vigna G, Bagchi S, Payer M. HALucinator: firmware
re-hosting through abstraction layer emulation. In: Proc. of the 29th USENIX Conf. on Security Symp. USA: USENIX Association,
2020: 1201–1218.
[12] Feng B, Mera A, Lu L. P2IM: scalable and hardware-independent firmware testing via automatic peripheral interface modeling. In:

吴华茂 等: 一种利用动态分析的嵌入式 Linux 应用托管方案

2221

Proc. of the 29th USENIX Conf. on Security Symp. USA: USENIX Association, 2020: 1237–1254.
[13] Liu Q, Zhang C, Ma L, Jiang M, Zhou Y, Wu L, Shen W, Luo X, Liu Y, Ren K. FirmGuide: boosting the capability of rehosting
embedded linux kernels through model-guided kernel execution. In: Proc. of the 36th IEEE/ACM Intl. Conf. on Automated Software
Engineering. Melbourne, Australia: IEEE, 2021: 792–804. [doi: 10.1109/ASE51524.2021.9678653]
[14] Jiang M, Ma L, Zhou Y, Liu Q, Zhang C, Wang Z, Luo XP, Wu L, Ren K. ECMO: peripheral transplantation to rehost embedded
linux kernels. In: Proc. of the 2021 ACM SIGSAC Conf. on Computer and Comm. Security. Republic of Korea: ACM, 2021:
734–748. [doi: 10.1145/3460120.3484753]
[15] Zhou W, Guan L, Liu P, Zhang Y. Automatic firmware emulation through invalidity-guided knowledge inference. In: Proc. of the
30th USENIX Conf. on Security Symp. USA: USENIX Association, 2021: 2007–2024.
[16] Dolan-Gavitt B, Hodosh J, Hulin P, Leek T, Whelan R. Repeatable reverse engineering with panda. In: Proc. of the 5th Program
Protection and Reverse Engineering Workshop. New York, NY, USA: Association for Comp. Machinery, 2015: 1–11. [doi: 10.1145/
2843859.2843867]
[17] Bellard F. QEMU, a fast and portable dynamic translator. In: Proc. of the 2005 USENIX Annual Technical Conf. USA: USENIX
Association, 2005: 41-46.
[18] O’Callahan R, Jones C, Froyd N, Huey K, Noll A, Partush N. Engineering record and replay for deployability. In: Proc. of the 2017
USENIX Annual Technical Conf. USA: USENIX Association, 2017: 377–389.
[19] Shoshitaishvili Y, Wang R, Salls C, Stephens N, Polino M, Dutcher A, Grosen J, Feng S, Hauser C, Kruegel C, Vigna G. SOK: (state
of) the art of war: offensive techniques in binary analysis. In: Proc. of the 2016 IEEE Symp. on Security and Privacy. San Jose,
CA,USA: IEEE, 2016: 138–157. [doi: 10.1109/SP.2016.17]
[20] Intermediate representation - angr documentation. [2023-01-12]. https://docs.angr.io/advanced-topics/ir.
[21] FirmDep artifacts [2023-12-18]. https://gitlab.com/firmdep/firmdep_artifacts.
[22] Buildroot - making embedded linux easy. [2023-01-12]. https://buildroot.org/.
[23] Binwalk. GitHub, [2023-03-22]. https://github.com/ReFirmLabs/binwalk/wiki/Usage.
[24] W3af - open source web application security scanner. [2023-06-16]. http://w3af.org/.
[25] RouterSploit - exploitation framework for embedded devices. [2023-12-18]. https://github.com/threat9/routersploit.

附中文参考文献:
[2] 于 颖 超 , 陈 左 宁 , 甘 水 滔 , 秦 晓 军 . 嵌 入 式 设 备 固 件 安 全 分 析 技 术 研 究 . 计 算 机 学 报 , 2021,44(5):859-881.
http://cjc.ict.ac.cn/online/onlinepaper/yyc-202159181446.pdf [doi: 10.11897/SP.J.1016.2021.00859]


软件学报 ISSN 1000-9825, CODEN RUXUEW
Journal of Software, [doi: 10.13328/j.cnki.jos.007022]
©中国科学院软件研究所版权所有.

E-mail: jos@iscas.ac.cn
http://www.jos.org.cn
Tel: +86-10-62562563

一种基于突变流量的在野黑产应用采集方法

∗

陈沛 1, 洪赓 1, 邬梦莹 1, 陈晋松 1, 段海新 2,3, 杨珉 1
1

(复旦大学

计算机科学技术学院,

2

(清华大学

网络科学与网络空间研究院, 北京

3

(中关村实验室, 北京 100081)

上海

201203)
100084)

通讯作者: 杨珉, E-mail: m_yang@fudan.edu.cn

摘 要:

随着经济社会的快速发展, 互联网黑色产业(也称互联网地下产业, 以下简称网络黑产)对人民群众的生

产生活带来的影响也在快速扩大. 近年来, 移动互联网的兴起使以诈骗、博彩和色情为主的网络黑产移动应用
(APP)变得更加猖獗, 亟待采取有效措施进行管控. 目前研究人员针对黑产应用的研究较少, 其原因是由于执法部
门持续对传统黑产应用分发渠道的打击, 已有的通过基于搜索引擎和应用商店的采集方法的效果不佳, 缺乏大规
模具有代表性的在野黑产应用数据集已经成为开展深入研究的一大掣肘. 为此, 本文尝试解决在野黑产应用大规
模采集的难题, 为后续深入全面分析黑产应用及其生态提供数据支撑. 本文提出了一种基于突变流量分析的黑产
应用批量捕获方法, 以黑产应用分发的关键途径为抓手, 利用其具有的突变和伴随流量特点, 批量快速发现正处
于传播阶段的新兴在野黑产应用, 为后续实时分析和追踪提供数据基础. 在测试中, 本方法成功获取了 3,439 条应
用下载链接和 3,303 个不同的应用. 捕获的移动应用中, 不但有 91.61%的样本被标记为恶意软件, 更有 98.14%的
样本为首次采集发现的零天应用. 上述结果证明了本文提出的方法在黑产应用采集方面的有效性.
关键词: 互联网地下产业;网络黑产;移动应用;流量分析
中图法分类号: TP311
中文引用格式: 陈沛,洪赓,邬梦莹,陈晋松,段海新,杨珉.一种基于突变流量的在野黑产应用采集方法.软件学报.
http://www.jos.org.cn/1000-9825/7022.htm
英文引用格式: Chen P, Hong G, Wu MY, Chen JS, Duan HX, Yang M. An underground industry application collection method
based on flow analysis. Ruan Jian Xue Bao/Journal of Software (in Chinese). http://www.jos.org.cn/1000-9825/7022.htm

An underground industry application collection method based on flow analysis
CHEN Pei1, HONG Geng1, WU Meng-Ying1, CHEN Jin-Song1, DUAN Hai-Xin2,3, YANG Min1
1

(School of Computer Science, Fudan University, Shanghai 201203, China)

2

(Institute for Network Sciences and Cyberspace, Tsinghua University, Beijing 100084, China)

3

(Zhongguancun Lab, Beijing 100081, China)

Abstract:

In recent years, with the rise of the mobile Internet, underground mobile applications primarily involved in scams, gambling,

and pornography have become more rampant, requiring effective control measures. Currently, there is a lack of research on underground

∗

基金项目: 国家自然科学基金(62302101)
陈沛和洪赓为共同第一作者, 作者顺序依据姓氏首字母排序.
收稿时间: 2023-09-11; 修改时间: 2023-10-30; 采用时间: 2023-12-15; jos 在线出版时间: 2024-01-05

陈沛 等: 一种基于突变流量的在野黑产应用采集方法

2205

applications by researchers. Due to the continuous crackdown by law enforcement agencies on traditional distribution channels for these
applications, the existing collection methods based on search engines and app stores have proven to be ineffective. The lack of
large-scale and representative datasets of real-world underground applications has become a major constraint for in-depth research.
Therefore, our paper aims to address the challenge of collection of large-scale real-world underground applications, providing data
support for a comprehensive in-depth analysis of these applications and their ecosystem. We propose a method to capture underground
applications based on traffic analysis. By focusing on the key distribution channels of underground applications and leveraging their
characteristics of mutation and accompanying traffic, we can discover in-the-wild underground applications in the propagation stage. In
the test, this method successfully obtained 3,439 application download links and 3,303 distinct applications. Among the apps, 91.61% of
the samples were labeled as malware by antivirus engine, while 98.14% of the samples were zero-days. The results demonstrate the
effectiveness of our proposed method in the collection of underground applications.
Key words:

underground ecosystem; underground app market; mobile apps; traffic analysis

随着经济社会快速发展, 互联网黑色产业(以下简称网络黑产)的犯罪手段和范围也迅速发展. 近年来,
随着移动互联网的兴起, 以诈骗、博彩、色情为代表的网络黑产移动应用(APP)日渐猖獗, 严重影响人民群
众生产生活. 360 数字安全集团发布的《2022 年度反诈报告》显示, 近一年新增了 2400 万恶意应用, 日均新
增 6.5 万, 其中主要以投资理财、色情和赌博三类为主[1]. 随着人民生产生活对于移动应用日渐依赖, 传统
意义的犯罪形式进一步向移动端迁移. 公安部数据表明, 截止 2021 年, 通过诈骗 APP 实施的电信网络诈骗
发案已经成为电信网络诈骗案的主要形式, 占比超过 60%[2]. 因此, 亟需采取措施对黑产应用加以管控.
当前, 已有对于网络黑产攻击方法研究主要关注于黑产网站检测方面, 网站数据多来自于搜索引擎和
公开数据集. Yang 等人[3]通过爬取合作搜索引擎获得了其索引的大量网站候选域名, 使用自然语言处理技术
过滤识别大量非法博彩网站, 并对博彩网站的推广策略、第三方支付渠道、网络存储服务等进行了研究. 此
外, 部分研究者使用内容审核领域中的公开图片或视频数据集, 结合文本及图像特征采用机器学习方法对
敏感内容进行检测[4-8]. 除了被动检测外, 网络空间资产测绘引擎 Shodan[9] 、Zoomeye[10] 等通过扫描互联网
IPv4 地址并匹配黑产指纹特征, 主动探测黑产网站.
已有研究中, 对于黑产应用的研究还较为匮乏, 其首要掣肘于缺乏鲜活的在野黑产应用数据集. 当前,
大多数研究的黑产应用样本需要研究者通过搜索引擎收集或需要研究者与有关部门建立联系. Gao 等人[11]
通过爬取搜索引擎结果中的博彩网站内嵌入的博彩应用, 系统地分析了博彩应用的生态和非法博彩应用程
序之间的联系. 这种基于在搜索引擎利用关键词搜索并过滤的收集方式依赖于搜索关键词, 一方面局限于
对于某些已知特定领域, 且由于搜索引擎存在安全检测机制, 大多数如博彩网站在内的违法网站并不会被
搜索引擎所收录, 因此这种应用收集方法效率较低. Hong 等人[12]基于匿名机构提供的 1,487 个赌博应用程序
开展了博彩诈骗的实证研究, 此种研究方法收集应用程序的丰富度和效率较低, 在样本采集的扩展性上较
为受限.
本文希望解决黑产应用的大规模采集问题, 为后续开展黑产应用深入研究奠定良好基础. 但在野黑产
应用的批量采集并非易事, 当前黑产应用分发传播往往呈现出高对抗性, 这为大规模采集工作带来了诸多
挑战: 首先, 黑产应用为了躲避监管多选择隐匿传播, 因此不会在常规应用商店中上架推广. 其次, 黑产团
伙在自身网站中推广少量黑产应用, 但黑产网站数量级远远超过其推广的黑产应用, 通过黑产网站采集方
案的效率低下.
针对现有黑产应用收集难题, 本文提出了一种基于突变流量分析的黑产应用发现方法. 该方法不依靠
单个黑产网站或人工收集, 而是基于黑产应用分发中的关键途径对黑产应用进行批量捕获. 该方法基于以
下观察:
（1）

随着正规应用商店(如华为应用市场、360 应用商店)中批量取缔下架黑产应用, 当前, 在黑产
应用往往通过一类特殊的网站, 分发黑产应用, 我们称作黑产门户. 黑产门户网站往往提供

Journal of Software 软件学报 Vol.XX, No.

2206

X, Jan 2024
了大量黑产应用的下载链接, 部分黑产门户同时为多个黑产团伙应用提供下载服务. 同时,
黑产门户网站还会定期更换应用下载链接, 从而保证下载应用的有效性. 因此, 黑产门户可
以作为下载新鲜、正处于传播过程中的黑产应用的渠道.
（2）

黑产门户网站除了分发黑产应用, 通常还添加多种黑产网站广告链接、为各类黑产网站引流
来获取利润. 首先, 由于黑产门户网站本身具有较大流量, 被引流网站在引流前后的访问量
量级差距明显, 存在访问量突变. 其次, 用户在连续访问黑产门户和被引流黑产网站时, 其访
问序列表现出时序临近关系. 因此, 域名流量突变特征可以作为批量发现黑产门户网站的重
要思路.

本文提出的方法利用黑产门户网站作为黑产应用采集来源, 可有效地解决传统应用收集方式存在的问
题. 首先, 基于被动 DNS 数据库计算获取发生访问量突变的域名, 它们是潜在的被引流对象. 然后, 通过伴
随算法反查访问者在临近时间内的访问序列, 得到的网站可能就是为突变域名引流的门户网站, 最后, 本文
构建了基于多种特征的二分类模型, 通过模型判别得到黑产门户, 通过这些黑产门户, 便可以下载到应用.
基于传播渠道的方案存在以下优点: 采集数量大、效率高, 从一个黑产门户中往往可以采集到大量应用; 数
据来源鲜活, 有助于找到正处于分发阶段的在野黑产应用, 更具分析与追踪价值.
我们使用 2023 年 5 月的被动 DNS 数据对该方法的发现能力进行了评估. 经测试, 累计获取 3,439 条应
用下载链接和 3,303 个不同的移动应用. 为了评估这些应用的类型, 我们使用 VirusTotal[13]对下载到的应用
进行检测, 发现其中 91.61%的应用被至少一个引擎检出为恶意软件, 98.14%的应用为零天应用, 证明了本
文方法可有效发现黑产应用.
本文第 1 节介绍本文使用到的相关技术的基础知识和概念, 包括被动 DNS 技术和黑产门户的概念. 第 2
节介绍本文构建的基于突变流量分析的黑产应用发现方法. 第 3 节通过真实数据测试实验验证所提算法的
有效性, 并对本方法进行评估. 第 4 节介绍网络黑色产业链的相关研究现状. 最后第 5 节总结全文.

1 背景知识
本文所提方法主要基于被动 DNS 和黑产门户概念, 本节就相关概念和基本知识予以介绍.
1.1 被动DNS原理
被动 DNS(Passive DNS)是一种用于收集和记录域名系统(DNS)活动的技术和方法[14-15]. 与主动 DNS 通
过用户向 DNS 服务器查询域名解析信息不同, 被动 DNS 将全球域名系统中可用的 DNS 数据信息重建到中
央数据库中, 以便研究人员对 DNS 解析进行反向检索和查询.
被动 DNS 的工作原理如下:
(1) 数据收集: 被动 DNS 服务器监听网络中的 DNS 查询, 并返回相应的响应数据包.
(2) 数据记录: 收集到的 DNS 查询和响应数据包会被记录下来, 并保存为结构化的数据. 通常, 被动
DNS 服务器将这些数据存储在数据库中, 以便后续的查询和分析.
(3) 数据分析: 通过对被动 DNS 数据进行分析, 可以获得有关域名的详细信息, 例如域名的解析历史、
IP 地址变化、域名关联关系等. 此类数据往往用于网络安全监测、威胁情报分析、恶意域名检测等方面.
在本研究中, 被动 DNS 数据项可以被形式化地概括为三元组
< IP, domain, timestamp >

(1)

该三元组的含义为, 某一个客户端 IP 在某一时刻(timestamp)发出了一个 DNS 查询请求, 该请求内容为
查询 domain 的实际地址. 当三元组数量足够大时, 通过聚合操作可以侧面表示更多用户信息. 例如, 对于
IP 相同的不同三元组, 此序列反映了该 IP 用户(或用户组)在不同时刻请求解析的域名及请求时刻, 这些时
序行为数据可被用于分析用户行为; 对于 domain 相同的不同三元组, 此序列反映了该域名在被不同用户(或
用户组)访问的时刻与次数, 此行为数据可被用于域名访问分析, 寻找潜在的恶意访问流量等.

陈沛 等: 一种基于突变流量的在野黑产应用采集方法

2207

值得注意的是, 出于对用户隐私保护的考量, 本文中所涉及的客户端 IP 均已匿名化处理, 不含任何个
人识别信息(PII, Personally Identifiable Information).
1.2 网络黑产应用产业链
网络黑产是指利用互联网技术实施网络违法行为, 以及为这些行为提供工具、资源、平台等准备和非法
获利变现的渠道与环节 [16]. 黑产相关违法行为对个人和企业都会造成严重的影响. 意大利信息安全协会在
最近一份报告中称, 仅 2021 年, 黑客攻击和各种网络犯罪就给全球经济造成了超过 6 万亿美元的损失, 预计
到 2025 年, 此类犯罪每年将使全球经济损失约 10.5 万亿美元[17].
随着监管及执法机构对黑产网站的快速封禁打击, 网络黑产逐渐将犯罪活动转移到移动端平台上. 这
些网络犯罪团伙通过批量制作、分发黑产应用, 将受害者引导至由黑产团伙所控制的应用环境内, 欺骗、诱
导受害者完成大量交易, 最终转化为犯罪分子的金钱收益.
网络黑产应用产业链主要有以下参与者:
(1)

应用制作者: 制作黑产应用的开发者、后台控制团伙. 由于黑产应用快速上线的特性, 他们可
能使用在线应用生成器(Online Application Generator, OAG) 或者私有的应用生成工具批量低
成本制作黑产应用 [12]. 在一些情况下, 应用制作者还可能会操控应用后台, 修改数据, 诱导受
害者上当.

(2)

黑产门户: 批量分发、传播黑产应用的平台. 一方面, 黑产门户是用户直接接触黑产应用的窗
口, 用户可以通过黑产门户浏览、选择和下载的黑产应用. 另一方面, 黑产门户是黑产应用的
流量入口, 黑产应用及其制作者通过购买广告的方式上架黑产门户, 增加其影响力进而提高
收入.

(3)

支付渠道: 为黑产应用中的交易提供支付鲁棒的收款服务. 在目前的监管政策下, 包括银行、
第三方支付渠道(如支付宝、微信支付等)在内的各大支付渠道都禁止黑产应用使用相关服务.
因此出现了专门为黑产团伙提供的高度稳定和匿名的支付渠道及服务, 实现非法交易变现.

(4)

用户: 黑产应用的使用者、受害者. 用户通过黑产门户下载黑产应用, 并在使用应用的过程中
受到犯罪团伙诱导投入个人资金, 或被黑产应用窃取敏感隐私, 导致移动设备无法正常使用
等. 同时, 其金钱投入为整个黑色产业链提供源源不断的外部资金输入.

以上四类参与者在网络黑产应用产业链上的关系如图 1.

图 1 黑产应用产业链示意图(彩印)

Journal of Software 软件学报 Vol.XX, No.

2208

X, Jan 2024
2 基于突变流量分析的黑产应用发现方案设计
本节首先介绍我们提出的基于流量分析的黑产应用发现方法的核心思路和设计上的考虑, 其次介绍方
案的总体框架流程, 以及发现流程中的 3 个主要步骤.
2.1 核心观察
发现并捕获在野黑产应用的关键在于掌握黑产应用的分发渠道. 对于该问题, 本文有以下两点核心观
察:


观察一: 黑产门户是黑产应用的重要分发渠道

在黑产应用的分发过程中, 存在一类特殊的分发网站, 其往往提供了大量黑产应用的下载链接, 并为
多个黑产应用提供分发服务, 我们将其称作黑产门户网站, 简称黑产门户. 同时, 黑产门户为保证下载应用
的有效性, 其运营者会定期更新维护黑产应用下载链接. 利用该特点, 我们可以将黑产门户作为捕获在野应
用的关键渠道.
图 2 展示了黑产门户示例. 如图, 这些网站界面与常规应用商店相似, 其中包含醒目的应用图标和诱
导用户下载的对应按键. 此外, 部分黑产门户还可能带有页面广告, 通过给相关地下产业导流获利.

图 2 黑产门户示例(彩印)
值得注意的是, 黑产门户和传统的黑产网站存在较大的区别: 黑产网站包括网络黑产团伙所部署的赌
博、色情、诈骗网站, 尽管部分黑产网站会通过直接在网站上推荐访问者下载来自同一开发集团的应用, 已
达到部署分发应用的目的. 但此类网站通常只会推广单个应用, 因此无法作为稳定、批量的应用收集渠道.
黑产门户网站往往集成了包含多种类型、多个利益集团的黑产应用下载链接, 并以醒目的色彩诱导用户下载
应用的网站, 是当前受害者下载黑产应用的最主要渠道之一. 同时, 黑产门户通常流量规模较大, 为通过分
发应用获利, 其内嵌的应用下载渠道普遍具有较高的及时性和多样性, 因此若能对通过黑产门户传播的应
用进行有效侦测, 则能有效捕获较多的在野黑产应用.


观察二: 黑产门户与其下游的网站的访问量存在显著关联特征

网站间的引流关系指的是不同网站之间通过直接嵌入广告跳转链接或间接文字提示建立起的流量引导
关系. 它的主要表现为, 引流网站通常访问量较大、流量稳定, 而且被引流网站在被引流前通常访问量较低,
被引流后发生访问量突增. 在本文所研究的问题中, 黑产门户网站通常具有稳定的流量, 它们通常还添加多
种黑产网站广告链接、为各类黑产网站引流来获取利润. 因此, 这种引流行为使得双方的流量关系可被建模.
本文进一步形式化地定义一种网站间的引流关系, 该关系建模目的为帮助定位黑产门户.
(1)

突变域名: 定义发生了访问量突变的被引流网站为突变域名. 黑产门户网站作为高流量平台, 向
下游网站导流行为会为被引流网站带来大量用户访问, 从而对被引流网站的访问量产生显著

陈沛 等: 一种基于突变流量的在野黑产应用采集方法

2209

的影响. 这将导致被引流网站在此前后的访问量量级差距明显, 导致被引流网站出现访问量
突然变化. 利用这一现象, 使得我们从突变现象出发寻找背后的黑产门户成为可能, 基于此,
可以进一步监测黑产活动, 从而采取相应的防范措施.
(2)

伴随域名: 定义与突变域名存在临近访问时序关系的域名集合为伴随域名. 引流关系的实质是,
用户在访问黑产门户后, 在临近时间内由黑产门户中的引流链接跳转到了被引流网站上. 这
一操作使得用户的访问序列表现出时序临近关系, 在这个临近关系宏观上表现为被动 DNS 数
据中的时间戳差值在一定阈值内. 基于这种现象, 可以设计伴随算法筛选满足访问时序临近
的域名, 从而能进一步得到黑产门户候选集合, 为批量发现黑产门户网站、黑产应用提供数据
来源. 根据以上定义, 可从与突变域名具有伴随关系的域名中筛选出黑产门户.

图 3 突变伴随关系示意图(彩印)
如图 3 所示, IP1 , IP2 ,..., IPn 为已知突变域名(图中红色网站), 在发生突变当天, 通过被动 DNS 数据反查
到的所有访问 IP. 假设 IPi 访问该突变域名的时间为 T0 时刻, 我们记 IPi 在 [T 0 − ∆T , T 0] 时间段内访问过的所
有域名集合为伴随域名, 那么黑产门户(图中蓝色网站)则可能出现在这一集合中. 为排除用户访问的偶然
性、降低筛选伴随域名数量级, 考虑与所有访问 IP 的产生时序关联的伴随域名, 若某域名在多个 IP 的访问
序列中同时出现, 那么其引流的概率也就越大.
2.2 总体流程
基于 2.1 节的设计思路, 本文提出一种基于突变流量分析的黑产应用发现方法, 该方法由 3 部分组成,
整体流程图如图 4.

Journal of Software 软件学报 Vol.XX, No.

2210

X, Jan 2024

图 4 系统流程图(彩印)
首先, 从被动 DNS 数据库出发, 利用域名的访问量特点完成初筛, 再基于突变指标计算得到突变域名.
由于互联网空间中域名数量众多, 我们在计算突变前将采用域名当日及过去某段时间内解析量、访问量阈值
过滤解析量和访问量较低的域名. 同时, 我们设计了突变指标的计算规则, 将短期访问量指标和长期访问量
指标作为基准值, 测算当日访问量的突变率, 进而得到最终的突变域名集合. 使用访问量预处理可以有效降
低筛选流程数据量, 减少后续流程中不必要的计算量, 提高整体运行效率.
其次, 通过对突变域名的访问序列反查, 建立伴随域名集合. 此步骤是本方法的关键部分, 它利用了用
户访问黑产门户网站和导流的黑产域名的序列在被动 DNS 数据中呈现的关联模式特征, 从突变域名扩展发
现门户域名. 该方法可从访问量发生突变的黑产网站关联分析至不具备明显访问量变化特征的黑产门户网
站, 可用于捕获黑产门户这种持续稳定的分发渠道, 进而更高效地获取到黑产应用. 同时, 由于被动 DNS 数
据只包含了用户请求域名解析信息, 不带有具体某个网站的跳转链路, 因此备选集合中含有大量噪音域名,
此过程需要通过相关算法控制待测网站集合规模.
最后, 利用分类模型判别伴随域名中的黑产门户网站, 并获取其中分发的应用. 此过程首先将针对性地
对上一步骤计算出的伴随域名(即待测网站)进行多维特征提取. 在特征选取时, 充分考虑了黑产门户的传播
特点、视觉特点, 结合图像识别技术和外链特征构造, 并使用了多种机器学习模型进行训练.
2.3 突变算法描述
在本文工作中, 突变概念选取本文 2.1 节的定义. 本文使用被动 DNS 数据库作为数据源, 首先利用访问
量预处理完成域名初筛, 过滤解析量和访问量较低、被引流概率较小的域名, 然后, 基于突变指标计算值测
算突变率, 使用突变阈值 Thresholdshort_spike 和 Thresholdlong_spike 评估域名的突变率是否满足本文的突变概念,
最终输出突变域名集合.
在域名初筛阶段, 本文依据表 1 标准过滤出可能被引流的域名.
表 1 突变域名初筛规则
规则名称

规则描述

初筛方法

域名有效性

判断是否为反向域名

忽略以”.in-addr.arpa”等为后缀的反向域名

域名解析
域名泛解析

判断域名是否被解析过
判断域名是否为泛解析域名

访问客户端数>=1
解析过的子域名数量<=50,000,000

域名解析量
域名流行度

判断域名是否有足够解析
判断是否为信誉较高的域名

30,000<=解析量<=1,000,000,000
Tranco[18]域名排名>1000

对完成过滤后的域名, 本文定义最近两日的总请求量比率、访问客户端 IP 数比率, 来反映当日访问较

陈沛 等: 一种基于突变流量的在野黑产应用采集方法

2211

前一日的突变程度:
Short _ Spike _ Rate域名总请求量 =

Short _ Spike _ Rate访问客户端IP数 =

域名请求量today
域名总请求量last _ day
访问客户端IP数today
访问客户端IP数last _ day

(2)

(3)

同时, 为了降低偶然性, 使用一段时间内总请求量、访问客户端 IP 总量的平均值代表该域名在被引流
前的平均情况, 重定义上述变量来反映当前访问情况较过去一段时间内的突变程度:
Long _ Spike _ Rate域名总请求量 =

Long _ Spike _ Rate访问客户端IP数 =

域名总请求量today
平均每日域名总请求量last _10 _ days
访问客户端IP数today
平均每日访问客户端IP数last _10 _ days

(4)

(5)

本节, 我们使用 Thresholdshort_spike 和 Thresholdlong_spike 作为突变阈值, 同时满足以下条件的即为突变域
名.
Short _ Spike _ Rate域名总请求量 > Threshold short _ spike

(6)

Long _ Spike _ Rate域名总请求量 > Thresholdlong _ spike

(7)

Short _ Spike _ Rate访问客户端IP数 > Threshold short _ spike

(8)

Long _ Spike _ Rate访问客户端IP数 > Thresholdlong _ spike

(9)

2.4 伴随算法描述
在本文工作中, 伴随概念选取本文 2.1 节的定义. 本节算法使用上一节输出的突变域名集合作为输入,
同时利用被动 DNS 作为数据源, 利用主动反查用户访问序列的方式从突变域名的时序流量信息中提取对应
的伴随域名, 最终输出伴随域名集合.
获取某日突变域名的伴随域名的算法阐述如下:
(1) 输入突变域名集合 spike_domain_set, 伴随时间窗口长度 ∆t , 伴随域名抽取阈值 ThresholdIP;
(2) 置伴随集合 accompany_domain_set 为空;
(3) 对 spike_domain_set 中的每个突变域名 domain spike:
a)

获取在发生访问量突变当天访问过该域名的所有客户端 IP 地址 visit_IPs;

b)

对每个访问

, 获取该客户端 IP 在时间窗口∆t 内的被动 DNS 请求序列 Query_Seqi,提取请

求序列中的请求域名 Query_Domainsi ;
c)

对 Query_Domainsi 中的每个请求域名 Query_Domainsi,j 计数,计数结果存入 Domain_Counter;

d)

Domain_Counter 中为所有关联 IP 的请求域名计数结果, 若某一请求域名 domaink 同时满足
(10)
domaink ∈ visit _ IPs
Domain _ Counter[domaink ]
> Threshold IP
num(visit _ IPs )

(11)

Journal of Software 软件学报 Vol.XX, No.

2212

X, Jan 2024
说明有超过 ThresholdIP 比例的客户端 IP 在访问突变域名 domainspike 前均访问了 domaink, 因
此 domaink 可能为造成突变域名 domainspike 的黑产门户域名, 则将 domaink 加入伴随集合
accompany_domain_set;
(4) 最终输出 accompany_domain_set 即为当日的所有突变域名对应的伴随域名集合.
2.5 黑产门户网站特征表示与判别
为了从大量伴随域名判别出黑产门户网站, 从黑产门户网站的引流功能出发, 深入解析引流网站的四
个方面的特征:
(1)

域名元数据: 黑产门户网站通常具有稳定且较高的访问流量, 以便支撑黑产门户为其他网站导
流, 这也是黑产门户能够通过放置推广链接盈利的原因. 表现在特征上, 黑产门户通常具有较
高的访问量绝对值和较小的方差. 此外, 网站的访问总量和访问 IP 数分别反映了网站被请求
的总次数和访问的用户数量, 在实践中可以代表网站流量和实际影响人数, 因此本文同时分
析这两个量;

(2)

网页外链特征: 黑产门户网站往往含有比普通网站更多的外链数、渠道数. 外链是指此网站之外
的外部链接, 外链数越多, 其引流特征越明显. 渠道是一类特殊的用于计算网站访问量的外
链,通常由第三方数据统计平台(如百度统计 [19])提供服务, 由渠道统计得到的访问量往往作为
结算门户网站推广费的依据,为此,黑产门户中嵌入的渠道量往往高于普通网页. 由于黑产门户
分发平台的特性, 其内容外链往往来自不同的二级域名(Second-Level Domain, SLD)、不同的完
全限定域名(Fully Qualified Domain Name, FQDN), 当 FQDN 相同的时候, 外链路径(path)不同
时也会传播不同的黑产应用, 因此以上特征均能表现黑产门户分发移动应用的能力.

(3)

网页源码特征: 网页布局、网页主题等信息可以特异性指征黑产门户. 首先, 由于黑产门户的设
计初衷是用于大量展示并分发应用, 因此在页面结构上倾向于频繁使用列表等并列结构. 其
次, 黑产门户不同于普通门户, 其页面内容中具有明显的黑产相关语义, 以及诱导用户下载、
点击的引导语义, 其中文本语义特点可供表征其黑产特性.

(4)

网页截图特征: 黑产门户网站需要吸引用户点击以增加访问量, 这一需求反映到网页图像上则呈
现出明亮、鲜艳等特征. 本方法在爬取待测网站的同时保存了网页截图, 利用截图中的视觉效
果, 将图像转化为可被量化的指标, 基于不同算法提取图像的亮度、饱和度、对比度可用于区
分黑产门户.

最终本文在以上四个方面共提取到了 21 个特征作为黑产门户的识别依据, 具体特征及特征类型见表 2.
表 2 黑产门户识别特征及特征类型
特征类别

域名元数据

网页外链内容

特征内容

特征类型

30 天内访问量
30 天内访问量方差

数值序列
数值

30 天内访问 IP 数
30 天内访问 IP 数方差

数值序列
数值

外链数量

数值

外链中图片的比例
外链中的 SLD 数量

数值
数值

SLD 数量/总外链数量
外链中的 FQDN 数量

数值
数值

FQDN 数量/SLD 数量

数值

外链的 path 数/FQDN 数量
渠道数

数值
数值

陈沛 等: 一种基于突变流量的在野黑产应用采集方法

网页源码内容

网页截图

2213

列表数

数值

最大列表宽度
“下载”相关语义词数

数值
数值

“黑产”相关语义词数

数值

亮度
亮度(luminosity 方法)

数值
数值

饱和度
空白对比度

数值
数值

对比度

数值

在提取到上述特征以后, 本方法将利用机器学习模型对提取到的特征进行处理, 精确识别黑产门户网
站, 并输出识别结果. 为避免不同维度的数据分布、数量级、单位的不兼容性, 因此选择采用不依赖量纲的
随机森林分类模型. 由于决策树的简洁性和森林构建的随机性, 该算法相比其他算法能够有效地在大数据
集上运行, 并且可以判断不同特征的重要程度和相互的影响, 泛化能力强. 对于不同类型机器学习模型的效
果评估见 3.2 节 RQ2. 具体而言, 本文首先构建了多个互不关联的二分类决策树, 将它们通过随机方式建立
成为森林, 在每次训练中, 利用 Bagging 策略算法从汇总得到的各分类树结果中计算并输出精确的甄别结
果. 通过对不同决策树数量及最小叶节点样本数、最小分割叶节点样本数进行了梯度实验, 确定最终的随机
森林模型中最小叶节点样本数为 1, 最小分割叶节点所需样本数为 8, 决策树数量为 200.

3 实验分析
本节以 2023 年 5 月 1 日至 2023 年 5 月 31 日共 31 天的被动 DNS 流量为数据源开展研究, 通过一系列
实验, 评估本文提出的基于突变流量分析的黑产应用发现方法的有效性. 本节首先介绍相关实验方法和主
要评估指标(第 3.1-3.2 节), 然后围绕以下 5 个研究问题开展实验验证(第 3.3 节).
RQ1: 本文方法累计发现的黑产门户及应用有多少?
RQ2: 本文方法效率相比基准方法有多大提升?
RQ3: 本文方法是否可以有效检测黑产门户?
RQ4: 本文方法是否可以有效获得零天应用?
RQ5: 本文方法捕捉到的应用类型有哪些?

3.1 实验方法
本实验使用来自某匿名的安全公司提供的被动 DNS 数据源(包含中国最大的 DNS 提供商 114DNS 的公
共 PDNS 数据), 实验时间为 2023 年 6 月 15 日-24 日共 10 天, 运行服务器环境为 5 台 Ubuntu 18.10 服务器
(16 核,32G 内存)执行分布式网络爬虫和模型判别的任务, 和 1 台 Ubuntu 18.04 服务器(32 核,224G 内存)执行
流水化任务管理和数据分析的管理.
对于突变算法模块和伴随算法模块, 本实验选取 2023 年 5 月 1 日至 2023 年 5 月 31 日共 31 天的被动
DNS 流量数据作为突变域名的数据源, 应用本方法自动化计算突变、伴随域名. 为平衡计算和网络资源限
制, 保证实验顺利进行, 我们设置伴随时间窗口长度

为 120 秒, 伴随域名抽取阈值

为 30%, 最

终平均每日突变伴随的计算时间为 7.74 小时.
对于门户判别模块, 我们在前期研究中人工标注了 500 个黑产门户网站样本, 选取 Tranco[18]中前 500 个域

Journal of Software 软件学报 Vol.XX, No.

2214

X, Jan 2024
名作为网站白样本. 在模型训练阶段, 我们将数据集按照 4:1 划分为训练集和测试集, 并使用五折交叉验证的
方法评估模型效果. 在实际运行阶段, 我们选取效果最佳的分类器模型, 并对域名进行特征提取, 最后判别是
否为门户域名.
为串联本实验多个流程, 提高运行效率, 我们采用了 redis 作为流水线任务管理工具, 使用 Mysql、
SeaweedFS 作为文件存储数据库, 设置 batch_size 为 100, 每台任务服务器从 redis 中读取 100 个任务后分别完
成对应任务(如爬虫任务、特征提取任务或模型判别任务), 并将数据结果写入数据库内, 多任务的批量执行减
少了每个任务的网络往返时间和服务器处理时间, 从而提高了任务处理的整体性能.
3.2 实验评价指标
本实验中在进行黑产门户网站判别时涉及机器学习模型训练. 该过程为二分类任务. 我们采用准确率、
精确率、召回率、F1 值作为判别结果评估指标. 计算方法如下:
=
准确率

TP + TN
TP
=
, 精确率
TP + TN + FP + FN
TP + FP

(12,13)

TP
2TP
=
, F1值
TP + FN
2TP + FP + FN

(14,15)

=
召回率

3.3 实验结果与分析
• RQ1:本方法累计发现的黑产门户及应用有多少?
本方法运行结果如表 3 所示, 此表展示了在系统测试的 31 日中, 每日发现突变域名、伴随域名、黑产
门户、下载应用等数量的平均值、最大值和最小值. 结果显示, 从每日被动 DNS 数据源出发, 经过突变伴随
算法、黑产门户判别后, 平均每日能发现 2222.48 个黑产门户、下载 1079.19 个应用, 平均每个门户能下载
到 0.48 个应用. 考虑不同门户可能会分发相同应用, 将收集到的应用按 MD5 去重后, 平均每日可以获取到
751.48 个不同的应用. 本方法在 31 天内一共识别出 10,921 个黑产门户, 获取 3,439 个不同的应用下载链接
和 3,303 个不同的移动应用.
进一步地, 本文还分析了捕获应用与黑产门户对应关系. 测试中, 我们观测到分发应用数最多的一个门
户累计分发了 55 个不同的应用, 该现象表明当前黑产应用通过门户进行集中传播.
表 3 发现黑产门户及应用数量(单位:个)
统计量
平均值

突变域名
8746.96

伴随域名
121803.64

黑产门户
2222.48

捕获应用
1079.19

捕获应用(去重)
751.48

最小值

6237

87833

1759

635

475

最大值

11122

147151

2608

2550

1381

• RQ2: 本文方法效率相比基准方法有多大提升?
为了对本文所提出的应用采集方法的效率进行评估, 本文采用通常使用的基于搜索引擎的黑产应用采
集技术作为基准方法. 具体而言, 本次测试中我们选取在中文互联网环境中常用的百度、搜狗、360 搜索引
擎进行实验. 对于每个搜索引擎, 我们采用 50 个黑产相关的关键词与“应用下载”结合, 如“博彩 应用下
载”、“娱乐城 应用下载”等, 并对每组关键词选取前 50 个搜索结果. 之后使用 RQ1 中相同的流程爬取网
站内的所有应用, 其结果如表 4 所示. 上述三家搜索引擎每家爬取了 2500 个网站, 平均采集到 21 个应用, 其
中最多的是百度, 爬取到了 40 个. 基准方法的平均应用采集率(捕获应用数/爬取网站数)为 0.0084, 远低于
本文方法的应用采集率 0.4856.由此表明, 采用基于突变流量分析的方法可以明显提升应用采集率 57.8 倍.

陈沛 等: 一种基于突变流量的在野黑产应用采集方法

2215

表 4 基于搜索引擎的黑产应用采集结果对比(单位:个)
采集渠道
百度搜索引擎
搜狗搜索引擎

疑似网站
2500
2500

捕获应用
40
20

应用采集率
0.016
0.008

360 搜索引擎
搜索引擎(平均)

2500

3

0.0012

2500

21

本文方法(平均)

0.0084

2222.48

1079.19

0.4856

• RQ3:本文方法是否可以有效检测黑产门户?
为了更加准确地判别黑产门户, 本文测试了多种不同的机器学习模型算法, 并对每种算法的参数进行
优化, 最终筛选出效果最佳的分类器模型算法及其对应的最优参数.
具体而言, 我们选取了 6 种较为经典的机器学习分类模型, 分别是决策树模型 [20],随机森林模型 [21],
KNN 模型[22],SVC 模型[23],ADA 模型[24] 和 MLP 模型[25],

并将准确率、精确率、召回率、和 F1 值作为指标

评估模型的效果, 以上指标已在 3.2 节中定义. 通过调整每个模型的关键参数, 最终得到的测试结果见表 5
至表 10.
表 5 决策树模型测试结果

表6

(h 为最大高度, n 为最小叶子节点样本数, s 为最小叶子节点分隔数)

KNN 模型测试结果

(k 为最近邻居的数量, n 为叶子节点的大小)
F1

(h,n,s)

准确率

精确率

召回率

F1

(k,n)

准确率

精确率

召回率

(3, 1, 3)

0.945

0.989

0.900

0.942

(3, 20)

0.785

0.766

0.820

0.792

(3, 1, 5)

0.945

0.989

0.900

0.942

(3, 30)

0.785

0.766

0.820

0.792

(3, 3, 3)
(3, 3, 5)

0.945
0.945

0.989
0.989

0.900
0.900

0.942
0.942

(3, 40)
(5, 20)

0.785
0.730

0.766
0.730

0.820
0.730

0.792
0.730

(5, 1, 3)
(5, 1, 5)

0.945
0.955

0.968
0.979

0.920
0.930

0.944
0.954

(5, 30)
(5, 40)

0.730
0.730

0.730
0.730

0.730
0.730

0.730
0.730

(5, 3, 3)
(5, 3, 5)

0.955
0.955

0.979
0.979

0.930
0.930

0.954
0.954

(7, 20)
(7, 30)

0.735
0.735

0.742
0.742

0.720
0.720

0.731
0.731

表 7 随机森林模型测试结果

表8

ADA 模型测试结果

(M 表示弱分类器数量, α 表示权重缩减系数)

(n 为最小叶子节点样本数, s 为最小分隔样本数, m 为决策树数量)

α

(n,s,m)

准确率

精确率

召回率

F1

(M,

准确率

精确率

召回率

F1

(1, 4, 100)

0.955

0.950

0.960

0.955

(50, 0.01)

0.865

0.974

0.750

0.847

(1, 4, 200)
(1, 8, 100)

0.960
0.955

0.951
0.950

0.970
0.960

0.960
0.955

(50, 0.1)
(50, 1)

0.935
0.915

0.939
0.928

0.930
0.900

0.935
0.914

(1, 8, 200)
(2, 4, 100)

0.970
0.955

0.970
0.950

0.970
0.960

0.970
0.955

(100, 0.01)
(100, 0.1)

0.880
0.940

0.975
0.949

0.780
0.930

0.867
0.939

(2, 4, 200)

0.960

0.960

0.960

0.960

(100, 1)

0.920

0.929

0.910

0.919

(2, 8, 100)
(2, 8, 200)
(2, 9, 200)

0.955
0.955
0.955

0.950
0.950
0.950

0.960
0.960
0.960

0.955
0.955
0.955

(150, 0.01)
(150, 0.1)
(150, 1)

0.935
0.945
0.920

0.958
0.949
0.938

0.910
0.940
0.900

0.933
0.945
0.918

表 9

)

表 10

MLP 模型测试结果
σ 表示激活函数)

(H 表示隐藏层的大小,
(H,

σ

)

(50,tanh)

SVC 模型测试结果

(C 表示惩罚参数, k 表示核函数)

准确率

精确率

召回率

F1

(C,k)

准确率

精确率

召回率

F1

0.855

0.890

0.810

0.848

(0.1,linear)

0.850

0.850

0.850

0.850

Journal of Software 软件学报 Vol.XX, No.

2216

X, Jan 2024
(50,relu)
(100,tanh)

0.870
0.870

0.911
0.911

0.820
0.820

0.863
0.863

(0.1,rbf)
(1,linear)

0.675
0.900

0.664
0.935

0.710
0.860

0.686
0.896

(100,relu)
150,tanh)

0.855
0.860

0.918
0.900

0.780
0.810

0.843
0.853

(1,rbf)
(10,linear)

0.755
0.940

0.726
0.940

0.820
0.940

0.770
0.940

(150,relu)

0.860

0.900

0.810

0.853

(10,rbf)

0.865

0.869

0.860

0.864

通过对比模型测试结果, 我们发现, 随机森林模型的效果显著优于其他模型算法, 在选取(1,8,200)作为
参数的情况下, 该模型的准确率和精确率高达 97%, 能够准确地判别门户域名.
• RQ4:本文方法是否可以有效获得零天应用?
面对执法部门和安全公司的打击和检测, 黑产应用的更新十分迅速. 为此, 作为黑产应用的采集方法,
本研究是否能够采集到鲜活的样本尤为重要. 为科学地评估本文方法是否能够有效捕获零天应用, 我们将
全球最大的恶意软件共享平台 VirusTotal[13] 中记录到的该应用首次上传时间近似为样本最早出现的时间, 并
将该时间与本方法采集到的样本发现时间进行对比, 统计差值, 得到了相应的累计密度分布图.
图 5-(a)记录了差值分布的整体趋势, 可以看到, 我们下载的应用中, 有 98.14%的样本之前从未被
VirusTotal 平台采集, 该结果表明我们的方法能够有效地获取零天应用. 进一步地, 为了能够更清晰地观察
差值的累计概率变化趋势, 我们挑选了差值在 60 天内的分布结果进行分析, 如图 5-(b)所示. 从图中我们可
以发现, 本方法捕获的应用中仅有约 1%的样本为非零天应用, 平缓的增加趋势表明这些应用的上传日期存
在一定的随机性, 从侧面反映了本文的方法能够更加集中、高效地发现零天应用.

图 5 采集应用累计密度分布(彩印)

• RQ5:本文方法捕捉到的应用类型有哪些?
前文实验已经证明, 利用本方法能够有效捕捉到大量零天在野应用. 为了进一步论述本采集方法的有
效性, 本实验将评估利用本文方法所采集到的应用的类型. 特别地,

我们选取了当前综合了大量病毒软件

检测引擎结果的 VirusTotal 作为判断应用类型的检测平台.
具体而言, 我们将采集到的应用上传到 VirusTotal 平台, 并通过解析报告内容, 可以得到 VirusTotal 中集
成的 75 个反病毒引擎对该应用的检测结果, 并从中提取应用所对应的恶意标签, 例如“virus”,“trojan”,
“scam app”等.
统计发现, 在上传的 3303 个应用中, 有 3026 个应用被平台判定为恶意应用, 其中部分应用甚至被检测
出多种不同类型的恶意行为, 例如 MD5 值为 6624151114ba45628c7acd59fa2700c2 的应用检测出的报告中含
有“Virus”、“Trojan”和“PUA”等多种恶意标签. 为了进一步了解应用的恶意标签分布情况,

我们筛选

出识别恶意应用数量排名前五的反病毒引擎, 并统计其提供的恶意标签, 得到了如图 6 所示的恶意标签分
布图. 我们观察到,

IKARUS 检测到的恶意应用最多, 占比 58.2%. 从整体结果上看, 标签为 PUA(46.9%)、

陈沛 等: 一种基于突变流量的在野黑产应用采集方法

2217

Trojan(31.8%) 和 Boogr(28.4%) 的 样 本 数 量 最 多 . 其 次 , 有 不 少 样 本 被 标 识 为 Adlibrary(24.3%) 、
Riskware(19.3%)软件, 表明此类应用有一定的潜在风险. 除此之外, 还有部分样本被判别为 Adware(5.2%)、
Dropper(0.4%)等其他类型的恶意应用. 此外, 由于各反病毒引擎对于样本的恶意标签并未存在统一标准,
因此图中所示的样本标签可能存在不同, 但该不一致情况并不影响本文方法检测在野黑产样本的有效性.
总体而言, 我们从门户中下载到的样本绝大多数(91.6%)都属于恶意应用, 证明了本文提出的方法在黑
产应用采集方面的有效性.

图 6 采集样本恶意标签分布图

4 总

结

在本研究中, 我们对当前网络黑产应用的分发传播现象开展了深入探讨, 并提出了一种有效的大规模
采集在野黑产应用的方法, 旨在解决当前研究中移动应用数据集收集困难的问题. 根据网络黑产应用分发
过程中的特点, 本文提出的基于突变流量分析批量捕获黑产应用的方法, 利用黑产门户这一传播渠道中的
关键环节, 显著改善了在野黑产应用采集过程的效率和质量, 为后续的实时分析与追踪提供了强有力的数
据支持.
我们的测试结果验证了该方法的有效性, 成功获取了 3,303 个不同的应用. 被捕获的移动应用中,
91.61%的样本被至少一个引擎检测为恶意软件, 而 98.14%的样本为首次采集发现的零天应用, 这为后续的
黑产应用的研究和分析提供了珍贵的数据资源.
References:
[1]

360 数字安全. 《2022 年度反诈报告》重磅发布！2023. https://wlaq.gmw.cn/2023-02/18/content_36375612.htm

[2]

中国政府网. 公安部组织开展新一轮集中收网行动依法严厉打击涉电信网络诈骗 APP 技术开发违法犯罪团伙 [EB/OL]. 2
021. http://www.gov.cn/xinwen/2021-05/12/content_5605957.htm

[3]

Yang H, Du K, Zhang Y, et al. Casino royale: a deep exploration of illegal online gambling[C]//Proceedings of the 35th

[4]

Lee P Y, Hui S C, Fong A C M. An intelligent categorization engine for bilingual web content filtering[J]. IEEE Transa

Annual Computer Security Applications Conference. 2019: 500-513.
ctions on multimedia, 2005, 7(6): 1183-1190.
[5]

Ali F, Khan P, Riaz K, et al. A fuzzy ontology and SVM–based Web content classification system[J]. IEEE Access, 201
7, 5: 25781-25797.

[6]

Platzer C, Stuetz M, Lindorfer M. Skin sheriff: a machine learning solution for detecting explicit images[C]//Proceedings
of the 2nd international workshop on Security and forensics in communication systems. 2014: 45-56.

2218

Journal of Software 软件学报 Vol.XX, No.
X, Jan 2024

[7]

Wehrmann J, Simões G S, Barros R C, et al. Adult content detection in videos with convolutional and recurrent neural
networks[J]. Neurocomputing, 2018, 272: 432-438.

[8]

Perez M, Avila S, Moreira D, et al. Video pornography detection through deep learning techniques and motion informati
on[J]. Neurocomputing, 2017, 230: 279-293.

[9]

Shodan. Shodan: Search Engine for the Internet of Everything. https://www.shodan.io/

[10]

Knownsec. Zoomeye – Cyberspace Search Engine. https://www.zoomeye.org/

[11]

Gao Y, Wang H, Li L, et al. Demystifying illegal mobile gambling apps[C]//Proceedings of the Web Conference 2021. 2
021: 1447-1458.

[12]

Hong G, Yang Z, Yang S, et al. Analyzing Ground-Truth Data of Mobile Gambling Scams[C]//2022 IEEE Symposium o
n Security and Privacy (SP). IEEE, 2022: 2176-2193

[13]

VirusTotal. VirusTotal - Free Online Virus, Malware and URL Scanner. https://www.virustotal.com/

[14]

Milly F. What is Passive DNS? A beginner’s guide. 2022. https://www.spamhaus.com/resource-center/what-is-passive-dns-abeginners-guide/

[15]

Reverse IP/DNS Blog & How To Guides to Obtain Reverse IP/DNS Data. 2022. https://dns-history.whoisxmlapi.com/blog/
passive-dns.

[16]

中国国家互联网信息办公室.“网络黑产”到底是啥? 普通人应怎样防范? 这些知识你要知道. 2023. http://www.cac.gov.cn/20
19-09/17/c_1570248615898997.htm

[17]

OF INVESTIGATION F B. 2020 internet crime report[EB/OL]. 2021. https: //www.ic3.gov/Media/PDF/AnnualReport/2020_
IC3Report.pdf.

[18]

Victor Le Pochat, Tom Van Goethem, Samaneh Tajalizadehkhoob, Maciej Korczyński, and Wouter Joosen. 2019. "Tranco:
A Research-Oriented Top Sites Ranking Hardened Against Manipulation," Proceedings of the 26th Annual Network and
Distributed System Security Symposium (NDSS 2019). https://doi.org/10.14722/ndss.2019.23386.

[19]

百度. 网站分析白皮书. https://tongji.baidu.com/web5/image/%E7%99%BE%E5%BA%A6%E5%8F%91%E5%B8%83%E3%8

[20]

Song Y Y, Ying L U. Decision tree methods: applications for classification and prediction[J]. Shanghai archives of psych

0%8A%E7%BD%91%E7%AB%99%E5%88%86%E6%9E%90%E7%99%BD%E7%9A%AE%E4%B9%A6V3.0%E3%80%8B.pdf
iatry, 2015, 27(2): 130.
[21]

Biau G, Scornet E. A random forest guided tour[J]. Test, 2016, 25: 197-227.

[22]

Peterson L E. K-nearest neighbor[J]. Scholarpedia, 2009, 4(2): 1883.

[23]

Gunn S R. Support vector machines for classification and regression[J]. ISIS technical report, 1998, 14(1): 5-16.

[24]

Schapire R E. Explaining adaboost[M]//Empirical Inference: Festschrift in Honor of Vladimir N. Vapnik. Berlin, Heidelber
g: Springer Berlin Heidelberg, 2013: 37-52.

[25]

Gardner M W, Dorling S R. Artificial neural networks (the multilayer perceptron)—a review of applications in the atmos
pheric sciences[J]. Atmospheric environment, 1998, 32(14-15): 2627-2636.

附中文参考文献:
[1] 360 数字安全. 《2022 年度反诈报告》重磅发布！2023. https://wlaq.gmw.cn/2023-02/18/content_36375612.htm
[2] 中国政府网. 公安部组织开展新一轮集中收网行动依法严厉打击涉电信网络诈骗 APP 技术开发违法犯罪团伙 [EB/OL]. 20
21. http://www.gov.cn/xinwen/2021-05/12/content_5605957.htm
[16] 中国国家互联网信息办公室.“网络黑产”到底是啥? 普通人应怎样防范? 这些知识你要知道. 2023. http://www.cac.gov.cn/20
19-09/17/c_1570248615898997.htm
[19] 百度. 网站分析白皮书. https://tongji.baidu.com/web5/image/%E7%99%BE%E5%BA%A6%E5%8F%91%E5%B8%83%E3%8
0%8A%E7%BD%91%E7%AB%99%E5%88%86%E6%9E%90%E7%99%BD%E7%9A%AE%E4%B9%A6V3.0%E3%80%8B.pdf


软件学报 ISSN 1000-9825, CODEN RUXUEW
Journal of Software,2024,35(3):1440−1465 [doi: 10.13328/j.cnki.jos.006825]
©中国科学院软件研究所版权所有.

E-mail: jos@iscas.ac.cn
http://www.jos.org.cn
Tel: +86-10-62562563

*

一种基于算法机制设计的社会法则合成方法
吴 骏 1,2, 曹 杰 1, 王崇骏 2, 谢俊元 2
1

(南京财经大学 江苏省电子商务重点实验室, 江苏 南京 210023)

2

(计算机软件新技术国家重点实验室 (南京大学), 江苏 南京 210023)

通信作者: 王崇骏, E-mail: chjwang@nju.edu.cn

摘

要: 社会法则是在多 Agent 系统中为确立某种目标属性而对各个 Agent 实施的行为限制集合. 在 Agent 具有

“个体理性”及“私有信息”的“策略情况”下, 社会法则合成问题不应建模成通常的优化问题, 而应建模成算法机制设
计问题. “最小化副作用”经常是社会法则需要满足的基本要求. 从博弈论的角度来看, “最小化副作用”与“最大化社
会福利”的概念紧密相关, 可以将“最小化副作用的社会法则合成”建模为一种效率机制设计问题. 不仅需要为给定
目标属性找到有效且社会福利最大的社会法则, 还需要向 Agent 支付适当的金额, 以实现激励相容性和个体理性.
首先基于 VCG 机制设计一种名叫 VCG-SLM 的效率机制, 证明它可满足所有必需的形式属性. 然而, 由于发现可
证明该机制的计算是一个 FPNP-完全问题, 针对性地提出该机制的一种基于整数规划的实现方式 VCG-SLM-ILP,
基于 ATL 语义将分配及支付的计算转化为整数规划, 并严格地证明其正确性, 从而可有效利用目前已非常成熟的
工业级整数规划求解器, 成功解决棘手的机制计算问题.
关键词: 多 Agent 系统; 社会法则; 算法机制设计
中图法分类号: TP18
中文引用格式: 吴骏, 曹杰, 王崇骏, 谢俊元. 一种基于算法机制设计的社会法则合成方法. 软件学报, 2024, 35(3): 1440–1465. http://
www.jos.org.cn/1000-9825/6825.htm
英文引用格式: Wu J, Cao J, Wang CJ, Xie JY. Social Law Synthesizing Method Based on Algorithmic Mechanism Design. Ruan Jian
Xue Bao/Journal of Software, 2024, 35(3): 1440–1465 (in Chinese). http://www.jos.org.cn/1000-9825/6825.htm

Social Law Synthesizing Method Based on Algorithmic Mechanism Design
WU Jun1,2, CAO Jie1, WANG Chong-Jun2, XIE Jun-Yuan2
1

(Jiangsu Provincial Key Laboratory of E-Business, Nanjing University of Finance and Economics, Nanjing 210023, China)

2

(State Key Laboratory for Novel Software Technology (Nanjing University), Nanjing 210023, China)

Abstract: A social law is a set of restrictions on the available actions of agents to establish some target properties in a multiagent system.
In the strategic case, where the agents have individual rationality and private information, the social law synthesizing problem should be
modeled as an algorithmic mechanism design problem instead of a common optimization problem. Minimal side effect is usually a basic
requirement for social laws. From the perspective of game theory, minimal side effect closely relates to the concept of maximum social
welfare, and synthesizing a social law with minimal side effect can be modeled as an efficient mechanism design problem. Therefore, this
study not only needs to find out the efficient social laws with maximum social welfare for the given target property but also pays for the
agents to induce incentive compatibility and individual rationality. The study first designs an efficient mechanism based on the VCG
mechanism, namely VCG-SLM, and proves that it satisfies all the required formal properties. However, as the computation of VCG-SLM
is an FPNP-complete problem, the study proposes an ILP-based implementation of this mechanism (VCG-SLM-ILP), transforms the
computation of allocation and payment to ILPs based on the semantics of ATL, and strictly proves its correction, so as to effectively
utilize the currently mature industrial-grade integer programming solver and successfully solve the intractable mechanism computing

*

基金项目: 国家自然科学基金 (91646204, 92046026, 61876080, 71871109); 国家重点研发计划 (2017YFD0401001, 2018YFB1403400);
江苏省重点研发计划 (BE2019105)
收稿时间: 2021-08-04; 修改时间: 2022-05-05, 2022-07-22; 采用时间: 2022-09-16; jos 在线出版时间: 2023-06-14
CNKI 网络首发时间: 2023-06-15

吴骏 等: 一种基于算法机制设计的社会法则合成方法

1441

problems.
Key words: multiagent system; social law; algorithmic mechanism design

社会法则 (social law) 最初由 Shoham 等人 [1,2] 作为一种协同多 Agent 系统的离线方法提出, 文献 [3−5]
进一步引入了模态和时态逻辑的形式系统, 扩展了社会法则的原始框架, 特别是引入了能对多 Agent 系统进行描
述与验证的交互时态逻辑 (alternating-time temporal logic, ATL)[6,7]及其变种, 从而能更好地表示协同目标, 建模并
发行动. 尽管文献中提出的各种社会法则具有不同的技术细节, 但是他们均拥有一些共同的基本特征, 即社会法则
是对 Agent 可用行动的一个约束集. 通过施加这些约束, 期望一些目标属性在系统中得到满足 [8], 例如保证某些
Agent 能可靠地访问某些资源、避免死锁的发生等.
信息的不完全性及 Agent 的理性行为近年来引起了广泛的关注, 但是却缺失于社会法则的现有框架. 这样的
理性 Agent 总是试图最大化自身的效用, 给社会法则合成带来了一些前所未有的根本挑战.
1) 尽可能少地修改原有系统, 即最小化副作用 [9,10], 是社会法则通常被期望满足的基本属性. 在原始的文献中,
最小化副作用被表述为最少化 Agent 被禁止的可行行动的总数. 而这显然可以被更一般地建模为最大化 Agent 对
最终系统结构属性的估值和. 但是如果这些估值被 Agent 作为私有信息的形式掌握时, 我们并不能直接确定哪个
社会法则是最优的.
2) Agent 选择服从社会法则当且仅当这是有利可图的. 有必要在实施社会法则时考虑每个 Agent 的得失, 而
不是简单地将其作为硬性约束实现 [1,2,8]. 一般可以考虑向各个 Agent 支付一定的金额, 以保证其获得非负的利润.
但是由于 Agent 的估值函数是其私有信息, 我们并不能确定应当向各个 Agent 支付多少钱.
上述事实意味着, 在信息不完全以及 Agent 具有理性行为的策略环境下, 社会法则合成并不适合被建模为通
常的优化问题, 一些必要的 Agent 私有信息必须首先被正确地导出并加以有效利用. 我们发现这个问题可以很自
然地在算法机制设计 (algorithmic mechanism design)[11−13]的理论框架下寻求解决方案: 求分配及支付规则 (函数),
该规则被确立并公布后, 接受各 Agent 同时投标 (汇报) 自己的估值函数, 进而输出一个满足最优化目标的有效社
会法则, 并为每个 Agent 确定一个合适的支付金额以保证所有的 Agent 诚实汇报是其最优选择. 在上述框架下, 问
题转化为寻求 Agent 的结构估值的紧凑表示方法, 使其能在投标中被 Agent 高效、清晰地表述; 正确地设计分配
函数及支付函数, 实现优化目标并激励 Agent 诚实投标、主动服从得到的社会法则的约束; 找出计算分配及支付
函数的有效算法, 使其能适用于社会法则合成的应用环境. 这些问题正是将机制设计引入社会法则领域的核心问
题与关键挑战.
本文围绕上述核心问题展开, 取得的主要进展如下.
1) 证明在基于交互时态逻辑 ATL 的社会法则合成问题中, 任何不区分交互互模拟 (alternating bisimulation)
等价结构 [14,15]的估值函数均可被等价地表示为“公式-数值”对的集合. 由于互模拟等价实质上描述的是数学上的
等价关系, 这意味着我们为通常情况下的估值函数均找出了一种具有良好理论基础的紧凑表示方法.
2) 发现策略情况下的社会法则合成可被建模成一类特殊的效率机制设计问题, 其中供选择的社会法则实质上
是一种非独占、非竞争的公共货物 (public goods)[16], 且 Agent 对社会法则的估值可正可负. 对于该类问题著名的
VCG (Vickrey-Clarke-Groves) 机制 [16−18] 显然能提供一种激励相容的解决方案, 但是如何保证个体理性 (所有
Agent 均自发服从机制) 并无已知解决方案. 然而我们进一步证明了基于 Clarke 基准规则 (Clarke pivot rule)[16]可
设计一种使所有 Agent 服从机制是纳什均衡的支付规则, 这意味着没有 Agent 会单方面地脱离机制的约束——这
仍然是一种有意义的个体理性保证.
3) 证明上述机制的计算是是 FPNP-完全的, 这意味着不能期望为分配与支付的计算找到多项式时间复杂度的
算法. 我们将分配与支付的计算转化为整数规划来实现, 基于 ATL 的语法及语义系统地导出了整数规划的优化目
标函数及约束集, 并严格地证明了其正确性以及多项式的时间复杂度. 由于整数规划是一类已被深入研究的难计
算问题, 已有许多成熟的工业级整数规划求解器, 这实际上为上述机制的计算问题设计了一种现实可行的算法; 同
时也为进一步的近似机制的研究奠定了良好的基础.

1442

软件学报 2024 年第 35 卷第 3 期

4) 鉴于求解整数规划往往不能得到精确的社会福利最大的分配, 而这会损害 VCG 机制的激励相容性, 本文
基于 Nisan 等人 [19]提出的再次机会机制 (second chance mechanism) 实现了一种克服该问题的机制.
本文第 1 节介绍相关的背景知识, 包括 ATL 逻辑及社会法则, 并总结现有的相关工作. 第 2 节导出一种紧凑
的估值函数表示方法, 将策略情况下的社会法则合成问题建模成一类面向公共货物的效率机制设计问题, 并提出
一种满足激励相容及个体理性的机制. 第 3 节研究上述机制的计算复杂性, 针对性地提出一种基于整数规划的实
现方法. 第 4 节总结全文, 并对未来值得关注的研究点进行探讨.

1 研究背景
1.1 交互时态逻辑
交互时态逻辑 ATL 扩充了经典的计算树逻辑 (computation tree logic, CTL)[20], 是一种已被深入研究的用于对
并发博弈结构 (concurrent game structure, CGS) 进行描述、推理与验证的逻辑. 而并发博弈结构被认为是一种置身
于外部环境中、行动的效果具有不确定性的开放系统的通用模型 [6,7].
定义 1 (并发博弈结构). 并发博弈结构可表示为元组 S = ⟨k, Q, qs , Π, π, ε, δ⟩ , 包含如下组件.
1) Agent 集合 Ag = [k] = {1, . . . , k} .
2) 状态的有限集 Q , 其中 qs ∈ Q 为初始状态.
3) 命题符的有限集 Π .
4) 标注函数 π (为每个状态 q ∈ Q 指定一个在该状态为真的命题符集 π(q) ⊆ Π ).
5) 行动函数 ε (为每个 Agent i ∈ Ag 以及每个状态 q ∈ Q 指定 Agent i 在该状态下可用的行动集合 ε(i, q) . 我们
假定该集合为非空集合; 所有 Agent 在任意状态 q ∈ Q 的联合行动为元组 ⟨ j1 , . . . , jk ⟩ , 其中 ji ∈ ε(i, q) 为 Agent i 采
取的行动; D(q) 表示状态 q 上的联合行动空间 ε(1, q) × . . . × ε(k, q) ).
6) 状态转换函数 δ (对于每个状态 q ∈ Q 以及每个联合行动 ⟨ j1 , . . . , jk ⟩ ∈ D(q) , δ(q, j1 , . . . , jk ) ∈ Q 为在状态 q 执
行联合行动 ⟨ j1 , . . . , jk ⟩ 导致的下一状态).
在并发博弈结构中, 环境的概念是相对的, 任意 Agent 集合 A ⊆ Ag 均可被看作一个联盟, 同时剩下的 Agent (即
Ag \ A ) 表示该联盟所处的环境. 由所有 Agent 组成的联盟 Ag 叫做宏联盟 (grand coalition), 其所处的环境是空集 ∅ .

我们将用符号 m
⃗ 来表示 (所有 Agent 的) 联合行动 ⟨ j1 , . . . , jk ⟩ ; 用 m
⃗ A 来表示联盟 A ⊆ Ag 的联合行动 (或者被叫
作 A-行动). 为了表述的方便, 我们通常把 A-行动看作行动的向量 (其中行动按所属 Agent 的 ID 从小到大排列), 而
不是看作行动的集合. 此外, 我们还用 DA (q) 来表示在状态 q 下所有可能的 A-行动; 并且用 m
⃗ 中联盟 A 的
⃗ |A 表示 m
联合行动, 即 m
⃗ A 是一个 A-行动, 且对于任意的 i ∈ A , 我们均有 m
⃗ A [i] = m
⃗ [i] .
⃗ |A = m
⃗ A , 其中 m
关于并发博弈结构的一些重要概念可定义如下.
1) 对于两个状态 q 和 q′ , q′ 被称为 q 的后继 (或者被称为一个 q -后继) 当且仅当存在一个联合行动 m
⃗ 使得
⃗ ) . 我们用 qδq′ 表示 q′ 为 q 的后继.
q′ = δ(q, m

2) 一个状态序列 λ = q0 , q1 , q2 , . . . 称作 S 的一个计算当且仅当对于每一个 i ⩾ 0 , 状态 qi+1 都是 qi 的后继.
3) 从状态 q 开始的一个计算称为一个 q -计算. 对于计算 λ 和位置 i (i ⩾ 0) , 我们用 λ[i] , λ[0, i] 和 λ[i, ∞] 来分别
表示 λ 的第 i 个状态, λ 的前缀 q0 , q1 , . . . , qi 和 λ 的后缀 qi , qi+1 , . . .
4) Agent a 的策略 fa ∈ Σ 是从有限非空的状态序列 λ ∈ Q+ 到行动的函数映射: 如果 λ 的最后一个状态是 q , 则
有 fa (λ) ∈ ε(a, q) .
5) 在状态 q ∈ Q , 联盟 A ⊆ Ag 的联合行动 m
⃗ A 的结果被记作 out(q, m
⃗ A ) , 包括此联合行动可能导致的所有下一
状态. 状态 q′ 在 out(q, m
⃗ |A = m
⃗ A 且 σ(q, m
⃗ A ) 中当且仅当 m
⃗ ∈ D(q) , 使得 m
⃗ ) = q′ .
6) 对于每一个联盟 A ⊆ Ag , 如果其中每个 Agent 采取一个策略, 那么所有这些策略的集合就被称为一个 A 的
联合策略 (或者被称为一个 A -策略), 记作 F A . 在任意状态 q ∈ Q 上, 联合策略 F A 的结果是一个包含所有的它可能
导致的计算的集合, 记作 out(q, F A ) . 计算 λ = q0 , q1 , q2 , . . . 在 out(q, F A ) 中, 当且仅当 q0 = q , 且对于任意 i 存在联合行

吴骏 等: 一种基于算法机制设计的社会法则合成方法

1443

动 ⟨ j1 , . . . , jk ⟩ ∈ D(qi ) , 使得 (1) 对于所有的 Agent a ∈ A , ja = fa (λ[0, i]) ; (2) δ(qi , j1 , . . . , jk ) = qi+1 .
由以上定义可以看出, 并发博弈结构实质上描述的是多 Agent 系统的状态转换: 在每个状态各个 Agent 可以
同时选择一个行动, 得到的行动组合决定系统的下一状态. ATL 把 CTL 中经典的路径选择量词 ∃ 和 ∀ 替换成用以
表示 α -效力的路径选择量词 《》 . 《A》ψ 表示联盟 A 有一个联合策略, 使得不管联盟 A 之外的 Agent 采取什么行
动, ψ 都能成立——也就是说联盟 A 中的 Agent 通过合作能克服环境的影响, 可靠地使 ψ 被满足.
定义 2 (ATL 语言). 令 Π 为命题的有限集且 Ag = {1, . . . , k} 为 Agent 的有限集, ATL 公式可由以下语法生成:
φ ::= p|¬φ|φ1 ∨ φ2《A》⃝
|
φ|《A》□φ|《A》φ1 Uφ2 ,

其中, p ∈ Π 是一个命题符, A ⊆ Ag 是一个 Agent 联盟. ATL 语言 LATL 为所有合法 ATL 公式的集合.
算子《》是路径选择量词, ⃝ (“下一状态”), □ (“总是”), 和 U (“直到”) 是时态算子. 《{a1 , . . . , an }》可简写成
《a1 , . . . , an 》, 《∅》可简写成《》. 额外的布尔连接符, 如 ∧ 、 → 、 ↔ 等, 可采用通常的方式由 ¬ 和 ∨ 定义出来.与

CTL 类似, 《A》♢ φ 可看作《A》⊤Uφ 的缩写, 其中 ♢ (“最终”) 也是时态算子.
定义 3 (ATL 语义). “并发博弈结构 S 的状态 q 满足公式 φ ”可表示为 S , q ⊨ φ . 当 S 可由上下文清楚地确定时,
它可被省略并写作 q ⊨ φ . 关系 ⊨ 可关于所有的状态 q 定义如下.
1) 对于所有 p ∈ Π , q ⊨ p 当且仅当 p ∈ π(q) .
2) q ⊨ ¬φ 当且仅当 q ⊭ ¬φ .
3) q ⊨ φ1 ∨ φ2 当且仅当 q ⊨ φ1 或 q ⊨ φ2 .
4) q ⊨《A》⃝ φ 当且仅当存在一个 A -策略 F A , 使得对于每个计算 λ ∈ out(q, F A ) , 都有 λ[1] ⊨ φ .
5) q ⊨《A》□φ 当且仅当存在一个 A -策略 F A , 使得对于每个计算 λ ∈ out(q, F A ) 和位置 i (i ⩾ 0) , 都有 λ[i] ⊨ φ .
6) q ⊨《A》φ1 Uφ2 当且仅当存在一个 A -策略 F A , 使得对于每个计算 λ ∈ out(q, F A ) 存在一个位置 i (i ⩾ 0) , 使得
λ[i] ⊨ φ2 ; 并且对于所有的位置 j (0 ⩽ j < i) , 都有 λ[ j] ⊨ φ1 .

注意, 算子 ⃝ 是局部性的, 等价地有: q ⊨ 《A》⃝ φ 当且仅当存在 A -行动 m
⃗ A , 对于任意 q′ ∈ out(q, m
⃗ A ) , 都有 q′ ⊨ φ .
算子《A》的对偶形式是 [[A]] , [[A]]ψ 的含义是“联盟 A 中的 Agent 不能通过合作来使 ψ 不被满足 (它们不能避免 ψ )”.
在状态 q ∈ Q 联盟 A 不能避免一个计算的集合 Λ 当且仅当对于所有 A -策略 F A , Λ ∩ out(q, F A ) , ∅ .可把 ¬《A》⃝
¬φ 写作 [[A]] ⃝ φ , 把 ¬《A》♢ ¬φ 写作 [[A]]□φ , 把 ¬《A》□¬φ 写作 [[A]]♢φ .

如果对于任意并发博弈结构 S , 任意其上的状态 q , 均有 S , q ⊨ φ 成立, 那么我们就说 φ 是有效的, 记为 ⊨ φ .
1.2 社会法则
社会法则可基于交互时态逻辑 ATL 进行形式化: 将多 Agent 系统的交互建模为并发博弈结构, 将需要在多
Agent 系统中实现的目标属性表述为 ATL 公式, 验证系统是否满足目标属性的问题可由 ATL 模型检测解决. 社会
法则可被定义为在各个状态下对各个 Agent 的可用行动的约束.一个对某个属性 (ATL 公式) 有效的社会法则实质
上将原并发博弈结构转化为新的并发博弈结构, 其中目标属性被初始状态满足.
定义 4 (社会法则). 并发博弈结构 S = ⟨k, Q, qs , Π, π, ε, δ⟩ 的一个社会法则为一个行为约束 η , 定义为一个函数
∀i ∈ Ag, q ∈ Q : η(i, q) ⊂ ε(i, q) .

注意, 在后文中为了书写更加简洁, 我们也把 η(i, q) 记作 ηi (q) , 把 ε(i, q) 记作 εi (q) .
实际上在并发博弈结构 S = ⟨k, Q, qs , Π, π, ε, δ⟩ 上实施一个社会法则 η 得到的新结构 S †η 为 S ′ = ⟨k, Q, qs , Π, π,
ε , δ ⟩ 其中 ∀i ∈ Ag, q ∈ Q : ε′i (q) = εi (q) \ ηi (q) , δ′ 为把 δ 的定义域限制到 ε′ 定义的子空间后的结果. 直观地, 在一个
′

′

并发博弈结构上实施一个社会法则即为从该结构上删去所有被该社会法则限制的行动. 我们要求 ∀i ∈ Ag, q ∈ Q :
ηi (q) , εi (q) 以保证在得到的新结构中每个状态均有至少一个后继, 这也保证了新结构 S †η 仍是一个并发博弈结构.

社会法则合成总是为了满足一个预先定义的协同目标 φ ∈ LATL , 即使公式 φ 在新并发博弈结构的初始状态上
得到满足; 如果 S †η, qs ⊨ φ , 那么我们说 η 是一个有效社会法则 (effective social law). 在朴素的非策略情形, 社会法
则被实现为针对 Agent 行动的硬性约束, 我们可以简单粗暴地通过搜索解空间的方式找出有效社会法则, 文
献 [3,4] 已系统地研究这种情况.

1444

软件学报 2024 年第 35 卷第 3 期

1.3 相关工作
由于不同的逻辑系统可以提供不同的语义结构或符号语言, 可分别建模不同的多 Agent 系统或描述不同的协
同目标. 尝试使用不同的逻辑以及相应的模型检测技术来实现社会法则是该领域的重要研究内容. 例如使用更简
单的 CTL 语言来表示协同目标, 简化 van der Hoek 等人 [3]的理论框架, 导出一些更深入的理论结果 [8,21−24]; 使用交
互时态认知逻辑 (alternating-time temporal epistemic logic, ATEL)[25]来表示协同目标, 实现表示 Agent 之间对于“知
识”的协同需求: 如“某个 Agent 能为另一个 Agent 带来某种知识”或者“如果某个 Agent 知道某件事, 那么它将以某
种特定的方式行动”[26]; 将约束作用于 Agent 联盟的联合行动而非单个 Agent 的行动, 得到一种新的语义结构, 针
对性地设计逻辑 Co-ATL, 并基于此实现一种能更灵活地对系统进行修改的社会法则 [4,27,28].
如何使自主 Agent 自觉服从社会法则的约束也是该领域被集中探讨的基础性问题.自从 Shoham 等人 [1,2]提出
社会法则开始, 该项研究就存在着“所有 Agent 都将无条件遵守社会法则”的假设, 并把放宽这项假设作为一个亟
待解决的问题. van der Hoek 等人 [3]提到“为什么一个理性且自利的 Agent 在明知会获得较低收益的情况下要选择
服从一个社会法则”是社会法则研究涉及到的悖论之一. 因此, “Agent 遵守一个社会法则的驱动机制是什么?”“如
果有些 Agent 不遵守社会法则会发生什么?”“系统对不遵守社会法则的 Agent 的鲁棒程度如何?”等成为需回答的
基本问题. 针对这些问题, Binmore 等人 [29,30] 从博弈论的角度对社会法则展开研究, 力图解释为什么在自利的
Agent 组成的多 Agent 系统中可以存在社会法则;在引入逻辑对社会法则重新进行形式化后, Ågotnes 等人进行的
一系列关于“服从性 (compliance)”的研究是这方面的重要工作, 如Ågotnes 等人 [8]为每个 Agent 关联一个层次化的
目标集, 并据此定义效用函数, “是否服从一个社会法则的约束”则被定义为 Agent 可选择执行的两个行动, 从而所
有 Agent 对行动的决策就成为一个博弈问题; 文献 [22] 在文献 [8] 的基础上研究了社会法则鲁棒性的相关问题;
而文献 [23] 则进一步提出了一种度量 Agent 对某个规范系统的是否成功的影响能力的方法.
本文的工作主要是从一个新的视角对社会法则合成问题进行探讨, 关注的主要问题是“在 Agent 掌握私有信
息并且具有最求最大自身效用的理性行为时, 如何可靠地合成对给定协同目标有效且能最大化社会福利 (social
welfare) 的社会法则”, 我们发现该问题可以转化为一个算法机制设计问题, 而亟待实现的 Agent 对社会法则的自
觉服从性可以被完美地建模为机制设计中的个体理性 (individual rationality), 即使服从社会法则的效用高于不服
从社会法则的效用. Bulling 等人 [31,32]引入了规范机制设计的概念, 乍看之下与我们的工作颇为相似, 但实际上有
根本的区别. 在 Bulling 等人的框架中的机制设计概念与传统情况有所不同, 它假定 Agent 的偏好是公开信息, 实
际上没有私有信息的概念. 而且, 他们仍然将社会法则定义为硬性约束. 这意味着 Bulling 等人的工作从研究动机、
研究目标到方法体系都与本文基于算法机制设计的工作有着本质的区别. 本文工作与现有社会法则相关工作的区
别还在于本文基于 ATL 语义导出了一种基于整数规划的算法, 并严格证明了其正确性, 而现有相关工作大多止于
对问题计算复杂性的探讨 [3].
博弈论 (game theory) 与机制设计 (mechanism design) 是经济学的传统研究方向. 简而言之, 博弈 (game) 是一
种理性个体策略交互的数学模型; 博弈论给出分析博弈的一套方法; 机制设计则给出设计博弈的一套方法. 在显示
原理 [12]等支柱性定理的作用下, 机制设计往往等同于“诚实拍卖设计 (truthful auction design)”, 即设计激励竞拍者
诚实汇报自己的估价的拍卖规则. 算法机制设计的初衷是把机制设计的概念与方法融入并改进传统算法的理论框
架 (从而得到一种“算法机制”) 使其解决问题的同时能提供正确的激励、适用于理性个体组成的多 Agent 系统的
相关优化问题 [12,33]. 本文工作是将上述研究思路实践于社会法则合成问题的一次尝试, 这进一步拓展了算法机制
设计的应用领域. 优化目标是区分机制设计问题的一个重要方面. 以社会福利最大为目标的 “效率机制设计问题”,
如路径拍卖 [34−37]、最小生成树拍卖 [38−40]以及 1990 年代开始的美国联邦通信局的频谱拍卖衍生出的组合拍卖 [19,41]
等, 均可以用著名 VCG (Vickrey-Clarke-Groves) 机制 [16−18]解决. 虽然该机制不具备多项式时间的实现 [42], 但是有
时可以设计计算可行的近似机制, 以获得接近全局最优的社会福利 [42,43]. 狭义的“最优机制设计”指以收益最大化
(revenue maximization)[44]为优化目标的机制设计问题; 而广义上它可泛指任何优化目标或约束与拍卖者的支付有
关的机制设计问题 [45−48]. 我们发现社会法则合成中考虑的“最小化副作用”可被建模为“社会福利最大”, 从而使考
虑最小化目标的社会法则合成问题成为一类典型的效率机制设计问题, 同时社会法则所拥有的独特的作为公共货
物的性质、可正可负的估值函数以及对个体理性的需求使该效率机制设计问题具有新颖性. 最优机制设计具有更

吴骏 等: 一种基于算法机制设计的社会法则合成方法

1445

强的挑战性, 是当前的研究热点, 为社会法则的其他优化目标的实现提供了有效的理论基础, 我们将在后续的工作
中对其进行探讨.

2 社会法则合成机制
2.1 Agent 的效用模型
我们将并发博弈结构 S = ⟨k, Q, qs , Π, π, ε, δ⟩ 上的所有可能社会法则的集合记为 SLS , 将所有可由 S 实施社会
法则转换而得的并发博弈结构记为 WS = {S ′ |∃η ∈ SLS : S †η = S ′ } . 显然, SLS 及 WS 均为有限集.
定义 5 (结构估值). 任意 Agent i 对任意结构 S ′ ∈ WS 均具有一个估值 vi (S ′ ) ∈ R+ .
描述估值函数 vi 要求为每个 S ′ ∈ WS 赋予一个值, 朴素的方法将消耗大量的空间且不便于在投标中被 Agent
表述. 接下来我们将试图导出一种用于紧凑表示估值函数的方法.首先, 基于文献 [14,15] 的工作, 我们可以在 WS
中的任意两个并发博弈结构的状态集之间定义一种互模拟关系.注意, 在接下来的定义中, 我们用不同的上标来区
分来自不同并发博弈结构的组件.
定义 6 (交互互模拟关系). 令 S 1 = ⟨k, Q1 , q1s , Π1 , π1 , ε1 , δ1 ⟩ 及 S 2 = ⟨k, Q2 , q2s , Π2 , π2 , ε2 , δ2 ⟩ 为任意两个基于同一多
Agent 集合 Ag = {1, . . . , k} 的并发博弈结构, 任意二元关系 Z ⊆ Q1 × Q2 被叫做一个交互互模拟 (alternating bisimulation) 关系, 当且仅当 q1s Zq2s 且对于任意 q1 Zq2 有:
1) π1 (q1 ) = π2 (q2 ) .
2) 对于每个联盟 A ⊆ Ag , 以及每个联合行动 m
⃗ 1A ∈ D1A (q1 ) , 存在 m
⃗ 2A ∈ D2A (q2 ) 满足对于任意的 q2x ∈ out(q2 , m
⃗ 2A ) ,
存在 q1x ∈ out(q1 , m
⃗ 1A ) , 使 q1x Zq2x .
3) 对于每个联盟 A ⊆ Ag , 以及每个联合行动 m
⃗ 2A ∈ D2A (q2 ) , 存在 m
⃗ 1A ∈ D1A (q1 ) 满足对于任意的 q1x ∈ out(q1 , m
⃗ 1A ) ,
存在 q2x ∈ out(q2 , m
⃗ 2A ) , 使 q1x Zq2x .
直觉上, 互模拟关系在数学和计算中都表达了非常自然的等价概念. 如果 S 1 和 S 2 之间存在交互互模拟关系,
我们就说两个结构 S 1 和 S 2 是交互互模拟等价的, 记作 S 1 ⇌ S 2 .
定理 1[14]. 对于两个并发博弈结构 S 1 和 S 2 , 若 S 1 ⇌ S 2 且 q1 Zq2 , 那么对于任意 ATL 公式 φ 有:
S 1 , q1 ⊨ φ 当且仅当 S 2 , q2 ⊨ φ .

上述结论实际上反映了 ATL 语言表达能力的限度, 对于任意两个交互互模拟等价的并发博弈结构, 存在交互
互模拟关系的状态上一定满足相同的 ATL 公式, 这意味着 ATL 语言不能区分满足交互互模拟关系的状态.
定理 2 (表示定理). 以下两项是等价的:
1) 结构估值函数 vi 不区分交互互模拟等价的并发博弈结构, 即满足: 对于 ∀S 1 , S 2 ∈ WS , 若 S 1 ⇌ S 2 , 那么有
vi (S 1 ) = vi (S 2 ) .

2) 存在一个特征集合 Fi = {(φ1 , c1 )i , . . . , (φk , ck )i } , 其中 k ∈ N , ∀ j ∈ {1, . . . , k} : φ j ∈ LATL , c j ∈ R+ , 满足对于任意
S ∈ WS 有:
vi (S ) = σ(Fi , S ) =

∑

c j.

(φ j ,c j )i ∈Fi ;S ,q s ⊨φ j

证明: “ 1) ⇒ 2) ”: 由于并发博弈结构 S 中具有有限的 Agent 及有限的状态, 且在任意状态任意 Agent 具有有
限的可用行动, SLS 是社会法则的有限集.我们可以在该集合中定义二元关系 ≃ :
∀η1 , η2 ∈ SLS , η1 ≃ η2 当且仅当 S †η1 ⇌ S †η2 .

显然 ≃ 满足自反、对称及传递性, 因而是一个等价关系. 令 E = {η1 , . . . , ηm } 为该等价关系的一个等价类, 于是
对于任意 η ∈ SLS , 一定存在 η j ∈ E , 满足 S †η ⇌ S †η j ; 且对于任意 η j , ηl ∈ E , S †η j ⇌ S †ηl 当且仅当 j = l .
因此, 对于任意 j, l ∈ {0, . . . , m} , 且 j , l , S †η j 与 S †ηl 不满足交互互模拟等价, 故存在 ATL 公式 ψ j−l 满足:
S †η j , q s ⊨ ψ j−l 且 S †ηl , qs ⊭ ψ j−l .

我们可以定义特征集 Fi = {(φ1 , c1 )i , . . . , (φm , cm )i } , 其中:

软件学报 2024 年第 35 卷第 3 期

1446

∀1 ⩽ j ⩽ m : φ j = ∧ ψ j−l , c j = v(S †η j ).
l, j

易见, 任意 φ j 均是一个我们构造出来的仅被 S †η j 的初始状态满足, 但不被 WS 中任何其他并发博弈结构的初
始状态满足的公式, 即有:
S †η j , q s ⊨ φl 当且仅当 j = l ,

而这可以导出:

∑

∀η j ∈ E : vi (S †η j ) = c j =

c x = σ(Fi , S †η j ); 且

(φ x ,c x )i ∈Fi :S †η j ,q s ⊨φ x

∑

∀η < E : ∃η j ∈ E, vi (S †η) = vi (S †η j ) = c j =

∑

cx =

(φ x ,c x )i ∈Fi :S †η j ,q s ⊨φ x

cx = σ(Fi , S †η).

(φ x ,c x )i ∈Fi :S †η,q s ⊨φ x

也就是说, 对于任意的 S ∈ WS , 我们都可以得到 vi (S ) = σ(Fi , S ) .
“ 1) ⇐ 2) ”: 假设 ∃S 1 , S 2 ∈ WS , S 1 ⇌ S 2 , 且 vi (S 1 ) , vi (S 2 ) .
由于存在 Fi = {(φ1 , c1 )i , . . . , (φk , ck )i } 满足:
vi (S 1 ) = σ(Fi , S 1 ) =

∑

(φ j ,c j )i ∈Fi ;S 1 ,q s ⊨φ j

vi (S 1 ) , vi (S 2 ) 能导出:

∑

∑

c j 且vi (S 2 ) = σ(Fi , S 2 ) =

c j.

(φ j ,c j )i ∈Fi ;S 2 ,q s ⊨φ j

∑

cj ,

(φ j ,c j )i ∈Fi ;S 1 ,q s ⊨φ j

c j.

(φ j ,c j )i ∈Fi ;S 2 ,q s ⊨φ j

即 S 1 和 S 2 在状态 qs 满足不同的公式集. 而这是不可能的, 因为 S 1 ⇌ S 2 .
上述结论意味着任何不区分互模拟等价结构的估值函数均可被等价且紧凑地表示为一个特征集. 对交互互模
拟等价结构不加区分的要求实质上根源于 ATL 语言的表达能力的局限性. 然而由于这类估值函数良好地反映了
数学意义上的等价, 适合于大多数应用场景的需求, 本文将集中讨论此类估值函数.
Agent 对不同社会法则的偏好关系实质上反映于它对不同社会法则的不同估值, 而整个多 Agent 群体对不同
社会法则的偏好关系则可以反映于社会福利, 定义为所有 Agent 的估值之和.
定义 7 (社会福利). 对于一个 Agent 集合 ∆ ⊆ Ag 而言, 社会法则 η ∈ SLS 导致的社会福利为其中所有 Agent 对
新结构 S †η 估值之和, 记为:
S W∆ (η) =

∑

σ(Fi , S †η).

i∈∆

在所有有效社会法则中对 Agent 集合 Ag 社会福利最大的那个叫做高效社会法则 (efficient social law). 可靠地
找出高效社会法则是本文的最终目标.
例 1: 考虑一个由两个机器人 (Agent) 组成的多机器人系统 (可扩充至 n ∈ N 个机器人). 这两个机器人分别归
属于不同的运营商, 均拥有独立的数据缓冲区, 且共享一个数据传输通道, 用以实现在无基础设施的环境与远程服
务器通信. 在运行过程中机器人会不断地产生数据并写入自己的缓冲区. 但缓冲区容量有限, 当缓冲区满时, 机器
人就必须使用数据传输通道把其中的数据转移到远程服务器才能继续运行. 为保证数据传输互不干扰, 机器人对
数据传输通道的使用必须互斥地进行.
该系统的运行可被表示为一个并发博弈结构 S = ⟨k, Q, qs , Π, π, ε, δ⟩ , 其中,
● k = 2 , Agent 集合为 Ag = {1, 2} .
● Q = {q0 , q1 , q2 , q3 , q4 } , qs = q0 , 即系统的初始状态是 q0 .
● Π = {α1 , α2 , β1 , β2 , ϵ} , 其中各命题的含义为:
αi : Agent i 优先申请对共享数据传输通道的控制 ( i ∈ {1, 2} ).
βi : Agent i 已被授权对共享数据传输通道的控制 ( i ∈ {1, 2} ).
ϵ : 对共享数据传输通道的使用违反互斥性, 出现错误.

● 标注函数 π 和行动函数 ε 如下:

吴骏 等: 一种基于算法机制设计的社会法则合成方法

1447

ε1 (q0 ) ={r, a};
ε1 (q1 ) ={r, a};
ε1 (q2 ) ={r, w};
ε1 (q3 ) ={r, w};
ε1 (q4 ) ={r};

π(q0 ) ={α1 };
π(q1 ) ={α2 };
π(q2 ) ={β1 };
π(q3 ) ={β2 };
π(q4 ) = {ϵ} ;

ε2 (q0 ) ={r, a};
ε2 (q1 ) ={r, a};
ε2 (q2 ) ={r, w};
ε2 (q3 ) ={r, w};
ε2 (q4 ) ={r}.

即在状态 q0 , q1 , Agent 1 和 Agent 2 分别优先申请对数据传输通道的控制; 在状态 q2 , q3 , Agent 1 和 Agent
2 分别获得对数据传输通道的控制权; q4 是错误状态; 行动 a 为 “申请控制数据传输通道”, 行动 r 为 “执行常规工
作”, 行动 w 为 “使用数据传输通道传输数据”; 例如, ε1 (q0 ) = {r, a} 表示在状态 q0 , Agent 1 能选择执行常规工作或
申请控制数据传输通道.
● 状态转换函数 δ 的定义如图 1 中箭头所示 (箭头上标注的元组即为导致该状态转换的联合行动).
〈r, r〉
〈w, w〉
〈r, r〉



〈w, w〉
〈r, r〉

q4
〈w, r〉

β1

〈r, w›

β2

q2
〈w, r〉 〈r, a〉
〈a, a〉

〈a, r〉 〈r, w〉
〈a, a〉
〈r, a〉

α1

〈a, r〉

qs=q0
〈r, r〉

图1

α2

q1

〈r, r〉

一个共享数据传输通道的多机器人系统的并发博弈结构

例如, 在状态 q0 , Agent 1 优先申请对数据传输通道的控制权, 表现在如果 Agent 1 选择行动 a , 那么不管
Agent 2 选择行动 r 还是行动 a , Agent 1 均能获得对数据传输通道的控制权 (系统状态转移到状态 q2 ); 而仅当
Agent 2 选择行动 a , Agent 1 选择行动 r 时, Agent 2 才能获得对数据传输通道的控制权 (系统状态转移到状态 q3 );
此外, 当两个 Agent 均选择行动 r 时, 系统状态不变. 在状态 q2 , 虽然 Agent 1 拥有对数据传输通道的控制权, 但是
Agent 2 仍能使用数据传输通道. 如果仅有 Agent 1 使用数据传输通道, 那么系统状态转移为 Agent 2 优先申请状
态 q1 ; 如果仅有 Agent 2 使用数据传输通道, 那么系统状态转移到 Agent 1 优先申请状态 q0 ; 当两个 Agent 均使用
数据传输通道时, 因违反互斥性, 系统状态转移到错误状态 q4 .
● 协同目标为: φ = 《》□¬ε , 表示系统运行的任意路径均不会到达一个错误的状态.现有的系统由于存在到达
错误状态的路径, 显然不满足该协同目标.通过社会法则禁止各 Agent 的某些行动, 从而删除系统中的一些状态转
换, 有望使协同目标得到满足.
● Agent 1 的特征集 F1 和 Agent 2 的特征集 F2 可分别表示为表 1 和表 2.
表1
属性

价值

特征集 F1
描述

φ1 =《》□(α1 →《1》⃝ β1 )

20

在 Agent 1 优先状态, Agent 1 能确保马上控制数据传输通道

φ2 =《》□(α1 →《1, 2》⃝ α1 )

5

在 Agent 1 优先状态, 如果 Agent 2 配合, 则 Agent 1 能使系统保持处于该状态

φ3 =《》□(β1 →《1》⃝ α2 )

5

Agent 1 控制数据传输通道时, Agent 1 能马上使用数据传输通道

φ4 =《》□(α2 →《1, 2》⃝ β1 )

8

在 Agent 2 优先状态, 如果 Agent 2 配合, 则 Agent 1 能马上控制数据传输通道

φ5 =《》□(β2 →《1, 2》⃝ α2 )

1

Agent 2 控制数据传输通道时, 如果 Agent 2 配合, 则 Agent 1 能马上使用数据传输通道

φ6 =《》□(β2 →《1》♢ α2 )

10

Agent 2 控制数据传输通道时, Agent 1 能确保后续能使用数据传输通道

软件学报 2024 年第 35 卷第 3 期

1448

表2

特征集 F2

属性

价值

ψ1 = 《》□(α2 → 《2》⃝ β2 )

描述

7

在 Agent 2 优先状态, Agent 2 能确保马上控制数据传输通道

ψ2 = □(α2 → 《1, 2》⃝ α2 )

4

在 Agent 2 优先状态, 如果 Agent 1 配合, Agent 2 能使系统保持处于该状态

ψ3 = 《》□(β2 → 《2》⃝ α1 )

6

Agent 2 控制数据传输通道时, Agent 2 能马上使用数据传输通道

ψ4 = 《》□(β2 → 《2》⃝ β2 )

2

Agent 2 控制数据传输通道时, Agent 2 能使系统保持处于该状态

ψ5 = 《》□(α1 → 《1, 2》⃝ β2 )

18

在 Agent 1 优先状态, 如果 Agent 1 配合, Agent 2 能马上使用数据传输通道

根据定理 2, 上述 F1 和 F2 实际上给出了 Agent 1 和 Agent 2 的结构估值函数. 基于此, 可以给出各 Agent 对任
意并发博弈结构的结构估值. 然而, 特征集实质上反映了 Agent 对各种系统属性的需求程度, 其具体包含哪些属性
及其对应的价值是仅为 Agent 自己了解的私有信息.
● 我们可以列出若干社会法则如表 3 所示, 其中 η1 – η6 均是有效社会法则. Agent 的特征集实质上确定了各个
社会法则的社会福利 (表 3 中, v1, v2 分别表示 Agent 1 和 Agent 2 的结构估值, SW 表示社会福利, 01a 表示“Agent 1
在状态 q0 的行动 a ”, “+”表示对应的公式被满足; “－”表示对应的公式不被满足).
表3

若干社会法则及其导致的社会福利

社会法则

禁止行动

φ1

φ2

φ3

φ4

φ5

φ6

ψ1

ψ2

ψ3

ψ4

ψ5

v1

v2

SW

η0

∅

+

+

－

+

+

－

+

+

－

－

+

34

29

63

η1

{01a , 02a }

－

+

+

+

+

+

+

+

+

+

－

29

19

48

η2

{01a , 11a , 31w }

－

+

+

+

－

－

+

+

+

+

+

18

37

55

η3

{22w , 31w }

+

+

+

+

－

－

+

+

+

+

+

38

37

75

η4

{02a , 12a , 22w }

+

+

+

+

+

+

－

+

+

+

－

49

12

61

η5

{22w , 31w , 32r }

+

+

+

+

－

+

+

+

+

－

+

48

35

83

η6

{21w , 32w }

+

+

－

+

+

－

+

+

－

+

+

34

31

65

● 例如, η1 表示在状态 q0 禁止 Agent 1 和 Agent 2 控制数据传输通道的申请. 实施该社会法则后, 系统状态将
保持留在 q0 , 没有 Agent 可以控制或使用数据传输通道, 从而简单粗暴地避免了进入错误状态, 是一个有效社会
法则. 但由于其严重影响了各个 Agent 的正常运行, 社会福利值只有 48, 低于其他许多社会法则的社会福利值, 因
而不是一个高效社会法则; η5 表示在状态 q2 , 禁止 Agent 2 的使用数据传输通道, 并且在状态 q3 禁止 Agent 1 的使
用数据传输通道以及 Agent 2 的常规工作. 实施该社会法则后, 在状态 q2 及 q3 均只有一个 Agent 可以对数据传输
通道进行写入, 从而避免了进入错误状态. 此外, 还避免了 Agent 2 在状态 q3 对数据传输通道的永久控制. 可以计
算出该社会法则的社会福利是 83, 是目前考虑的 6 个有效社会法则中的社会福利最大者, 如果不存在其他社会福
利更大的社会法则, η5 将是我们需要寻找的高效社会法则.
● 然而, 由于 F1 和 F2 的私有性, 我们并不能像上面那样直接计算并比较各个社会法则的社会福利. 试图通过
询问各 Agent 获取 F1 和 F2 也不可行, 因为各 Agent 会选择汇报对自己最有利的特征集而非诚实地汇报. 如何可
靠地选出高效社会法则是后文要解决的关键挑战.
例 1 实质上展示了如下场景: 由于系统中存在信息不完全性, 且 Agent 具有博弈论角度的理性, 它们总是选择
最大化自身效用的行动. 这意味着找出高效社会法则的问题有别于通常的优化问题. 我们的目的是在上述存在信
息不完全性及 Agent 的理性行为的情况下, 可靠地找出高效社会法则, 这可以很自然地在算法机制设计的理论框
架下寻求解决方案——通过设计社会法则合成机制来可靠地找出高效社会法则.
定义 8 (社会法则合成机制). 令 ∆ = {r(1), r(2), . . . , r(n)} ⊆ Ag 为所有参与社会法则合成的 Agent, 其中 r : N →
{1, . . . , k} 为单射函数, Θr(i) 为 Agent r(i) 的特征集空间. 社会法则合成机制为一个二元组, 其中 a : Θr(1) × . . . × Θr(n) → SLS

吴骏 等: 一种基于算法机制设计的社会法则合成方法

1449

为分配函数, ∀i ∈ ∆ : ti : Θr(1) × . . . × Θr(n) → R 为 (Agent i 的) 支付函数.
社会法则合成机制通过如下 3 步进行实施.
1) 首先, 机制 (a, t) 被宣布并为所有 Agent 所了解.
2) 然后, 所有的 Agent 可选择是否参与机制.若参与, 则从其特征集空间中自由选择一个特征集进行投标 (允
许谎报自己的特征集), 并承诺服从得到的社会法则的约束; 否则就不参与投标, 但也不需服从社会法则的约束.
3) 最后, 将投标向量 (所有 Agent 的投标) 输入分配函数 a 和支付函数 t , 分别得到选用的社会法则以及各个
参与 Agent 的支付金额.
社会法则实际上是一种非独占、非竞争的公共货物 (public goods). 一旦选用一个社会法则, 它将影响系统中
的任意 Agent (产生价值或造成损失). 我们不能阻止任何 Agent 享用社会法则的价值, 任意 Agent 也无法避免社会
法则对自己造成的损失. 然而, 社会法则作为公共货物又有其特殊性, 因为它是对系统中若干 Agent 的行为约束,
它的成功实施需要这些 Agent 积极配合、自觉遵守. 社会法则并不是强制性约束, 任何 Agent 是否遵守社会法则
完全出于其自愿. 在社会法则合成机制实施前, 所有的 Agent 都有机会决定是否要参与, 但如果决定参与那就必须
承诺服从得到的社会法则的约束.
对于任意 ∆ ⊆ Ag , 我们把仅约束集合 ∆ 中的 Agent 的行动的社会法则叫做 ∆ -社会法则.
定义 9 ( ∆ -社会法则). 给定并发博弈结构 S = ⟨k, Q, qs , Π, π, ε, δ⟩ , ∆ ⊆ Ag 以及社会法则 η , 若 ∀q ∈ Q , ∀i ∈
Ag \ ∆ : ηi (q) = ∅ , 那么我们说 η 是一个 ∆ -社会法则.
给定一个并发博弈结构 S , 一个协同目标 φ , 我们记所有有效 ∆ -社会法则的集合为:
SL∆S ,φ = {η ∈ SL s |S †η, qs ⊨ φ 且 ∀q ∈ Q , ∀i ∈ Ag \ ∆ : ηi (q) = ∅} .

为了书写上的方便, 我们将 SLSAg,φ 简写为 SLS ,φ , 并且将 SLSAg\{i}
简写为 SL−i
.
S ,φ
,φ
我们用 Fi 表示 Agent i 的真实特征集, 用 Fbi 表示 Agent i 投标的特征集, 并采用如下常用的记号:
Fb = (Fb1 , . . . , Fbk ), Fb−i = (Fb1 , . . . , Fbi−1 , Fbi+1 , . . . , Fbk ), Fbi , Fb−i = Fb.

定义 10 (社会法则估值). 任意 Agent i ∈ Ag 对任意社会法则 η ∈ SLS 的估值 visl (η) 为该社会法则带来的结构估
值增益, 即有 visl (η) = vi (S †η) − vi (S ) = σ(Fi , S †η) − σ(Fi , S ) .
根据社会法则估值的定义, 对于任意 Agent i 而言, 不限制任何行动的社会法则 η∅ 满足 ∀i ∈ Ag, q ∈ Q :
η∅ (i, q) = ∅ , 不会对原结构作任何修改, 因此估值为 0; 任意其他社会法则可导致对结构估值增加, 也可导致结构估

值降低, 这意味着 visl 的取值范围包括正实数、负实数和 0.
例 2: 表 3 中各社会法则的估值如表 4 所示.
表4

若干社会法则的估值

估值函数

η0

η1

η2

η3

η4

η5

η6

v1

34

29

18

38

49

48

34

v2

29

19

37

37

12

35

31

v1sl

0

−5

−16

4

15

14

0

v2sl

0

−10

8

8

−17

6

2

可 见 , v1sl (η0 ) 、 v1sl (η6 ) 、 v2sl (η0 ) 均 = 0 ; v1sl (η1 ) 、 v1sl (η2 ) 、 v2sl (η1 ) 、 v2sl (η4 ) 均 < 0 ; v1sl (η3 ) 、 v1sl (η4 ) 、 v1sl (η5 ) 、
v2sl (η2 ) 、 v2sl (η3 ) 、 v2sl (η5 ) 、 v2sl (η6 ) 均 > 0 .

定义 11 (效用). 在社会法则合成机制的实施中, Agent 获得的效用为其对所选社会法则的估值与支付之差, 即
若参与社会法则合成的 Agent 集合为 ∆ ⊆ Ag , 投标集为 Fb∆ , 那么对于任意具有特征集 Fi 的 Agent i ∈ Ag :
1) 如果 i ∈ ∆ (参与社会法则合成机制), 那么获得的效用为:
ui (Fi , Fb∆ ) = vsl (a(Fb∆ )) − ti (Fb∆ ) = σ(Fi , S †a(Fb∆ )) − σ(Fi , S ) − ti (Fb∆ ).
i

2) 如果 i < ∆ (不参与社会法则合成机制), 那么获得的效用为:

软件学报 2024 年第 35 卷第 3 期

1450

ui (Fi , Fb∆ ) = visl (a(Fb∆ )) = σ(Fi , S †a(Fb∆ )) − σ(Fi , S ).

由此可见, 由于社会法则作为公共货物的特殊性, 对于任意 Agent 而言, 不管它是否参与机制, 选用的社会法
则均能给它带来价值 (结构估值的变化). 如果它参与机制, 那么它还需承担机制指定的支付; 如果它不参与机制,
那么它的结构估值将不会在社会法则的选择时被考虑 (因为 F∆ 中不包含它的投标). Agent 的效用实质是其在社会
法则实施中获得的综合收益.
2.2 一种高效社会法则合成机制
为了在具有信息不完全性及 Agent 的理性行为的环境中可靠地得到高效社会法则, 本文专注于设计满足激励
相容和个体理性的高效社会法则合成机制.
定义 12 (高效社会法则合成机制). 一个社会法则合成机制是一个针对协同目标 φ ∈ LATL 的高效社会法则合成
机制当且仅当下列属性被满足.
1) (高效性) 机制总是为协同目标 φ 分配一个高效社会法则.
2) (激励相容) 诚实投标是所有参与机制的 Agent 的占优策略.
3) (个体理性) 任意 Agent i ∈ Ag 参与机制且服从社会法则获得的效用不低于不参与机制获得的效用.
Agent 的自觉服从性可以完美地由个体理性反应, 并且 Agent 诚实揭示自己私有信息的行为可由激励相容保
证. 高效社会法则机制设计问题实质上是一种面向公共货物的效率机制设计问题, 通常可以考虑基于 VCG 机制设
计解决方案.
定义 13 (VCG 社会法则机制). 我们把包含如下分配函数与支付函数的机制叫做 VCG 社会法则机制 (VCG
social law mechanism, VCG-SLM).
令参与机制的 Agent 集合为 ∆ ⊆ Ag , 获得的投标集为 Fb∆ , 且令 Fb∆−i 为其中 Agent 集合 ∆ \ {i} 的投标.
1) 分配函数从所有有效 ∆ -社会法则中选出对 ∆ 社会福利最高的社会法则, 即:
∑
a(Fb∆ ) = arg max
σ(Fbi , S †η).
η∈SL∆S ,φ

i∈∆

2) 基于 Clarke 基准规则, 对于任意 Agent i ∈ ∆ , 令
∑
hi (Fb∆−i ) = max
σ(Fbj , S †η).
∆\{i}
η∈SLS ,φ

j∈∆\{i}

由此确定 Agent i 的支付为:
∑
∑
∑
bj , S †η) −
ti (Fb∆ ) =hi (Fb∆−i ) −
σ(Fbj , S †a(Fb∆ )) = max
σ(
F
σ(Fbj , S †a(Fb∆ ))
∆\{i}
=

∑
j∈∆\{i}

j∈∆\{i}

σ(Fbj , S †a(Fb∆−i )) −

∑

η∈SLS ,φ

j∈∆\{i}

j∈∆\{i}

σ(Fbj , S †a(Fb∆ )).

j∈∆\{i}

社会法则实质上被实现为一种权利与义务的综合体. Agent 作为社会成员的权利包括自己的结构估值在社会
福利的优化中被考虑, 以及获得支付; Agent 作为社会成员的义务即参与投标并服从社会法则的约束. Agent 想享
受作为社会成员的权力就必须承担作为社会成员的义务.
实际上, 根据 VCG 机制上述函数 hi (Fb∆−i ) 可以是任何与 Agent i 的投标无关的实质函数, 这样就能保证激励相
容 [12,13]. 问题是如何实现本文要求的个体理性. 如果所有 Agent 的估值非负, 那么可使用 Clarke 基准规则 (如上设
置函数 hi (Fb∆−i ) ) 使 Agent 选择服从机制成为占优策略 [12], 从而实现个体理性. 然而, 本文考虑的问题与上述通常的
情况有本质的区别, 社会法则是一种公共货物, 且 Agent 的估值并不满足非负性, 如何设置 hi (Fb∆−i ) 以实现个体理
性并无已知的理论保证, 是一个需要探索的问题. 有趣的是, 我们可以证明采用 Clarke 基准规则的 VCG 社会法则
机制仍能保证激励相容并提供一种有意义的个体理性保证.
2.3 形式化属性
首先我们可以证明当 SL∆S ,φ , ∅ (意为存在有效 ∆ -社会法则) 且 ∀i : SL∆\{i}
(意为没有哪个 Agent 不可或
S ,φ , ∅
缺) 时, VCG-SLM 是激励相容的, 即对于所有的 Agent i ∈ ∆ , 诚实投标自己的特征集是其最优选择.

吴骏 等: 一种基于算法机制设计的社会法则合成方法

1451

定理 3. 当 SL∆S ,φ , ∅ 且 ∀i : SL∆\{i}
时, 诚实投标是所有选择参与社会法则合成并承诺服从选定的社会法则
S ,φ , ∅
的 Agent 的占优策略.
证明: 令 Fb∆−i 和 Fbi , Fb∆−i 分别为集合 ∆ \ {i} 和 ∆ 中所有 Agent 的投标. 对于任意 Agent i ∈ ∆ , 其效用为:
∑
σ(Fbj , S †a(Fbi , Fb∆−i ))
ui (Fi , (Fbi , Fb∆−i )) =σ(Fi , S †a(Fbi , Fb∆−i )) − σ(Fi , S ) − ti (Fbi , Fb∆−i ) = σ(Fi , S †a(Fbi , Fb∆−i )) +
∑

− max
∆\{i}
η∈SLS ,φ

j∈∆\{i}

∑

− max
∆\{i}
η∈SLS ,φ

σ(Fbj , S †η) − σ(Fi , S ) ⩽ σ(Fi , S †a(Fi , Fb∆−i )) +

∑

j∈∆\{i}

σ(Fbj , S †a(Fi , Fb∆−i ))

j∈∆\{i}

σ(Fbj , S †η) − σ(Fi , S ) = ui (Fi , (Fi , Fb∆−i )).

j∈∆\{i}

这意味着对于任意 Agent i ∈ ∆ 而言, 诚实汇报其类型是其占优策略.
定理 4. 当 SLS ,φ , ∅ 且 ∀i : SL−i
时, 在 VCG-SLM 中所有的 Agent 参与社会法则合成、服从选定的社会
S ,φ , ∅
法则并诚实投标是一个纳什均衡.
证明: 任意 Agent 实际上有且仅有 3 种可供自由选择的策略: ① 不参与社会法则合成, 不服从选择的社会法
则的约束; ② 参与社会法则合成, 承诺服从选定社会法则的约束, 并诚实投标; ③ 参与社会法则合成, 承诺服从选
定社会法则的约束, 但不诚实投标. 以下证明, 所有 Agent 均选择策略②是一个纳什均衡, 也就是要证对于任意
Agent i ∈ Ag , 若其单方面地改变策略, 选择策略①或策略③均不能获得更高的效用.
情况 1: 若 Agent i 单方面把它的策略变为策略③, 此时仍有所有的 Agent 选择参与社会法则合成并承诺服从
选定的社会法则, 由定理 3 知, 诚实投标是 Agent i 的占优策略, 故策略③不会导致更高的效用.
情况 2: 若 Agent i 单方面把它的策略变为策略①, 那么仍有除 i 外的所有 Agent 选择参与社会法则合成, 承诺
∑
服从选定的社会法则并诚实投标. 此时, 投标向量为 F−i , 系统选择的社会法则为 η∗ = arg maxη∈SL−i
σ(Fbj , η) , 从
S ,φ
j,i

而 Agent i 的效用为 σ(Fi , S †η∗ ) − σ(Fi , S ) .

而如果 Agent i 保持其策略②不变, 那么投标向量为 F = {F1 , . . . , Fk } , 其中任意特征集均为对应 Agent 的真实
特征集. 此时 Agent i 获得的效用为:
ui (Fi , (Fi , F−i )) = σ(Fi , S †a(Fi , F−i )) +

∑

σ(F j , S †a(Fi , F−i )) −

j,i

= σ(Fi , S †a(Fi , F−i )) +

∑

∑

σ(F j , S †η∗ ) − σ(Fi , S )

j,i

σ(F j , S †a(Fi , F−i )) − (σ(Fi , S †η∗ ) +

j,i

∑

σ(F j , S †η∗ )) + σ(Fi , S †η∗ ) − σ(Fi , S ).

j,i

由于 a(F ) 为对 Ag 社会福利最高的有效 Ag -社会法则, 我们可以得到:
k
k
∑
∑
∑
∑
σ(Fi , S †a(Fi , F−i )) +
σ(F j , S †a(Fi , F−i )) − (σ(Fi , S †η∗ ) +
σ(F j , S †η∗ )) =
σ(Fi , S †a(F )) −
σ(Fi , S †η∗ ) ⩾ 0.
j,i

j,i

i=1

i=1

故有 ui (Fi , (Fi , F−i )) ⩾ σ(Fi , S †η∗ ) − σ(Fi , S ) , 即策略①也不会导致更高的效用.
综上, VCG-SLM 中所有 Agent 参与社会法则合成、服从选定的社会法则并诚实投标是一个纳什均衡.
至此我们可以确定, 在投标开始前, 所有的 Agent 出于自身效用的考虑都不会单方面地选择不参与投标 (定
理 4 保证), 并且当投标开始后, 所有的 Agent 均会诚实投标 (定理 3 保证). 注意, 为获得更简单清晰的表述方式,
在后文中投标向量通常可以不失一般性地记为 Fb = (Fb1 , . . . , Fbk ) .
推论 5. VCG-SLM 是一个高效社会法则合成机制当且仅当 SLS ,φ , ∅ 且 ∀i : SL−i
, 即存在有效社会法则
S ,φ , ∅
并且没有哪个 Agent 不可或缺.
证明: “ ⇒ ”: 若 VCG-SLM 是一个高效社会法则合成机制, 那么显然要求满足 SLS ,φ , ∅ , 否则分配无法计算;
且要求满足 ∀i : SL−i
, 否则支付无法计算.
S ,φ , ∅
“ ⇐ ”: 若 SLS ,φ , ∅ 且 ∀i : SL−i
, 那么由分配函数的定义知, 该函数会返回一个高效社会法则, 从而高效
S ,φ , ∅
性被满足; 由定理 3 知, 激励相容被满足; 此外, 由定理 4 知, 个体理性也被满足. 因此, 在该机制下, 所有的 Agent

软件学报 2024 年第 35 卷第 3 期

1452

均会选择参与机制并服从选出的社会法则的约束, 所有的 Agent 均会诚实地汇报自己的特征集, 进而分配函数能
可靠地选择出高效的社会法则. 从而根据定义 12, VCG-SLM 是一个高效社会法则合成机制.
基于上述结论, 我们实际上为 SLS ,φ , ∅ 且 ∀i : SL−i
的情况找出了一种高效社会法则合成机制 VCG-SLM,
S ,φ , ∅
但是并不能说明 VCG-SLM 是唯一的高效社会法则合成机制. 事实上, 函数 hi (Fb∆−i ) 的设置仍然存在探索的空间.
是否可以采用 Clarke 基准规则之外的 hi (Fb∆−i ) 设置方式, 使所有的 Agent 选择参与机制满足某种 (不限于纳什均
衡) 的博弈解概念, 从而实现个体理性, 是一个值得进一步探究的问题.
此外, 我们也可以划分出一些不存在任何高效社会法则合成机制的情况.
定义 14 (ATL 的存在性与通用性子语言). ATL 的存在性子语言 Le 与通用性子语言 Lu 可分布由以下语法 ϵ
和 υ 定义:
ϵ ::= p|ϵ ∧ ϵ|ϵ ∨ ϵ|《Ag》⃝ ϵ|《Ag》□ϵ|《Ag》ϵUϵ,
υ ::= p|υ ∧ υ|υ ∨ υ|《》⃝ υ|《》□υ|《》υUυ,

其中, p ∈ Π .
定理 6.
1) 如果 SLS ,φ = ∅ , 那么不存在任何高效社会法则合成机制.
2) 如果目标为 φ , 且 φ ∈ Le 或 ¬φ ∈ Lu 但是 S , qs ⊭ φ , 那么不存在任何高效社会法则合成机制.
证明:
1) 当 SLS ,φ = ∅ 时, 不存在有效社会法则, 因此不存在高效社会法则, (定义 12 要求的) 高效性无法满足, 故不
存在任何高效社会法则合成机制.
2) 如果 S , qs ⊭ φ , 那么等价地有 S , qs ⊨ ¬φ .
如果 φ ∈ Le , 那么对于任意社会法则 η , S †η, qs ⊨ φ ⇒ S , qs ⊨ φ 均满足, 即 S , qs ⊨ ¬φ ⇒ S †η, qs ⊨ ¬φ 均满足 [3], 这
意味着对于任意的社会法则 η , 我们均有 S †η, qs ⊭ φ , 即 SLS ,φ = ∅ .
如果 ¬φ ∈ Lu , 那么对于任意社会法则 η , S , qs ⊨ ¬φ ⇒ S †η, qs ⊨ ¬φ 均满足 [3] , 这意味着对于任意的社会法则 η ,
我们均有 S †η, qs ⊭ φ , 即 SLS ,φ = ∅ .
综上所述, 在上述情况下, 我们均有 SLS ,φ = ∅ , 因此根据 1) 的结论, 在上述情况下均不存在任何高效社会法
则合成机制.
实质上 Le 为所有其满足性不能通用实施社会法则确立 (由不满足变成满足) 的 ATL 的公式的集合; Lu 为所
有其满足性不能通过实施社会法则避免 (由满足变成不满足) 的公式的集合. 以上结论说明, 当协同目标为某些特
定的形式时, 由于不存在有效社会法则, 不存在任何高效社会法则合成机制.

3 社会法则合成机制的计算
3.1 计算复杂性
定义 15 (VCG-SLM 的计算问题). VCG-SLM 的计算问题为: 给定并发博弈结构 S = ⟨k, Q, qs , Π, π, ε, δ⟩ , 协同目
b1 , . . . , F
bk } , 求选定的社会法则 a(Fb) 及对各个 Agent 的支付 (t1 (Fb), . . . , tk (Fb)) .
标 φ , 以及 Agent 的投标向量 Fb = {F
由 VCG-SLM 的定义 (定义 13) 可见社会法则合成机制的计算包括分配函数的计算以及支付函数的计算, 其
中支付函数的计算实质上涉及若干次分配函数的计算, 而分配函数的计算要基于任意 Agent i 的估值函数 σ(Fi , S )
的计算.
引理 7. σ(Fi , S ) 的计算具有多项式时间复杂度算法.
证明: 由于我们有:
σi (Fi , S ) =

∑
(φ j ,c j )i ∈Fi ;S ,q s ⊨φ j

c j.

吴骏 等: 一种基于算法机制设计的社会法则合成方法

1453

实质上是对 Fi 中的若干个形如 (φ j , c j )i 元组, 逐一判断是否有 S , qs ⊨ φ j 成立, 若成立, 则把对应的 c j 累加. 而判
断是否有 S , qs ⊨ φ j 成立的问题是 ATL 模型检测问题. 该问题已被证明是一个 P 问题 [7]. 因此 σi (Fi , S ) 的计算具有
多项式时间复杂度算法.
引理 8. a(Fb) 的计算是 FPNP-完全的.
证明: 该问题关联的判定问题为: 给定并发博弈结构 S = ⟨k, Q, qs , Π, π, ε, δ⟩ , 协同目标 φ , 以及 Agent 的投标向
k
∑
b1 , . . . , F
bk } , 判定是否存在社会法则 η ∈ SLS ,φ 满足 S W(η) =
量 Fb = {F
σ(Fi , S †η) ⩾ l ∈ R+ . 由于 σ(Fi , S ) 的计算具
i=1

有多项式时间复杂度算法 (引理 7), 上述问题对应的验证问题是一个 P 问题, 因此上述问题属于 NP, 进而可以确
定 a(Fb) 的计算属于 FPNP.
为了证明 a(Fb) 的计算 FPNP-难, 我们将最大权重布尔可满足问题 (MAX WEIGHT SAT)[49]规约到该问题. MAX
WEIGHT SAT 的任意实例为: 给定布尔变量 ϕ1 , . . . , ϕm 上的一个命题集 {ψ1 , . . . , ψn } 以及每个命题关联的一个权重
{w1 , . . . , wn } , 找出对上述布尔变量的一个最大化“被满足命题的权重和”的赋值 v∗ , 即:
∑
v∗ = arg maxn
w j.
v∈{0,1}

j∈{1⩽i⩽n|v⊨ψi }

我们构造一个具有 2 个 Agent, m + 2 个状态的并发博弈结构 S , 状态转换关系如图 2 所示.

〈1, 1〉

〈1, 1〉

ϕ1
q1

〈1, 1〉

〈2, 2〉

〈i, j〉, i≠j

ϕ2

〈m, m〉

…

q2
qs

qm+1

〈1, 1〉

ϕm

〈1, 1〉

qm

图2

从 MAX WEIGHT SAT 到 a(F ) 计算的规约

● qs 为初始状态, ∀i ∈ {1, . . . , m} : π(qi ) = {ϕi } 且 π(qs ) = π(qm+1 ) = ∅ .
● 若 q = qs , ε1 (q) = ε2 (q) = {1, . . . , m} , 否则 ε1 (q) = ε2 (q) = {1} .
● 对于任意 i = j , δ(qs , ⟨i, j⟩) = qi , 否则 δ(qs , ⟨i, j⟩) = qm+1 ; 且对于任意 q , qs , δ(q, ⟨1, 1⟩) = q .
bi 为将 ψi 中的任意命题变量 ϕ j 替换为《1, 2》♢ ϕ j 后所得的公式, 显然有:
令ψ
v(ψi ) = 1 , 其中 v(ϕ j ) = 0 当且仅当 j ∈ η1 (qs ) ∪ η2 (qs ) .
S †η, qs ⊨ b
ψi 当且仅当 b

令 F1 = {(ψb1 , w1 )1 , . . . , (ψbn , wn )1 } , F2 = {(ψb1 , w1 )2 , . . . , (ψbn , wn )2 } , 协同目标为 ⊤ ,
令:



ψi

 1, S †η, qs ⊨ b
b
SAT(S †η, ψi ) = 
.

 0, 否则

于是有:
a(F1 , F2 ) = arg max 2 · σ({(ψb1 , w1 )1 , . . . , (ψbn , wn )1 }, S †η)
η∈SLS ,⊤

= arg max

η∈SLS ,⊤

令 η∗ = a(F1 , F2 ) , 易证:

n
∑

bi ).
wi · SAT(S †η, ψ

i=1


∗
∗


 0, i ∈ η1 (qs ) ∪ η2 (qs )
v (ϕi ) = 
.

 1, 否则
∗

软件学报 2024 年第 35 卷第 3 期

1454
综上所述, a(Fb) 的计算是 FPNP-完全的.

定理 9. VCG-SLM 的计算问题是 FPNP-完全的.
证明: VCG-SLM 的计算包含 a(Fb) 的计算及 k 次 ti (Fb) 的计算, 可转化为 O(k) 次 a(Fb) 的计算. 由于引理 8 已证
NP
NP
a(Fb) 的计算是 FP -完全的, 故 VCG-SLM 的计算问题是 FP -完全的.

3.2 转化为整数规划
定理 9 意味着我们不能期待为 VCG-SLM 的计算找出多项式时间复杂度的算法.由于整数规划 (integer
programming) 是一种针对此类计算难问题的被广泛采用的有效方法, 我们试图将 VCG-SLM 的计算问题转化为整
数规划进行求解, 基本思路是首先将分配函数 a(Fb) 的计算转化为整数规划, 然后在此基础上进一步得到基于整数
规划的支付函数 ti (Fb) 的计算方法.

∪
引理 10. a(Fb∆ ) = arg maxη∈SLS ,φ σ( Fbi , S †η) .
i∈∆

证明:
a(Fb∆ ) = arg max

∑

η∈SLS ,φ

∑

∑

c j = arg max

η∈SLS ,φ

i∈∆ (φ j ,c j ) ∈F
bi ;S †η,q s ⊨φ j
i

(φ j ,c j )i ∈

∪b
Fi ;S †η,q s ⊨φ j

∪
c j = arg max σ( Fbi , S †η).
η∈SLS ,φ

i∈∆

可见分配函数选择的是满足目标且最大化社会福利的社会法则, 这实际上取决于

i∈∆

∪
i∈∆

bi 中的各个公式 φi 是
F

否在状态 qs 被满足, 并进一步取决于 Agent 在当前状态有哪些可用的行动及 φi 的子公式是否在当前状态及与当
前状态相关联的后续状态被满足. 所有上述选择都可以分别用一个布尔变量来反映, 并且对这些布尔变量的联合
赋值最终受到 ATL 语义的约束.
定义 15 (公式的闭包). 对于每个公式 φ , 它相关的子公式可以被记为 cl(φ) , 叫做 φ 的闭包, 定义为:
cl(φ) = {φ} ∪ sub(φ),

其中,



cl(ψ) ∪ cl(χ), 如果φ = ψ ∨ χ或φ =《A》(ψUχ)






cl(ψ),
如果φ = ¬ψ或φ =《A》⃝ ψ或φ =《A》□ψ .
sub(φ) = 





 {φ} ,
如果φ ∈ Π

根据上述定义, 公式的闭包实质上构成了一种树状结构, 如图 3 所示.
《A1》□《A2》◯p1 《A3》p2 p3)

《A1》□《A2》◯p1 《A3》p2 p3)

《A2》◯p1

p1

图3

《A3》p2 p3

p2

p3

一个 ATL 公式的闭包及其潜在的树形结构

例 3: 根据定义 2, 给定 Agent 集合 Ag , 命题集合 Π , 对于联盟 A1 , A2 , A3 ⊆ Ag 以及命题 p1 , p2 , p3 ⊆ Π , ¬《A1 》□
《
( A2 》⃝ p1 ∨《A3 》p2 U p3 ) 是一个 ATL 公式, cl(¬《A1 》□(《A2 》⃝ p1 ∨ 《A3 》p2 U p3 )) 包含的公式如图 3 所示可表

示为一个树形结构.
易见, 根节点即为给定公式, 且对于任意树结点 v 及其对应的公式 φ , cl(φ) 即为以结点 v 为根节点的子树包含
的所有结点; sub(φ) 为所有以结点 v 的子女为根节点的子树的所有结点 (若结点 v 不是叶子结点) 或仅包含结点 v
的集合 (若结点 v 是叶子结点).

吴骏 等: 一种基于算法机制设计的社会法则合成方法

1455

公式的闭包所含公式的数量叫做该公式的规模 (或长度). 公式的规模是其结构复杂性的直接体现, 紧密关联
于该公式相关的一些算法的时间复杂度. 如 ATL 模型检测问题存在时间复杂度为 O(t · l) 的算法, 其中 t 为给定并
发博弈结构的转换的数量, l 为公式的长度 [7]. 本文也将使用这些参数来刻画社会法则合成问题的规模.
对于任意投标的特征集 Fbi = {(φi , ai )i , . . . , (φi , ai )i } , 我们令:
1

ni

1

ni

cl(Fbi ) = cl(φi1 ) ∪ . . . ∪ cl(φini ),


∪  ∪
c∆ ) = cl  F
bi  =
bi ).
cl(F
cl(F


i∈∆

(∪

i∈∆

)
bi 是所有可能影响优化目标函数取值的公式的集合. 我们可以引入如下 4 类布尔变量.
实质上, cl
F
i∈∆
c∆ ) .
1) xφq ∈ {0, 1} , ∀q ∈ Q, φ ∈ cl(F

2) yqi:a ∈ {0, 1} , ∀q ∈ Q, i ∈ {1, . . . , k}, a ∈ εi (q) .
3) yqA:⃗mA ∈ {0, 1} , ∀q ∈ Q, A ⊆ {1, . . . , k}, m
⃗ A ∈ DA (q) .
q,φ
c
4) z
⃗ A ∈ DA (q) .
∈ {0, 1} , ∀q ∈ Q, φ ∈ cl(F∆ ), A ⊆ {1, . . . , k}, m
A:⃗
mA

其中,
● xφq = 1 当且仅当 S †η, q ⊨ φ .
● yqi:a = 1 当且仅当在 S 的状态 q Agent i 的行动 a 被禁止.
● yqA:⃗mA = 1 当且仅当在 S 的状态 q Agent 联盟 A 的联合行动 m
⃗ A 被禁止.
● zq,φ
⃗ A 可以保证 φ 在下一状态被满足.
= 1 当且仅当在 S †η 的状态 q Agent 集合采取联合行动 m
A:⃗
mA
关于 yqi:a 和 yqA:⃗mA 的关系, 由于显然有一个联合行动被禁止当且仅当其中一个行动被禁止, 我们可以确定如下
结论成立. 注意, 我们用 m
⃗ A 中 Agent i 选择的行动.
⃗ A [i] 表示联合行动 m
命题 11. yqA:⃗mA = 1 当且仅当 ∃i ∈ A : yqi:⃗mA [i] = 1 .
实质上描述的是联合行动在下一状态可靠地实现某种属性的能力. 基于 ATL 的语义定义不难证明如下结论.
zq,φ
A:⃗
mA
δ(q,(⃗
m ,⃗
m ))
引理 12. zq,φ
= 1 当且仅当 ∀⃗
mĀ ∈ DĀ (q) : (yqĀ:⃗m = 1 或 xφ A Ā = 1) .
A:⃗
mA
Ā

证明: Agent 联盟 A 采取联合策略 m
⃗ A 导致的下一状态的不确定性来自于联盟 Ā 的联合策略选择的不确定性.
具体而言, 联盟 Ā 所有可选择的联合行动为 DĀ (q) 中所有不被社会法则禁止的联合行动, 即联合策略集合
{⃗
mĀ ∈ DĀ (q)|yqĀ:⃗m = 0} . 根据定义, 我们有:
Ā

δ(q,(⃗
m ,⃗
m ))
zq,φ
= 1 当且仅当 ∀⃗
mĀ ∈ {⃗
mĀ ∈ DĀ (q)|yqĀ:⃗m = 0} : xφ A Ā = 1 ,
A:⃗
mA
Ā

δ(q,(⃗
m ,⃗
m ))
当且仅当 ∀⃗
mĀ ∈ DĀ (q) : (yqĀ:⃗m = 1 或 xφ A Ā = 1) .
Ā

直观来看, zq,φ
⃗ A 时, 环境 Ā 的任意联合行动 m
⃗ Ā 要么被禁止, 要么将
= 1 意为当 Agent 联盟 A 采取联合行动 m
A:⃗
mA
导致一个满足 φ 的后续状态.
由 ATL 的语义定义, 我们还可以导出如下结论.
命题 13. 如下两个公式是有效的.
1) 《A》ψUχ ↔ (χ ∨ (ψ ∧《A》⃝《A》ψUχ)) .
2) 《A》□φ ↔ φ ∧《A》⃝《A》□φ .
更进一步, 我们可以得到如下结论.
引理 14. 如果 xφq = 1 当且仅当 S †η, q ⊨ φ , 那么:
1) xqp = 1 当且仅当 p ∈ π(x) .
q
2) x¬ψ
= 1 当且仅当 xψq = 0 .
q
3) xψ∨χ
= 1 当且仅当 xψq = 1 或 xχq = 1 .
q
4) x《A》
= 1) .
mA ∈ DA (q) : (yqA:⃗mA = 0 且 zq,φ
= 1 当且仅当 ∃⃗
A:⃗
mA
⃝φ
q
5) x《A》
= 1 当且仅当至少以下一条成立:
ψUχ

软件学报 2024 年第 35 卷第 3 期

1456

● xχq = 1 .
● xψq = 1 且 ∃⃗
mA ∈ DA (q) : (yqA:⃗mA = 0 且 zq,A:《→−mA》ψUχ = 1) .
A

q
6) x《A》
= 1 当且仅当以下两条均成立:
□φ

● xφq = 1 .
● ∃⃗
mA ∈ DA (q) : (yqA:⃗mA = 0 且 zq,A:《→−mA》□ϕ = 1) .
A

证明: 1) 由定义 xqp = 1 当且仅当 S †η, q ⊨ p 当且仅当 p ∈ π(x) .
q
2) x¬ψ
= 1 当且仅当 S †η, q ⊨ ¬ψ 当且仅当 S †η, q ⊭ ψ 当且仅当 xψq = 0 .
q
3) xψ∨χ
= 1 当且仅当 S †η, q ⊨ ψ ∨ χ 当且仅当 S †η, q ⊨ ψ 或 S †η, q ⊨ χ 当且仅当 xψq = 1 或 xχq = 1 .
q
4) x《A》
⃗ A , 对于任意 q′ ∈ out(q, m
⃗ A) ,
= 1 当且仅当 S η, q ⊨《A》⃝ φ 当且仅当在并发博弈结构 S †η 中存在 A-行动 m
φ

都有 S †η, q′ ⊨ φ 当且仅当在并发博弈结构 S 中存在 A-行动 m
⃗ A , 满足 yqA:⃗mA = 0 且对于任意 m
⃗ Ā , 若 yqĀ:⃗m = 0 则有
δ(q,(⃗
mA ,⃗
mĀ ))

xφ

Ā

= 1 当且仅当 ∃⃗
= 1) .
mA ∈ DA (q) : (yqA:⃗mA = 0 且 zq,φ
A:⃗
mA

q
5) x《A》
= 1 当且仅当 S η, q ⊨ 《A》ψUχ (由命题 13 的 1)) 当且仅当 S †η, q ⊨ χ ∨ (ψ ∧《A》⃝《A》ψUχ) 当
ψUχ

且仅当 S †η, q ⊨ χ 或 S †η, q ⊨ ψ ∧《A》⃝《A》ψUχ ,
当且仅当 S †η, q ⊨ χ 或 ( S †η, q ⊨ ψ 与 S †η, q ⊨《A》⃝《A》ψUχ) ,
(由引理 14 的 4)) 当且仅当 xχq = 1 或 ( xψq = 1 且 ∃⃗
mA ∈ DA (q) : (yqA:⃗mA = 0 且 zq,A:《→−mA》ψUχ = 1)) .
A

q
6) x《A》
= 1 当且仅当 S †η, q ⊨ 《A》□φ (由命题 13 的 2)) 当且仅当 S †η, q ⊨ φ ∧《A》⃝《A》□φ 当且仅当 S †η, q ⊨
□φ

φ 且 S †η, q ⊨《A》⃝《A》□φ ,

(由引理 14 的 4)) 当且仅当 xφq = 1 且 ( ∃⃗
mA ∈ DA (q) : (yqA:⃗mA = 0 且 zq,A:《→−mA》□ϕ = 1) ).
A

综合命题 11, 引理 12 及引理 14 的结论, 实质上为我们完善地展现了各引入变量之间的约束关系, 将公式的
满足性最终规约为“各 Agent 可用行动的选择”及“子公式在后续状态的满足性”. 接下来我们试图进一步引入一些
∪
bi , φ∗ ∈
中间变量, 将上述变量之间的约束关系进一步等价地表述为整数规划的约束集.令 ∆ ⊆ Ag 且 Fb∆ =
F
i∈∆

LATL 为协同目标, 我们可以得到如下整数规划:

ILP-SWA( ∆ ): Maximize

∑

c j · xφqsj

(1)

(φ j ,c j )i ∈Fb∆

s.t.
xφq ∈ {0, 1}, ∀q ∈ Q, φ ∈ cl(Fb∆ )

(2)

y ∈ {0, 1}, ∀q ∈ Q, i ∈ {1, . . . , k}, a ∈ εi (q)

(3)

y = 0, ∀q ∈ Q, i ∈ {1, . . . , k} \ ∆, a ∈ εi (q)
∑
(1 − yqi:a ) ⩾ 1, ∀q ∈ Q, i ∈ {1, . . . , k}

(4)

q
i:a

q
i:a

(5)

a∈εi (q)

⃗ A ∈ DA (q)
yqA:⃗mA ∈ {0, 1}, ∀q ∈ Q,《A》或《Ā》∈ cl(Fb∆ ), m
⃗ A ∈ DA (q), i ∈ A
yqA:⃗mA ⩾ yqi:⃗mA [i] , ∀q ∈ Q,《A》或《Ā》∈ cl(Fb∆ ), m
∑
q
q
⃗ A ∈ DA (q)
yA:⃗mA ⩽
yi:⃗mA [i] , ∀q ∈ Q,《A》或《Ā》∈ cl(Fb∆ ), m

(6)
(7)
(8)

i∈A

⃗ A ∈ DA (q), m
⃗ Ā ∈ DĀ (q)
sq,φ
∈ {0, 1}, ∀q ∈ Q, φ ∈ cl(Fb∆ ),《A》∈ cl(Fb∆ ), m
A:⃗
mA ,⃗
mĀ

(9)

⃗ A ∈ DA (q), m
⃗ A ∈ DA (q)
sq,φ
⩾ yqA:⃗m , ∀q ∈ Q, φ ∈ cl(Fb∆ ),《A》∈ cl(Fb∆ ), m
A:⃗
mA ,⃗
m

(10)

⃗ A ∈ DA (q), m
⃗ Ā ∈ DĀ (q)
sq,φ
⩾ xφδ(q,(⃗mA ,⃗mĀ )) , ∀q ∈ Q, φ ∈ cl(Fb∆ ),《A》∈ cl(Fb∆ ), m
A:⃗
mA ,⃗
mĀ

(11)

A

q,φ
A:⃗
mA ,⃗
mA

s

A

δ(q,(⃗
mA ,⃗
mA ))
φ

⩽ yA:⃗m + x
q

A

⃗ A ∈ DA (q), m
⃗ A ∈ DA (q)
, ∀q ∈ Q, φ ∈ cl(Fb∆ ),《A》∈ cl(Fb∆ ), m

(12)

吴骏 等: 一种基于算法机制设计的社会法则合成方法

⃗ A ∈ DA (q)
zq,φ
∈ {0, 1}, ∀q ∈ Q, φ ∈ cl(Fb∆ ),《A》∈ cl(Fb∆ ), m
A:⃗
mA
⃗ A ∈ DA (q), m
⃗ Ā ∈ DĀ (q)
zq,φ
⩽ sq,φ
, ∀q ∈ Q, φ ∈ cl(Fb∆ ),《A》∈ cl(Fb∆ ), m
A:⃗
mA
A:⃗
mA ,⃗
mĀ
∑
⃗ A ∈ DA (q)
zq,ϕ
⩾ 1−
(1 − sq,ϕ
), ∀q ∈ Q, ϕ ∈ cl(Fb∆ ),《A》∈ cl(Fb∆ ), m
A:⃗
mA
A:⃗
mA ,⃗
mĀ

1457

(13)
(14)
(15)

⃗ Ā ∈DĀ (q)
m

xqp = 1, ∀q ∈ Q, p ∈ (Π ∩ cl(Fb∆ )) ∩ π(q)

(16)

xqp = 0, ∀q ∈ Q, p ∈ (Π ∩ cl(Fb∆ )) \ π(q)

(17)

q
x¬φ
= 1 − xφq , ∀q ∈ Q, ¬φ ∈ cl(Fb∆ )

(18)

q
xψ∨χ
⩾ xψq , ∀q ∈ Q, ψ ∨ χ ∈ cl(Fb∆ )

(19)

q
xψ∨χ
⩾ xχq , ∀q ∈ Q, ψ ∨ χ ∈ cl(Fb∆ )

(20)

⩽ x + xχq , ∀q ∈ Q, ψ ∨ χ ∈ cl(Fb∆ )

(21)

⃗ A ∈ DA (q)
∈ {0, 1}, ∀q ∈ Q, ϕ ∈ cl(Fb∆ ),《A》∈ cl(Fb∆ ), m

(22)

x
q,ϕ
A:⃗
mA

e

q
ψ∨χ

q
ψ

⃗ A ∈ DA (q)
eq,φ
⩾ zq,φ
− yqA:⃗mA , ∀q ∈ Q, φ ∈ cl(Fb∆ ),《A》∈ cl(Fb∆ ), m
A:⃗
mA
A:⃗
mA
q,φ
A:⃗
mA

e

⩽z

q,φ
A:⃗
mA

⃗ A ∈ DA (q)
, ∀q ∈ Q, φ ∈ cl(Fb∆ ),《A》∈ cl(Fb∆ ), m

⃗ A ∈ DA (q)
eq,φ
⩽ 1 − yqA:⃗mA , ∀q ∈ Q, φ ∈ cl(Fb∆ ),《A》∈ cl(Fb∆ ), m
A:⃗
mA
q
⃗ A ∈ DA (q)
x《A》
⩾ eq,φ
, ∀q ∈ Q,《A》⃝ φ ∈ cl(Fb∆ ), m
A:⃗
mA
⃝φ
∑
q
q,φ
x《A》⃝φ ⩽
eA:⃗mA , ∀q ∈ Q,《A》⃝ φ ∈ cl(Fb∆ )

(23)
(24)
(25)
(26)
(27)

⃗ A ∈DA (q)
m
q
r《A》
∈ {0, 1}, ∀q ∈ Q,《A》ψUχ ∈ cl(Fb∆ )
ψUχ
q
r《A》
⩽ xψq , ∀q ∈ Q,《A》ψUχ ∈ cl(Fb∆ )
ψUχ
∑
q
《A》ψUχ
r《A》
⩽
eq,A:⃗
, ∀q ∈ Q,《A》ψUχ ∈ cl(Fb∆ )
mA
ψUχ

(28)
(29)
(30)

⃗ A ∈DA (q)
m
q
《A》ψUχ
⃗ A ∈ DA (q)
r《A》
⩾ 1 − ((1 − xψq ) + (1 − eq,A:⃗
)), ∀q ∈ Q,《A》ψUχ ∈ cl(Fb∆ ), m
mA
ψUχ
q
《A》ψUχ

x

⩾ xχq , ∀q ∈ Q,《A》ψUχ ∈ cl(Fb∆ )

(31)
(32)

q
q
x《A》
⩾ r《A》
, ∀q ∈ Q,《A》ψUχ ∈ cl(Fb∆ )
ψUχ
ψUχ

(33)

q
q
x《A》
⩽ xχq + r《A》
, ∀q ∈ Q,《A》ψUχ ∈ cl(Fb∆ )
ψUχ
ψUχ

(34)

q
x《A》
⩽ xφq , ∀q ∈ Q,《A》□φ ∈ cl(Fb∆ )
□φ
∑
q
《A》□φ
x《A》
⩽
eq,A:⃗
, ∀q ∈ Q,《A》□φ ∈ cl(Fb∆ )
mA
□φ

(35)
(36)

⃗ A ∈DA (q)
m
q
《A》□φ
⃗ A ∈ DA (q)
x《A》
⩾ 1 − ((1 − xφq ) + (1 − eq,A:⃗
)), ∀q ∈ Q,《A》□φ ∈ cl(Fb∆ ), m
mA
□φ

(37)

xφqs∗ = 1

(38)

q
q
q
从 ILP-SWA( ∆ ) 的约束集可以看出, 形如 x《A》
, x《A》
和 x《A》
的变量的取值通过若干引入的中间变量,
⃝φ
φUψ
□φ

最终受制于形如 xψq 和 yqi:a 的变量的取值. 部分变量之间的约束关系可清晰地描绘为图 4.
易见 ILP-SWA( ∆ ) 的一个解实质上是一个赋值 ℓ , 在满足约束集的前提下为任意变量赋以 0 或 1 的值. 令
ηℓ (i, q) = {a ∈ εi (q)|ℓ(yqi:a ) = 1} , 即赋值 ℓ 对应的社会法则. 进一步我们可以证明如下结论.

软件学报 2024 年第 35 卷第 3 期

1458

yiq:a
y Aq :m

xϕq′′

A

s Aq ,:ϕm′A , m A

yiq:a

z Aq ,:ϕm′A

y Aq :m A
eAq ,:ϕm′A

xψq
r《qA》ϕ ψ

xψq

q

xχ

x《qA》◯ϕ

x《qA》ϕ ψ

图4

x《qA》ϕ

ILP-SWA(Δ) 中部分变量之间的依赖关系

引理 15.
1) ∀q ∈ Q, i ∈ {1, . . . , k}, a ∈ εi (q) : ℓ(yqi:a ) = 1 当且仅当 a ∈ ηℓ (i, q).
2) ∀q ∈ Q, A ⊆ {1, . . . , k}, m
⃗ A ∈ DA (q) : ℓ(yqA:⃗mA ) = 1 当且仅当 ∃i ∈ A : ℓ(yqi:⃗mA [i] ) = 1.
δ(q,(⃗
m ,⃗
m ))
3) ∀q ∈ Q,《A》∈ cl(Fb∆ ), m
⃗ A ∈ DA (q), m
⃗ A ∈ DA (q) : ℓ(sq,ϕ
) = 1 当且仅当 ℓ(yqĀ:⃗m ) = 1 或 ℓ(xφ A Ā ) = 1.
A:⃗
mA ,⃗
m
Ā

A

4) ℓ(zq,φ
) = 1 当且仅当 ∀⃗
mĀ ∈ DĀ (q) : ℓ(sq,φ
) = 1.
A:⃗
mA
A:⃗
mA ,⃗
mĀ
5) ∀q ∈ Q, φ ∈ cl(Fb∆ ), A ⊆ {1, . . . , k}, m
⃗ A ∈ DA (q) : ℓ(eq,φ
) = 1 当且仅当 ℓ(yqA:⃗mA ) = 0 且 ℓ(zq,φ
) = 1.
A:⃗
mA
A:⃗
mA
6) ∀q ∈ Q,《A》ψUχ ∈ cl(Fb∆ ) :
q
mA ∈ DA (q) : (ℓ(yqA:⃗mA ) = 0 且 ℓ(zq,A:《→−mA》ψUχ ) = 1) .
ℓ(r《A》
) = 1 当且仅当 ℓ(xψq ) = 1 且 ∃⃗
ψUχ
A

证明:
1) 由 ηℓ (i, q) 的定义可直接导出.
2) 若 ℓ(yqA:⃗mA ) = 1, 那么由约束 (8) 得
束 (7) 得 ℓ(yqA:⃗mA ) ⩾ 1, 故 ℓ(yqA:⃗mA ) = 1.

∑

q
q
ℓ(yqi:⃗mA [i] ) ⩾ 1, 故 ∃i ∈ A : ℓ(yi:⃗mA [i] ) = 1; 若 ∃i ∈ A : ℓ(yi:⃗mA [i] ) = 1, 那么由约

i∈A

δ(q,(⃗
m ,⃗
m ))
3) 若 ℓ(sq,φ
) = 1, 由约束 (12) 得 ℓ(yqĀ:⃗m ) + ℓ(xφδ(q,(⃗mA ,⃗mĀ )) ) ⩾ 1, 故 ℓ(yqĀ:⃗m ) = 1 或 ℓ(xφ A Ā ) = 1; 若 ℓ(yqĀ:⃗m ) = 1
A:⃗
mA ,⃗
mĀ
Ā

Ā

Ā

或 ℓ(xφδ(q,(⃗mA ,⃗mĀ )) ) = 1, 由约束 (10) 和约束 (11) 得 ℓ(sq,φ
) ⩾ 1, 故 ℓ(sq,φ
) = 1.
A:⃗
mA ,⃗
mĀ
A:⃗
mA ,⃗
mĀ
4) 若 ℓ(zq,φ
mĀ ∈ DĀ (q) :
) = 1, 那么由约束 (14) 得 ∀⃗
mĀ ∈ DĀ (q) : ℓ(sq,φ
) ⩾ 1, 故 ∀⃗
mĀ ∈ DĀ (q) : ℓ(sq,φ
) = 1; 若 ∀⃗
A:⃗
mA
A:⃗
mA ,⃗
mĀ
A:⃗
mA ,⃗
mĀ
) ⩾ 1, 故 ℓ(zq,φ
) = 1.
ℓ(sq,φ
) = 1, 那么由约束 (15) 得 ℓ(zq,φ
A:⃗
mA
A:⃗
mA
A:⃗
mA ,⃗
mĀ

5) 若 ℓ(eq,φ
) = 1, 那么由约束 (24) 得 ℓ(zq,φ
) ⩾ 1, 故有 ℓ(zq,φ
) = 1; 且由约束 (25) 得 ℓ(yqA:⃗mA ) ⩽ 0, 故有 ℓ(yqA:⃗mA )
A:⃗
mA
A:⃗
mA
A:⃗
mA
= 0; 若 ℓ(yqA:⃗mA ) = 0 且 ℓ(zq,φ
) = 1, 那么由约束 (23) 得 ℓ(eq,φ
) ⩾ 1, 因此可得 ℓ(eq,φ
) = 1.
A:⃗
mA
A:⃗
mA
A:⃗
mA
q
6) 若 ℓ(r《A》
) = 1, 那么由约束 (29) 得 ℓ(xψq ) ⩾ 1, 因此 ℓ(xψq ) = 1, 且由约束 (30) 可得
ψUχ

∑

《A》ψUχ
ℓ(eq,A:⃗
) ⩾ 1,
mA

⃗ A ∈DA (q)
m
《A》ψUχ
《A》ψUχ
这意味着 ∃⃗
mA ∈ DA (q) : (ℓ(yqA:⃗mA ) = 0 且 ℓ(zq,A:⃗
mA ∈ DA (q) : ℓ(eq,A:⃗
) = 1, 由引理 15 的 5) 可以得到 ∃⃗
) = 1) ; 若
mA
mA
《A》ψUχ
mA ∈ DA (q) :
ℓ(xψq ) = 1 且 ∃⃗
) = 1) , 那么由引理 15 的 5) 可以得到 ℓ(xψq ) = 1 且 ∃⃗
mA ∈ DA (q) : (ℓ(yqA:⃗mA ) = 0 且 ℓ(zq,A:⃗
mA
《A》ψUχ
q
q
ℓ(eq,A:⃗
) = 1, 进一步由约束 (31) 可以导出 ℓ(r《A》
) ⩾ 1, 这意味着 ℓ(r《A》
) = 1.
mA
ψUχ
ψUχ

接下来, 我们可以证明整数规划 ILP-SWA 的约束集等价地表述了 ATL 语义.
引理 16. 对于任意 φ ∈ cl(Fb∆ ) , ℓ(xφq ) = 1 当且仅当 S †ηℓ , q ⊨ φ .
证明: 对于任意 φ ∈ cl(Fb∆ ) , 以下我们将基于结构归纳法证明该结论.
1) 若 φ = p , p ∈ Π : 由约束 (16) 和约束 (17) 得 ℓ(xqp ) = 1 当且仅当 p ∈ (Π ∩ cl(Fb∆ )) ∩ π(q) 当且仅当 p ∈ π(q) 当且
仅当 S †ηℓ , q ⊨ φ .

吴骏 等: 一种基于算法机制设计的社会法则合成方法

1459

q
2) 若 φ = ¬ψ : 由约束 (18) 得 ℓ(x¬ψ
) = 1 当且仅当 ℓ(xψq ) = 0 (由归纳假设知) 当且仅当 S †ηℓ , q ⊭ φ (由 ATL 语义

知) 当且仅当 S †ηℓ , q ⊨ ¬φ .
q
3 ) 若 φ = ψ ∨ χ : 由 约 束 ( 1 9 – 2 1 ) 得 ℓ(xψ∨χ
) = 1 当 且 仅 当 ℓ(xψq ) = 1 或 ℓ(xχq ) = 1 ( 由 归 纳 假 设 知 ) 当 且 仅 当

S †ηℓ , q ⊨ ψ 或 S †ηℓ , q ⊨ χ (由 ATL 语义知) S †ηℓ , q ⊨ ψ ∨ χ .

4) 若 φ =《A》⃝ ψ :
q
若 ℓ(x《A》
) = 1 , 那么由约束 (27) 得
⃝ψ

DA (q) : ( ℓ(yqA:⃗mA ) = 0 且 ℓ(zq,ψ
) = 1 ).
A:⃗
mA

∑
⃗ A ∈DA (q)
m

mA ∈
mA ∈ DA (q) : ℓ(eA:⃗mA ) = 1 , 由引理 15 的 5) 有 ∃⃗
ℓ(eq,ψ
) ⩾ 1 , 因此 ∃⃗
A:⃗
mA
q,ψ

若 ∃⃗
) = 1) , 那么由引理 15 的 5) 知 ∃⃗
mA ∈ DA (q) : ℓ(eq,ψ
) = 1 , 进一步由约束
mA ∈ DA (q) : (ℓ(yqA:⃗mA ) = 0 且 ℓ(zq,ψ
A:⃗
mA
A:⃗
mA
q
q
(26) 得到 ℓ(x《A》
) ⩾ 1 , 因此 ℓ(x《A》
)=1.
⃝ψ
⃝ψ
q
以上意味着: ℓ(x《A》
) = 1) .
mA ∈ DA (q) : (ℓ(yqA:⃗mA ) = 0 且 ℓ(zq,ψ
) = 1 当且仅当 ∃⃗
A:⃗
mA
⃝ψ

(由引理 15 的 1), 2) 及 4)) 当且仅当 ∃⃗
⃗ A [i] < ηℓ (i, q) 且 ∀⃗
mA ∈ DA (q) : (∀i ∈ A : m
mĀ ∈ DĀ (q) : ℓ(sq,ψ
) = 1 ) (由引
A:⃗
mA ,⃗
mĀ
δ(q,(⃗
m ,⃗
m ))
理 15 的 3)) 当且仅当 ∃⃗
⃗ A [i] < ηℓ (i, q) 且 ∀⃗
mA ∈ DA (q) : (∀i ∈ A : m
mĀ ∈ DĀ (q) : (ℓ(yqĀ:⃗m ) = 1 或 ℓ(xψ A Ā ) = 1 )).
Ā

(由引理 15 的 1) 及 2)) 当且仅当 ∃⃗
⃗ A [i] < ηℓ (i, q) 且 ∀⃗
mA ∈ DA (q) : (∀i ∈ A : m
mĀ ∈ DĀ (q) : ( ∃i ∈ Ā : ℓ(yqi:⃗m [i] ) = 1 或
Ā
δ(q,(⃗
mA ,⃗
mĀ ))

ℓ(xψ

) = 1 )).

(由归纳假设) 当且仅当 ∃⃗
⃗ A [i] < ηℓ (i, q) 且 ∀⃗
mA ∈ DA (q) : (∀i ∈ A : m
mĀ ∈ DĀ (q) : ∃i ∈ Ā : (ℓ(yqi:⃗mĀ [i] ) = 1 或 S †ηℓ , δ(q,
⃗ Ā )) ⊨ ψ )).
(⃗
mA , m

(由 ATL 语义) 当且仅当 S †ηℓ , q ⊨《A》⃝ ψ .
5) 若 φ =《A》ψUχ :
q
q
q
q
若 ℓ(x《A》
) = 1 , 那么由约束 (34) 得 ℓ(xχq ) + ℓ(r《A》
) ⩾ 1 , 因此 ℓ(xχ ) = 1 或 ℓ(r《A》
)=1.
ψUχ
ψUχ
ψUχ
q
q
若 ℓ(xχq ) = 1 或 ℓ(r《A》
) = 1 , 那么由约束 (32) 及约束 (33) 得 ℓ(x《A》
)=1.
ψUχ
ψUχ
q
q
q
q
以上意味着: ℓ(x《A》
) = 1 当且仅当 ℓ(xχ ) = 1 或 ℓ(r《A》
) = 1 , (由引理 15 的 6)) 当且仅当 ℓ(xχ ) = 1 或 ( ℓ(xψq ) =
ψUχ
ψUχ

1 且 ∃⃗
mA ∈ DA (q) : (ℓ(yqA:⃗mA ) = 0 且 ℓ(zq,A:《→−mA》ψUχ ) = 1) ), (由上述 4 的证明过程) 当且仅当 ℓ(xχq ) = 1 或 ( ℓ(xψq ) = 1 且
A

q
ℓ(x《A》
) = 1 ), (由归纳假设) 当且仅当 S †ηℓ , q ⊨ χ 或 ( S †ηℓ , q ⊨ ψ 且 S †ηℓ , q ⊨《A》⃝《A》ψUχ ), (由 ATL 语
⃝《A》ψUχ

义) 当且仅当 S †ηℓ , q ⊨ χ ∨ (ψ ∧《A》⃝《A》ψUχ) , (由命题 13 的 1)) 当且仅当 S †ηℓ , q ⊨《A》ψUχ .
6) 若 φ =《A》□ψ :
q
若 ℓ(x《A》
) = 1 , 那么由约束 (35) 得 ℓ(xψq ) ⩾ 1 , 故有 ℓ(xψq ) = 1 , 并且由约束 (36) 可得
□ψ

∑

《A》□ψ
ℓ(eq,A:⃗
) ⩾ 1 , 因此
mA

⃗ A ∈DA (q)
m

《A》□ψ
∃⃗
mA ∈ DA (q) : ℓ(eq,A:⃗
)=1.
mA

《A》□ψ
q
q
若 ℓ(xψq ) = 1 且 ∃⃗
mA ∈ DA (q) : ℓ(eq,A:⃗
) = 1 , 那么由约束 (37) 得 ℓ(x《A》
) ⩾ 1 , 因此 ℓ(x《A》
)=1.
mA
□ψ
□ψ
《A》□ψ
q
以上意味着: ℓ(x《A》
mA ∈ DA (q) : ℓ(eq,A:⃗
) = 1 , (由引理 15 的 5)) 当且仅当 ℓ(xψq ) = 1
) = 1 当且仅当 ℓ(xψq ) = 1 且 ∃⃗
mA
□ψ
《A》□ψ
q
且 ∃⃗
mA ∈ DA (q) : (ℓ(yqA:⃗mA ) = 0 且 ℓ(zq,A:⃗
) = 1 ), (由上述 4 的证明过程) 当且仅当 ℓ(xψq ) = 1 且 ℓ(x《A》
) = =1,
mA
⃝《A》□ψ

(由归纳假设) 当且仅当 S †ηℓ , q ⊨ ψ 且 S †ηℓ , q ⊨《A》⃝《A》□ψ , (由 ATL 语义) 当且仅当 S ηℓ , q ⊨ ψ ∧《A》⃝《A》□
ψ , (由命题 13 的 2)) 当且仅当 S †ηℓ , q ⊨《A》□ψ .

接下来我们可以证明整数规划 ILP-SWA 的解对应着一个满足协同目标且最大化社会福利的高效社会法则,
这实质上意味着机制 VCG-SLM 的分配函数的计算已被正确地转化为整数规划.
定理 17. 若 ℓ 为 ILP-SWA( ∆ ) 的一个解, 那么 ηℓ 是一个 ∆ -社会法则, 满足 S †ηℓ , qs ⊨ φ∗ 且最大化 S W∆ (η) .
证明: 若 ℓ 为 ILP-SWA( ∆ ) 的一个解, 那么约束 (5) 保证每个状态每个 Agent 均至少会有一个被允许的行动,
因此 ηℓ 是一个 ∆ -社会法则; 约束 (38) 要求 ℓ(xφqs∗ ) = 1 , 由引理 16 可以导出 S †ηℓ , qs ⊨ φ∗ , 且优化目标函数满足:
∑
∑
c j · xφqsj =
σ(Fi , S †η) = S W∆ (η).
(φ j ,c j )i ∈Fb∆

i∈∆

综上所述, ηℓ 是一个 ∆ -社会法则, 满足 S †ηℓ , qs ⊨ φ∗ 且最大化 S W∆ (η) .

软件学报 2024 年第 35 卷第 3 期

1460

基于上述结论, 我们可以提出如下基于整数规划的机制 VCG-SLM-ILP, 并证明其与 VCG-SLM 机制的等价性.
算法 1. FIND-OSL.
输入: 并发博弈结构 S = ⟨k, Q, qs , Π, π, ε, δ⟩ , 投标 Fb1 , . . . , Fbk , 目标 φ∗ , Agent 集合 ∆ ⊆ [k] ;
1. 生成 ILP-SWA( ∆ ) 并求其解 ℓ ;
2. 令 ηℓ (i, q) = {a ∈ εi (q)|ℓ(yqi:a ) = 1} ;
输出: 选择的社会法则 ηℓ .
机制 1. VCG-SLM-ILP.
输入: 并发博弈结构 S = ⟨k, Q, qs , Π, π, ε, δ⟩ , 投标 Fb1 , . . . , Fbk , 目标 φ∗ ;
1. η∗ ← FIND-OSL( Ag );
2. 对于每个 Agent i ∈ Ag :
● η ← FIND-OSL( Ag \ {i} );
∑
∑
● ti ←
σ(Fbj , S †η) −
σ(Fbj , S †η∗ );
j,i

j,i

输出: 选择的社会法则 η∗ 以及对各个 Agent 的支付 t1 , . . . , tk .
定理 18. 当 SLS ,φ , ∅ 且 ∀i : SL−i
时, VCG-SLM-ILP 激励相容且所有的 Agent 参与投标并服从社会法则
S ,φ , ∅
是一个纳什均衡.
证 明 : 由 定 理 1 7 的 结 论 知 η∗ = arg maxη∈SLS ,φ
∑
j,i

σ(F̂ j , S †η) −

∑

∗

σ(F̂ j , S †η ) = max−i

η∈SLS ,φ

j,i

∑

k
∑

σ(Fbi , S †η) , 且 对 于 每 个 A g e n t i , 对 其 支 付 ti 满 足 ti =

i=1

σ(Fbj , S †η) −

∑

j,i

σ(Fbj , S †a(Fb)) .

j,i

故 VCG-SLM-ILP 实质上是 VCG-SLM 的一种基于整数规划的正确实现, 故由定理 2 及定理 3 得, SLS ,φ , ∅
且 ∀i : SL−i
时, VCG-SLM-ILP 激励相容且所有的 Agent 服从社会法则是一个纳什均衡.
S ,φ , ∅
以上结论说明, 借助于求解整数规划, VCG-SLM-ILP 是对机制 VCG-SLM 的一种正确的实现方式. 接下来还
需进一步探讨的问题是机制 VCG-SLM-ILP 的计算可行性.
定理 19. 生成 ILP-SWA( ∆ ) 的时间复杂度为 O(|Q| · kt · l2 ) , 其中 |Q| 、 k 、 t 分别为给定并发博弈结构的状态
数量、Agent 数量及状态转换的数量, l 为 ∆ 中所有 Agent 各自的特征集中的所有 ATL 公式的总长度之和.
证明: 由于对于长度为 l 的 ATL 公式 ψ , cl(ψ) 中包含的 ATL 公式的数量为 O(l) [7], 于是可以得到:
∪
∪
c∆ )| = | cl(F
bi )| = | (cl(φi ) ∪ . . . ∪ cl(φi ))| = |cl( ∨ ∨ φi )| = O(l).
|cl(F
1

i∈∆

ni

i∈∆ 1⩽ j⩽ni

i∈∆

j

c∆ ) 中形如 p 、 ¬φ 、 φ1 ∨ φ2 、《A》⃝ φ 、《A》□φ 及《A》φ1 Uφ2 的公式的数量, 以及涉及的 Agent 联
故 cl(F

盟的数量均为 O(l) . 此外, 根据并发博弈结构的定义, 各状态下联合行动与状态转换一一对应, 即有:
∑
|D(q)| = t.
q∈Q

因此, 对于任意的 A ⊆ {1, . . . , k} , 均有 |DA (q)| = O(t) 成立.
根据 ILP-SWA( ∆ ) 的定义, 其约束集由分别形如约束 (2)–约束 (38) 构成:
c∆ )| .
- 形如约束 (2) 限定所有变量 xφq 的取值范围, 其数量为 |Q| · |cl(F
- 形如约束 (3)–约束 (5) 限定所有变量 yqi:q 的取值范围, 其数量不超过:
∑∑
∑
∑
2
|εi (q)| + k|Q| ⩽ 2
Π1⩽i⩽k |εi (q)| + k|Q| = 2
|D(q)| + k|Q| = 2t + k|Q|.
q∈Q 1⩽i⩽k

q∈Q

- 形如约束 (6)–约束 (8) 限定所有变量 y

q
A:⃗
mA

q∈Q

的取值范围, 其数量不超过:

吴骏 等: 一种基于算法机制设计的社会法则合成方法

1461

c∆ )| · |DA (q)| + k · |Q| · 2|cl(F
c∆ )| · |DA (q)| ⩽ 6kt · |Q| · |cl(F
c∆ )|.
2 · |Q| · 2|cl(F
c∆ )|2 .
- 形如约束 (9)–约束 (12) 限定所有变量 sq,φ
的取值范围, 其数量不超过 4t · |Q| · |cl(F
A:⃗
mA ,⃗
m
Ā

- 形如约束 (13)–约束 (15) 限定所有变量 zq,φ
的取值范围, 其数量不超过:
A:⃗
mA
2
c
c∆ )|2 · t = 3t · |Q| · |cl(F
c∆ )|2 .
2 · |Q| · |cl(F∆ )| · t + |Q| · |cl(F
c∆ )| .
- 形如约束 (16) 和约束 (17) 限定所有变量 xqp 的取值范围, 其数量不超过 |Q| · |cl(F
q
c∆ )| .
- 形如约束 (18) 限定所有变量 x¬φ
的取值范围, 其数量不超过 |Q| · |cl(F
q
c∆ )| .
- 形如约束 (19)–约束 (21) 限定所有变量 xφ∨χ
的取值范围, 其数量不超过 3 · |Q| · |cl(F
q,φ
c∆ )|2 .
- 形如约束 (22)–约束 (25) 限定所有变量 e
的取值范围, 其数量不超过 4t · |Q| · |cl(F
A:⃗
mA

q
c∆ )| + |Q| · |cl(F
c∆ )| .
- 形如约束 (26) 和约束 (27) 限定所有变量 x《A》
的取值范围, 其数量不超过 t · |Q| · |cl(F
⃝ϕ
c∆ )| + t|Q| · |cl(F
c∆ )| .
- 形如约束 (28)–约束 (31) 限定所有变量 rq
的取值范围, 其数量不超过 3|Q| · |cl(F
《A》ψUχ

q
c∆ )| .
- 形如约束 (32)–约束 (34) 限定所有变量 x《A》
的取值范围, 其数量不超过 3 · |Q| · |cl(F
ψUχ
q
c∆ )| + t · |Q| · |cl(F
c∆ )| .
- 形如约束 (35)–约束 (37) 限定所有变量 x
的取值范围, 其数量不超过 2|Q| · |cl(F
《A》□ϕ

- 形如约束 (38) 要求协同目标在初始状态被满足, 其数量仅有 1 个.
综上所述, 由于约束集中的约束由上述各类约束构成, 其数量至多为:
c∆ )|2 + (6kt + 3t + 15)|Q| · |cl(F
c∆ )| + 2t + k|Q| + 1 ⩽ 39kt · |Q| · |cl(F
c∆ )|2 .
11t · |Q| · |cl(F
因此, 整数规划 ILP-SWA( ∆ ) 中包含约束的数量为 O(|Q| · kt · l2 ) . 由于生成每条约束需要单位时间, 生成该整
数规划需要的时间也为 O(|Q| · kt · l2 ) .
根据机制 VCG-SLM-ILP 的定义, 其计算的时间开销主要源于对 FIND-OSL 函数的 k + 1 次调用, 而函数
FIND-OSL 的计算开销主要来自 ILP-SWA( ∆ ) 的生成以及 ILP-SWA( ∆ ) 的求解. 定理 19 意味着 ILP-SWA( ∆ ) 的
生成所需的时间是问题规模的多项式函数, 此外 ILP-SWA( ∆ ) 的求解借助于目前已深入大规模工业应用的整数
规划求解器, 可在多项式时间内得到精度较高的解, 这意味着函数 FIND-OSL 以及机制 VCG-SLM-ILP 的计算都
是现实可行的. 此外, 由于建模为整数规划是进一步寻找具有可证明性能下界保证的近似算法的良好开端, 本文的
工作也为后续的近似机制的研究奠定了良好的基础.
3.3 提高不精确分配下机制的激励相容性
值得注意的是, 我们借助于求解器实现的其实是一个与 FIND-OSL 略有区别的算法 (不妨叫做 FIND-OSLX),
其中得到的是整数规划 ILP-SWA ( ∆ ) 的近似解而非精确解.同时, 我们不妨把相应的调用 FIND-OSLX 实现的机
制叫做 VCG-SLM-ILPX. 而已有结论表明, 当 VCG 机制实施时, 如果不能精确地得到社会福利最大的分配, 那么
机制的激励相容性就可能会受到影响 [43]. 该问题在本文的场景中同样存在.
回顾机制激励相容性 (定理 3) 的证明, 我们可以发现, 其关键步骤在于确定下面 “ ⩽ ”成立:
对于任意 Agent i ∈ ∆ ,
ui (Fi , (Fbi , Fb∆−i )) = σ(Fi , S †a(Fbi , Fb∆−i )) +

∑

σ(Fbj , S †a(Fbi , Fb∆−i )) − max
∆\{i}
η∈SLS ,φ

j∈∆\{i}

⩽ σ(Fi , S †a(Fi , Fb∆−i )) +

∑

σ(Fbj , S †a(Fi , Fb∆−i )) − max
∆\{i}
η∈SLS ,φ

j∈∆\{i}

∑

σ(Fbj , S †η) − σ(Fi , S )

j∈∆\{i}

∑

σ(Fbj , S †η) − σ(Fi , S ) = ui (Fi , (Fi , Fb∆−i )).

j∈∆\{i}

进而可以证明对于任意 Agent i ∈ ∆ , 其非诚实投标的效用 ui (Fi , (Fbi , Fb∆−i )) 不超过其诚实投标的效用 ui (Fi , (Fi , Fb∆−i )) ,
由此说明诚实投标是任意 Agent 的占优策略.
实际上任意 Agent i 仅能改变 a(Fbi , Fb∆−i ) 中自己的投标 Fbi , 从而影响以下两项的取值:
∑
σ(Fi , S †a(Fbi , Fb∆−i )) +
σ(Fbj , S †a(Fbi , Fb∆−i )).
j∈∆\{i}

不难发现这实质上是选择社会法则 a(Fbi , Fb∆−i ) 时, 一组特征集为 (Fi , Fb∆−i ) 的 Agent 的社会福利. 而根据定义,
b
a(Fi , Fb∆−i ) 是一个使一组特征集为 (Fbi , Fb∆−i ) 的 Agent 社会福利最大的社会法则. 因此当 Agent i 选择 Fbi = Fi 时, 上

软件学报 2024 年第 35 卷第 3 期

1462

述式子会取得最大值. 现在的问题在于, 我们不能期待在有意义的时间内精确地算得 a(Fbi , Fb∆−i ) , 而仅能得到一个
社会福利接近的社会法则 a′ (Fbi , Fb∆−i ) , 因此以下不等式并不一定成立:
∑
∑
σ(Fi , S †a′ (Fbi , Fb∆−i )) +
σ(Fbj , S †a′ (Fbi , Fb∆−i )) ⩽ σ(Fi , S †a′ (Fi , Fb∆−i )) +
σ(Fbj , S †a′ (Fbi , Fb∆−i )),
j∈∆\{i}

j∈∆\{i}

因而不一定能得到 ui (Fi , (Fbi , Fb∆−i )) ⩽ ui (Fi , (Fi , Fb∆−i )) .
即有可能存在某个 F ′ , 满足 ui (Fi , (F ′ , Fb∆−i )) > ui (Fi , (Fi , Fb∆−i )) .
i

i

此处可见如下两项事实.
1) VCG-SLM-ILPX 不满足激励相容性. 诚实投标并不是 Agent 的占优策略. 任意 Agent i 存在策略性地投标
Fi 从而获得更高的效用的可能.
′

2) 对任意 Agent i 而言, 是否选择诚实投标取决于能否在有意义的时间内算得一个上述 Fi′ . 如果不能, 那么
由于诚实投标能为 Agent 带来接近最优值的效用, 仍是 Agent 的合理选择.
简而言之, VCG-SLM-ILPX 并不是严格的激励相容机制. 它虽然保持了一定的激励诚实投标的属性, 但是如
果 Agent 能找出优于诚实投标的策略, 其激励相容性就会遭到一定程度地削弱. 针对此问题, 一方面我们可以试图
进一步证明 Agent 计算上述更优投标的问题是难计算问题 (由于本文利用第三方的求解器来计算整数规划, 隐藏
了实现了细节, 证明该结论存在一定的困难); 另一方面, 我们可基于 Nisan 等人 [19]提出的再次机会机制 (second
chance mechanism), 提高不精确分配下 VCG 机制的激励相容性. 具体做法是允许任意 Agent i 在规定的时间 T 内
c′ 是一个该 Agent 找出的会
c′ , 其中 F
随投标上交一个函数 pi (·) . 对于任意的投标向量 Fb = ⟨Fb1 , . . . , Fbk ⟩ , pi (Fb) = F
带来更大社会福利的投标向量; 在此基础上可将 VCG-SLM-ILPX 机制扩充成如机制 2.
机制 2. VCG-SLM-ILPX-SC.
输入: 并发博弈结构 S = ⟨k, Q, qs , Π, π, ε, δ⟩ , 投标 Fb1 , . . . , Fbk , 函数 p1 , . . . , pk , 目标 φ∗ ;
1. η∗ ← FIND-OSLX( Ag , Fb );
0

对每个 Agent i ∈ Ag : η∗i ← FIND-OSLX( Ag , pi (Fb) );
η∗ ← η∗ , . . . , η∗ 中对 Fb 具有最大的社会福利的社会法则;
0

k

2. 对于每个 Agent i ∈ Ag :
● η0 ← FIND-OSLX( Ag \ {i} , Fb );
● 对每个 Agent j ∈ Ag \ {i} : η j ← FIND-OSLX( Ag \ {i} , pi (Fb) );
● η ← η0 , . . . , ηi−1 , ηi+1 , . . . , ηk 中对 Fb 具有最大的社会福利的社会法则;
∑
∑
● ti ←
σ(Fbj , S †η) −
σ(Fbj , S †η∗ );
j,i

j,i

输出: 选择的社会法则 η∗ 以及对各个 Agent 的支付 t1 , . . . , tk .
VCG-SLM-ILPX-SC 机制的核心思想是, 各个 Agent 可在规定的时间内将自己能找到的社会福利更高的非诚
实投标告知机制. 机制将据此优化社会法则的选择并确定支付. 基于 Nisan 等人 [19]的结论, 该机制能得到社会福利
接近最优的社会法则, 且在一定程度上克服不精确分配的影响, 激励 Agent 诚实投标.

4 结论与展望
在信息不完全的策略环境下可靠地合成社会法则, 并实现 Agent 对社会法则的自觉服从是社会法则合成领域
亟待解决的重要问题. 本文在算法机制设计的框架下为上述问题寻求系统的解决方案, 发现策略环境下的社会法
则合成问题可被建模为一类特殊的以社会福利最大化为目标的公共货物拍卖问题, 上述意义的“可靠性”与“自觉
服从性”可分别由拍卖机制的激励相容性和个体理性保证; 社会法则即公共货物, Agent 对其估值不具有非负性.
这意味着著名的 VCG 机制可被考虑采用, 但在非负估值环境中能实现个体理性的 Clarke 基准规则似乎并不直接

吴骏 等: 一种基于算法机制设计的社会法则合成方法

1463

适用于当前问题的支付函数设计. 本文证明任何不区分交互互模拟等价结构的估值函数均可被等价地表示为特征
集, 从而为通常情况下的结构估值函数找出了一种具有良好理论基础的紧凑表示方法, 使 Agent 在投标中能清晰、
准确地表述自己的结构估值函数; 进而以 VCG 机制为基础设计了一种社会法则合成机制 VCG-SLM, 把社会法则
实现为一种权利与义务的综合体, Agent 想要享受作为社会成员的权利就必须服从社会法则的约束, 并进一步证
明了基于 Clarke 基准规则可设计一种使所有 Agent 选择参与投标、承诺服从社会法则的约束并诚实投标是纳什
均衡的支付规则, 这意味着没有 Agent 会单方面地脱离社会法则的约束; 本文证明了上述机制的计算是 FPNP-完全
的, 这意味着不能期望找到多项式时间复杂度的有效算法.本文从而将分配与支付的计算转化为整数规划来实现,
基于 ATL 的语法及语义系统地导出了整数规划的优化目标函数及约束集, 严格地证明了其正确性, 并证明了上述
到整数规划的转化仅需多项式时间. 由于整数规划是一类已被深入研究的难计算问题, 已有许多成熟的工业级整
数规划求解器, 这实际上为上述机制的计算问题设计了一种现实可行的算法. 同时考虑到不精确的分配函数会损
害 VCG 机制的激励相容性, 本文基于再次机会机制实现了一种克服该问题的方法.
本文的工作可看作是将算法机制设计的理论与技术应用于基于 ATL 逻辑的社会法则, 解决其中亟待解决的
问题的初步的尝试. 同时我们发现, 社会法则合成所带来的公共货物拍卖问题为算法机制设计的研究提供了新的
场景, 带来了新的挑战. 基于本文的框架, 主要可进行如下 4 方面进一步的工作: 一是, 探索 Clarke 基准规则之外
的支付函数设置方式, 使所有的 Agent 选择参与机制满足某种 (不限于纳什均衡) 的博弈解概念, 从而实现个体理
性并展现出不同的计算属性; 二是寻找保持激励相容和个体理性的优良属性且具有可证明性能下界的近似机制,
寻求机制计算方面更深入的结果, 并在此基础上尝试证明 Agent 操纵机制是难计算问题, 从而从另一角度确立本
文提出的非精确分配 VCG 机制的激励相容性; 三是探讨优化目标或约束与支付相关的场景, 这将紧密联系于最
优机制设计的工作, 是算法机制设计领域当前研究的重点与热点; 最后, 探讨无支付机制在社会法则领域的理论与
应用, 从而使机制适用于那些不能包含支付的场景, 大大拓宽机制的适应范围.
References:
[1]

Shoham Y, Tennenholtz M. On the synthesis of useful social laws for artificial agent societies. In: Proc. of the 10th National Conf. on
Artificial Intelligence. San Jose: AAAI Press, 1992. 276–281.

[2]

Shoham Y, Tennenholtz M. On social laws for artificial agent societies: Off-line design. In: Agre PE, Rosenschein SJ, eds. Computational
Theories of Interaction and Agency. Cambridge: MIT Press, 1996. 597–618.

[3]

van der Hoek W, Roberts M, Wooldridge M. Social laws in alternating time: Effectiveness, feasibility, and synthesis. Synthese, 2007,
156(1): 1–19. [doi: 10.1007/s11229-006-9072-6]

[4]

Wu J, Wang CJ, Xie JY. A framework for coalitional normative systems. In: Proc. of the 10th Int’l Conf. on Autonomous Agents and
Multiagent Systems. Taipei: Int’l Foundation for Autonomous Agents and Multiagent Systems, 2011. 259–266.

[5]

Ågotnes T, van der Hoek W, Wooldridge M. Conservative social laws. In: Proc. of the 20th European Conf. on Artificial Intelligence.
Montpellier: IOS Press, 2012. 49–54.

[6]

Alur R, Henzinger TA, Kupferman O. Alternating-time temporal logic. In: Proc. of the 38th Annual Symp. on Foundations of Computer
Science. Washington: IEEE, 1997. 100–109.

[7]

Alur R, Henzinger TA, Kupferman O. Alternating-time temporal logic. Journal of the ACM, 2002, 49(5): 672–713. [doi: 10.1145/585265.
585270]

[8]

Ågotnes T, van der Hoek W, Wooldridge M. Normative system games. In: Proc. of the 6th Int’l Conf. on Autonomous Agents and Multiagent Systems. Honolulu: ACM, 2007. 129. [doi: 10.1145/1329125.1329284]

[9]

Fitoussi D, Tennenholtz M. Minimal social laws. In: Proc. of the 15th National Conf. on Artificial Intelligence and the 10th Innovative
Applications of Artificial Intelligence Conf. Wisconsin: AAAI Press, 1998. 26–31.

[10]

Fitoussi D, Tennenholtz M. Choosing social laws for multi-agent systems: Minimality and simplicity. Artificial Intelligence, 2000,
119(1–2): 61–101. [doi: 10.1016/S0004-3702(00)00006-0]

[11]

Nisan N, Ronen A. Algorithmic mechanism design. In: Proc. of the 31st Annual ACM Symp. on Theory of Computing. Atlanta: ACM,
1999. 129–140. [doi: 10.1145/301250.301287]

[12]

Nisan N, Ronen A. Algorithmic mechanism design. Games and Economic Behavior, 2001, 35(1–2): 166–196. [doi: 10.1006/game.1999.
0790]

[13]

Nisan N, Roughgarden T, Tardos É, Vazirani VV. Algorithmic Game Theory. New York: Cambridge University Press, 2007.

1464

[14]

软件学报 2024 年第 35 卷第 3 期

Alur R, Henzinger TA, Kupferman O, Vardi MY. Alternating refinement relations. In: Proc. of the 9th Int ’l Conf. on Concurrency
Theory. Nice: Springer, 1998. 163–178. [doi: 10.1007/BFb0055622]

[15]

van der Hoek W, Ruan J, Wooldridge M. Strategy logics and the game description language. In: Proc. of the 2007 Workshop on Logic
Rationality & Interaction. 2007.

[16]
[17]

Clarke EH. Multipart pricing of public goods. Public Choice, 1971, 11(1): 17–33. [doi: 10.1007/BF01726210]
Vickrey W. Counterspeculation, auctions, and competitive sealed tenders. The Journal of Finance, 1961, 16(1): 8 –37. [doi: 10.1111/j.
1540-6261.1961.tb02789.x]

[18]
[19]

Groves T. Incentives in teams. Econometrica, 1973, 41(4): 617–631. [doi: 10.2307/1914085]
Nisan N, Ronen A. Computationally feasible VCG mechanisms. Journal of Artificial Intelligence Research, 2007, 29: 19–47. [doi: 10.
1613/jair.2046]

[20]

Emerson EA. Temporal and modal logic. In: van Leeuwen J, ed. Handbook of Theoretical Computer Science (Vol. B): Formal Models
and Semantics. Cambridge: MIT Press, 1991. 995–1072.

[21]

Ågotnes T, van der Hoek W, Rodríguez-Aguilar JA, Sierra C, Wooldridge M. On the logic of normative systems. In: Proc. of the 20th Int’l
Joint Conf. on Artifical Intelligence. Hyderabad: Morgan Kaufmann Publishers Inc., 2007. 1175–1180.

[22]

Ågotnes T, van der Hoek W, Wooldridge M. Robust normative systems. In: Proc. of the 7th Int ’l Conf. on Autonomous Agents and
Multiagent Systems. Estoril: Int’l Foundation for Autonomous Agents and Multiagent Systems, 2008. 747–754.

[23]

Ågotnes T, van der Hoek W, Tennenholtz M, Wooldridge M. Power in normative systems. In: Proc. of the 8th Int’l Conf. on Autonomous
Agents and Multiagent Systems. Budapest: Int’l Foundation for Autonomous Agents and Multiagent Systems, 2009. 145–152.

[24]

Ågotnes T, Wooldridge M. Optimal social laws. In: Proc. of the 9th Int’l Conf. on Autonomous Agents and Multiagent Systems. Toronto:
Int’l Foundation for Autonomous Agents and Multiagent Systems, 2010. 667–674.

[25]

van der Hoek W, Wooldridge M. Cooperation, knowledge, and time: Alternating-time temporal epistemic logic and its applications.
Studia Logica, 2003, 75(1): 125–157. [doi: 10.1023/A:1026185103185]

[26]

van der Hoek W, Wooldridge M. On the logic of cooperation and propositional control. Artificial Intelligence, 2005, 164(1–2): 81–119.
[doi: 10.1016/j.artint.2005.01.003]

[27]

Wang CJ, Wu J, Wang ZC, Xie JY. Strategic ability updating in concurrent games by coalitional commitment. IEEE Trans. on Systems,
Man, and Cybernetics, Part B (Cybernetics), 2011, 41(6): 1442–1457. [doi: 10.1109/TSMCB.2011.2146248]

[28]

Wang CJ, Wu J, Zhang L, Xie JY. On the limitation of the power of coalitional normative systems. Ruan Jian Xue Bao/Journal of
Software, 2012, 23(7): 1796−1804 (in Chinese with English abstract). http://www.jos.org.cn/1000-9825/4135.htm [doi: 10.3724/SP.J.
1001.2012.04135]

[29]
[30]
[31]

Binmore K. Game Theory and the Social Contract, Vol. 1: Playing Fair. Cambridge: MIT Press, 1994.
Binmore K. Game Theory and the Social Contract, Vol. 2: Just Playing, Cambridge: MIT Press, 1998.
Bulling N, Dastani M. Verifying normative behaviour via normative mechanism design. In: Proc. of the 22nd Int ’l Joint Conf. on
Artificial Intelligence. Barcelona: AAAI Press, 2011. 103–108.

[32]
[33]

Bulling N, Dastani M. Norm-based mechanism design. Artificial Intelligence, 2016, 239: 97–142. [doi: 10.1016/j.artint.2016.07.001]
Dughmi S, Hartline J, Kleinberg RD, Niazadeh R. Bernoulli factories and black-box reductions in mechanism design. Journal of the
ACM, 2021, 68(2): 1–30. [doi: 10.1145/3440988]

[34]
[35]

Archer A, Tardos É. Frugal path mechanisms. ACM Trans. on Algorithms, 2007, 3(1): 3. [doi: 10.1145/1186810.1186813]
Elkind E, Sahai A, Steiglitz K. Frugality in path auctions. In: Proc. of the 15th Annual ACM-SIAM Symp. on Discrete Algorithms. New
Orleans: Society for Industrial and Applied Mathematics, 2004. 701–709.

[36]

Zhang L, Chen HB, Wu J, Wang CJ, Xie JY. False-name-proof mechanisms for path auctions in social networks. In: Proc. of the 22nd
European Conf. on Artificial Intelligence. The Hague: IOS Press, 2016. 1485–1492. [doi: 10.3233/978-1-61499-672-9-1485]

[37]

Cheng H, Zhang WT, Zhang Y, Zhang L, Wu J, Wang CJ. Fast core pricing algorithms for path auction. Autonomous Agents and Multiagent Systems, 2020, 34(1): 18. [doi: 10.1007/s10458-019-09440-y]

[38]

Talwar K. The price of truth: Frugality in truthful mechanisms. In: Proc. of the 20th Annual Symp. on Theoretical Aspects of Computer
Science. Berlin: Springer, 2003. 608–619. [doi: 10.1007/3-540-36494-3_53]

[39]

Bikhchandani S, de Vries S, Schummer J, Vohra RV. Linear programming and vickrey auctions. IMA Volumes in Mathematics and its
Applications, 2001, 127: 75–116.

[40]

Garg R, Kumar V, Rudra A, Verma A. Coalitional games on graphs: Core structures, substitutes and frugality. In: Proc. of the 4th ACM
Conf. on Electronic Commerce. New York: ACM, 2003. 248–249. [doi: 10.1145/779928.779982]

[41]

Tang PZ. Computational economics and the optimal mechanism design problem. Communications of CCF, 2013, 9(10): 18 –23 (in
Chinese with English abstract).

吴骏 等: 一种基于算法机制设计的社会法则合成方法

[42]

1465

Sandholm T. Algorithm for optimal winner determination in combinatorial auctions. Artificial Intelligence, 2002, 135(1–2): 1–54. [doi:
10.1016/S0004-3702(01)00159-X]

[43]

Dobzinski S, Nisan N. Limitations of VCG-based mechanisms. In: Proc. of the 39th Annual ACM Symp. on Theory of Computing. San
Diego: ACM, 2007. 338–344. [doi: 10.1145/1250790.1250842]

[44]
[45]
[46]

Myerson RB. Optimal auction design. Mathematics of Operations Research, 1981, 6(1): 58–73. [doi: 10.1287/moor.6.1.58]
Hartline J. Lectures on optimal mechanism design. 2006. http://users.eecs.northwestern.edu/~hartline/omd.pdf
Wu J, Qiao Y, Zhang L, Wang CJ, Liu ML. A multi-unit profit competitive mechanism for cellular traffic offloading. In: Proc. of the 34th
AAAI Conf. on Artificial Intelligence. Palo Alto: AAAI Press, 2020. 2294–2301. [doi: 10.1609/aaai.v34i02.5607]

[47]

Wu J, Zhang L, Wang CJ, Xie JY. Synthesizing optimal social laws for strategical agents via bayesian mechanism design. In: Proc. of the
16th Int’l Conf. on Autonomous Agents and Multiagent Systems. São Paulo: Int’l Foundation for Autonomous Agents and Multiagent
Systems, 2017. 1214–1222.

[48]

Wu J, Zhang L, Wang CJ, Xie JY. Mechanism design for social law synthesis under incomplete information. In: Proc. of the 16th Int’l
Conf. on Autonomous Agents and Multiagent Systems. São Paulo: Int ’l Foundation for Autonomous Agents and Multiagent Systems,
2017. 1757–1759.

[49]

Papadimitriou CH. Computational Complexity. Boston: Addison-Wesley, 1994.

附中文参考文献:
[28]

王崇骏, 吴骏, 张雷, 谢俊元. 联盟规范系统及其规范能力极限. 软件学报, 2012, 23(7): 1796–1804. http://www.jos.org.cn/1000-9825/
4135.htm [doi: 10.3724/SP.J.1001.2012.04135]

[41]

唐平中. 计算经济学与最优机制设计问题. 中国计算机学会通讯, 2013, 9(10): 18–23.

吴骏(1981－), 男, 博士, 讲师, 主要研究领域为

王崇骏(1976－), 男, 博士, 教授, CCF 高级会员,

算法机制设计, 多智能体系统.

主要研究领域为多智能体系统, 智能信息处理,
机器学习.

曹杰(1969－), 男, 博士, 教授, CCF 专业会员, 主

谢俊元(1961－), 男, 教授, 博士生导师, 主要研

要研究领域为商务智能, 数据挖掘基础理论, 电

究领域为人工智能.

子商务平台支撑技术.


软件学报 ISSN 1000-9825, CODEN RUXUEW
Journal of Software, [doi: 10.13328/j.cnki.jos.007105]
©中国科学院软件研究所版权所有.

E-mail: jos@iscas.ac.cn
http://www.jos.org.cn
Tel: +86-10-62562563

一种聚类分析驱动种子调度的模糊测试方法

∗

张文 1, 2, 陈锦富 1, 2, 蔡赛华 1, 2, 张翅 1, 2, 刘一松 1
1

(江苏大学 计算机科学与通信工程学院, 江苏 镇江 212013)

2

(江苏省工业网络安全技术重点实验室, 江苏 镇江 212013)

通讯作者: 陈锦富, E-mail: jinfuchen@ujs.edu.cn
刘一松, E-mail: liuyisong@ujs.edu.cn

摘 要: 作为当前被广泛应用的自动化软件测试技术,模糊测试的首要目标是尽可能多地探索被测程序的代码区域
以达到更高的覆盖率,从而检测出更多的漏洞或者错误.现有的模糊测试方法大多是根据种子的历史突变数据来调
度种子,实现起来较简单,但忽略了种子所探索程序空间的分布情况,导致测试工作可能会陷入只对程序的某单一区
域进行探测,造成测试资源的浪费.本文提出了一种基于聚类分析驱动种子调度的模糊测试方法 Cluzz.首先,Cluzz
结合种子执行路径覆盖的分布来分析种子在特征空间上的区别,使用聚类分析对种子在程序空间中的执行分布情
况进行划分.然后,根据不同种子簇群的路径覆盖模式与聚类分析结果对种子进行优先级评估,探索稀有代码区域并
优先调度评估得分较高的种子.其次,通过种子评估得分为种子分配能量,将突变得到的有趣输入保留并进行归类以
更新种子簇群信息.Cluzz 根据更新后的种子簇群重新评估种子,以确保测试过程中种子的有效性,从而在有限时间
内探索更多的未知代码区域,提高被测程序的覆盖率.最后,将 Cluzz 实现在 3 个当前主流的模糊器上,并在 8 个流行
的真实程序上进行大量测试工作.结果表明,Cluzz 检测独特崩溃的平均数量是普通模糊器的 1.7 倍,在发现新边缘数
量方面平均优于基准模糊器 22.15%.此外,通过与现有种子调度方法进行对比,Cluzz 的综合表现要优于其它基准模
糊器.
关键词: 模糊测试;软件安全;聚类分析;种子调度;能量分配
中图法分类号: TP311
中文引用格式: 张文,陈锦富,蔡赛华,张翅,刘一松. 一种聚类分析驱动种子调度的模糊测试方法.软件学报.
http://www.jos.org.cn/1000-9825/7105.htm
英文引用格式: Zhang W, Chen JF, Cai SH, Zhang C, Liu YS. A Novel Fuzzing Approach of Clustering Analysis-driven in Seed
Scheduling. Ruan Jian Xue Bao/Journal of Software (in Chinese). http://www.jos.org.cn/ 1000-9825/7105.htm

A Novel Fuzzing Approach of Clustering Analysis-driven in Seed Scheduling
ZHANG Wen1,2,

CHEN Jin-Fu 1,2,

CAI Sai-Hua1,2,

ZHANG Chi1,2,

LIU Yi-Song1

1

(School of Computer Science and Communication Engineering, Jiangsu University, ZhenJiang 212013, China)

2

(Jiangsu Key Laboratory of Security Technology for Industrial Cyberspace, Jiangsu University, Zhenjiang, 212013, China)

Abstract:

As a widely used automated software testing technique, the primary goal of fuzzy testing is to explore as many code areas of

the program under test as possible, thereby achieving higher coverage as well as detecting more bugs or errors. Most of existing fuzzy
testing methods schedule the seed based on the historical mutation data of the seed, which is simpler to implement but ignores the
distribution of program space explored by the seed, resulting in the testing may fall into only a single region of the program to be probed,
∗

基金项目: 国家自然科学基金(62172194, 62202206, U1836116); 江苏省自然科学基金(BK20220515); 中国博士后科学基金资

助项目(2023T160275); 江苏省自然科学基金前沿引领项目(BK20202001); 江苏省研究生科研与实践创新计划项目(KYCX21_3375,
SJCX23_2092); 江苏省青蓝工程项目(2022)
收稿时间: 2023-09-09; 修改时间: 2023-10-30; 采用时间: 2023-12-14; jos 在线出版时间: 2024-01-05

2205

张文 等: 一种聚类分析驱动种子调度的模糊测试方法

and causing the waste of testing resources. In this paper, we propose the Cluzz, a fuzzing approach of clustering analysis-driven in seed
scheduling. Firstly, Cluzz analyzes the difference between seeds in the feature space by combining the distribution of seed execution path
coverage, and uses cluster analysis to classify the distribution of seeds execution in the program space. And then, Cluzz prioritizes the
seeds according to the path coverage patterns of different seed clusters and the results of cluster analysis, explores the rare code regions
and prioritizes the seeds with higher evaluation scores. Secondly, energy is allocated to the seeds by their evaluation scores, and the
interesting inputs obtained from mutations are retained and categorized to update the seed cluster information. Cluzz re-evaluates the
seeds based on the updated seed clusters to ensure the validity of seeds during testing process, thereby exploring more unknown code
regions in a limited time and improving the coverage of the program under test. Finally, we implement the Cluzz on three current
mainstream fuzzers and conduct extensive testing work on eight popular real-world programs. The results show that Cluzz can detect an
average of 1.7 times more unique crashes than a regular fuzzer, and it also outperforms a benchmark fuzzer by an average of 22.15% in
terms of the number of new edges found. In addition, compared with the existing seed scheduling methods, the comprehensive
performance of Cluzz is better than that of other benchmark fuzzers.
Key words:

1 引

Fuzzing; Software Security; Cluster Analysis; Seed Scheduling; Energy Allocation

言

在软件安全测试领域中,模糊测试(Fuzzing)是其中一种被广泛应用的自动化软件测试技术,用于暴露软件
中的安全问题,它已经应用到现实世界的程序测试中并识别出上千个错误和漏洞[1-4].其关键思想是通过自动化
的方法对输入进行随机变异以不断生成新的测试输入,从而得到涵盖程序潜在错误和异常情况的新的测试输
入.然后,监控被测程序的运行状态以检测其是否存在漏洞、错误、异常行为或者崩溃等问题.现如今大多数流
行的模糊器通过进化搜索算法来生成新的测试输入,这些进化搜索算法方法能够有效地引导模糊测试以探索
被测程序中更多的未知代码区域.在模糊过程中,对一组种子输入进行突变的过程仅保留最有趣的新输入(即触
发了新的程序状态的输入),并在这些有趣输入基础上进行下一步的突变 [5-7].最后,利用突变得来的有趣测试输
入在程序的输入空间中搜索潜在可利用的错误行为.
然而在现实中,实际的程序输入空间往往很大,通过随机变异方式无法穷尽地对其进行探索,此外无引导的
测试输入生成方式在探索被测程序错误方面也并不有效.因此,现有的模糊器大多采用边覆盖引导的进化方法
来指导输入生成过程,确保生成得到的新输入能够探索目标程序不同的控制流边 [8,9] 以尽可能地探索程序的输
入空间.这种覆盖引导的灰盒模糊测试(Coverage-guide Greybox Fuzzing,CGF)从一个种子输入语料库开始,不
断重复地从语料库中挑选一个种子对其进行突变,只将那些产生了新的边缘覆盖的突变输入添加到种子语料
库中以用于后续的模糊测试工作.CGF 的主要工作流程如图 1 所示.这种模糊器的性能在很大程度上依赖于种
子的前后调度顺序,即选择要突变种子的先后顺序[10].
目标程序

初始种子

种子选择

能量
分配

突变

生成

新输入

Bug报告

覆盖率报告

执行
添加

新的种子

是

触发新状态

种子队列

图 1 覆盖引导的灰盒模糊测试工作流程
随着人工智能的发展,基于机器学习的技术也已经应用于模糊测试当中 [11].基于机器学习的模糊测试方法
通常使用现有的测试输入来训练一个模型,通过训练得到的模型来指导模糊测试.在定向灰盒模糊测试中,
Fuzzguard[12] 提出在执行测试输入之前通过神经网络模型预测测试输入的可达性,过滤掉不可达的输入.这种方
法能够有效减少由于执行不必要测试输入所带来的资源浪费,但模型训练所需要的时间成本过高.Neuzz[13] 与
MTFuzz[14] 利用神经网络模型来近似现实世界程序的分支行为,通过神经网络的梯度信息去识别输入序列的关
键字节来指导测试的突变,并针对关键字节区域中字节的突变来提高测试覆盖率.SmartFuSE[15] 提出了一种基
于深度学习的符号执行与模糊测试的混合测试方法.然而,与其他所有的有监督学习任务相同,这些方法在模型

Journal of Software 软件学报

2206

的训练上很大程度依赖于训练样本的数量与多样性.此外,在模糊测试中,用于模型训练的数据样本为测试输
入,而程序的输入空间往往很大,测试输入的字节分布往往是稀疏的且存在输入样本不平衡现象,导致模型的训
练容易出现过拟合问题.而且,这些方法并没有考虑对突变种子选择的先后调度顺序是否会影响模糊器的工作
效率.
通过优化 CGF 过程中的种子调度与变异算子选择是当前对模糊测试研究的两个关键问题.合适的突变策
略对模糊测试发现新的边缘覆盖和暴露崩溃方面有着很大的改善,现有的变异调度方法根据历史突变数据来
优化变异算子概率分布[16,17],通过根据概率分布选择最优的突变策略.对于种子优先级调度,其主要难题是如何
确定种子语料库中哪些种子突变所产生的新输入更有希望探索被测程序中尽可能多的新边,从而对这些更有
希望的种子进行更多的突变以获得更高的边缘覆盖.一种简单的策略是根据种子执行路径在程序控制流程图
(Control Flow Graph,CFG)中所有可达边的数量来调度种子,但这种理想的方法假设 CFG 中所有的边都是可以
通过突变可达的.然而,现实世界的程序会存在复杂的约束条件,难以通过随机突变的方式绕过约束,因而这种
理想化的策略不太可能有效.先前的研究证据也表明通过对输入随机突变的方法,到达子节点的难度通常比到
达父节点更大 [18],即通过对输入字节随机突变的方式突破约束条件以到达新的边缘是很有难度的.对于一些嵌
入在较为简单的约束条件中的浅边,可能可以通过大量的突变输入到达.而对于嵌入在很多复杂约束条件中的
深边,在模糊过程中只能被少数的几个突变得到的新输入(如果有发生的情况)到达(因为许多分支可能是难以
通过随机突变到达的).另外一个理想的策略是根据一个种子可以通过突变到达的所有潜在可行边的数量来进
行种子调度.计算得到的潜在可行边的数量越高的种子,应该被优先调度并且分配更多的能量.然而,计算所有
种子沿所有边是不切实际的,一方面是因为计算将产生巨大计算成本,另一方面是在程序控制流程图中某条边
是否可达受很多因素的影响(如程序上下文因素、变量因素等),所以边的可达性难以通过形式化的方式去衡量.
目前已有的关于种子调度的模糊测试方法主要基于种子历史突变产生的边缘覆盖数量来选择种子,这些
种子在历史突变过程中产生了更高的路径覆盖 [19],或根据测试过程反馈信息对种子进行优先级排序以确定哪
个种子是更有希望的种子,例如种子执行触发了更稀有的边 [20].通过使用图分析的中心性度量方法来近似种子
到达未访问边的可能性来选择种子[21].然而,种子的数量会随着模糊工作的进行而爆发性的增长,这会造成大量
的计算过程,时间成本是不可预估的.并且,这些现有方法在测试过程中并没有考虑不同种子的执行路径在程序
空间中的整体状态分布.由于种子输入之间的差异性,不同种子所探索的程序空间也有所不同.相似的种子有着
相似的输入模式以及执行行为,种子之间的差异越大则所探索的程序空间越不相同.由于忽略了不同类别种子
探索区域的信息,会使得探测容易陷入对程序空间中某单一区域的探索,从而限制了对程序空间的全面探索能
力.其次,在现有的模糊测试方法中,大多是通过历史运行数据推断下一时刻的最优选择,这种方式涉及到大量
的计算,会导致高额的时间开销.
为了解决现有方法存在的问题,本文提出了一种基于种子聚类分析的模糊调度方法,并在此想法上实现了
轻量级模糊测试框架 Cluzz.我们根据种子的执行状态序列特征,分析种子在程序空间中的分布情况,通过聚类
分析将种子分为若干个种子簇群,其中相似的种子在特征空间上更接近,而不同类别之间的种子在特征空间上
有明显的区分.由于不同种子簇群之间的路径探索区域有着明显差异,因此通过执行程序空间某区域的种子分
布权重占比来衡量代码区域的稀有度以及相较于全局的种子稀有度得分.我们优先考虑能够探索程序稀有代
码区域的测试输入和稀有种子.在完成对种子的选取后,Cluzz 根据种子的稀有度得分为种子分配能量,对稀有
度得分越高的种子分配更多的能量,确保能够全面探测未知代码区域.在模糊测试过程中,通过分析上下文中的
种子选择情况来平衡调度各个种子簇群中的种子,保证后代种子的多样性,实现对程序路径探索的多样性和测
试输入的多样化.随着语料库中的种子数量的增加,阶段性地重新评估队列中种子的优先级,避免对无用种子模
糊导致的测试时间和资源的浪费,从而提高测试的效率与覆盖率.
本文的主要贡献包括:
(1)

提出了一种基于种子聚类分析的模糊调度方法,通过分析不同类别之间的种子在特征空间上的区别,
并结合种子执行状态在程序空间中的分布情况来划分种子簇群,用于指导模糊测试中的种子调度.

(2)

基于划分得到的种子簇群信息与种子执行信息,提出了种子簇群权重与种子稀有度得分评估策略,并
将两种分数用于种子的能量分配工作.

2207

张文 等: 一种聚类分析驱动种子调度的模糊测试方法

(3)

基于以上方法实现了轻量级模糊测试框架 Cluzz,并在当前三个主流模糊器 AFL、MOPT 和 NEUZZ
上实现并设计相关实验,以证明我们方法的通用性与实用性.

本文其余部分的组织如下.第 2 节介绍种子调度、能量分配以及聚类分析等基础知识.第 3 节介绍了本文
提出的 Cluzz 方法,包括多重随机初始化聚类分析、种子优先级调度和能量分配.第 4 节将本文提出的 Cluzz 方
法集成到现有模糊器中,并和基准模糊器进行了实验对比.第 5 节对有效性威胁进行了分析.第 6 节对相关工作
进行了总结.最后,第 7 节对本文的研究进行了总结并给出了未来展望.

2 基础知识
2.1 无监督学习-聚类分析
无监督学习中的聚类方法对于挖掘数据的结构和模式以及不同数据点之间的相似性和差异性至关重要.
这种将数据划分为不同类别的过程可以帮助我们发现数据的内在结构和模式,从而更好地理解和分析数据.目
前已有多种聚类方法用于数据挖掘与数据聚类中 [22-25].例如,K-Means[23] 是一种基于质心的聚类算法,其目标是
使得每个数据点都属于离其最近的簇中心点所对应的簇,K-means++[22] 是它的一种改进算法.此外,DBSCAN[24]
是一种基于密度的聚类算法,它在聚类前不需要事先指定聚类的数量.对于我们的应用场景,初始数据集相对较
小并且需要通过对程序和初始种子的静态分析并通过迭代来确定聚类的数量,因此不需要 DBSCAN 聚类的自
适应性质.并且 DBSCAN 算法的时间复杂度也较高,所以使用不会产生过多时间开销(在第 4 节中通过实验对其
证明)的 K-Means 聚类算法会更加高效和可靠.并且由于我们数据集中数据本身是通过执行输入而得到路径覆
盖的输出结果,所以数据本身是准确的、无异常值,噪声极小,有利于 K-Means 算法明确地划定簇群的边界,在较
短的时间内收敛到稳定的结果.
由于种子输入与执行状态序列是反映种子特征与状态分布情况的数据,具有输入空间的高维与稀疏的性
质,而高维稀疏的数据会导致聚类算法难以识别数据间的实际距离.同时,高维稀疏数据也会增加算法的运行时
间,导致聚类算法难以选择出数据中最重要的特征,从而影响聚类效果.这显然不符合我们的期待,因此需要在
聚类之前对数据进行降维操作.对于每个数据样本,通过计算它与 K 个聚类中心之间的距离,并将该样本分配给
距离最近的聚类中心所在的簇.对于每个样本 Xi,计算它与每个聚类中心 Cj 的距离,然后将该样本分配到距离最
近的聚类中心所在的簇 Sj 中,其计算过程如公式(1)所示.
S j = argmin j∈{1,2,..., k}dist ( xi , C j )

每完成一次样本所属的簇(即 Sj)的计算后,通过公式(2)重新计算每个簇的聚类中心 Cj：
C j = ∑ x ∈S xi S j
i

j

(1)
(2)

通过上面描述的步骤不断迭代,直到所有数据所分配的簇编号不再发生变化则完成一次聚类操作.由于不
同的初始聚类中心可能导致不同的聚类结果,而初始聚类中心的选取充满了随机性.因此,我们通过多随机初始
化的方式来减小随机因素的影响,避免算法陷入局部最优,提高聚类的准确性和可靠性.
2.2 模糊测试中的种子调度优化
提高模糊测试效率的一个主要方式是种子调度,即种子的选择顺序.对模糊测试中的种子调度进行优化,可
以选择正确的种子以极大地提高模糊测试的能效 [10].与之相比,没有种子调度优化的模糊器(例如 MOPT[16])在
读入初始测试输入时,仅仅按照文件名称顺序将其加入种子队列,且后续突变得到的新输入也没有通过某种方
法对其进行调度选择.模糊测试的进行会产生大量的新输入,不同的输入可能在发现新的覆盖方面具有不同的
潜力,并且不同类别的种子有着截然不同的覆盖模式.同样,通过突变得到的大量新输入中也会存在对测试没有
贡献的输入,通过种子调度避免执行这些无用的输入可以提高测试效率.因此,选择适当的策略优先调度有效种
子可以提高测试覆盖率并尽快发现更多的程序错误.
2.3 模糊测试中的能量分配优化
另一种提高模糊测试性能的方法是通过优化模糊过程中的能量分配模块 [26-28].对于一般的模糊器(例如
AFL[5]),它们为每个种子分配几乎相同的能量.然而,不同的种子在发现新的覆盖方面有着不同的潜力.因此,在

Journal of Software 软件学报

2208

基于模糊测试规则的基础上,覆盖引导的模糊测试方法 [29] 通过给那些更有可能发现新覆盖的种子分配更多的
能量.如何选取为种子计算所分配能量的指标,以及如何计算是关键问题所在.不合理的计算方式会导致能量的
浪费,或者导致种子突变不充分,造成模糊器探测新边缘的能力出现下降.在 AFL 中,种子的执行速度与路径覆
盖大小都是计算种子能量大小的指标.Truzz[30]提出了一种基于路径转换的能量调度优化方法,它通过观察每个
突变的结果来构建路径转换的概率分布,识别并且保护与验证检查有关的字节不受突变,将更多的能量将分配
给那些非验证检查的字节,从而更加充分地探索程序的功能代码.

3 一种聚类分析驱动种子调度的模糊测试方法
3.1 方法动机
在现有的模糊测试方法中,FairFuzz[20] 通过统计识别测试输入很少执行到的程序分支(该方法将 AFL[5] 中
的边的表示方式称为分支),并将这些分支称为稀有分支. FairFuzz 通过在这些罕见的分支上生成更多的随机输
入,极大地增加了由它们保护的代码部分的覆盖率.同样, AFL++[4] 会优先并且额外地关注那些执行边缘很少被
其他种子覆盖的种子,这是一个有效的度量方式.然而,在真实的测试环境中被测程序的边与分支的数量非常庞
大,且模糊测试所生成的测试输入数量会急剧的增加最后趋于平缓.种子数量的增长与这种精细化的统计反馈
来指导种子调度,所带来的计算开销是不可预估的.此外,这种根据种子覆盖到的稀有边缘(或分支)数量指导的
种子调度,其方法中对于稀有边缘(或分支)所定义的边界比较模糊(即：选取统计结果中覆盖次数最少的 k 条边
作为稀有边,或是选取 2k 条边作为稀有边).
在测试过程中,差异性较大的测试输入之间有着截然不同的执行覆盖路径,不同的测试输入执行了被测程
序不同的代码区域.如图 2 所示,对于一个被测程序,我们希望能够探测到的代码区域最大化.因此,对于表示被
测程序的高维空间(图 2 中以三维为例),我们希望通过执行多样化的测试输入使得测试能够更全面地覆盖整个
程序状态空间.因此,我们提出一种宏观直接的方法,根据种子执行覆盖的路径对种子进行聚类分析,有着相同
路径覆盖模式以及执行行为的种子被归为一类.不同类之间的种子所探索的区域可能大不相同,而同属于某一
类别的种子可能只会探索某处单一的代码区域,如图 2(a)中所表示的情况.对于种子数量较少的类,代表对应代
码区域被探索度较低,本质即所处代码区域中的边缘与分支被覆盖的次数较少(即为稀有边、稀有分支).通过对
这些覆盖罕见边以及分支的种子突变生成更多的随机输入,极大的增加了突破对应分支约束的概率,使得探测
由其保护的代码部分.因此,探索度越低的代码区域越稀有,对该类区域的探索更有可能发现新的边缘覆盖.通
过种子的分类信息对种子优先级评估,优先调度那些探索了稀有区域的种子,并为其分配更多的能量以提高模
糊测试的覆盖率.如果在种子调度过程中按照种子簇群为单位去调用种子,某类种子总优先于另外一类种子被
调度,这会导致后代种子的多样性下降,从而引起对程序的探索不均匀.我们根据分析的结果来平衡不同类别种
子的调度数量,确保后代种子的多样性.最终通过不断的突变和探测,使得测试能够执行程序的多样性路径,让
突变得到的输入更全面地覆盖整个程序空间,实现程序空间的均匀的探索(如图 2(c)所表示的情况),从而可能多
地检测出被测程序中的错误行为.

Fuzz

Fuzz

(a)
(b)
(c)
图 2: Cluzz 的动机和目标.图 2(a)展示了初始种子语料库对被测程序空间探索的状态分布簇,每个种子的执行状
态用一个坐标表示,同一簇中的种子探索程序空间的某一相同区域.图 2(b)展示了对程序空间的探索过程.图
2(c)展示了在理想情况下最终要达到均匀地探索整个目标程序空间的目标.
根据种子输入覆盖的特征与执行状态序列对种子采用聚类分析,可以帮助发现种子探测空间的结构和特

2209

张文 等: 一种聚类分析驱动种子调度的模糊测试方法

征.通过分析聚类结果,我们可以了解种子覆盖的分布情况,从而发现特定聚类中的种子群体可能表示某些特定
的测试场景或输入域.这些信息可以指导我们对模糊测试的改进和优化.得到的种子聚类结果中每个类别种子
之间彼此相似且具有共性,但与其他类别中的种子相比具有明显的区别.同一类别中的种子具有相似的输入模
式、路径覆盖模式以及执行行为,可以被看作是在特征空间中相互接近的种子集合,在某种程度上代表了程序
空间的特定区域与行为模式.如图 3 所示,给出一个程序的控制流图,分别使用输入 A、输入 B 以及输入 C 作为
该程序的测试输入,执行路径的覆盖情况用不同颜色表示.输入 A 的执行路径集合通过代码执行的基本块可以
表示 为 Path(A)={A,B,D,G},输 入 B 的执行 路径集合 为 Path(B)={A,C,F,J,K,N},输 入 C 的执行路径集 合为
Path(C)={A,B,D,H}.结合图 3 给出控制流程图可知,输入 A 与输入 C 的路径覆盖模式明显区别于输入 B, 输入 A、
C 与输入 B 探索了完全不同的程序区域,按照聚类的思想将输入 A 与 C 归为一类.在所有被覆盖到的边中,被命
中过两次的边为{AB,BD},仅命中一次的边为{DG,DH,AC,CF,FJ,JK,KN}.可以发现,所属类中种子输入数量较少
的输入 B,其覆盖了更多被命中次数较少的边,且输入 B 执行覆盖所临近的未执行边的数量更多.在模糊测试中,
优先调度这类种子并为其分配更多的能量产生更多的随机输入,更有可能得到产生新覆盖的输入.

(a)
(b)
(c)
图 3: 一个体现种子类别差异的例子.(a)、(b)和(c)是同一个程序的控制流程图,由不同输入执行的路径覆盖结
果.不同类别的种子输入的执行路径覆盖有着明显的差异.
基于这个问题,我们提出了一种基于聚类分析驱动的种子调度的模糊测试方法 Cluzz.我们将 Cluzz 设计为
一个轻量级的工具,可以集成到基准模糊器中以提高基准模糊器的测试效率与代码覆盖率.与其他模糊测试方
法相比,我们通过将测试输入的执行路径覆盖在程序空间中的分布情况进行聚类分析,以便根据种子状态分布
对种子调度以更好地探索整个程序空间的各个区域,进而实现测试探索路径的多样性.通过种子稀有度为种子
分配能量并随着模糊工作的进行对种子动态评估以更新种子调度的优先级,让模糊器优先调度具有高优先级
的种子,从而提高测试效果.
3.2 Cluzz的工作流程
图 4 描述了集成 Cluzz 方法的模糊器的整体工作流程.给定一个种子语料库和目标被测程序,我们首先执行
预演,并通过 AFL[5] 中的 AFL-showmap 工具来统计被测程序的路径执行情况,获取种子语料库中所有种子的执
行路径覆盖.然后,通过多重随机初始化的聚类分析,将种子划分为不同的执行状态分布簇.这些种子分布簇代
表种子在程序执行状态空间中的不同区域.
2

…

…

…

执行
收集

目标程序

多重随机初始化聚类分析

种子语料库

指
导

5

种子队列
种子优先级
评估

评估信息
3

能量分配

种子语料库
更新

指导

4

种子突变

执行结果
分析

突变新输入

输入执行

种子执行覆盖序列

图 4 集成 Cluzz 的模糊器工作流程

覆盖率报告

有趣输入

崩溃检测报告

Journal of Software 软件学报

2210

接着,利用执行状态分布簇的权重占比因子来指导种子的评估过程.具体的,通过计算种子的稀有度得分
(①),可以确定种子在探索程序空间中稀有代码区域的能力.这个稀有度得分将指导后续种子的调度与能量分
配(②).模糊器将优先考虑那些能够探索稀有代码区域且稀有度得分较高的种子,并分配更多的能量给它们,从
而让模糊测试可以更好地探索程序的稀有区域.在种子的调度过程中,我们选择具有高稀有度得分的种子,并对
它们进行突变(③).突变操作会生成新的输入,而我们只保留那些能够触发新的程序状态的突变输入,也就是具
有趣味性的新输入.为了评估这些新输入的质量,使用新输入与各个种子簇群之间的语义相似度来评估新输入
的所属集群(④).随着模糊测试的进行,新输入的加入使得种子簇群之间的权重发生变化,因而重新评估种子簇
群的优先级以动态调整测试策略,确保模糊测试的效果和效率并保证种子的样本多样性.同时,我们将这些新输
入保存到种子语料库中,并随着模糊工作的进行定期对种子语料库进行重新评估(⑤).这个阶段性的重新评估
将更新种子队列中种子的优先级,确保始终选择最有潜力的种子进行后续的突变和探索.通过上述优化过程,我
们能够在模糊测试中更好地利用种子的执行状态和特征信息,从而高效地探索程序的不同区域并提高测试覆
盖率.
通过种子执行序列进行聚类分析的详细过程如算法 1 所示.
算法 1. 种子的多重随机初始化聚类分析算法
输入: 种子语料库: S,目标程序: P;
变量: 聚类分析的目标数量: K,初始化的聚类中心集合: C,种子执行状态序列集合: M;
输出: 最优聚类结果: BestResult,种子簇群权重占比: FOP;
1.

InitWithEmptyNumpy(M);

2.

BestResult ← ∅, Best_Score ← +∞, FOP ← ∅;

3.
4.
5.
6.
7.
8.
9.
10.
11.

for s ∈ S do

// 种子执行信息采集

executionSeq ← GetCoverage(P,s);
M ←M ⋃ executionSeq;

end for

M ← DataProcess(M);

for c ∈ C do

// 数据预处理

// 执行聚类分析

cluster_result ← AssignCluster(M,c,K);

eval_score ← EvalClusterResult(cluster_result);

12.

Best_Score ← eval_score;

BestResult ← cluster_result;

end if

15.

end for

16.

FOP ← CalcWeightProportions(BestResult);

17.

// 评估聚类结果

if eval_score < Best_Score then

13.
14.

// 初始化参数

// 计算种子簇群权重信息

return BestResult, FOP

3.3 多重随机初始化聚类分析
在本节中,我们通过种子的执行状态序列对种子进行聚类分析.在聚类分析过程中,过高的数据维度往往稀
疏且难以处理.而种子执行状态序列中的许多状态可能是重复或者无关紧要的.这些无用信息会占据序列的大
部分空间,导致序列的稀疏性,而直接进行聚类分析可能会带来过高的计算成本且影响聚类的效果.因此,我们
只关注在训练数据中至少执行过一次的边.对于单个输入的执行状态序列通过公式(3)的形式进行表示.
execSeq ( seed ) =

( edge1 , 0 ) , ( edge2 ,1) , , ( edgek ,1) ,

(3)

其中,通过 0 或 1 来标识某条边是否被执行,0 表示未执行,1 表示被执行.不同种子输入的边缘覆盖会存在
偏差,即某输入的边缘覆盖只包含程序所有边的少部分标签.这一现象会导致训练数据中的一组标签之间存在

2211

张文 等: 一种聚类分析驱动种子调度的模糊测试方法

多重共线性,使得模型的损失难以收敛.为了解决这个问题,我们通过将所有输入都未覆盖的边信息进行消除,
只关注所有测试输入中至少执行过一次的边,从而可以减少数据中的冗余信息和噪声.例如,种子 a 和种子 b 在
某被测程序上的执行状态序列如公式(4)上半部所示,通过上述的预处理方式得到的目标执行序列如公式(4)的
下半部所示.
edgeid :

1

2

3

4

5

6

7

8

9

10

execSeq (a ) = 0 1 0 1 0 0 0 0 0 1
execSeq (b) = 0 1 1 0 0 0 1 0 0 0

(4)

⇓
execSeq (a ) = ( edge2 ,1) , ( edge3 ,0 ) , ( edge4 ,1) , ( edge7 ,0 ) , ( edge10 ,1)
execSeq (b) = ( edge2 ,1) , ( edge3 ,1) , ( edge4 ,0 ) , ( edge7 ,1) , ( edge10 ,0 )

其中,种子 a 与种子 b 至少执行过一次的边的集合为{edge2,edge3,edge4,edge7,edge10}.在减少数据中的冗余
信息和噪声后,数据更易于可视化和理解,同时保留有意义的特征.通过降维,我们可以更好地理解数据之间的
关系、发现数据的内在规律和模式,从而提升聚类分析的效果.
此外,为了获得更具有鲁棒性的聚类结果,我们采用多重随机初始化的迭代方法.每次迭代后计算聚类结果
的评价得分,并保存具有最优得分的聚类中心和标签以将其作为最终结果.这样做可以减少聚类结果受随机因
素影响的程度,确保得到更可靠的聚类结果.最后,基于聚类的结果计算每个种子的权重占比,这个指标将在后
续步骤中指导种子的评估、调度和能量分配.算法 1 提供了通过种子执行状态序列进行多重随机初始化聚类分
析的过程,接下来的讨论中将详细介绍每个步骤.
多随机初始化聚类分析.种子语料库中每个种子的执行序列都是由 0 和 1 组成的二进制序列,计算每个序
列与 K 个聚类中心之间的距离,并将该数据样本分配给距离最近的聚类中心所在的簇.算法 1 中的第 1 行至第 8
行对应数据预处理过程,第 10 行至第 17 行详细叙述了对数据进行聚类的过程.在对种子执行聚类分析的 C 次
迭代过程中,每次迭代都随机初始化聚类中心 c,并且保存每次聚类迭代的结果 cluster_result.在多次随机初始化
迭代过程中,对每次迭代得到的聚类结果进行评估以获得最优聚类结果.通过聚类损失作为衡量此次聚类质量
的指标,使用如公式(5)所示的误差平方和(Sum of Squared Errors,SSE)来计算.
SSE
=

∑
∑ i 11{ y = k}
=
k 1=
K

n

i

xi − µ k

2

(5)

其中,K 表示簇的数量,n 表示样本的总数量.xi 代表样本点 i,yi 表示样本点 i 所属的簇.μk 表示簇 k 的质
心.1{yi=k}是一个指示函数,如果样本点 i 属于簇 k 则取值为 1,否则取值为 0.公式的最后一部分表示样本点 i 到
簇 k 中心点的欧几里得距离的平方.通过多次聚类迭代优化该损失函数,使其达到最小,从而获取最优聚类结果.
计算种子簇群权重占比.对于聚类结果中的每一个种子簇,都为其计算一个权重.每个簇的权重可以帮助确
定簇中的种子所执行代码区域的稀有程度.例如图 5 所示,对一个程序输入域空间的探索.对于所有测试输入在
输入域中的分布情况(以二维形式表示),红色(即类别 0 的输入)所在的区域中测试输入数量最少,也是被探索最
不充分的区域.如果将更多的机会放到对类别 0 所属区域的探索,能够更加充分地穷尽所有输入组合,进而更有
机会触发程序的潜在崩溃.更准确的说,根据种子的执行状态序列对种子进行聚类分析,那些包含相对较少种子
的簇所对应的代码区域更为稀有,即探索度较低的区域.对从相关稀有区域采样的种子分配更高的执行优先级
与突变次数,产生执行了相关区域新的代码覆盖的有效测试用例的概率要大于在其余代码区域采样的种子.

Journal of Software 软件学报

2212

图 5: 随机生成的测试用例在被测程序输入域中的分布情况.
在模糊测试中,通过将更多的能量分配给这些簇中的种子,并优先考虑对稀有区域的探索,从而提高模糊测
试的效率和效果.因此,根据某个簇中种子数量与所有种子的数量比值的倒数来计算该种子簇群的权重,具体如
公式(6)所示.
fi = N

/ ni

(6)

其中,fi 表示的是第 i 个簇的权重占比,N 表示的是所有种子的数量,ni 表示被划分到第 i 个簇的种子数量.簇
中种子数量越少,该簇对应的权重就越大.占比因子较大的簇表明该簇所对应的代码区域更为稀有,且簇中的种
子更为稀有.我们是通过种子的执行状态序列得到聚类结果,所以簇中种子数量很少代表该类种子所探索的程
序空间被探索的很浅,因此需要给予这些种子更多的能量以让其能够更加充分地探索该代码区域.这样的计算
结果可以更直观地反映出簇内种子的相对稀缺程度,在后续种子的能量分配过程中能够更直接地通过权重占
比因子去衡量种子的稀有度,并进一步根据种子的稀有度得分进行能量分配.
3.4 种子调度与能量分配
通过种子聚类分析结果,对种子进行稀有度得分的计算与种子优先级的评估.我们的目标是通过探索稀有
代码区域以获得更高的测试覆盖率,并通过平衡对不同类别种子的调度来保持后续生成新的测试输入的多样
性与均衡性.
通过种子簇群信息对种子优先级评估的详细过程如算法 2 所示.
算法 2. 种子的优先级评估算法
输入: 种子语料库: S,种子聚类结果: R,种子簇群权重信息: FOP;
变量: 记录种子评估信息: V,已加入种子队列的种子集合: was_selected,种子评估循环检查变量: check_done;
输出: 种子优先级队列: Q;
1.
2.
3.

InitTheSeedQueue(Q);
V ← ∅, check_done = 0, was_selected ← ∅;

R ← GetSeedGroupPriority(R,FOP);

5.

for s ∈ S do

6.

end for

7.

while check_done == 0 do

4.

8.
9.
10.
11.

// 初始化参数和种子队列

// 评估种子簇群的优先级

// 评估种子稀有度得分

V[s] = GetRarityScore(FOP,s,R);
// 种子优先级评估

for r ∈ R do

seed ← ChoseBestSeed(r,V,was_selected);
if seed equals NULL then
continue;

12.

end if

13.

Q ← Q ⋃ seed;

2213

张文 等: 一种聚类分析驱动种子调度的模糊测试方法

14.

was_selected ← was_selected ⋃ seed;

15.

end for

16.

if was_selected equals S then

17.
18.

check_done = 1;
end if

19.

end while

20.

return Q

通过算法 2,我们首先对当前的种子语料库中的所有种子进行优先级评估,并生成一个种子优先级队列.在
模糊测试器的工作流程中,我们只需要按照评估的优先级顺序来调度种子.首先在算法 2 中,根据聚类分析的结
果和各个种子簇群的权重,结合种子执行信息计算种子的稀有度得分(如算法 2 中的 4-6 行),后续根据种子的稀
有度对种子进行优先级评估.种子簇群中越稀有的种子被调度的优先级越高.权重越大的种子簇群代表执行的
代码区域探索度越低,这类种子簇群中的种子越应该被优先调度并分配更多的能量.此外,为了确保对程序空间
进行均匀探索并获得路径探索的多样性,我们进行集群一致性检查.这意味着在每次调度种子时确保所选的种
子来自不同的种子簇群,并且所挑选的种子在之前的工作中没有被模糊过(如算法 2 的 7-19 行).重复这个过程,
直到种子语料库中所有种子都被评估并且加入队列.这样可以避免测试过程陷入程序空间的某个单一区域,保
证新生成的输入具有多样性.通过这种方式,能够同时实现对程序多样性路径的探测并保持生成输入的多样性.
种子稀有度得分的计算.种子稀有度得分通过聚类分析结果中各种子簇群的权重占比来计算.某组中的种
子数量越少,其权重占比越大,代表相应代码区域被探索得越浅(聚类分析的结果是根据种子执行状态序列得到
的).权重越大的种子簇群所探索的相应区域越稀有,被探索度越低,相应组中的种子应该被分配更多的能量.对
于所属同一组的种子,它们之间的表现也有差异.通常情况下,执行路径较长的种子有着更好的潜力.如图 3 中所
示的情况,使用程序控制流程图中基本块编号来表示一条路径,种子输入 A 和种子输入 B 的执行路径分别可以
表示为 Path(A)={AB,BD,DG}与 Path(B)={AC,CF,FJ,JK,KN}.种子输入 B 的执行路径更长,且与种子输入 B 执行
路径临近的未执行边的集合为{AB,CE,EI,JL,KM},而与种子输入 A 执行路径临近的未执行边的集合为{BE}.我
们发现,种子 B 执行路径上的未执行临近边数量更多,为种子 B 分配更多的能量,更有希望通过突变得到覆盖这
些未执行边的新输入,即发现程序新边缘的可能性更大.
因此,在计算的时候也将种子的执行路径长度作为衡量种子得分的指标.与执行路径临近的边比较容易通
过突变的方式到达.通过对一个执行了更多边的输入进行突变(未执行的临近边数量更多),更有可能探索程序
的未执行代码区域.稀有度得分计算如公式(7)所示.
(s)
Scorerarity=

fops
× [1 + β × (len path ( s ) Avglen )]
fopavg

(7)

其中,fops 表示种子 s 所在种子簇群的权重占比,fopavg 表示所有种子簇群权重的平均值.β 是路径长度的权
重系数,反映执行路径长度参数对种子得分的影响程度(在这里我们设置为 β =0.75,以避免能量溢出造成资源
的浪费),lenpath(s)表示种子 s 的执行路径长度.在公式(7)中,种子执行路径的长度与平均执行路径长度的比值越
大则得分越高,分数越高表示种子越稀有.通过为越稀有的种子分配越多的能量,能够鼓励模糊器探索那些稀有
的、不常见的路径,从而提高覆盖率.
种子能量的计算.种子的能量大小直接决定对一个种子的突变次数.为更有潜力的种子分配更多能量,更有
可能通过突变发现程序的潜在崩溃,从而提高测试覆盖率.我们通过上述步骤得到了种子的稀有度得分,并根据
种子优先级队列挑选优先级最高的种子进行模糊工作.在进行模糊之前,为当前选择的种子分配能量,以确定后
续种子突变的次数.我们为稀有度得分越高的种子分配更多的能量,根据 AFL[6] 中对能量的计算方式,我们对于
所选种子 q 的能量计算公式(8)如下.

((

E (q ) perf _ score ⋅ 1 + ω ⋅ S r (q) − Avg Sr
=


)

)

Avg Sr 


(8)

其中,perf_score 代表种子的表现得分,在 AFL 中种子的表现得分由执行速度与路径覆盖大小等因素决定,
最终通过表现得分计算得到种子的能量.在我们的方法中,我们并没有舍弃基准模糊器中为种子计算能量大小
的工作,而是将我们的工作作为一项新的指标,用于种子能量的计算.公式中的权重系数𝜔𝜔控制了对于当前种子

Journal of Software 软件学报

2214

稀有度得分和平均得分差异的关注程度(我们根据基准模糊器中计算能量得分的数量级,取值𝜔𝜔=1).当稀有度得
分高于平均值的种子时,权重系数𝜔𝜔所乘的项为正值,这使得高于平均稀有度得分的种子(稀有度更高)获得了更
多的能量,反之使得低于平均稀有度得分的种子获得较少的能量,相当于受到了一定的惩罚.
我们在算法 3 中给出了集成了 Cluzz 的模糊器工作流程.根据算法 2 得到种子的优先级队列后,在算法 3 中
根据评估得到的种子优先级队列 Q,选择下一个要模糊的种子(算法 3 第 11 行).根据种子的稀有度对其分配能
量(算法 3 第 12 行)以确保对稀有代码区域的深入探索.通过对当前种子进行突变生成新的输入,并对其进行评
估,筛选出有趣的输入并将其添加到种子语料库中(算法 3 第 14-15 行).同时,随着模糊工作进行,原本稀有度高
的种子被分配更多的能量,产生更多的新输入.我们通过计算新输入与各个种子簇群之间的语义相似度,对突变
得到的新输入进行归类.原本权重值高的种子簇群会因为更多的新种子加入,权重值会下降.为了平衡模糊过程
中新输入的多样性,模糊过程会重新评估各个种子簇群的权重,并且根据状态更新阈值来重新评估整个种子语
料库的种子优先级,以适应测试过程的变化.这一过程的执行频率由变量 execute_num 和 step_size 控制.
新输入的评估归类.随着模糊工作的不断进行,突变得到的新输入数量也会急剧增长,我们需要平衡各类种
子之间的数量平衡.被评估为稀有的种子会随着突变产生更多的同类新输入,从而变得不再稀有.然而,由于随
机突变的不确定性,突变得到的新输入可能从语义上面不同于父类种子所属的类别.其中,语义相似度描述了一
个种子输入的执行路径和某个种子簇群所覆盖的执行路径的相似性.该种子与某个种子簇群的执行路径越相
同,它们就越相似,将突变得到的新输入归入相似度最高的种子簇群中.相似度得分通过以下公式(9)来计算.
Scoersim ( s, R ) = Path( s )  Pathall ( R ) Pathall ( R )

(9)

其中,Pathall(R)表示种子簇群 R 中所有种子的执行路径的集合,Path(s)是得到的新输入 s 的执行路径集合.
例 如 , 有 一 个 新 输 入 S 与 一 个 种 子 簇 群 R, 他 们 的 执 行 路 径 集 合 分 别 为 Path(S)={A,B,C,D} 与
Pathall(R)={A,B,C,E,F,G,H,I}.我们发现,种子 S 和种子簇群 R 有着相同的执行路径{A,B,C},种子簇群的执行路径
集合的大小为 8,所以可计算得到种子 S 与种子簇群 R 的语义相似度得分为 0.375.
为避免过多聚类分析带来的时间开销,对于得到的新输入,暂时根据公式(9)评估其所属的种子簇群,用于该
阶段中后续对新种子的评估.在当前种子语料库中所有种子都被执行完一轮后进入下一测试阶段,重新调用聚
类分析评估所有种子的聚类结果,并根据新的权重与种子类别信息,重新对种子语料库中的所有种子进行优先
级的评估,从而指导后续测试工作.
配备 Cluzz 的模糊器的详细工作流程如算法 3 所示.
算法 3. 配备 Cluzz 的模糊器工作流程算法
输入: 种子语料库: S,目标程序: P,种子聚类结果: R,种子簇群权重信息: FOP;
变量: 已执行种子集合: was_fuzzed,种子优先级队列: Q,已执行种子数量: exe_num;状态更新阈值: step_size;
输出: 测试覆盖率报告,程序缺陷检测报告;
1.

InitTheThreshold(step_size);

2.

InitTheSeedQueue(Q);

3.

Was_fuzzed ← ∅, exe_num = 0;

4.
5.
6.
7.

Q ← EvalSeedPriority(S,R,FOP);
while fuzzer is running do

// 初始化参数和种子队列
// 基于算法 2

// 模糊循环

if exe_num equals step_size then

8.

Q ← EvalSeedPriority(S,R,FOP);

9.

step_size.update();

// 基于算法 2

exe_num.reset();

10.

end if

11.

seed, rarity_score ← GetTopSeed(Q,was_fuzzed);

12.

energy ← CalcEnergyScore(seed, rarity_score);

13.

new_seeds ← SeedMutation(seed,energy);

2215

张文 等: 一种聚类分析驱动种子调度的模糊测试方法

14.

eval_info ← EvalOffSpring(new_seeds);

15.

S.update(new_seeds,eval_info);

16.

R.update(new_seeds,eval_info);

17.

FOP.update(S,R);

18.

was_fuzzed ← was_fuzzed ⋃ seed;

19.

exe_num = exe_num + 1;

20.

end while

21.

return 覆盖率报告,崩溃检查报告

4 实验评估
在本节中,我们通过设计实验来证明 Cluzz 的性能,并通过实验结果来回答以下研究问题：
⦁RQ1:相比于基准模糊器,Cluzz 的集成能够使得模糊器在覆盖率方面带来多大程度的提升？
⦁RQ2:Cluzz 是否提高了模糊器检测独特崩溃(Unique Crash)的能力？

⦁RQ3:Cluzz 中对种子进行的聚类分析会造成的额外时间开销是多少？

⦁RQ4:相比其它有关种子调度及能量分配的模糊测试研究工作,Cluzz 的表现如何？

4.1 实验设计

为了评估我们的 Cluzz 方法的有效性,我们在三个当前最流行的模糊器上面实现了 Cluzz,并从发现新边缘
的能力与崩溃等方面将集成了 Cluzz 的模糊器与基准模糊器进行比较.具体来说,我们将每个模糊器在主流的 8
个目标程序上运行 24 小时,每个模糊器与对应的基准模糊器都在相同实验环境下进行试验,并在每个目标程序
上重复 5 次实验,并对 5 次实验结果求取平均值,与该研究领域的通行做法一致.此外,我们选择了 4 个主流的有
关种子调度与能量分配的模糊测试工具,验证我们 Cluzz 中种子调度与能量分配的研究工作的有效性.我们的
所有实验均在 Ubuntu 20.04 系统上进行,系统硬件环境为 Intel(R) Core(TM) i5-10400F CPU 和 16G 内存.
基准模糊器.如前面所述,我们选择了 3 个当前主流的模糊器来评估我们方法的通用性.其中 NEUZZ[13]利
用神经网络对输入字节和程序分支行为之间的关系进行建模,通过模型推断输入字节与约束条件关系来解析
约束.与传统模糊器运行不同,NEUZZ 是作为一个独立的进程运行,它与模糊器通信.NEUZZ 中对测试种子的选
择是随机的,将我们的聚类分析整合到模型训练之后根据分析的结果挑选 NEUZZ 模糊过程中使用的种子,并
随着 NEUZZ 的增量学习实现种子语料库的动态评估.AFL[5] 是最为经典的覆盖引导的模糊测试工具,目前有很
多模糊测试方法是在 AFL 基础之上进行改进的.此外,还选择 AFL 的扩展方法 MOPT[16],MOPT 在 AFL 基础上
采用粒子群优化(PSO)算法实现变异算子调度以实现更高的代码覆盖率.为了将我们的方法整合到 AFL 与
MOPT 中,我们将直接根据种子簇群的信息、种子稀有度得分来对种子进行优先级排序,并根据种子的稀有度
得分来计算种子的能量.
在 我 们 选 择 的 4 个 有 关 种 子 调 度 与 能 量 分 配 的 模 糊 器 中 ,Ecofuzz[19] 提 出 了 一 种 对 抗 性 多 臂 老 虎 机
(Multi-Armed Bandit,MAB)的变体,它对 AFL 的能量调度建模并阐述种子集的三种状态,并开发自适应调度算
法和基于概率的搜索策略.AFL++[4] 是一种结合现今所有基于 AFL 框架的改进方案,即取其精华去其糟粕形成
的一款测试工具,它在种子调度与能量分配中结合了 AFLFast[29] 的调度算法,并增加了最新种子用例的分数以
深入研究新发现的路径;在调度过程中忽略了种子的运行时间,将重点放在边缘很少被其他种子用例执行后覆
盖的种子上.Fairfuzz[20] 对测试中的稀有分支进行识别,优先调度能够命中稀有分支路径的种子,并使变异偏向
于产生覆盖稀有分支的输入.AFLFast 提出了一种使模糊测试中种子调度偏向于执行低频路径的种子策略,即
给予低频路径更多机会,以便在相同的模糊测试量下探索更多的路径.这四个模糊器都是在 AFL 基础上进行的
研究开发.为了对比 Cluzz 中种子调度与能量分配的有效性,我们将配备了 Cluzz 的 AFL 模糊器与之进行对比.
表 1 目标程序信息
目标程序

程序来源

测试输入格式

测试指令

Journal of Software 软件学报

2216

readelf -a @@1

readelf
objdump

objdump -D @@

nm

binutils-2.30

ELF

nm -C @@

size

size @@

strip

strip -o /dev/null @@

harfbuzz

harfbuzz-1.7.6

TTF

harfbuzz @@

djpeg

libjpeg-9c

JPEG

djpeg @@

libxml2-2.9.7

XML

xmllint @@

xmllint
1

@@: 为指令中的占位符,表示输入是文件.

目标程序.我们在 8 个不同的真实程序(如表 1 所示)上对 Cluzz 方法进行了评估.选用的被测程序来源于开
源的 GNU 编程语言工具程序以及其他开源工具程序.我们的被测程序与 NEUZZ 实验中的被测程序相同,并且
所有对比试验都 在相同环境 与相同的初始种 子下进行.其 中,readelf 用于显示 有 关 ELF 二进制文件 的信
息,objdump 用于查看目标文件或可执行文件的组成信息,nm 的功能为列出对象文件中的符号,size 的功能为列
出每个二进制文件的节大小,strip 用于去除程序文件中的调试信息,djpeg 是一种广泛使用的处理 JPEG 图像文
件的工具,harfbuzz 是一个开源的文本整形引擎,xmllint 是一个很方便的处理及验证 xml 的工具.
初始种子.为了对比实验的公平,所用的种子输入全部从 NEUZZ 和 MOPT 公布的种子集中收集得到,并且
集成 Cluzz 的模糊器与基准模糊器使用同样的初始种子去运行被测程序.NEUZZ 与其他的模糊器不同,因为它
需要初始数据集,因此直接选择使用 NEUZZ 中的公开数据集作为实验的初始种子输入,用于 Cluzz 在 NEUZZ
上面的对比试验.
4.2 RQ1:新边缘覆盖的探测能力
被测程序的代码覆盖率是模糊测试的一个重要指标.因为程序的错误是隐藏在代码中的,只有通过对各个
代码区域的探索才会触发潜在的程序崩溃.我们在 8 个被测程序上同时运行两个版本的模糊器(集成 Cluzz 的版
本和普通版本),并且每组对照模糊器在同一个被测程序上进行 5 次实验(每次实验为 24 小时).通过 AFL[5] 中的
AFL-showmap 工具来统计被测程序的代码覆盖情况,并通过绘制折线图反映实验中的各个模糊器在发现新路
径方面的平均性能.如图 6 所示,除了 harfbuzz 的结果之外,集成 Cluzz 的模糊器相比于基准模糊器,在所有目标
程序上都有着较为明显的增长趋势.
3500

1200

AFL
Cluzz(AFL)
MOPT
Cluzz(MOPT)
NEUZZ
Cluzz(NEUZZ)

3000

1600

新边缘覆盖

800

2000
1500

1200

800

400

1000

400

500
0

0

5

10

15

时间(小时)

20

0

25

0

(a) readelf 覆盖率情况变化图

10

15

时间(小时)

20

(d) size 覆盖率情况变化图

15

时间(小时)

20

0

25

0

25

5000

1500

2000

1000

10

15

时间(小时)

20

(e) strip 覆盖率情况变化图

20

25

3000

500

5

15

时间(小时)

4000

1000

0

10

AFL
Cluzz(AFL)
MOPT
Cluzz(MOPT)
NEUZZ
Cluzz(NEUZZ)

6000

2000

0

5

(c) nm 覆盖率情况变化图

新边缘覆盖

新边缘覆盖

新边缘覆盖

2500

400

5

10

AFL
Cluzz(AFL)
MOPT
Cluzz(MOPT)
NEUZZ
Cluzz(NEUZZ)

3000

800

0

5

(b) objdump 覆盖率情况变化图

AFL
Cluzz(AFL)
MOPT
Cluzz(MOPT)
NEUZZ
Cluzz(NEUZZ)

1200

0

AFL
Cluzz(AFL)
MOPT
Cluzz(MOPT)
NEUZZ
Cluzz(NEUZZ)

2000

新边缘覆盖

新边缘覆盖

2500

2400

AFL
Cluzz(AFL)
MOPT
Cluzz(MOPT)
NEUZZ
Cluzz(NEUZZ)

25

0

0

5

10

15

时间(小时)

20

25

(f) harfbuzz 覆盖率情况变化图

2217

张文 等: 一种聚类分析驱动种子调度的模糊测试方法

AFL
Cluzz(AFL)
MOPT
Cluzz(MOPT)
NEUZZ
Cluzz(NEUZZ)

500

0

0

5

AFL
Cluzz(AFL)
MOPT
Cluzz(MOPT)
NEUZZ
Cluzz(NEUZZ)

1000

新边缘覆盖

新边缘覆盖

1000

10

15

时间(小时)

20

500

0

25

(g) djpeg 覆盖率情况变化图

0

5

10

15

时间(小时)

20

25

(h) xmllint 覆盖率情况变化图

图 6 不同模糊器运行 24 小时的覆盖率情况变化图
通过观察,我们发现对于 MOPT 实验中新边缘覆盖的增长趋势并不平滑,相比于另外两组对比,MOPT 没有
一个很稳定的增长趋势.这是因为 MOPT 认为在对一个种子模糊的过程中,AFL 在确定性阶段(deterministic
stage)花费的时间比在浩劫(havoc)和拼接(splice)阶段花费的时间要多得多(MOPT 证明在这两个阶段可以发现
更多独特崩溃和路径).因此,MOPT 基于 AFL 的模糊器提供了一种优化,称为起搏器模糊模式(Pacemaker
Fuzzing Model),可以选择性地避免耗时的确定性阶段.具体来说,如果 MOPT 在很长一段时间内没有发现任何
新的唯一崩溃或路径,它将有选择地禁用后续测试用例的确定性阶段.所以,可以发现在我们图 6 所有 MOPT 的
实验中,在发现新边缘覆盖的趋势激增之前,都会有一段较长的时间新边缘覆盖趋势几乎没有波动.这是因为在
这段时间 MOPT 并没有发现新的边缘覆盖或崩溃,使得后续 MOPT 进入了起搏器模糊模式.
表 2 原始模糊器与配备 Cluzz 的模糊器在 24 小时发现的新边缘数量的对比
# 新边缘数量

目标程序

AFL

Cluzz(AFL)

MOPT

Cluzz(MOPT)

NEUZZ

readelf

926

1531(+65.33%↑)

1862

2110(+13.32%↑)

2839

3085(+8.67%↑)

objdump

713

865(+21.32%↑)

732

890(+21.58%↑)

975

1160(+18.97%↑)

nm
size

358
534

511(+32.73%↑)
679(+27.15%↑)

1904
647

2199(+15.49%↑)
970(+49.92%↑)

672
1165

1047(+55.80%↑)
1267(+8.76%↑)

773
2010

1014(+31.18%↑)
2372(+18.01%↑)

1402
4157

1684(+20.11%↑)
4012(-3.48%↓)

2465
5765

2824(+14.56%↑)
6091(+5.65%↑)

djpeg

296

328(+10.81%↑)

622

665(+6.91%↑)

843

1061(+25.86%↑)

xmllint

633

807(+27.49%↑)

1010

1171(+16.00↑)

940

1123(+19.47%↑)

strip
harfbuzz

Cluzz(NEUZZ)

如表 2 所示,配备了 Cluzz 的模糊器相比于基准模糊器在大多数的被测程序上都实现了更好的新边缘覆盖.
平均来说,Cluzz(AFL)在 8 个目标程序上发现的新边缘数量平均要比 AFL 多 29.25%.Cluzz(MOPT)在除 harfbuzz
之外的 7 个被测程序上都表现得比 MOPT 要好,并且在 8 个被测程序上发现新边缘的数量平均比 MOPT 高
17.48%.最后,Cluzz(NEUZZ)在 8 个被测程序上的表现都有提升,发现新边缘的数量平均比 NEUZZ 多 19.72%.
表 3 配备 Cluzz 的模糊器相较普通模糊器 24 小时发现的新边缘数量的提升
模糊器

Cluzz(AFL)

Cluzz(MOPT)

被测程序

Average

Min

Max

Median

CV

readelf

+65.33%

+57.22%

+83.10%

+62.16%

0.14

objdump

+21.32%

+12.28%

+36.70%

+24.15%

0.38

nm
size

+32.73%
+27.15%

+23.47%
+6.96%

+37.06%
+68.81%

+34.13%
+15.96%

0.15
0.82

strip
harfbuzz

+31.18%
+18.01%

+24.75%
+9.14%

+41.60%
+41.59%

+28.81%
+13.44%

0.19
0.66

djpeg

+10.81%

+7.57%

+13.38%

+11.27%

0.19

xmllint

+27.49%

+9.19%

+73.62%

+20.94%

0.85

readelf

+13.32%

+7.59%

+24.15%

+11.79%

0.46

objdump

+21.58%

+18.73%

+28.75%

+19.62%

0.18

Journal of Software 软件学报

2218
nm

+15.49%

+5.49%

+37.18%

+11.35%

0.73

size
strip

+49.92%
+20.11%

-1.21%
+6.39%

+89.13%
+35.67%

+77.67%
+15.91%

0.79
0.54

-3.48%
+6.91%

-12.79%
-5.53%

+3.40%
+28.08%

-1.48%
+4.53%

1.61
1.65

harfbuzz
djpeg

Cluzz(NEUZZ)

xmllint

+16.00%

-3.48%

+98.81%

+4.58%

1.12

readelf
objdump

+8.67%
+18.97%

+5.54%
+16.51%

+11.73%
+24.48%

+8.69%
+17.19%

0.24
0.16

nm

+55.80%

+7.50%

+101.61%

+44.79%

0.64

size
strip

+8.76%
+14.56%

+0.13%
-3.62%

+15.82%
+44.58%

+8.19%
+7.19%

0.60
1.13

harfbuzz
djpeg

+5.65%
+25.86%

+1.19%
+4.49%

+10.71%
+63.29%

+5.57%
+20.53%

0.56
0.76

xmllint

+19.47%

+15.69%

+21.38%

+20.67%

0.11

此外,我们通过对每组对比实验的结果进行计算统计形成表 3 中的内容.具体来说,我们将集成 Cluzz 的模
糊器与基准模糊器的 8 组对比实验的结果进行统计,统计实验结果提升率的最小值、最大值以及中位数,并计
算新边缘数量提升的平均大小.此外,还根据统计的实验结果计算离散系数(coefficient of variation,CV)来度量
每组实验结果之间的离散程度.离散系数越大,说明每组实验结果之间的离散程度也大,代表模糊器的运行结果
越不稳定,离散系数越接近 0 则说明离散程度越小.其中,集成 Cluzz 的模糊器在大多数的被测程序上面,每一次
的实验结果都比基准模糊器有着提升.且 Cluzz(AFL)在 8 个被测程序上的发现新边缘数量提升的平均离散系
数为 0.42,比平均离散系数分别为 0.89 和 0.53 的 Cluzz(MOPT)与 Cluzz(NEUZZ)稳定性要好.表 3 中各项数据
的统计以及离散系数表明 Cluzz 方法的有效性以及实验结果的稳定性(离散系数少有大于 1 的情况).
综上实验结果回答 RQ1.配备 Cluzz 的模糊器相较于基准模糊器可以显著提高测试覆盖的发现(平均增加
22.15%的新边缘覆盖).在 24 组对比试验中,有 23 组配备了 Cluzz 的模糊器比普通的模糊器实现了更多的代码
覆盖率.并且,在给出的新边覆盖数量随时间变化对比图中,配备了 Cluzz 的模糊器普遍能够保持优势直到测试
结束.通过对所有实验进行统计发现新边缘数量的提升,能够说明配备 Cluzz 的模糊器几乎在所有实验中都优
于普通模糊器,Cluzz 在发现新边缘数量方面有着显著的提升.
4.3 RQ2:崩溃检测能力
程序崩溃,是指程序在执行过程中发生了无法处理的错误或异常,导致程序无法正常继续执行并终止运行.
崩溃检测能力是衡量测试能效的重要指标.根据测试过程中触发的崩溃数量,我们将集成 Cluzz 的模糊器与普
通模糊器进行比较.如表 4 所示,我们在每个程序上进行 5 次实验,所有的数据都是根据实验结果计算得到的平
均值.
表 4 配备 Cluzz 的模糊器相较普通模糊器 24 小时内检测独特崩溃数量的对比
目标程序

readelf
objdump
nm
size
strip

# 独特崩溃数量
AFL

Cluzz(AFL)

MOPT

Cluzz(MOPT)

NEUZZ

Cluzz(NEUZZ)

20.6
0

44
0

27.2
0

42.4
0

32
0

41.6
2

0

0.6

1

3.8

0

1

0.6
0

2.6
0

0.6
0.4

3.6
1.4

0.6
0

2
0

其中,在被测程序 readelf、size、nm 上面,三个集成 Cluzz 的模糊器都要比对应的基准模糊器检测到更多的
崩溃.对于被测程序 objdump,只有 Cluzz(NEUZZ)发现了崩溃,其余的模糊器都没有发现额外的崩溃.对于被测
程序 strip,模糊器 AFL 与 NEUZZ 对应的实验都没有发现额外崩溃,Cluzz(MOPT)相比于 MOPT 在 24 小时内多
发现额外崩溃.此外,对于实验中没有在表 4 中出现的三个被测程序 harfbuzz、jpeg 与 xmllint,在我们实验中的
所有模糊器上都没有检测出崩溃.
综上实验结果回答 RQ2.集成了 Cluzz 的模糊器在暴露程序崩溃方面的能力要优于普通的模糊器,并且在

2219

张文 等: 一种聚类分析驱动种子调度的模糊测试方法

能够发现崩溃的 11 组实验中,配备了 Cluzz 的模糊器全都比普通模糊器检测出更多的崩溃数量.而且,所有实验
在发现独特崩溃的数量方面,配备了 Cluzz 的模糊器检测数量平均是普通模糊器的 1.7 倍.实验结果能够证明提
出的 Cluzz 方法在改善模糊器检测独特崩溃的能力方面有着显著的提升.
4.4 RQ3:Cluzz中有关种子聚类的开销
在这个实验中,我们统计将 Cluzz 集成到模糊器中所造成的额外时间开销.额外的运行开销是模糊器运行
期间对种子进行聚类分析开销.为了测量这些开销,我们针对集成了 Cluzz 的模糊器进行记录,记录使用这些模
糊器运行被测程序 24 小时所有有关种子聚类总的时间开销.我们在表 5 中给出了配备了 Cluzz 的模糊器在各
个被测程序上运行过程中,由种子聚类分析所造成的额外时间开销,以及这些时间开销相较于整个运行周期(24
小时)的占比大小.
通过表 5 中的统计可以发现,Cluzz(AFL)在 8 个被测程序上的平均额外开销为 0.75%,Cluzz(MOPT)的平均
额外开销为 1.25%,Cluzz(NEUZZ)的平均额外开销为 1.42%.同一个模糊器在不同被测程序上的额外开销大不
相同,最直接的原因是被测程序的差异性所造成.不同被测程序之间的功能差异以及代码复杂度各不相同,导致
相同时间内的同一个模糊器在不同被测程序上发现新边缘的能力不同,所生成的有趣测试用例数量也不同.间
接导致种子语料库中种子数量的差异,使得模糊器在种子聚类分析上面的时间开销不同.尽管种子聚类分析带
来了额外的时间开销,但是相较于整个测试周期(24 小时),额外开销的占比很小,几乎对测试结果没有影响.
表 5 配备 Cluzz 的模糊器在 24 小时中额外时间开销
目标程序

Cluzz(AFL)

Cluzz(MOPT)

Cluzz(NEUZZ)

时间(s)

开销占比

时间(s)

开销占比

时间(s)

readelf

1056.22

1.22%

1399.21

1.62%

1651.55

开销占比
1.91%

objdump
nm

530.64
371.79

0.61%
0.43%

549.26
1479.28

0.64%
1.71%

622.43
913.15

0.72%
1.06%

size

481.26

0.56%

967.18

1.12%

1217.49

1.41%

strip
harfbuzz

852.59
1177.63

0.99%
1.36%

1106.28
1896.73

1.28%
2.20%

1617.29
2249.17

1.87%
2.60%

djpeg
xmllint

220.64
511.29

0.26%
0.59%

417.21
824.16

0.48%
0.95%

726.31
779.64

0.84%
0.90%

Average

650.26(s)

0.75%

1079.91(s)

1.25%

1222.13(s)

1.42%

综上实验结果回答 RQ3.集成 Cluzz 的基准模糊器相较于对应的普通模糊器,在实验中最多增加 2.6%的时
间开销,最小只有 0.26%,所有的实验平均只增加了 1.14%的额外时间开销.这一开销相较于我们整个的测试实
验周期 24 小时是非常少的.所以,可以证明我们方法能够通过少量的时间开销换取更好的测试效果.综合,融入
Cluzz 并未对测试工作造成负面的影响.
4.5 RQ4:Cluzz中种子调度与能量分配策略的性能
在 该 实 验 中 ,我 们 选 取 了 四 个 当 前 先 进 的 模 糊 测 试 工 具 ,分 别 为 EcoFuzz[19],AFL++[4],AFLFast[29] 以 及
FairFuzz[20].这四个模糊测试工具均在 AFL 的基础上展开了与种子调度以及能量分配有关的研究工作,这与我
们提出的研究工作方向相同.为了验证提出的 Cluzz 在种子调度与能量分配工作的优劣,我们将配备了 Cluzz 的
AFL 模糊器与这四个模糊器设计实验进行对比.我们对 GNU 的 5 个编程语言工具进行测试,并统计在每个被测
程序上测试运行 24 小时后各个模糊器发现新边缘的数量和独特崩溃的数量.在模糊测试中,能量分配阶段所计
算的能量大小代表了种子在浩劫(havoc)和拼接(splice)阶段的突变次数.为了体现各个模糊器在能量分配阶段
的优劣,我们选择在对所有被测程序进行测试时使所有模糊器跳过种子突变的确定性阶段(deterministic stage).
我们在相同实验环境下,分别使用 Cluzz(AFL)以及所选取的四个模糊器对一个被测程序同时开展测试工作，并
对同一个被测程序开展 4 轮实验,最后统计实验结果的平均值,形成表 6 中的实验数据.
表 6 有关种子调度与能量分配的模糊器与 Cluzz(AFL)在 24 小时中发现新边缘数量与独特崩溃数量的对比
目标程序

EcoFuzz

AFL++

AFLFast

Fairfuzz

Cluzz(AFL)

Journal of Software 软件学报

2220
新边缘
数量

独特崩溃
数量

新边缘
数量

独特崩溃
数量

新边缘
数量

独特崩溃
数量

新边缘
数量

独特崩溃
数量

新边缘
数量

独特崩溃
数量

readelf

2526

90

2843

74.5

2673

92

2444

48

2909

107

objdump

1596

2.5

1135

0

1641

0

1278

2

1658

5

nm

1282

5

1226

0

1513

5

762

0

1590

8

size

1046

3

569

0

981

5.5

1037

3

1034

8

strip

1883

4

969

0

1760

1

1884

0

1774

4

Total

8333

104.5

6742

74.5

8568

103.5

7405

53

8965

132

通过表 6 中的统计可以发现,对于被测程序 readelf,objdump 以及 nm,Cluzz(AFL)在发现新边缘数量以及独
特崩溃数量方面的表现皆优于其它四个模糊器.而对于被测程序 size 和 strip,Cluzz(AFL)在发现独特崩溃的数
量方面优于其它四个模糊器,在发现新边缘数量方面与表现最好的模糊器相差无几.总的来说,在五个被测程序
上,Cluzz(AFL)在发现总的新边缘数量方面相比 EcoFuzz,AFL++,AFLFast 以及 Fairfuzz 分别提升了 7.58%、
32.97%、4.63%以及 21.07%,在发现总的独特崩溃数量方面优于其它四个模糊器.
综上实验结果回答 RQ4.集成 Cluzz 的 AFL 模糊器 Cluzz(AFL)相较于其它四个基于 AFL 的模糊器,其在五
个被测程序上的综合表现最好.在发现新边缘数量以及总的独特崩溃数量方面皆优于其他模糊器,并且测试效
果有着显著的提升,这也可以说明 Cluzz 在种子调度与能量分配方面的有效性.相较于现有的种子调度与能量
分配研究工作,Cluzz 有着更优秀的表现.

5 有效性威胁
内部有效性.对内部效度的一个威胁在于我们评估阶段中所选取的对比模糊器的实施.为了减少这种威胁,
我们在实验阶段中重用了它们论文公开的原始源代码.此外,还由各位作者手动仔细检查了所有代码,以确保对
比模糊器中代码实现的正确性,以及与其对应的研究内容方法的功能一致性.并且,所有的模糊器在运行时采用
相同的运行命令参数、相同的初始测试输入以及测试命令.
外部有效性.外部效度的威胁主要存在于测试对象和基准上.为了减少外部威胁,我们在验证 Cluzz 有效性
的实验中选择了 3 个具有代表性的、最先进的模糊测试器,包括基线模糊器 AFL[5] 、基于 AFL 的拓展模糊器
MOPT[16]和基于神经程序平滑的模糊器 NEUZZ[13].在验证 Cluzz 相较其它种子调度研究工作有效性的实验中,
我们选择了 4 个先进且具有代表性的模糊测试器[4,19,20,29],该 4 个模糊器均在 AFL 的基础上展开有关种子调度
的研究工作,所以在我们的对比实验中,我们将这 4 个模糊器与配备了 Cluzz 的 AFL 模糊器进行对比.此外,根据
所研究的模糊测试领域相关原始论文,选择了在实验评估中使用频率最高的 8 个主流被测程序(具体信息如表 1
内容所示).另一个威胁外部有效性的因素是评估结果的随机性.为了减少这种威胁,我们在 RQ1-RQ3 的实验中,
对所有评估结果进行五次运行并取平均值.在 RQ4 的实验中,我们对所有实验运行四次并计算平均值,以减少随
机性对实验评估的影响.
结构有效性.对构造有效性的威胁主要在于本文评估阶段使用的主要度量指标,即反映代码覆盖率的边缘
覆 盖 数 量 .为 了 减 少 这 种 威 胁 ,虽 然 可 以 有 各 种 方 法 来 测 量 边 缘 覆 盖 ,但 我 们 选 择 遵 循 许 多 现 有 的 模 糊 器
[6,13,14,20]

所使用的方法,通过 AFL 内置的名为 AFL-showmap 的工具来收集边缘覆盖.此外,我们还根据实验发现

的独特崩溃数量评估了 Cluzz 模糊测试方法的有效性.

6 相关工作
6.1 聚类分析
聚类分析,也被称为簇分析,是数据挖掘和探索性数据分析中的基本技术之一.通过将相似的对象归为一组
来识别数据集中的内在模式和结构.多年来,研究人员已经进行了大量的研究,提出了很多种聚类方法,其中一
种常见的聚类方法是 K-means 聚类算法[23].该算法将数据集分为 K 个簇，每个簇由与之最相似的数据点组成,
通过迭代优化簇的中心点来最小化数据点与其所属簇中心点之间的距离,从而得到聚类结果.层次聚类算法 [25]

张文 等: 一种聚类分析驱动种子调度的模糊测试方法

2221

通过不断地将最相似的对象归并为一组来构建一个层次结构,层次聚类可以根据簇之间的相似度进行凝聚聚
类(自底向上)或分裂聚类(自顶向下).DBSCAN 聚类算法 [24] 是一种基于密度的聚类方法,它将密度较高的数据
点归为一组,而较低密度的数据点被视为噪声.DBSCAN 算法可以有效地处理具有不规则形状的簇,并对噪声数
据具有一定的鲁棒性.这些聚类分析方法已经被广泛应用于自然语言处理、图像处理等各个领域.目前,在模糊
测试领域中有根据种子相似度对种子归类的方法,通过自适应的方式将种子归类,并为每个类的种子维护一组
突变算子的概率分布,来挑选种子最优的突变策略.但是并没有通过聚类分析指导种子调度的方法.
6.2 模糊测试
目前为止,已经提 出很多用 于 改进模糊测试 的技术,如符 号 执行 [31,32] 、动态 污染分析 [33,34] 和机器学习
[11,13,14]

.这些方法通过覆盖率来引导种子变异,确定应该修改测试输入中的哪一部分,能够提高代码覆盖率.一些

工作侧重于定位那些用于安全操作的字节.如 TaintScope[35] 和 BuzzFuzz[36] 使用污染跟踪技术来识别输入中对
于系统安全指令或库调用的输入字节,然后专注于修改这些字节.Angora[6] 侧重于引导模糊测试去解决程序分
支约束来扩大测试的分支覆盖.它利用字节级污染分析来定位流向分支的字节.然后,通过改变这些字节来绕过
约束条件.在本文中,我们的主要目标是优化模糊测试中的种子调度阶段.通过对种子聚类分析结果指导测试输
入 的 优 先 级 和 对 种 子 能 量 分 配 的 大 小 .现 在 已 有 的 优 化 的 模 糊 测 试 方 法 ,通 常 是 基 于 历 史 边 缘 覆 盖 情 况
[19,20,27,29]

或者其他反馈指标指导种子调度,例如执行时间 [37,38].也有通过内存访问情况 [39] 指导种子能力调度.如

K-Scheduler[21] 基于运行过程间的程序控制流程图,构建包含种子节点的边缘水平图,通过计算种子节点的 Katz
中心性对种子进行优先排序.但是,对于这些运行过程数据的获取以及指标的评价通常需要复杂的计算过程.相
反,我们通过分析种子在程序空间中的执行状态分布,指导种子调度与能量分配,这一个过程是轻量级的,时间
和空间上的开销极小.根据我们的初步实验表明,Cluzz 对种子调度的优化可以改善模糊测试效果.

7 总

结

在本文中,我们提出了一种基于聚类分析驱动的种子调度的模糊测试方法.通过分析种子在特征空间上的
区别,结合种子执行覆盖状态在程序空间中的分布情况对种子进行分类.根据不同类别种子所探索的程序空间
不同以及不同种子簇群的路径覆盖模式与聚类分析结果,对种子进行优先级评估,优先调度探索稀有区域的种
子.通过种子簇群的信息与种子执行信息为种子计算所分配的能量大小.并且,通过种子聚类结果平衡种子调度
过程中各个类别种子的数量,从而确保后代种子的多样性与平衡性,避免测试在某个代码区域消耗过多的测试
资源,实现对程序空间均衡性的探索.
在我们的方法中,我们按照种子执行路径进行聚类分析并对种子进行划分.为执行了对应探索度较低区域
的种子分配更多的能量,这些种子覆盖到了更多的稀有边以及稀有分支,在其基础之上突变生成更多的随机输
入,极大地提高了突破对应代码区域中分支约束的概率,从而实现对程序代码空间更深入的探索.此外,通过模
糊过程中动态重新评估种子优先级,确保有趣的种子总是能被优先调度,从而提高模糊测试效果.根据本文叙述
的方法实现了 Cluzz,为了验证所提出的 Cluzz 方法的有效性,将 Cluzz 集成到 3 个现有的模糊测试工具并设计
实验对真实世界的 8 个开源程序进行实验评估.评估表明配备了 Cluzz 的模糊器的在新边缘的发现方面与崩溃
检测能力方面明显优于普通模糊器.此外,通过与现有的种子调度与能量分配研究工作进行对比,进一步证明了
Cluzz 在该方面工作的有效性.
虽然提出的 Cluzz 方法取得了较好的实验效果,但也存在一些不足需要在今后的工作中加以克服.首先,需
要对种子聚类所依赖的指标加以完善,充分依据程序信息与种子执行覆盖的情况.这可以通过程序分析技术与
静态分析方法来完成,将种子聚类做得更加完善以提高指导种子调度的有效性.其次,将新的字节推断与符号执
行,以及学习算法等指导突变的方法与我们的工作相结合,进一步提高测试覆盖与漏洞检测的能力.
References:
[1]

Aschermann C, Schumilo S, Blazytko T, Gawlik R, Holz T. REDQUEEN: Fuzzing with Input-to-State Correspondence. In: Proc. of
the Network and Distributed System Security Symp. (NDSS). 2019. [DOI: 10.14722/ndss.2019.23xxx]

2222
[2]

Journal of Software 软件学报

Zheng Y, Davanian A, Yin H, Song C, Zhu H, Sun L. FIRM-AFL: High-Throughput Greybox Fuzzing of IoT Firmware via
Augmented Process Emulation. In: Proc. of the 28th USENIX Security Symp. (USENIX Security 19). 2019. 1099-1114.

[3]

Schumilo S, Aschermann C, Gawlik R, Schinzel S, Holz T. kAFL: Hardware-Assisted Feedback Fuzzing for OS Kernels. In: Proc.
of the 26th USENIX Security Symp. (USENIX Security 17). 2017. 167-182.

[4]

Fioraldi A, Maier D, Eißfeldt H, Heuse M. AFL++: Combining incremental steps of fuzzing research. In: Proc. of the 14th USENIX
Workshop on Offensive Technologies. (WOOT 20). 2020. 1-12.

[5]
[6]

Michał Zalewski. 2021. AFL (american fuzzy lop). https://github.com/google/AFL accessed 21-January-2021.
Chen P, Chen H. Angora: Efficient fuzzing by principled search. In: Proc. of the IEEE Symp. on Security and Privacy (SP). IEEE,
2018. 711-725.

[7]

Rawat S, Jain V, Kumar A, Cojocar L, Giuffrida C, Bos H. Vuzzer: Application-aware evolutionary fuzzing. In: Proc. of the
Network and Distributed System Security Symp. (NDSS). 2017. [DOI: 10.14722/ndss.2017.23404]

[8]

libFuzzer – a library for coverage-guided fuzz testing. https://llvm.org/docs/LibFuzzer.html, 2021.

[9]

Zhao L, Duan Y, Yin H, Xuan J. Send hardest problems my way: Probabilistic path prioritization for hybrid fuzzing. In: Proc. of

[10]

Herrera A, Gunadi H, Magrath S, Norrish M, Payer M, Hosking AL. Seed selection for successful fuzzing. In: Proc. of the 30th

the Network and Distributed System Security Symp. (NDSS). 2019. [DOI: 10.14722/ndss.2019.23504]
ACM SIGSOFT Int’l Symp. on Software Testing and Analysis (ISSTA). ACM, 2021. 230-243.
[11]

Godefroid P, Peleg H, Singh R. Learn&fuzz: Machine learning for input fuzzing. In: Proc. of the 32nd IEEE/ACM Int’l Conf. on
Automated Software Engineering (ASE). IEEE, 2017. 50-59.

[12]

Zong P, Lv T, Wang D, Deng Z, Liang R, Chen K. Fuzzguard: Filtering out unreachable inputs in directed grey-box fuzzing
through deep learning. In: Proc. of the 29th USENIX Security Symp. (USENIX security 20). 2020. 2255-2269.

[13]

She D, Pei K, Epstein D, Yang J, Ray B, Jana S. Neuzz: Efficient fuzzing with neural program smoothing. In: Proc. of the IEEE
Symp. on Security and Privacy (SP). IEEE, 2019. 803-817.

[14]

She D, Krishna R, Yan L, Jana S, Ray B. MTFuzz: fuzzing with a multi-task neural network. In: Proc. of the 28th ACM Joint
Meeting. on European Software Engineering Conference and Symposium on the Foundations of Software Engineering (ESEC/FSE).
ACM, 2020. 737-749.

[15]

Gao FJ, Wang Y, Situ LY, Wang LZ. Deep learning-based hybrid fuzz testing. Ruan Jian Xue Bao/Journal of Software,
2021,32(4):988-1005.

[16]

Lyu C, Ji S, Zhang C, Li Y, Lee WH, Song Y, Beyah R. MOPT: Optimized Mutation Scheduling for Fuzzers. In: Proc. of the 28th

[17]

Jauernig P, Jakobovic D, Picek S, Stapf E, Sadeghi AR. DARWIN: Survival of the Fittest Fuzzing Mutators. In: Proc. of the

USENIX Security Symp. (USENIX security 19). 2019. 1949-1966.
Network and Distributed System Security Symp. (NDSS). 2023. [DOI: 10.14722/ndss.2023.23159]
[18]

Peng H, Shoshitaishvili Y, Payer M. T-Fuzz: fuzzing by program transformation. In: Proc. of the IEEE Symp. on Security and
Privacy (SP). IEEE, 2018. 697-710.

[19]

Yue T, Wang P, Tang Y, Wang E, Yu B, Lu K, Zhou X. Ecofuzz: Adaptive energy-saving greybox fuzzing as a variant of the
adversarial multi-armed bandit. In: Proc. of the 29th USENIX Security Symp. (USENIX security 20). 2020. 2307-2324.

[20]

Lemieux C, Sen K. Fairfuzz: A targeted mutation strategy for increasing greybox fuzz testing coverage. In: Proc. of the 33rd
ACM/IEEE Int’l Conf. on Automated Software Engineering (ASE). ACM, 2018. 475-485.

[21]

She D, Shah A, Jana S. Effective seed scheduling for fuzzing with graph centrality analysis. In: Proc. of the IEEE Symp. on
Security and Privacy (SP). IEEE, 2022. 2194-2211.

[22]

Arthur D, Vassilvitskii S. K-means++: the advantages of careful seeding. In: Proc. of the eighteenth annual ACM-SIAM Symp. on
Discrete algorithms (SODA). 2007. 1027-1035.

[23]

Hartigan J A, Wong M A. Algorithm AS 136: A k-means clustering algorithm. Journal of the royal statistical society. series c
(applied statistics), 1979, 28(1): 100-108. [DOI: 10.2307/2346830]

[24]

Ester M, Kriegel H P, Sander J, et al. A density-based algorithm for discovering clusters in large spatial databases with noise. kdd.
1996, 96(34): 226-231.

[25]

Ward Jr J H. Hierarchical grouping to optimize an objective function. Journal of the American statistical association, 1963, 58(301):
236-244.

张文 等: 一种聚类分析驱动种子调度的模糊测试方法

[26]

2223

Lyu C, Liang H, Ji S, Zhang X, Zhao B, Han M, Li Y, Wang Z, Wang W, Beyah R. SLIME: program-sensitive energy allocation
for fuzzing. In: Proc. of the 31st ACM SIGSOFT Int’l Symp. on Software Testing and Analysis (ISSTA). ACM, 2022. 365-377.

[27]

Böhme M, Manès V J M, Cha SK. Boosting fuzzer efficiency: An information theoretic perspective. In: Proc. of the 28th ACM
Joint Meeting. on European Software Engineering Conference and Symposium on the Foundations of Software Engineering
(ESEC/FSE). ACM, 2020. 678-689.

[28]

Chen J, Wang S, Cai S, Zhang C, Chen H, Chen J, Zhang J. A Novel Coverage-guided Greybox Fuzzing based on Power Schedule
Optimization with Time Complexity. In: Proc. of the 37th IEEE/ACM Int’l Conf. on Automated Software Engineering (ASE).
ACM, 2022. 1-5.

[29]

Böhme M, Pham VT, Roychoudhury A. Coverage-based greybox fuzzing as markov chain. In: Proc. of the 2016 ACM SIGSAC
Conf. on Computer and Communications Security (CCS). ACM, 2016. 1032-1043.

[30]

Zhang K, Xiao X, Zhu X, Sun R, Xue M, Wen S. Path transitions tell more: Optimizing fuzzing schedules via runtime program
states. In: Proc. of the 44th Int’l Conf. on Software Engineering (ICSE). ACM, 2022. 1658-1668.

[31]

Yun I, Lee S, Xu M, Jang Y, Kim T. QSYM: A practical concolic execution engine tailored for hybrid fuzzing. In: Proc. of the 27th
USENIX Security Symp. (USENIX Security 18). 2018. 745-761.

[32]

Xie XF, Li XH, Chen X, Meng GZ, Liu Y. Hybrid testing based on symbolic execution and fuzzing. Ruan Jian Xue Bao/Journal of
Software, 2019,30(10):3071-3089.

[33]

Gan S, Zhang C, Chen P, Zhao B, Qin X, Wu D, Chen Z. GREYONE: Data flow sensitive fuzzing. In: Proc. of the 29th USENIX
Security Symp. (USENIX Security 20). 2020. 2577-2594.

[34]

Liang J, Wang M, Zhou C, Wu Z, Jiang Y, Liu J, Liu Z, Sun J. Pata: Fuzzing with path aware taint analysis. In: Proc. of the IEEE
Symp. on Security and Privacy (SP). IEEE, 2022. 1-17.

[35]

Wang T, Wei T, Gu G, Zou W. TaintScope: A checksum-aware directed fuzzing tool for automatic software vulnerability detection.
In: Proc. of the IEEE Symp. on Security and Privacy (SP). IEEE, 2010. 497-512.

[36]

Ganesh V, Leek T, Rinard M. Taint-based directed whitebox fuzzing. In: Proc. of the IEEE 31st Int’l Conf. on Software
Engineering (ICSE). IEEE, 2009. 474-484.

[37]

Lemieux C, Padhye R, Sen K, Song D. Perffuzz: Automatically generating pathological inputs. In: Proc. of the 27th ACM

[38]

Petsios T, Zhao J, Keromytis A D, Jana S. Slowfuzz: Automated domain-independent detection of algorithmic complexity

SIGSOFT Int’l Symp. on Software Testing and Analysis (ISSTA). ACM, 2018. 254-265.
vulnerabilities. In: Proc. of the 2017 ACM SIGSAC Conf. on Computer and Communications Security (CCS). ACM, 2017.
2155-2168.
[39]

Coppik N, Schwahn O, Suri N. Memfuzz: Using memory accesses to guide fuzzing. In: Proc. of the 12th IEEE Conf. on Software
Testing, Validation and Verification (ICST). IEEE, 2019. 48-58.

附中文参考文献:
[15] 高 凤 娟 , 王 豫 , 司 徒 凌 云 , 王 林 章 . 基 于 深 度 学 习 的 混 合 模 糊 测 试 方 法 . 软 件 学 报 ,2021,32(4):988-1005. [DOI:
10.13328/j.cnki.jos.006225]
[32] 谢 肖飞 ,李 晓 红 , 陈 翔 ,孟 国 柱 ,刘 杨 .基 于 符 号 执 行 与 模 糊 测 试 的混 合 测 试 方 法 . 软 件 学 报 ,2019,30(10): 3071-3089. [DOI:
10.13328/j.cnki.jos.005789]


软件学报 ISSN 1000-9825, CODEN RUXUEW
Journal of Software, [doi: 10.13328/j.cnki.jos.007123]
©中国科学院软件研究所版权所有.

E-mail: jos@iscas. ac. cn
http://www. jos. org. cn
Tel: +86-10-62562563

一种面向图神经网络安卓恶意代码检测的通用解释定位方法

∗

郭燕慧 1, 王东 1, 王晓煊 1, 王柳 2, 徐国胜 1
1

北京邮电大学 网络空间安全学院, 北京

2

北京邮电大学 计算机学院(国家示范性软件学院) 北京 100876

100876

通讯作者: 郭燕慧, E-mail: yhguo@bupt.edu.cn

摘 要:

面对不断涌现的安卓恶意应用, 虽然大量研究工作采用图神经网络分析代码图实现了准确高效的恶意

应用检测, 但由于未提供应用内恶意代码的具体位置信息, 难以对后续的人工复核工作提供有效帮助. 可解释技
术的出现为此问题提供了灵活的解决方法,在基于不同类型神经网络及代码特征表示实现的检测模型上展示出了
较好的应用前景. 本研究聚焦于基于图神经网络的安卓恶意代码检测模型上, 使用可解释技术实现安卓恶意代码
的准确定位:（1）提出了基于敏感 API 及多关系图特征的敏感子图提取方法. 根据敏感 API, 控制流逻辑以及函
数调用结构三类特征与恶意代码子图分布的关联性, 细致刻画恶意代码特征, 精简可解释技术关注的代码图规模;
（2）提出了基于敏感子图输入的可解释技术定位方法. 使用基于扰动原理的可解释技术, 在不改变检测模型结构
的情况下对代码图边缘进行恶意性评分, 为各类基于图神经网络安卓恶意代码检测提供解释定位; （3）设计实验
验证敏感子图提取对于与恶意代码特征的刻画效果以及基于敏感子图提取的解释定位效果. 实验结果显示, 本文
的敏感子图提取方法相较于 MsDroid 固定子图半径的方法更为精确, 能够为可解释技术提供高质量的输入; 基
于此方法改进后得到的可解释技术定位方法相较于 GNNExplainer 通用解释器及 MsDroid 定位方法, 在保证定
位适用性和效率的同时, 恶意代码平均定位准确率分别提高了 8.8%和 2.7%.
关键词: 代码定位;图神经网络;可解释性;敏感子图;恶意应用检测
中图法分类号: TP311
中文引用格式: 郭燕慧, 王东, 王晓煊, 王柳, 徐国胜.一种面向图神经网络安卓恶意代码检测的通用解释定位方法. 软件
学报. http://www.jos.org.cn/1000-9825/7123.htm
英文引用格式: Guo YH, Wang D, Wang XX, Wang L, Xu GS. A Generic Explaining & Locating Method for Malware Dedection
based on Graph Neural Networks. Ruan Jian Xue Bao/Journal of Software (in Chinese). http://www.jos.org.cn/1000-9825/7123.htm

A Generic Explaining & Locating Method for Malware Detection based on Graph Neural
Networks
Guo Yan-hui1,

Wang Dong1,

Wang Xiao-Xuan1,

Wang Liu2,

Xu Guo-Sheng1

1

(School of Cyber Security, Beijing University of Posts and Telecommunications, Beijing100876, China)

2

(School of Computer Science (National Pilot Software Engineering School), Beijing University of Posts and Telecommunications ,
Beijing100876, China)

Abstract:

Faced with the ever-emerging malicious Android applications, many researchers have used deep learning technology to

achieve accurate and efficient malware detection. However, such detection models can only output application classification results, but
cannot provide the specific location of malicious code in the application, which makes it difficult to support the subsequent manual review
work.

The emergence of explainable technology provides a flexible solution to the malicious code location problem of many existing

detection models, and shows good application prospects on detection models with different kinds of neural networks and code

∗

基金项目: 国家重点研发计划(2021YFB3101500);
收稿时间: 2023-09-11; 修改时间: 2023-10-30; 采用时间: 2023-12-15; jos 在线出版时间: 2024-01-05

郭燕慧 等:一种面向图神经网络安卓恶意代码检测的通用解释定位方法

2205

representations. Our work targets graph neural network-based Android malware detection models, using explainable techniques to achieve
accurate localization of Android malicious code. First, we propose a sensitive subgraph extraction method based on sensitive API and
multi-relational graph, which characterizes malicious code in detail based on the correlation between the three features (i.e., sensitive API,
control-flow logic, and function-call structure) and the distribution of malcode subgraphs, to streamline the size of the graphs that are of
interest to explainable technology. Second, we propose a generic explaining method to locate malicious code with sensitive subgraph input,
which uses the perturbation-based explainable technology to score the code maliciousness without changing the structure of the detection
model, providing malicious code location for any graph neural network-based Android detection method. Third, we design extensive
experiments to verify the effectiveness of sensitive subgraph extraction method in portraying malicious code features, as well as the
effectiveness of malicious code localization based on sensitive subgraph extraction. The experimental results show that, compared with
MsDroid, the sensitive subgraph extraction method in this paper can extract more accurate sensitive subgraphs to provide high-quality
inputs for the interpretable technique. Compared with GNNExplainer and MsDroid, our improved explainable and location method can
improve the average accuracy of malicious code location by 8. 8% and 2. 7%, respectively, while ensuring the applicability and efficiency.
Key words:

code location, graph neural networks, explainability, sensitive subgraph, malware detection

安卓恶意应用检测技术是遏制安卓恶意软件威胁的重要研究课题, 基于深度学习模型直接进行复杂的应
用特征学习和预测, 使得恶意应用检测准确率大幅度提升, 也使得在面对海量应用检测工作时人工分析工作
占比大幅度降低. 然而, 深度学习检测模型仅能给出应用分类结果, 无法提供证据支撑模型的分类判断, 导
致安全分析人员仍然需要耗费大量时间手动分析引发模型告警的应用. 考虑到注意力机制能够帮助模型关注
重要数据且易在模型上实现, 很多研究基于该机制实现基于深度学习的安卓恶意代码定位. 然而, 此类方法
需要修改原有模型架构或者重新定制新模型, 具有一定的侵入性, 无法快速解决既有模型的代码定位问题.
随着深度学习可解释技术在图像、文本等其他领域的关键信息定位上的成功应用, 可解释技术引起了代
码分析领域的关注, 开始出现利用深度学习的可解释技术, 将检测和定位结构解耦, 为已有深度学习模型赋
予代码定位能力的一些工作[1]

[2]

. 然而, Warnecke[3] 等人和 Fan[4] 等人的工作发现不同解释方法在重要代

码判定上存在较大差别, 解释结果与恶意代码的关联性较弱, 这意味着依赖当前可解释技术定位到的恶意代
码不一定准确.
近年来，图神经网络在代码分析领域兴起，以 GNNExplainer[5] 为代表的基于扰动的图神经网络可解释
技术开始应用于代码分析结果的解释与定位[1]

[6]

. 根据 Freitas

[7]

等人对 1,262,024 个 Android APK 文

件的函数调用图（Function Call Graph, FCG）进行的图提取工作, 每个图平均包含 15,378 个节点和 35,167 条
边. 但整个 FCG 图内恶意代码相关子图仅占其中一小部分, 剩余的大量与定位目标无关的正常代码相关子
图的存在不仅影响定位工作的执行效率, 还会对可解释技术的关注点产生错误引导, 使得重要性评分结果与
恶意代码的关联性不强, 进而影响定位准确性. MsDroid[1] 采用提取敏感子图的方式降低对解释工作的干扰,
但其敏感子图仅覆盖以敏感 API 为中心、固定半径范围内的区域, 无法充分表征恶意代码行为的特点, 且该
工作仅关注对自身检测模型的解释, 没有涉及对不同检测模型的解释定位.
为此, 本文以基于图神经网络的安卓恶意代码检测模型作为被解释对象, 分析代码图特征, 从敏感 API,
控制流图及函数间调用图等安卓代码语法及语义特征中发现恶意行为、提取敏感子图, 借助图检测模型常规
预测能力对敏感子图边缘进行重要性评分, 实现灵活、准确地安卓恶意代码定位. 论文的主要贡献如下:
（1）

基于敏感 API、控制流图及函数间调用图等安卓代码语法及语义特征, 提取表征恶意代码的敏
感子图, 精确刻画恶意代码特征, 消除应用代码图中大量良性代码子图对可解释技术的干扰;

（2）

综合图神经网络检测模型输入元素与代码的映射关系, 以及检测模型所提供的、可反映其对恶
意代码理解的辅助信息, 应用基于扰动原理的重要边缘图神经网络可解释技术, 定位恶意行为
的函数调用路径, 实现对基于图神经网络的各类安卓恶意代码检测模型通用可解释定位.

（3）

在安卓应用公开数据集 Drebin [8] 和 Androzoo [9] 数据集上进行实验, 验证了本文所提出的方
法相较于 GNNExplainer 通用解释器及 MsDroid 定位方法, 能够在保证定位效率的同时, 将恶
意代码平均定位准确率分别提高 8.8%和 2.7%.

2206

Journal of Software 软件学报 Vol.32, No.7, July 2021

本文第 1 节介绍基于深度学习的安卓恶意代码定位的相关方法和研究现状. 第 2 节介绍本文所需的基础
知识,包括安卓代码特征、图社区发现算法、图神经网络检测模型和图神经网络可解释技术. 第 3 节介绍本文
构建的基于敏感 API 及多关系图特征的敏感子图提取方法以及基于敏感子图输入的可解释技术定位方法.
第 4 节通过对比实验验证了所提方法的有效性. 最后总结全文.

1 安卓恶意代码定位的相关工作
注意力机制因其帮助模型关注重要数据的特性以及易嵌入深度学习模型的优势, 在基于深度学习模型的
安卓恶意代码定位研究中得到了广泛应用.

XMAL[3]

在基于多层感知器（MLP）实现的简单分类模型的基

础上, 增加了注意力层预先对应用敏感 API 及权限的联合布尔向量进行各维特征的注意力分布计算, 再将
加权特征向量输入到后续网络中进行应用分类预测, 而注意力值较高的 API 或权限特征将成为恶意代码关键
证据. Droidetec[10]

受自然语言处理领域标记自然语句关键词工作的启发, 选择函数调用图中入度为 0 的方

法作为入口, 深度优先遍历生成方法内及方法间的 API 调用序列, 输入到双向长短期记忆网络（Bi-LSTM）
模型进行恶意应用检测, 通过在隐藏层结束后引入注意力层计算各 API 的注意力值, 并进一步计算调用序
列的总注意力值, 以表示每个调用序列对应用分类的贡献程度, 从而实现可疑代码的定位工作. Wu

[11]

等人

也采用了增加注意力层的方法实现定位工作, 该工作通过将函数调用图直接输入到图卷积神经网络（GCN）
进行节点嵌入表示后, 借助注意力层的可优化参数向量与节点嵌入向量内积生成图内各节点的注意力值, 并
结合候选节点邻域内敏感 API 调用等情况识别恶意代码. 注意力层的存在直接提供了应用检测过程中各项
特征的注意力值评分结果, 并可作为恶意代码判定依据帮助安全分析人员实现初步的恶意代码定位. 然而,
该类方法需要修改原有模型架构或者重新定制新模型, 具有一定的侵入性, 无法快速解决既有模型的代码定
位问题. 考虑到过往研究中提出的深度学习检测模型众多, 其中不乏优秀的工作, 为了可以继续使用已有模
型, 同时赋予其恶意代码定位能力, 最近的工作尝试将检测及定位结构解耦, 以研究更加灵活的安卓恶意代
码定位方法, 深度学习的可解释性研究恰好为此提供了新思路.
可解释性的提出旨在帮助人类理解复杂模型的决策依据. 在安卓恶意代码定位中, 可解释性的目标是帮
助安全分析人员理解检测模型对恶意预测的依据. Fan 等人对安卓应用中 API、权限及 Intent 过滤器等特征
进行多维布尔向量形式的应用表示, 构建基于多层感知机（MLP）的深度学习模型以及基于随机森林（RF）,
K-近邻算法（KNN）, 支持向量机（SVM）的传统机器学习模型用于恶意应用识别以及家族识别, 然后将
LIME[12] , Anchors[13] , LORE [14] , SHAP [15] 以及 LEMNA [16] 五种基于代理的可解释技术应用到恶意代码
关键特征发现中. Iadarola[17] 等人将代码中每个字节转化为[0, 255]范围内的像素值, 并根据字节先后顺序生
成灰度图像形式的应用表示, 训练得到基于卷积神经网络（CNN）的检测模型以实现安卓恶意应用分类及家
族识别, 然后利用基于梯度的 Grad-CAM[18] 解释方法生成激活图, 激活图中不同位置的贡献度权重则可用
于定位对分类结果做出关键贡献的代码. 上述可解释技术定位方法主要是对以特征向量、二进制字节码序列
或图像形式的应用表示为输入并基于 MLP、RNN、CNN 等神经网络搭建的深度学习检测模型实现安卓恶意
代码定位信息的提取.
最新的安卓恶意应用检测工作 [19]

[20]

[21]

中出现了较多基于应用代码图表示及图神经网络（GNN）的

模型设计方案. 这些模型可以直接学习安卓代码的语义特征, 例如函数调用图（FCG）和控制流图（CFG）,
应用于检测任务. 而目前这对此类工作的解释尚处于起步探索阶段,相关工作较少. MsDroid 从安卓应用代码
中提取函数调用图, 并将包含多个敏感 API 节点的子图输入到基于图神经网络（GNN）的检测模型中进行训
练. 为了进一步对检测结果进行解释, 论文采用基于扰动的可解释技术 GNNExplainer 对被模型判断为恶意应
用的可疑子图的内部边缘权重进行学习, 并根据权重大小抽取出对恶意行为实现至关重要的函数调用关系.
不同于已有的 MsDroid 的解释工作, 本文基于 GNNExplainer 借助检测模型的常规预测能力以及表征代码执行
逻辑和社区特性的敏感子图, 对检测结果进行解释。细致刻画恶意行为的敏感子图有助于提升恶意代码定位
的准确性和效率, 恶意应用检测与恶意代码定位解耦, 可为逐渐兴起的各类基于图神经网络及代码图特征的

2207

郭燕慧 等:一种面向图神经网络安卓恶意代码检测的通用解释定位方法

安卓恶意应用检测提供有效的解释.

2 基础知识
本文所提方法主要基于安卓代码特征、图社区发现算法、图神经网络检测模型和图神经网络可解释技术,
下面就相关概念和基本知识予以介绍.
2.1 安卓代码特征
安卓恶意应用是经攻击者自主开发或恶意篡改代码生成的攻击软件, 在应用运行时触发权限提升, 服务
创建, 文件删改等侵害用户合法权益的恶意行为. 恶意行为同良性行为的实际功能差异, 以及不同恶意行为
的破坏目标差别, 往往体现在代码实现模式上. 受安卓基础框架及安全机制等对代码语法的约束, 以及代码
自身所需遵循的程序设计逻辑等语义信息的影响, 代码实现模式的不同体现在安卓应用的各类代码特征中.
这些特征是识别恶意应用, 认知恶意行为的关键. 常用的特征主要分为两大类: 权限, 组件, API 等代码语法
特征, 以及包、类、方法、Dalvik 指令等代码语义特征. 这些特征之后将被转化为向量、序列、网格或图等
表示形式, 输入到深度学习检测模型中进行特征学习及恶意预测.
2.2 图社区发现算法
图的社区结构是现实世界图数据中常见的一种特征. 不同节点之间的连接紧密度因现实条件而异, 同一
社区内的节点之间有着较强的连接, 而不同社区之间的连接相对较少. 通过利用图的社区结构, 可以有效地
缩减大型图的分析规模, 并提取有用的信息. 经典的图社区发现算法，包括基于图分割的 GN[22] 算法，基于
标签传播的 LPA[23] 算法，基于随机游走的 Infomap[24] 算法，以及基于模块度的 Louvain

[25]

算法等。采用

模块度 Q 的社区划分标准，对上述四种经典算法进行社区划分效果评估，Louvain 算法的 Q 值平均可达 0.56，
多次划分的社区个数也较稳定. 因此，本文敏感子图提取工作中选取了 Louvain 算法。
2.3 图神经网络检测模型
图神经网络（GNN）是一种用于对社交网络等图结构数据进行表示学习的神经网络, 其核心思想是利用
输入图的拓扑结构, 对各节点进行邻域内信息的传递与聚合, 并用聚合后的信息及当前节点信息更新节点嵌
入, 使得各节点被映射到可表示局部子图的嵌入空间, 从而达到图表示学习的目的, 并用于节点类别、边存在
性、图类别、节点社群以及子网络相似性等下游预测任务中. GNN 的工作机制可被总结为消息传递神经网络
（MPNN）[26] . 假设输入图 G 由一组节点 V 和节点间连接关系 E 构成, 且各节点嵌入可表示为 hi , 各边嵌入
可表示为 rij 中, 节点 vi 和 v j 之间传递的消息可表示为:

(

mijl = MSG hil −1 , hlj −1 , rij

)

(1)

其中 hil −1 和 hlj −1 是 vi 和 v j 进行消息传递前的节点嵌入, MSG 是消息传递函数, 之后聚合节点 v j 的领域内所有
消息可以得到:

(

=
M il AGG mijl |v j ∈ N vi

)

(2)

其中, N vi 是 vi 相邻节点的集合, AGG 是消息聚合函数. 结合邻域内信息, 节点 vi 在该轮的节点嵌入可更新
为:

(

hil = UPDATE M il , jil −1

)

(3)

2208

Journal of Software 软件学报 Vol.32, No.7, July 2021

其中, UPDATE 函数是节点更新函数. 对于本研究涉及的恶意应用检测这种图类别预测任务, 在使用多层

GNN 对图内各节点进行了表示学习后, 还需借助 READOUT 读出函数综合全局节点信息, 从而生成可输入分
类器的图嵌入表示, 公式如下:

(

=
hG READOUT hvl |v ∈ G

)

(4)

就基于 GNN 的安卓恶意应用检测模型而言,代码在包、类、函数、API 使用上表现出的各类程序依赖关
系因其天然的图结构成为这类模型输入中常用的应用特征, 其中使用函数调用图的研究工作最多. 对于图中
的节点, 除了使用函数名或序号进行区分外, 较多工作还结合函数内权限, 操作码等其他特征对节点信息进
行补充, 并表示为节点嵌入向量: 对于图的边, FCG 图中所有边可采用邻接矩阵或节点对的形式进行表示.
从后续检测流程来看, 在提取并构造出输入图后, 检测模型首先使用 GNN 网络对输入图进行表示学习, 经
k 层 GNN 处理后, 图内各节点将被更新为包含其 k-hop 邻域信息的嵌入表示. 接下来, 图内节点的表示学
习结果借助全局池化层综合生成全图的嵌入向量表示, 最后, 对图嵌入向量进行恶意应用和良性应用分类.
基于 GNN 的安卓恶意应用检测模型的完整处理流程如图 1 所示.
GNN

特征提取

...

GNN

pool

Malware

……

图表示学习

APK

图嵌入生成

分类

Benign

图1

GNN 检测模型处理流程

2.4 基于扰动图神经网络可解释技术
基于扰动的方法是一种观察不同程度的输入扰动下输出变化的解释方法, 其关键思想是对输入对象进行
多次扰动, 通过对比扰动前后的输入在 GNN 图神经网络输出上的差异来判断被扰动对象对 GNN 图神经网络
输出的重要程度. 该方法仅需借助检测模型的常规预测能力来计算输出损失以优化扰动权重，不涉及检测模
型结构，具有很强的泛化性，适用于各种不同类型 GNN 检测模型的解释.

GNNExplainer 是其中的一项代表

性工作, 其使用掩码扰动方式，结合原始模型的参数，估计掩码应当的取值，来为给定实例(例如节点或图)
提供解释。后续的 PGExplainer[1] 等工作在掩码估计以及模型解释方面做出了更多的尝试，以期理解深度学
习模型本身的运作机制. 在代码检测领域，研究人员更倾向于了解代码行为的执行路径，如 Hu[6] 等人在漏
洞检测中采用 GNNExplainer 对检测结果予以解释, 他们的研究表明, 在面向漏洞的解释中, GNNExplainer 的
解释准确率较 PGExplainer 高 17%左右。
基于函数调用图采用 GNN 可解释技术的代码检测任务, 可以看作是一个多目标优化问题, 即:
给 定 应 用 程 序 的 函 数 调 用 图 G = (V , E ) , 对 于 边 掩 码 M , 其 对 应 的 子 图 为 MG = (VMG , EMG ) , 其 中

EMG =
{e |?e ∈ E , M ( e ) =
1} , VMG = V ( EMG ) , M ( e ) 表示边掩码中对应边的取值. 对于 GNN 而言, 其所学习的内
容可用函数
=
y Φ (V , E ) → {0,1} 表示. 对于可解释而言, 其所要做的事情是利用 GNN 学习到的内容对由 M 生

=
成的子图进行预测,
即 yˆ Φ (VMG , EMG ) → {0,1} , 使得二者的不同尽可能小, 同时 M 中所包含的边数量应尽可
能少, 即

郭燕慧 等:一种面向图神经网络安卓恶意代码检测的通用解释定位方法

 min y − yˆ

 min ∑ mi
s.t. m ∈ M
i


2209

(5)

3 研究动机
3.1 安卓恶意代码检测的解释机制
基于图数据及图神经网络实现的安卓恶意代码检测，其输入的函数调用图内部节点或边缘等元素同应用
内代码存在着天然的映射关系，经过训练的图神经网络其内部复杂的网络参数以及其所提供的检测能力，是
深入学习恶意代码特征后的结果。基于扰动的可解释技术在对输入图内的待解释特征进行扰动后，借助检测
模型的预测能力来计算输出损失以优化扰动权重，可充分发挥检测模型自身在恶意代码特征分析上的独特优
势，简便快捷地实现对各类 GNN 模型检测结果的解释。
但是，不同 GNN 检测模型输入的函数调用图在节点嵌入的表示上通常会使用不同的特征, 如: 节点中心
性、函数修饰符、传入参数类型、操作码、API 调用序列、权限等, 而且为了增强节点嵌入的表示能力, 这
些特征一般采用向量拼接的组合方式生成单个节点的完整嵌入表示。因此, 若对节点嵌入的各维度进行恶意
性评分, 不同类型特征的评分标准无法实现有效区分, 评分结果容易缺乏合理性. 此外, 根据应用内函数类
型不同, 函数调用图内的节点嵌入的侧重点也不同, 例如: 除了应用内自定义函数, 安卓系统提供的函数无
法从应用代码内获取其定义并进行代码特征提取, 但这类函数与敏感权限的使用存在关联, 第三方库函数的
定义来自于其他开发者, 并不反映本应用开发者的真实意图, 其内部代码特征可适当忽略. 可见, 由于不同
类型节点的评分标准无法统一, 基于节点的恶意性评分结果可能无法正确指出恶意代码所在的函数位置. 而
在所有输入图中, 边的定义始终是函数间的调用关系, 且一般没有附加特征或类型的限制.
为此, 本文采用基于边扰动的解释方式, 对输入的函数调用图边缘进行恶意性评分, 指示恶意代码所在
函数及其行为实现路径，以适用于各类图神经网络检测模型的解释定位。
3.2 函数调用图中敏感子图分布规律
分析基于扰动图神经网络可解释的原理（见公式 5），其第一个优化目标 min y − yˆ 使得 GNN 模型对应
用程序的分类结果和对由边掩码生成的子图的分类结果相同. 考虑到一种极端的情况, 即不对边掩码加以限
制,则 GNN 模型对两者的分类结果必然相同, 这显然不是解释所需要的结果. 为此, 第二个优化目标 min ∑ mi
对边的数量进行了限制. 即在寻求分类结果差距最小的同时，包含重要函数调用关系的边掩码之和也尽量小,
只包含少数的函数调用关系.
以安卓某恶意应用(sha256: 5e82d73a3b2d4df192d674729f9578c4081d5096d5e3641bf8b233e1bee248d4)为
例，其部分恶意代码的调用关系如图 2 所示：


onCreate 函数，直接或间接调用 getDeviceId、getSubscriberId、query、delete 等安卓 API, 窃取了手



onStart 函数，调用 getActiveNetworkInfo 获取了本机网络信息, 调用 update 修改了手机接入点, 调

机设备 IMEI、IMSI 等敏感数据, 并对短信数据库进行了查询或删除操作；
用 DefaultHttpClient.execute 访问了某些 http 链接, 除此之外, 还实现了删除短信和操作短信相关
SQLite 数据库表等行为.

2210

Journal of Software 软件学报 Vol.32, No.7, July 2021

Start

BroadcastReceiver:
MyBoolService
method: onReceive()
AndroidAPI: AlarmManager
method: set()
BroadcastReceiver:
MyAlarmlService
method: onReceive()
Service: MyService
method: onCreate()

AndroidAPI:TelephoneManager
method: getDeviceId()
AndroidAPI:TelephoneManager
method: getSubscriberId()
AndroidAPI:ContentResolver
method:
registerContentObserver()

AndroidAPI:ContentResolver
method: query()
AndroidAPI:SMSObserver
method: onChange()

AndroidAPI:ContentResolver
method:delete()

AndroidAPI:
ConnectivityManager
method: getActiveNetworkinfo()
Service: MyService
method: onStart()

Service: MyService
method: openAPN()

ContentResolver:
SMSObserver
method: deleteSpecSMS()

AndroidAPI:ContentResolver
method: update()
AndroidAPI: DefaultHttpClient
method: execute()

……
Self-defined Class: qzl
Method: GG()

……

End

AndroidAPI: SmsManager
method: sendTextMessage()
AndroidAPI: SQLiteDatabase
method:
query()delete()/insert()

图2

恶意行为代码执行流程图(彩印)

观察其代码执行逻辑，可以发现恶意行为是围绕敏感权限的 API 调用(getDeviceId、getSubscriberId、
getActiveNetworkInfo 等)展开的. 以敏感 API 为基准点向外延伸寻找恶意行为，可以看到应用内恶意函数同
基准点直接的可达性高于良性节点, 如短信的删除操作 deleteSpecSMS 与敏感 API delete 具有直接的可达性,
而良性函数 openAPN 与敏感 API 无可达性. MsDroid 基于应用行为子图的检测模型进行解释定位，解释对象
是安卓应用内各敏感 API 固定半径范围内的 FCG 子图，使解释能力得到以优化。但是，基于固定半径提取
的敏感子图，在缩小解释范围的同时却会出现良性代码遗留或部分恶意代码缺失等问题，影响解释的准确性。
进一步基于应用的完整函数调用图观察与基准点合作实现应用行为的函数节点全貌. 图 3 是应用的完整
函数调用图, 图中黑色节点表示敏感 API, 深灰色节点表示除敏感 API 外的恶意代码相关函数, 浅灰色节点表
示普通函数, 深灰色边缘是恶意代码相关函数调用边, 浅灰色边缘是良性调用边. 考虑到控制流图（CFG）反
映了应用程序中代码的执行顺序, 利用“invoke-”调用指令与函数的关联性对各函数的 CFG 图进行转换, 生
成可以描述函数执行顺序的函数间控制流图集合，进而可以得出基准点可达分布情况，如图 4 所示. 其中, 红
色节点是基准点, 蓝色节点是标注出的可达节点（图中 2301 个节点中被标记为可达节点的有 641 个, 全图
规模大约缩小至 28%）, 黄色节点是未被标记出的恶意节点, 良性节点是浅灰色节点, 深灰色边缘是恶意函
数节点间的调用关系, 浅灰色边缘则是正常函数节点间的调用关系.

郭燕慧 等:一种面向图神经网络安卓恶意代码检测的通用解释定位方法

图3

图4

2211

恶意应用的函数调用图

函数调用图内基准点可达节点分布情况(彩印)

虽然经控制流分析后函数调用图规模已经大幅度缩减, 但是由于应用正常行为也会频繁调用敏感 API ，
当前获得的敏感子图中仍存在部分良性节点.采用社区发现算法对其进行社区划分, 得到图 5 所示的节点划分
结果, 其中共包括 14 个社区, 同一社区节点采用相同颜色标注, 图内红色节点是基准点节点, 蓝色节点是
当前社区内的其他节点, 深灰色节点是人工标注的恶意函数节点.

2212

Journal of Software 软件学报 Vol.32, No.7, July 2021

图5

敏感子图社区划分结果(彩印)

样例应用中 MyService 类的 onStart 函数中的部分恶意行为子图及其函数节点如图 6 所示, 其中主要涉
及 qzl.GG 函数内部线程的 run 方法根据远程指令执行短信发送等敏感操作.

图6

恶意社区示例(彩印)

可见，以敏感 API 作为敏感子图的基准点，配合恶意行为程序逻辑和社区特点，可以更加全面的刻画函
数调用图中敏感子图的分布规律。为此，本文将基于敏感 API 及多关系图特征提取敏感子图, 解决良性代码
遗留和恶意代码缺失问题，为解释提供精准的输入。

4 研究方法
给定应用程序的函数调用图 G = (V , E ) , 以敏感 API 为基准点, 设基准点 A ⊆ V , 则敏感子图可表示为

SG = (VSG , ESG ) , 其中 VSG =∈
{v | v V , R ( v, A ) =
1} , ESG = E (VSG ) , R 表示二者是否具有某种关系. 对于边掩码
{e |?e ∈ E , M ( e ) =
1} , VMG = V ( EMG ) , M ( e ) 表示边掩码中对
M , 其对应的子图为 MG = (VMG , EMG ) , 其中 EMG =
应 边 的 取 值 . 则 敏 感 子 图 的 边 掩 码 可 以 表 示 为 SM ,

SM 是 M 的 一 部 分 , 其 生 成 的 子 图 为

{e |?e ∈ EMG , MG ( e ) =
1} , VSMG = V ( ESMG ) , MG ( e ) 表示敏感子图边掩码中对
SMG = (VSMG , ESMG ) , 其中 ESMG =

2213

郭燕慧 等:一种面向图神经网络安卓恶意代码检测的通用解释定位方法

应边的取值. 满足此条件的 SM 所对应的 SMG 即为可解释技术输出的结果:
 min y − yˆ

 min ∑ mi
s.t. m ∈ SM
i


(6)

据此, 本文基于恶意代码执行路径及拓扑结构提取敏感子图，将检测模型内隐含的恶意代码抽象描述显
式化到敏感子图图内元素的恶意性评分上, 最终实现安卓应用内恶意代码的准确定位. 首先, 由检测模型判
定为恶意的 APK 文件生成函数调用图, 据此提取敏感子图, 然后对敏感子图进行边掩码扰动, 将扰动后的敏
感子图输入给检测模型, 得到的结果与基于原图的检测结果进行比较, 不断迭代直至差距最小, 最后将定位
结果输出。整体方案如图 7 所示。

恶意
APK

GNN模型
恶意应用检测

函数调用图
提取

基于原图的
检测结果

定位
输出

损失计算

敏感子图提取

边掩码
扰动

0.70.90.2

GNN模型
恶意应用检测

0.8 0.9 0.2 0.1

基于敏感子图
的检测结果

边掩码更新

图7

安卓恶意代码定位流程

其中，敏感子图的提取独立于检测模型，解释定位则依据检测模型对敏感子图元素评分的高低输出结果。
4.1 基于敏感 API 及多关系图特征的敏感子图提取
1）敏感API基准点识别

输入
FCG图

基准点FCG图

基准点
标注

…
…

函数间
控制流图生成

基准点

一类节点

二类节点

三类节点

删除节点

2）基于控制流分析初步提取敏感子图

图8

检测模型
分类预测

社区
划分

控制流
分析

最终敏感子图

FCG社区子图

初级敏感子图

函数间CFG图

函数内CFG图

基准点

社区一

社区二

社区三

删除节点

基准点

社区一

删除节点

3）基于社区划分生成敏感子图

敏感子图提取流程(彩印)

首先, 使用 Androguard 分析 APK 文件, 得到函数调用图(FCG)和每个函数的控制流图(CFG), 然后将此
两类图作为输入，执行以下步骤(如图 8 所示):
1） 识别敏感 API 节点
函数调用图中节点的 sensitive 属性定义为该节点是否存在敏感权限的使用，在函数使用上可细分为敏感

2214

Journal of Software 软件学报 Vol.32, No.7, July 2021

API 和敏感资源访问 API 两类，前者主要用节点完整函数名与安卓 API 进行比对，并根据 API 与权限的映射
关系得到节点的敏感权限使用情况，后者则通过标注调用 ContentResolver 增删改查方法的父函数节点，并分
析函数源码中“content://”字符串来确定访问资源 URI，最后根据 URI 与权限的映射关系得到节点的敏感权
限使用情况。
2）基于控制流分析初步提取敏感子图
i.生成函数间控制流图。首先根据每个 CFG 图中基本块内“invoke-”指令顺序进行被调用函数节点的顺
序连接，并保留基本块中控制流的首尾函数节点，然后根据基本块间顺序进行各基本块间首尾函数节点的连
接，最后通过将当前 CFG 图对应的函数节点与 CFG 图入口块的首函数节点进行连接，即可沟通多个 CFG 图，
得到关于整个应用的函数间控制流图；
ii.查找可达节点。获取 FCG 图内基准点调用路径上的祖先函数节点，接着在各祖先节点对应的函数间
CFG 图中分析其要调用基准点或下级祖先节点时还需依次经过的路径函数节点，然后对于路径节点中功能实
现所需的浅层辅助节点进行获取，最终将这三类节点同基准点构成的子图作为初级敏感子图；
3）基于社区划分生成敏感子图
i.社区划分。根据初级敏感子图中存在的各应用功能模块的社区结构分布，使用 Louvain 算法进一步划
分出各社区子图；
ii. 预测模型评分。借助检测模型对应用恶意行为的认知，将各社区子图映射回检测模型的输入图中以赋
予其节点嵌入，然后输入到检测模型中进行图嵌入学习并进行分类预测，预测概率将被用作社区子图的恶意
性评分以进一步删除无关子图，保留下的多个子图将综合生成最终的 FCG 敏感子图.
4.2 基于敏感子图输入的解释定位
基于敏感子图输入的解释定位对敏感函数调用子图使用边缘掩码的形式扰动生成新图, 计算检测模型对
原图和新图的检测结果差异, 并以缩小该差异为目标优化边缘掩码, 最后将掩码内的边缘权重赋予各函数调
用边, 并在排序后保留部分重要边缘以输出关键函数调用子图, 具体见算法 1.
算法 1
输入: 敏感函数调用子图 SG = ( VSG , ESG ): 恶意应用检测模型 Φ: 最大迭代次数𝑒𝑒: 保留边缘总数𝑘𝑘
输出: 关键函数调用子图 SMG = ( VSMG , ESMG )
1

ŷ : = Φ( GSG ); //计算原图的类别预测概率

2

VSMG : = VSG ; ESMG ← ESG ; //初始化新图表示

3

SM : = init(size( ESG )); //初始化边缘掩码

4
5
6
7

𝑜𝑜𝑝𝑝𝑡𝑡𝑖𝑖𝑚𝑚: = 𝐴𝐴𝑑𝑑𝑎𝑎𝑚𝑚(𝑝𝑝𝑎𝑎𝑟𝑟𝑎𝑎𝑚𝑚𝑠𝑠 = [ SM ], 𝑙𝑙𝑟𝑟 = 0. 01); //初始化优化器
lossmin : = 0; SM best : = SM ; //初始化最小损失及最优边缘掩码
for 𝑖𝑖: = 1 to 𝑒𝑒 do
ESMG : = ESG ⨀ SM ; //利用边缘掩码扰动生成新图

8


yi : = Φ( GSMG ); //计算新图的类别预测概率

9

yi , SM ); //计算损失
lossi : = loss( ŷ , 

10

if lossi < lossmin then

11
SM best : = SM ; //当存在更小损失时, 更新最优边缘掩码
12
end if
13
𝑜𝑜𝑝𝑝𝑡𝑡𝑖𝑖𝑚𝑚. zero_grad(); lossi . 𝑏𝑏𝑎𝑎𝑐𝑐𝑘𝑘𝑤𝑤𝑎𝑎𝑟𝑟𝑑𝑑(); 𝑜𝑜𝑝𝑝𝑡𝑡𝑖𝑖𝑚𝑚. step(); //更新边缘掩码
14 end for
15 ESMG : = 𝑔𝑔𝑒𝑒𝑡𝑡_𝑖𝑖𝑚𝑚𝑝𝑝𝑜𝑜𝑟𝑟𝑡𝑡𝑎𝑎𝑛𝑛𝑡𝑡_𝑒𝑒𝑑𝑑𝑔𝑔𝑒𝑒𝑠𝑠( ESG , SM best , 𝑘𝑘); //保留 k 条关键边缘
16

VSMG : = get_important_nodes( ESMG ); //保留关键边缘的起止节点

郭燕慧 等:一种面向图神经网络安卓恶意代码检测的通用解释定位方法

2215

17 return GSMG = ( VSMG , ESMG ) //返回关键子图
其中敏感子图边缘掩码 SM 是与敏感子图的边缘集合 ESG 同等规模的张量, 掩码内每个元素对应着各调
用边权重, 理想情况下若仅保留必要边缘, 每个元素取值应为 0 或 1, 即边缘掩码是二值化离散掩码, 但是
离散掩码在反向传播时不能直接优化, 且完全删除某些边缘可能引入新的噪声, 因此本研究中采用近似离散
掩码的形式为每条边赋予权重, 即每个元素为(0,1)范围内的某个值且趋近于 0 或 1, 在代码实现中借助

Sigmoid 函数限制元素大小. 另外, 对于优化边缘掩码最为关键的损失函数 loss 需要保证掩码保留子图的预
测结果应尽可能与原图一致, 且子图的边缘应该是稀疏的, 这意味着子图可以最大程度捕捉重要特征而忽略
不相关特征, 同时边缘掩码的取值还需近似离散, 基于上述要求, 本研究中损失函数定义如公式 7 所示:
Lexp =Lce + α Ll1 + β Lent
其中, 𝛼𝛼, 𝛽𝛽是比例系数, Lce 是原图 GSG 与新图 GSMG 的类别预测概率 ŷ 及 
yi 间的交叉熵损失, 即

L=
− ∑ PΦ (=
Y c|=
G GSG ) logPΦ (=
y c|=
G GSMG ) Cc
ce

(7)

(8)

其中, 𝐶𝐶在此处包括良性 0 和恶意 1 两类标签, 该值越小, 说明利用掩码保留的子图对于预测的产生越重要.
Ll1 是用于鼓励产生稀疏掩码的正则项限制, 即

Ll1 =
∑ m, m ∈ SM

(9)

其中, m 是边缘掩码 SM 中任意边的权重, 该值越小, 说明保留的子图规模越精简, 但在损失中过度引入该
惩罚项容易导致边权重普遍偏小, 因此该项比例系数𝛼𝛼相较其他项较小: Lent 是用于鼓励产生近似离散掩码的
正则项限制, 即

Lent = mean ( H )

(10)

其中, 所有边缘在子图内存在与否遵循多元伯努利分布, 若将边缘掩码 SM 中各边权值作为其出现概率 pi ,
则总的信息熵为

H = − ∑ pilogpi + (1 − pi ) log (1 − pi )

(11)

其中, 当 pi 趋近于 0 或 1 时熵值越小, 该值越小, 说明边缘掩码可以近似为离散掩码.

5 实验分析
5.1 实验数据
参考 MsDroid 数据集的构造方式, 本文利用 Drebin 和 Androzoo 构造恶意代码定位效果分析所需的数据
集.首先从 Drebin 中随机选取 1000 个旧恶意样本,接下来从 Androzoo 与 Drebin 时间段(2010 年-2012 年)重叠
的部分中随机选取 1000 个旧良性样本. 鉴于 Androzoo 中每个样本都经过了许多个反病毒引擎的检测, 并记
录了判定该样本为恶意的检测引擎统计值 vt_detection, 本文在采集样本时将 vt_detection=0 的样本视为良性
样本,将 vt_detection≥10 的样本视为恶意样本. 考虑到恶意代码随时间不断演化, MsDroid 补充了 2013 年
-2016 年间的新样本, 本文做了进一步更新, 随机选取了 Androzoo 中 2018 年-2022 年的数据作为新样本, 具
体的数据集构成如表 1 所示. 从该数据集中, 本文随机选取 200 个恶意样本，依照 4.1 节的样例分析过程对

2216

Journal of Software 软件学报 Vol.32, No.7, July 2021

其中的函数调用边进行了人工标注.
表 1
类别
良性
恶意

名称
Benign_old
Benign_new
Malware_old
Malware_new

数据集

数据集
Androzoo
Androzoo
Drebin
Androzoo

采集时间
2010. 10-2012. 8
2018. 1-2022. 6
2010. 10-2012. 8
2018. 1-2022. 6

数量
1000
3684
1000
4677

5.2 实验方法
鉴于安卓恶意代码定位过程中需要使用预训练的图神经网络恶意应用检测模型对应用的分类预测概率作
为输入数据, 而目前对于 Android 恶意应用进行检测的 GNN 模型众多, 为此本文构造并训练了一个基础检测
模型. 基于上面提到的实验数据集, 用 PyTorch 库和 PyG 库提供的各类网络来搭建图表示学习网络、图嵌
入生成网络以及分类器网络, 用于应用表示图的学习及类别预测. 恶意代码定位方法的评估将在此之上进行.
该基础模型反映了当前图神经网络进行安卓恶意代码检测的基本能力, 考虑到基于扰动的解释机制所采用的
解释模型和检测模型是同一个, 在目前的工作中, 尽管出现了各式各样的变体, 但其大多将函数调用图通过
特征提取转化为图神经网络中的边与节点, 边的定义始终是函数间的调用关系, 因此本方法对于更加先进的
图神经网络检测模型同样适用, 这种通用性可为各类新型图神经网络检测模型提供便捷的解释定位.
在训练基础检测模型时, 将数据集按照 4:1 的比例划分为训练集和测试集, 分别用于模型训练、模型超参
数选择及模型检测效果评估, 在训练过程中采用 10 折交叉验证的方式对测试集中样本进行再次划分以寻找
合适的超参数, 然后再用训练集中样本训练得到最终模型参数.
在模型训练阶段, 首先确定模型的结构参数, 其中图表示学习网络默认使用了 2 层图卷积层, 该网络初
始输入维度对应着应用表示图内各节点嵌入维度, 取值为 492 维, 中间经各层处理后输出维度不断压缩, 取
值依次为 256 维和 128 维: 图嵌入生成网络默认使用了最大及平均池化层混合结果, 输出的图嵌入维度取
值为 256 维: 分类器网络默认使用了 2 层全连接层实现, 其输出维度与预测类别一致, 取值为 2 维, 全连
接层间 Dropout 层的丢弃率设置为 0. 25. 接着又对训练过程涉及的超参数进行调整, Adam 优化器的学习率
设为 0. 001, 每折交叉验证中设置最大训练轮次 max_epoch 为 100, 每轮训练中 batch_size 为 64, 根据
10 折交叉验证中最优模型的经验选出合适的训练轮次, 并在确定超参数后使用全部训练集样本重新训练检
测模型.得到的基础 GNN 检测模型性能如表 2 所示.
表2

基础 GNN 检测模型性能

模型

准确率

精确率

召回率

F1

基础 GNN检测
模型

0.9628

0.9640

0.9683

0.9661

本 文 还 借 鉴 了 DIG[27] 中 提 供 的 GNNExplainer 代 码 以 及 MsDroid 论 文 中 提 供 的 代 码 , 实 现 了
GNNExplainer 中解释模块以及 MsDroid 中 3-hop 子图提取及解释模块的接口, 以支持本文的对比实验.
5.3 评价指标
对于恶意代码的定位效果, 本文借鉴安卓恶意应用检测模型常用评估指标, 包括精确率、准确率、召回
率以及 F1 分数，结合定位目标，定义了定位准确率（Location Accuracy, LA）, 定位精确率（Location Precision,
LP）, 定位召回率（Location Recall, LR）和定位 F1 分数（Location F1, LF）四个定位评估指标. 考虑到安卓
恶意代码定位方法在实际应用中对时间效率方面的要求, 本文除了对敏感子图提取和解释定位两步处理的阶
段性结果进行效果评估外, 又对各阶段的时间开销进行统计以体现本文方法的定位效率, 共包括敏感子图生
成时长（Subgraph generation Time, 即 ST）, 解释定位时长（Explaination generationTime, 即 ET）以及总运
行时长（Total Time, 即 TT）三个评估指标.

2217

郭燕慧 等:一种面向图神经网络安卓恶意代码检测的通用解释定位方法

对于敏感子图的提取效果, 本文采用恶意调用边的提取比例（Malicious edge Ratio, 即 MR） 和良性调
用边的提取比例（Benign edge Ratio, 即 BR）两个指标随应用规模的变化情况来评估.
以下介绍敏感子图提取效果与定位效率的评估指标的具体定义:
（1）

定位准确率（LA）: 该指标用于计算定位正确的恶意及良性调用边占所有函数调用边的比例, 即
LA =

（2）

∑ 
y = 1, y j = 1
 j

∑ 
yj =
1


∑ 
y = 1, y j = 1
 j

∑  y j =
1

定位 F1 分数（LF）: 该指标用于计算精确率和召回率的加权平均值,
2 * LP * LR
LF =
LP + LR

（5）

(13)

定位召回率（LR）: 该指标用于计算被人工标记为恶意的调用边中定位正确的比例, 即
LR =

（4）

(12)

定位精确率（LP）: 该指标用于计算被定位方法标记为恶意的调用边中定位正确的比例, 即

LP =
（3）

∑ 
y =
yj 
 j

N

(14)
即
(15)

恶意调用边提取比例（MR）: 该指标用于计算敏感子图含有的恶意调用边占全图恶意调用边的
比例, 即
MR =

（6）

∑ u j = 1, y j = 1
∑  y j =
1

(16)

良性调用边提取比例（BR）: 该指标用于计算敏感子图含有的良性调用边占全图良性调用边的
比例, 即
BR =

∑ u j = 1, y j = 0 
∑  y j =
0 

(17)

y j 三种属性, y j 表示人工标注的
其中, 各边 e j 分别具有敏感子图标记 u j 、恶意性评分 s j 和恶意性标记 
恶意调用边.
5.4 实验结果与分析
为了评估基于敏感子图输入的安卓恶意代码定位方法, 本文分别从定位效果及敏感子图特征影响两方面
进行实验分析.
5.4.1

基于该敏感子图输入的可解释方法定位效果

1)定位准确性
这里, 本文使用两种不同的定位方法作为比较基准.


GNNExplainer

GNNExplainer 是经典的基于扰动的图神经网络通用可解释器, 经常被各类代码检测任务用于结果的解
释. 图 9 展示了在 LA, LP, LR, LF 四个评估指标上, GNNExplainer 和本文方法在基础 GNN 检测模型上对
200 个经人工标注的安卓恶意应用进行解释定位的不同效果.
在定位准确率指标（LA）上, 二者分别具有 90. 0%及 81. 2%的定位准确率. 出现较高值的原因是, 在恶

2218

Journal of Software 软件学报 Vol.32, No.7, July 2021

意应用中良性代码占比较多, 仅依靠近似离散掩码即可区分出原图中的大量良性调用边: 在定位精确率指标
（LP）上, 本文方法的平均 LP 值较 GNNExplainer 方法高出 14.3%, 这说明在二者标注出的恶意调用边中,
本文方法的有效结果占比更大. 考虑到本文方法在提取子图时对其定位输入的规模进行了干涉, 因此还需关
注 LR 指标以排除由于分母减小带来的指标提高现象: 在定位召回率指标（LR）上, 本文方法的平均 LR 值
较 GNNExplainer 方法提高了约 20%, 除离散点外最低值也有了大约 26%的提高, 这说明对于人工标注出
的恶意调用边, 本文方法确实能够定位到更多的恶意代码信息, 敏感子图提取的方式可以预先借助恶意代码
的启发式规则删除子图中可能会对可解释技术定位产生干扰的部分良性节点, 从而在掩码更新过程中突出考
虑更有可能是恶意代码的调用边权重分配, 而非将部分贡献分散给其他良性调用边, 定位 F1 分数（LF）也
表现出了相同的提高效果.
通过与 GNNExplainer 的对比, 可以认为在安卓恶意代码定位任务中,启发性地引入任务特定领域知识,精
简定位输入, 可以在发挥基于扰动解释技术的通用性优势的同时, 提升定位的准确性.

图9


本文方法与 GNNExplainer 定位效果对比(彩印)

MsDroid

MsDroid 是一种基于安卓恶意代码片段输入的解释. 出于比对一致性的考虑, 本文将 MsDroid 解释方
法中用到的基于行为子图集的安卓恶意应用检测模型替换为本文实现的基础 GNN 检测模型, 并按照其子图
提取方法提取各应用内多个敏感 API 节点的 3-hop 邻域子图, 在 200 个人工标注恶意应用上进行对比实
验.
i.定位输入的效果
将人工标注过的 200 个应用按照调用边规模由小到大排序后划分为等量的 5 个批次, 各批次应用的平
均调用边数量依次为 481, 1870, 4086, 8376 以及 13449, 然后观察恶意调用边（MR） 和良性调用边（BR）
保留情况.
图 10 是恶意调用边提取比例随不同规模的函数调用边的变化情况. 序号 1 至 5 批次的应用内函数调
用边数量逐渐增大, 曲线上的点是各批次 40 个应用的 MR 均值. 随着调用边数量的增大, 虽然二者提取的
敏感子图中恶意调用边的数量都有一定程度的下降, 但本文方法的 MR 均值曲线几乎持续高于 MsDroid 方
法, 这说明本文方法能够保留更多的恶意调用边给解释器定位. 当应用较小时, MsDroid 在半径限制范围内
已经大致覆盖完整调用图的敏感子图, 恶意调用边缺失问题还不明显. 而本文方法在使用控制流分析敏感
API 所在函数执行路径后, 为了进一步过滤涉及敏感操作的良性调用边, 借助函数调用图内敏感社区选择的
方式精简了调用图, 社区划分误差会带来一些恶意调用边的损失, 因此本方法的 MR 均值开始稍低于
MsDroid 方法, 但其下降幅度在可接受范围内; 当应用较大时,

MsDroid 恶意调用边提取比例下降, 凸显了

其半径限制的问题. 本文方法虽然也会因为单个社区规模的增大, 导致一个社区的删除伴随着更多被错误分
区的恶意节点缺失, 但是在前期已利用控制流探索了敏感 API 执行路径上的几乎所有节点, 因而对图中恶
意调用边的保留更加完整, MR 值也会相应较高些.

郭燕慧 等:一种面向图神经网络安卓恶意代码检测的通用解释定位方法

图 10

2219

恶意调用边提取比例随调用边规模变化情况(彩印)

图 11 是良性调用边提取比例随不同规模的函数调用边的变化情况, 随着调用边数量的增加, 本文方法
的 BR 均值曲线几乎置于 MsDroid 的 BR 均值曲线下方, 这说明本文方法能够过滤掉更多的良性调用边.
当应用较小时, MsDroid 方法使用敏感 API 的固定半径领域作为敏感子图容易在覆盖更多恶意调用边的同
时引入许多良性调用边, 相比之下, 本文方法从控制流执行路径的角度更加精确地保留可疑调用边以涵盖更
全面的恶意调用边: 当应用逐渐增大时, MsDroid 方法的 BR 均值在不断减小, 一个主要因素是范围受限导
致混入敏感子图的良性调用边也将相应减少. 但是, 在去除敏感子图的良性调用边时, MsDroid 使用固定长
度, 本文方法的 BR 均值虽然由于敏感 API 的控制流可达路径的不断丰富导致混入的良性调用边有增加的
趋势, 但后期趋于平稳后与 MsDroid 的效果差不多, 这主要是因为本文方法在后期关注到应用行为在函数
调用图内的社区结构特点, 并增加了检测模型对社区子图的恶意性判断, 从而又删减了部分涉及敏感操作的
良性子图, BR 值也不会持续增高.

图 11

良性调用边提取比例随调用边规模变化情况(彩印)

ii.定位准确性
图 12 展示了本文方法与 MsDroid 方法的解释定位效果在 LA, LP, LR, LF 四个评估指标上的得分分布
情况. 在定位准确率指标上（LA）, 二者均可得到不错的平均定位准确率. 究其原因在于, 该指标的计算中加
入了良性代码的正确预测比例. 应用内存在大量良性代码, 在敏感子图提取阶段已经根据启发式特征识别出
一部分, 再加上 GNNExplainer 解释定位中用到的近似离散掩码又尽可能地拉开了各边的贡献值差距, 因此
许多良性调用边都能被区分; 在定位精确率指标上（LP）, 本文方法较 MsDroid 平均定位精确率高出约 5%,
这是由于相较于 MsDroid 采用敏感 API 节点周围固定半径的子图作为敏感子图, 本文方法提取的敏感子图
规模更具有弹性, 即 LP 指标的分母将会更加贴近真实恶意代码所在子图的规模. 这意味着, 在解释定位阶
段对恶意调用边判断相同的情况下, 更合适的敏感子图选取将带来更高的定位精确率. 本文方法的定位召回
率（LR）与定位 F1 值均超过 MsDroid, 也进一步证实了这一点.

2220

Journal of Software 软件学报 Vol.32, No.7, July 2021

图 12

本文方法与 MsDroid 定位效果对比(彩印)

通过对定位输入与定位效果的分析可知,本文方法与 MsDroid 在 GNNExplainer 解释定位的基础上对子
图输入环节进行的优化, 使得二者在定位准确性均有不错的表现. 但本文的敏感子图提取使用敏感 API, 控
制流路径以及函数调用图社区等特征, 较 MsDroid 采用敏感 API 固定半径邻域提取的方法, 在过滤无关子
图/保留恶意子图上受应用函数调用图规模变化的影响较小, 为定位提供了更为优质的输入, 定位表现更佳.
(2)定位效率
本文分别统计了本文方法, GNNExplainer 基线方法以及 MsDroid 先进方法对数据集中 5677 个恶意应
用的敏感子图提取及解释定位过程的时间开销, 以及迭代次数与准确率的变化情况.
对单个恶意应用的定位而言, 上述三种方法在敏感子图提取时长（ST）, 解释定位时长（ET）和总运行
时长（TT）三项指标上的平均取值如表 3 所示. 从敏感子图提取时长来看, GNNExplainer 直接对原始图进
行解释, 该阶段不存在时间消耗因此不做比较, 而相较于 MsDroid 的敏感子图提取方法, 本文提出的方法的
精简效率明显更高. 观察发现 MsDroid 在该阶段提取出原始图中多个敏感 API 附近的 3-hop 邻域子图后,
会单独对各子图进行内部剪枝和恶意检测, 当敏感 API 较多时此项处理较为耗时. 而本文方法则保留了与敏
感 API 存在控制依赖关系的函数节点, 根据函数调用关系对节点进行社区划分得到多个敏感子图, 经外部
剪枝快速生成子图并输入到检测模型中进行恶意性判断, 因而时间开销更小. 从解释定位时长来看,
GNNExplainer 定位方法对原始图所有边进行掩码学习, MsDroid 定位方法对应用内多个敏感 API 的相关子
图单独进行重要边缘掩码学习, 而本文方法不仅提前对原始图规模进行精简, 同时还将多个敏感子图进行合
并, 减少了无关掩码的学习时间, 同时也减少了掩码学习的总轮次, 因此相较于前两者在解释定位阶段的时
间开销明显降低. 从总运行时长来看, 本文方法的平均定位时间处于 GNNExplainer 和 MsDroid 之间, 以少
量的时间开销换取了定位准确性的提升.
三种方法的定位准确率随训练次数的变化如图 13 所示. 本文方法和 MsDroid 由于预先通过提取敏感子图
的方法去除大量良性代码, 在边缘掩码更新的初期同 GNNExplainer 相比, 准确率获得一个较大的提升. 在迭
代相同次数的情况下, 本文方法的定位准确率优于其他两种方法, 且随着迭代次数的增加能够快速到达收敛
平衡.
表3
定位方法
本文方法
GNNExplainer
MsDroid

ST（s）
12. 344
-111. 190

时间开销对比
ET（s）
13. 391
19. 724
101. 061

TT（s）
30. 735
19. 724
212. 251

郭燕慧 等:一种面向图神经网络安卓恶意代码检测的通用解释定位方法

图 13

2221

各方法准确率随迭代次数的变化情况(彩印)

在定位输入环节引入优化, 可提升恶意代码定位准确性, 但也会对定位效率产生影响. 不过, 从本文工
作和 MsDroid 的对比中可以发现, 基于扰动解释原理, 面向恶意代码定位任务目标, 充分挖掘恶意代码特征,
不仅能显著提升定位准确率, 还有助于降低对定位效率的影响.
5.4.2

敏感子图各类特征对恶意代码定位的影响

对于本文精简函数调用图规模时用到的两步处理: 基于敏感 API 及控制流的敏感子图提取, 基于函数
调用社区子图恶意检测结果的敏感子图提取, 依次选择全不保留, 保留第一项, 全部保留的敏感子图处理方
式, 分析子图提取对代码定位的影响.
图 14 体现了使用不同代码特征提取出的敏感子图在解释定位后得到的 LA, LP, LR, LF 分布情况. 从整体
效果来看, 随着子图处理从无到有,从一步处理到两步处理, 定位效果不断提升. 但从两阶段的指标增长幅度
来看, 特别是从更能体现人工分析所需定位效果的 LR 指标增长情况来看, 基于敏感 API 及控制流特征的
处理将 LR 最大值和最小值分别提高了 10%和 20%, 而在此基础上增加的基于函数调用社区子图处理, 则
将 LR 值最大值和最小值分别提高了 5%左右, 这说明基于敏感 API 及控制流特征在对于定位输入的优化
作用显著. 而函数调用图社区结构特征依赖于社区划分结果与应用行为的相关性, 以及检测模型对社区子图
的评分情况, 需要和基于确定执行逻辑分析的敏感 API 及控制流特征相结合才能发挥其对过滤良性子图的
作用.

图 14

不同敏感子图提取方式定位准确率分布(彩印)

由此可知, 要在恶意代码定位任务中取得比较满意的定位解释效果, 关键是要根据定位目标和定位解释
方法, 选择稳定的, 能反映恶意代码本质的特征作为定位输入, 再辅以其他特征做进一步的输入优化.

6 总结与展望
可解释技术为深度学习检测模型上的安卓恶意代码定位提供了新的解决方法, 对于新兴的基于图神经网
络及代码图特征实现的安卓恶意应用检测模型, 可用的可解释技术定位工作仍然较少. 本文提出了基于敏感

2222

Journal of Software 软件学报 Vol.32, No.7, July 2021

子图输入的通用可解释技术定位方法. 考察安卓恶意代码语法, 语义特征以及图社区结构, 结合检测模型应
用表示图中潜在的应用行为社区结构, 代码间控制流逻辑以及与恶意代码高度相关的敏感 API 节点, 提出
了基于敏感 API 及多关系图特征的敏感子图提取算法, 并在此基础上提出了适用于敏感子图的安卓恶意代
码定位算法. 实验证明了本文的敏感子图提取方法在函数调用图精简工作上的精确性优势以及在恶意代码准
确定位工作上的帮助作用.
在实际应用过程中，本文提出的方法仍存在许多可以继续优化和改进的地方:
（1）本文使用的函数调用图以及控制流图虽然是该领域最常用的一些特征, 但是归根结底仍是通过静态
分析提取的特征, 更容易受到代码混淆、动态加载 so 库等代码保护方案的限制, 导致应用表示结果不完善，
影响定位效果评估;
（2）现有数据集普遍缺乏对恶意代码的标注, 这限制了包括本研究在内的定位工作的效果评估, 后续可
构建针对恶意代码位置的可靠数据集用于代码定位模型的分析验证工作.
在未来工作中,我们将继续提升基于敏感子图的可解释技术在图类检测模型的恶意代码定位问题上的效
果, 推动图神经网络在安卓恶意代码检测领域的应用，更好地辅助专家进行恶意代码的分析和监控工作.
References:
[1]

He Y, Liu Y, Wu L, et al.

MsDroid: Identifying Malicious Snippets for Android Malware Detection[J].

IEEE Transactions on

Dependable and Secure Computing, 2022.
[2]

Luo D, Cheng W, Xu D, et al. Parameterized explainer for graph neural network[J]. Advances in neural information processing
systems, 2020, 33: 19620-19631.

[3]

Warnecke A, Arp D, Wressnegger C, et al.

Evaluating explanation methods for deep learning in security[A].

// 2020 IEEE

european symposium on security and privacy (EuroS&P)[C], Piscataway: IEEE, 2020: 158-174.
[4]

Fan M, Wei W, Xie X, et al.

Can we trust your explanations? sanity checks for interpreters in android malware analysis[J].

IEEE

Transactions on Information Forensics and Security, 2020, 16: 838-853.
[5]

Ying Z, Bourgeois D, You J, et al.

GNNExplainer: Generating explanations for graph neural networks[J].

Advances in neural

information processing systems, 2019, 32.
[6]

Hu YT, Wang SY, Wu YM, Zou DQ, Li WK, Jin H. A slice-level vulnerability detection and interpretation method based on graph
neural network. Ruan Jian Xue Bao/Journal of Software (in Chinese)[J], 2023, 34(6): 0.

[7]

Freitas S, Duggal R, Chau D H.

MalNet: A Large-Scale Image Database of Malicious Software[A].

// Proceedings of the 31st

ACM International Conference on Information & Knowledge Management[C], New York: ACM, 2022: 3948-3952.
[8]

Arp D, Spreitzenbarth M, Hubner M, et al.

Drebin: Effective and explainable detection of android malware in your pocket[A].

//

21st Annual Network and Distributed System Security Symposium[C], Rosten: ISOC, 2014, 14: 23-26.
[9]

Allix K, Bissyandé T F, Klein J, et al.

Androzoo: Collecting millions of android apps for the research community[A].

//

Proceedings of the 13th international conference on mining software repositories[C], New York: ACM, 2016: 468-471.
[10]

Ma Z, Ge H, Wang Z, et al.

Droidetec: Android malware detection and malicious code localization through deep learning[EB/OL].

https://arxiv. org/abs/2002. 03594, 2020-02-10.
[11]

Wu Q, Sun P, Hong X, et al.
Network[A].

[12]

An Android Malware Detection and Malicious Code Location Method Based on Graph Neural

// 2021 The 4th

Ribeiro M T, Singh S, Guestrin C.

" Why should i trust you?" Explaining the predictions of any classifier[A].

// Proceedings of

the 22nd ACM SIGKDD international conference on knowledge discovery and data mining[C], New York: ACM, 2016: 1135-1144.
[13]

Ribeiro M T, Singh S, Guestrin C.

Anchors: High-precision model-agnostic explanations[A].

// Proceedings of the AAAI

conference on artificial intelligence[C], Palo Alto: AAAI Press, 2018, 32(1).
[14]

Guidotti R, Monreale A, Ruggieri S, et al.

Local rule-based explanations of black box decision systems[EB/OL].

https://arxiv.

org/abs/1805. 10820, 2018-05-28.
[15]

Lundberg S M, Lee S I.
systems, 2017, 30.

A unified approach to interpreting model predictions[J].

Advances in neural information processing

2223

郭燕慧 等:一种面向图神经网络安卓恶意代码检测的通用解释定位方法

[16]

Guo W, Mu D, Xu J, et al.

LEMNA: Explaining deep learning based security applications[A].

// proceedings of the 2018 ACM

SIGSAC conference on computer and communications security[C], New York: ACM, 2018: 364-379.
[17]

Iadarola G, Martinelli F, Mercaldo F, et al.
family identification[J].

[18]

Towards an interpretable deep learning model for mobile malware detection and

Computers & Security, 2021, 105: 102198.

Selvaraju R R, Cogswell M, Das A, et al.

Grad-CAM: Visual explanations from deep networks via gradient-based localization[A].

// Proceedings of the IEEE international conference on computer vision[C], Piscataway: IEEE, 2017: 618-626.
[19]

Cai M, Jiang Y, Gao C, et al.

Learning features from enhanced function call graphs for Android malware detection[J].

Neurocomputing, 2021, 423: 301-307.
[20]

Pei X, Yu L, Tian S.

AMalNet: A deep learning framework based on graph convolutional networks for malware detection[J].

Computers & Security, 2020, 93: 101792.
[21]

Wu Y, Shi J, Wang P, et al.

DeepCatra: Learning flow‐and graph‐based behaviours for Android malware detection[J].

IET

Information Security, 2023, 17(1): 118-130.
[22]

Girvan M, Newman M E J.

Community structure in social and biological networks[J].

Proceedings of the national academy of

sciences, 2002, 99(12): 7821-7826.
[23]

Raghavan U N, Albert R, Kumara S.

Near linear time algorithm to detect community structures in large-scale networks[J].

Physical review E, 2007, 76(3): 036106.
[24]

Rosvall M, Bergstrom C T.

Maps of random walks on complex networks reveal community structure[J].

Proceedings of the

national academy of sciences, 2008, 105(4): 1118-1123.
[25]

Blondel V D, Guillaume J L, Lambiotte R, et al.

Fast unfolding of communities in large networks[J].

Journal of statistical

mechanics: theory and experiment, 2008, 2008(10): P10008.
[26]

Gilmer J, Schoenholz S S, Riley P F, et al.

Neural message passing for quantum chemistry[A].

// International conference on

machine learning[C], New York: PMLR, 2017: 1263-1272.
[27]

Liu M, Luo Y, Wang L, et al.

DIG: A turnkey library for diving into graph deep learning research[J].

The Journal of Machine

Learning Research, 2021, 22(1): 10873-10881.

附中文参考文献:
[6] 胡雨涛, 王溯远, 吴月明等.

基于图神经网络的切片级漏洞检测及解释方法[J].

软件学报, 2023, 34(6): 0.


软件学报 ISSN 1000-9825, CODEN RUXUEW
Journal of Software,2024,35(3):1341−1356 [doi: 10.13328/j.cnki.jos.006824]
©中国科学院软件研究所版权所有.

E-mail: jos@iscas.ac.cn
http://www.jos.org.cn
Tel: +86-10-62562563

*

一种预测流程剩余时间的可解释特征分层方法
郭 娜 1, 刘 聪 2,3, 李彩虹 2, 陆 婷 1, 闻立杰 4, 曾庆田 3
1

(山东理工大学 电气与电子工程学院, 山东 淄博 255000)

2

(山东理工大学 计算机科学与技术学院, 山东 淄博 255000)

3

(山东科技大学 计算机科学与工程学院, 山东 青岛 266590)

4

(清华大学 软件学院, 北京 100084)

通信作者: 刘聪, E-mail: liucongchina@163.com

摘

要: 流程剩余时间预测对于业务异常的预防和干预有着重要的价值和意义. 现有的剩余时间预测方法通过深

度学习技术达到了更高的准确率, 然而大多数深度模型结构复杂难以解释预测结果, 即不可解释问题. 此外, 剩余
时间预测除了活动这一关键属性还会根据领域知识选择若干其他属性作为预测模型的输入特征, 缺少通用的特征
选择方法, 对于预测的准确率和模型的可解释性存在一定的影响. 针对上述问题, 提出基于可解释特征分层模型
(explainable feature-based hierarchical model, EFH model) 的流程剩余时间预测框架. 具体而言, 首先提出特征自选
择策略, 通过基于优先级的后向特征删除和基于特征重要性值的前向特征选择, 得到对预测任务具有积极影响的
属性作为模型输入. 然后提出可解释特征分层模型架构, 通过逐层加入不同特征得到每层的预测结果, 解释特征值
与预测结果的内在联系. 采用 LightGBM (light gradient boosting machine) 和 LSTM (long short-term memory) 算法
实例化所提方法, 框架是通用的, 不限于选用算法. 最后在 8 个真实事件日志上与最新方法进行比较. 实验结果表
明所提方法能够选取出有效特征, 提高预测的准确率, 并解释预测结果.
关键词: 流程挖掘; 剩余时间预测; 特征选择; 可解释; 分层模型
中图法分类号: TP311
中文引用格式: 郭娜, 刘聪, 李彩虹, 陆婷, 闻立杰, 曾庆田. 一种预测流程剩余时间的可解释特征分层方法. 软件学报, 2024, 35(3):
1341–1356. http://www.jos.org.cn/1000-9825/6824.htm
英文引用格式: Guo N, Liu C, Li CH, Lu T, Wen LJ, Zeng QT. Explainable Feature-based Hierarchical Approach to Predict Remaining
Process Time. Ruan Jian Xue Bao/Journal of Software, 2024, 35(3): 1341–1356 (in Chinese). http://www.jos.org.cn/1000-9825/6824.
htm

Explainable Feature-based Hierarchical Approach to Predict Remaining Process Time
GUO Na1, LIU Cong2,3, LI Cai-Hong2, LU Ting1, WEN Li-Jie4, ZENG Qing-Tian3
1

(School of Electrical and Electronic Engineering, Shandong University of Technology, Zibo 255000, China)

2

(School of Computer Science and Technology, Shandong University of Technology, Zibo 255000, China)

3

(College of Computer Science and Engineering, Shandong University of Science and Technology, Qingdao 266590, China)

4

(School of Software, Tsinghua University, Beijing 100084, China)

Abstract: Remaining process time prediction is important for preventing and intervening in abnormal business operations. For predicting
the remaining time, existing approaches have achieved high accuracy through deep learning techniques. However, most of these techniques
involve complex model structures, and the prediction results are difficult to be explained, namely, unexplainable issues. In addition, the

*

基金项目: 国家自然科学基金 (61902222); 山东省泰山学者工程专项基金 (ts20190936, tsqn201909109); 山东省自然科学基金优秀青年
基金 (ZR2021YQ45); 山东省高等学校青创科技计划创新团队项目 (2021KJ031); 山东科技大学领军人才与优秀科研团队计划
(2015TDJH102)
收稿时间: 2022-07-18; 修改时间: 2022-09-12; 采用时间: 2022-10-05; jos 在线出版时间: 2023-05-10
CNKI 网络首发时间: 2023-05-11

1342

软件学报 2024 年第 35 卷第 3 期

prediction of the remaining time usually uses the key attribute, namely activity, or selects several other attributes as the input features of
the predicted model according to the domain knowledge. However, a general feature selection method is missing, which may affect both
prediction accuracy and model explainability. To tackle these two challenges, this study introduces a remaining process time prediction
framework based on an explainable feature-based hierarchical (EFH) model. Specifically, a feature self-selection strategy is first proposed,
and the attributes that have a positive impact on the prediction task are obtained as the input features of the model through the backward
feature deletion based on priority and the forward feature selection based on feature importance. Then an EFH model is proposed. The
prediction results of each layer are obtained by adding different features layer by layer, so as to explain the relationship between input
features and prediction results. The study also uses the light gradient boosting machine (LightGBM) and long short-term memory (LSTM)
algorithms to implement the proposed approach, and the framework is general and not limited to the algorithms selected in this study.
Finally, the proposed approach is compared with other methods on eight real-life event logs. The experimental results show that the
proposed approach can select effective features and improve prediction accuracy. In addition, the prediction results are explained.
Key words: process mining; remaining time prediction; feature selection; explainability; hierarchical model

随着企业数字化转型和智能化发展, 企业信息系统中的事件数据得到了高质量的储存. 流程挖掘技术可以从
历史事件日志中提取有价值的信息 [1−3], 帮助企业提高生产效率和产品质量, 避免不必要的风险, 提高企业的竞争
力. 经典的流程挖掘技术是指从现有事件日志中挖掘知识以发现、监控和改进实际流程 [4,5], 建立起了数据挖掘与
业务流程管理之间的联系, 为业务流程管理提供了更加系统、高效的思路和技术. 这些方法主要是通过已有的历
史数据分析业务流程, 但在业务的执行过程中, 对流程未来执行情况的了解更有利于风险的提前掌握、早期预备
和有效防范. 因此, 流程预测性监控是当前流程挖掘领域中的一个研究热点 [6].
流程预测性监控中常见的预测任务包括剩余时间预测、结果预测、下一事件预测、下一事件执行时间预测、
后缀预测以及其他属性预测等 [7,8]. 剩余时间预测任务可以对正在运行实例的未来执行时间做出预判, 帮助用户尽
早调整后续的执行步骤和时间, 避免因超时而带来的风险. 目前对于业务流程剩余时间预测的研究中存在两个局
限: (1) 大多数现有剩余时间预测方法采用活动及从时间戳中提取的时间信息作为输入特征, 或根据领域知识人为
选择特征作为模型输入, 缺少一个有说服性、较为通用的特征选择方法; (2) 现有基于深度学习的预测模型难以解
释输入特征和预测结果的内在联系, 导致了不可解释的问题, 用户难以信任并基于此做出相应决策.
为解决上述局限, 本文提出了一个可解释特征分层框架预测流程剩余时间, 主要贡献为: (1) 提出一种通用的
特征自选取策略, 包括基于优先级的后向特征删除和基于特征重要性值的前向特征选择两部分操作, 在保证准确
率的前提下减少选取特征的数量, 降低后续构建模型的复杂程度; (2) 提出一种可解释的特征分层输入预测模型结
构, 每层接收不同的特征, 并将训练好的模型逐层输出进行可视化, 以解释特征与预测结果的关联关系. 通过
LightGBM (light gradient boosting machine) 和 LSTM (long short-term memory) 算法实现所提框架, 并可选用其他
方法进行替换. 本文基于 8 个真实事件日志对所提方法进行实验对比和验证, 选用其中一个日志进行案例分析, 详
述了特征选择和预测的整个过程.
本文第 1 节介绍相关工作. 第 2 节介绍基础知识. 第 3 节介绍可解释特征分层的流程剩余时间预测框架. 第 4
节展示实验结果和案例分析. 第 5 节对全文进行总结和展望.

1 相关工作
流程预测性监控是针对业务执行过程中未来执行步骤、时间、业务结果及执行资源等各种属性的预测 [9−11],
通过对历史事件发生规律的探索, 预测正在发生业务流程的未来情况. 流程预测性监控包括基于流程感知和非流
程感知两大类方法 [12,13].
基于流程感知的方法是借助流程发现从事件日志中挖掘模型的基础上进行预测 [14,15], 主要结合发现模型和对
历史事件的统计分布. van der Aalst 等人 [16]提出一种带标记变迁系统作为预测模型的剩余时间预测方法, 并研发
支持工具 FSM Analyzer. Rogge-Solti 等人 [17]采用一种随机 Petri 网 GDT_SPN (stochastic Petri net with generally
distributed transitions) 作为业务流程预测模型, 用于预测流程剩余时间. 流程感知方法通过显示模型进行预测, 相
对容易理解和解释, 但所发现的模型质量依赖于不同的挖掘算法, 因此预测结果不一定准确.

郭娜 等: 一种预测流程剩余时间的可解释特征分层方法

1343

非流程感知依赖机器学习方法进行预测, 随着事件日志中数据质量和数量的提升, 深度学习模型也得到了广
泛的应用. Tax 等人 [18]将 LSTM 神经网络应用于业务流程预测中, 并将活动和时间属性作为输入特征, 相比流程感
知方法, 深度学习技术显著地提升了预测的准确率. Bukhsh 等人 [19]将 Transformer 模型应用至剩余时间预测任务
中, 仅采用活动作为输入, 显示了先进模型的优越性. Ni 等人 [20]采用了 encoder-decoder 模型编码活动特征, 获取了
有效的上下文关系. Camargo 等人 [21]采用 LSTM 神经网络构建预测模型, 将资源属性加入到输入特征中并采用了
词向量的编码方式. Pegoraro 等人 [22]将文本属性经过编码作为输入特征之一, 进一步提高了预测的准确率. 可以看
出, 除了不同的编码方式, 特征选取也是影响预测模型准确率的重要因素.
特征选择是机器学习方法中的数据预处理步骤, 可以对原始特征进行降维, 降低模型的复杂性, 消除干扰特
征, 是提高算法性能的重要手段 [23]. 特征选择主要包含过滤式、包裹式和嵌入式 [24]. 许行等人 [25]采用小样本数据,
提出一种基于互信息的过滤式特征选择方法来降低数据的维度, 但过滤式通常默认特征间相互独立, 因此在一些
情况下计算出来的相似度并不准确, 进而影响学习模型的准确率. 黄南天等人 [26]提出分类回归树的特征选择与最
优决策树构建方法, 采用嵌入式特征选择方法, 获取特征重要性排序, 确定最优特征子集, 并进行剪枝操作获得最
优分类树. 李占山等人 [27]提出一种基于 LightGBM 的新型包裹式特征选择算法, 采用 LightGBM 对原始特征构建
迭代提升树模型并对特征重要性进行度量, 结合前向搜索策略对特征进行选择, 能够同时兼顾特征子集的计算效
率和学习精度.
相比于流程感知和传统机器学习方法来说, 虽然深度学习等技术不断被提出和应用, 显著提升了预测的准确
率, 但可解释性不强难以理解, 企业不能完全信任并基于此做出决策. Sindhgatta 等人 [28]将机器学习可解释技术
LIME 应用至 XGBoost 算法构建的预测模型中, 对基于 XGBoost 算法的预测结果提供了一定的后验解释. Harl 等
人 [29]将 GGNN 应用于业务流程预测中, 通过可视化案例执行过程中不同活动对预测的影响程度, 使预测具有可解
释性. Hsieh 等人 [30]设计了一个扩展的 DiCE 反事实方法, 支持在轨迹的不同阶段推导具有里程碑感知的反事实条
件, 以促进可解释性. 当前的可解释工作主要是通过后验解释和流程可视化的方法来解释不同活动和输入特征对
预测结果的影响, 但难以在轨迹级别上解释各特征值与预测结果的关系.
综上, 本文针对预测模型输入特征的选择问题和预测模型可解释的问题进行探索, 尝试找到一种通用的包裹
式特征选择方法, 并为多特征输入的业务流程剩余时间预测任务构建可解释的预测模型.

2 基础知识
事件日志是一组轨迹的集合, 每条轨迹都表示一次业务流程的执行. 每条轨迹由一系列事件组成, 每个事件都
表示一个活动的执行. 每个活动表示业务流程中的一个步骤.
定义 1 (事件, 属性). 设 E 为事件空间, 即所有可能的事件标识符的集合, 事件可以由不同的属性来描述. 设
AN 为属性名的集合, 对于任意事件 e∈E 和属性名 n∈AN : #n(e) 是事件 e 的属性 n 的值.
设 UA 为活动空间, #act(e)∈UA 表示事件 e 相关联的活动.
定义 2 (轨迹, 案例和事件日志). 一条轨迹 δ∈E*是一个有限活动序列, 其中每个事件只出现一次. 设 C 为案例空
间, 案例同事件一样也包含属性, 对于任意 c∈C 和属性名 n∈AN: #n(c) 是案例 c 的属性 n 的值, 每个案例包含一
个特殊的属性, 即轨迹#trace(c)∈E*. 一个事件日志 L⊆C 是一组轨迹的集合, 其中每个事件在整个日志中仅出现一次.
定义 3 (轨迹前缀). tpk(δ) 是轨迹 δ 的轨迹前缀, 是由 δ 的前 k 个元素组成的序列.
表 1 展示了 BPIC2012 日志的部分内容, 仅显示了两条轨迹, 包含 16 个事件, 每个事件有唯一标识 id 和一些
属性. 例如 e4 活动 A_PREACCEPTED 的实例, 它发生于 2011/10/1 15:46, 由编号 112 人员执行, 其贷款金额为 13 500.
案例 173703 的轨迹 δ = 〈e1, e2, e3, e4, e5, e6, e7〉, k = 3 时的轨迹前缀为 tp3(δ) = 〈e1, e2, e3〉.
业务流程剩余时间预测的含义、目的和意义为: “我的业务还需要多长时间才能完成?”, 为确保业务在需要的
时间内完成办理, 可根据预测结果进行调整, 若出现延期情况可及时采取措施, 不影响后续进展. 剩余时间预测任
务的输入是已发生的事件序列, 输出是从当前时刻到业务完成的预测时长. 一般来说用户可处于业务流程的任意
一个事件节点执行预测任务.

软件学报 2024 年第 35 卷第 3 期

1344

表1
Case id

173703

173709

...

BPIC2012 部分事件日志

Event id
e1

Start time

Complete time

Activity

AMOUNT_REQ

Resource

2011/10/1 15:45

2011/10/1 15:45

A_SUBMITTED

13 500

112

e2

2011/10/1 15:45

2011/10/1 15:45

A_PARTLYSUBMITTED

13 500

112

e3

2011/10/1 15:46

2011/10/1 15:46

A_PREACCEPTED

13 500

112

e4

2011/10/1 15:46

2011/10/1 15:46

A_PREACCEPTED

13 500

112

e5

2011/10/1 17:37

2011/10/1 17:40

W_Completeren aanvraag

13 500

10 912

e6

2011/10/1 18:40

2011/10/1 19:02

W_Completeren aanvraag

13 500

10 912

e7

2011/10/1 19:02

2011/10/1 19:02

A_CANCELLED

13 500

10 912

e8

2011/10/1 15:57

2011/10/1 15:57

A_SUBMITTED

11 000

112

e9

2011/10/1 15:57

2011/10/1 15:57

A_PARTLYSUBMITTED

11 000

112

e10

2011/10/1 15:58

2011/10/1 15:58

A_PREACCEPTED

11 000

112

e11

2011/10/1 15:58

2011/10/1 15:58

A_PREACCEPTED

11 000

112

e12

2011/10/1 16:26

2011/10/1 16:27

W_Completeren aanvraag

11 000

10 912

e13

2011/10/1 17:40

2011/10/1 17:43

W_Completeren aanvraag

11 000

10 912

e14

2011/10/10 23:42

2011/10/10 23:43

W_Completeren aanvraag

11 000

10 982

e15

2011/11/1 16:15

2011/11/1 16:15

W_Completeren aanvraag

11 000

10 982

e16

2011/11/1 16:15

2011/11/1 16:15

A_CANCELLED

11 000

112

...

...

...

...

...

...

3 可解释特征分层的流程剩余时间预测框架
本节介绍可解释特征分层的流程剩余时间预测框架, 其方法概述如图 1 所示, 共分为以下 3 个阶段.
(1) 特征选择: 将事件日志作为输入, 自动选取出对剩余时间预测任务积极有效的特征.
(2) 特征编码: 将 (1) 中的已选特征作为输入, 针对活动特征选用 Word2Vec 的编码方式, 针对其他分类特征根
据分类数的不同, 选用随机向量或基于索引的编码方式.
(3) 构建、训练可解释特征分层模型: 将编码后的特征按重要性排序依次作为模型每一层的输入, 自动构建可
解释特征分层模型, 训练预测模型.

(2) 特征编码

(1) 特征选择
事件日志

已选特征
···
隐含层 1

可解释特征分
层预测模型

(3) 构建、训练可解释
特征分层模型
FC
隐含层 2

图1

数据集

可解释特征分层的剩余时间预测框架概述

3.1 特征自选取策略
特征选择的目的是在日志中选取对预测任务有积极影响的属性作为模型输入特征, 这一过程不仅能够去除具
有消极影响的属性, 还可以减少输入特征保证模型的简洁性. 由于不同特征对预测结果具有不同程度的影响, 所以

郭娜 等: 一种预测流程剩余时间的可解释特征分层方法

1345

需要判断是否将某特征保留或删除. 另外, 不同的特征组合可能也会影响到预测结果的准确性. 因此, 需要提供一
种计算特征重要性的方法和特征自选取策略. 为判断特征自选取策略的有效性, 本文采用平均绝对误差 (mean
absolute error, MAE) 对剩余时间预测任务进行评价, MAE 值是回归模型常用的评价指标, 计算真实值与预测值的
平均绝对误差, MAE 值越小说明预测的准确率越高, 其计算公式如下:
n
1∑
MAE =
|yi − ŷi |
n i=1

(1)

其中, y 为真实值, ŷ 为预测值.
每次删除或增加某特征后, 需通过训练预测模型得到 MAE 值来判断某特征对预测的影响程度. 决策树是通过
计算各节点的信息增益进行节点分裂, 因此特征的信息增益越高, 说明该特征在进行决策时越重要信息. 信息增益
表示了各特征的重要程度, 为特征选择提供了初始参考指标. LightGBM 是一种高效的梯度提升决策树算法框架,
它采用了基于梯度的单侧抽样和互斥特征捆绑方法, 有效提高了算法的训练速度同时保证了算法的预测精度. 因
此, 本文选用 LightGBM 实现特征选择阶段的预测模型 [31].
本文提出了一种通用的特征自选取策略, 共包含两个部分.
第 1 部分是基于优先级的后向特征删除策略, 主要用于筛除对预测任务具有消极影响的特征 (即增加该特征
后降低了预测准确率). 考虑到两个或以上的某些捆绑特征可能共同对预测结果产生积极影响 (即增加该特征后提
高了预测准确率), 而其中一个特征单独加入会造成消极影响, 故采取后向的删除策略, 以避免破坏特征间的组合
关系. 然而, 当特征集合中包含两个或以上的消极特征时, 删除一个消极特征可能会造成暂时的积极影响, 因此需
要进行多轮逐步特征选择. 为特征设置优先级用于控制后向特征删除的轮数, 并且保证活动这一关键标志性特征
不被删除.
在筛除过程中计算各特征删除前后的 MAE 差值, 作为新的特征重要性评判标准, 得到重要性值排序. 将决策
树算法得到的特征重要性值集合 Idt 作为初始参考标准, 将全部属性作为初始已选特征集合 F={f1,…, fn}, 在迭代
筛除过程中计算 fi∈F 删除前后的 MAE 差值, 作为特征重要性评判标准, 记为 IMAE(fi), 计算公式如下:
I MAE ( fi ) = MAE(F) − MAE(F − { fi } )

(2)

其中, MAE(F) 表示采用 F 所得的平均绝对误差, MAE(F – {fi}) 表示采用删除 fi 后的 F 集合得到的平均绝对误差.
基于优先级的后向特征删除策略详述步骤如下.
Step 1. 设置活动为最高优先级, 其他特征为 0, 采用已选特征集合 F, 训练预测模型得到 Idt.
Step 2. 删除 F 中优先级最低且在 Idt 集合中值最低的特征 fi.
Step 3. 将删除特征 f i 后的 F 重新训练预测模型, 计算预测模型在测试集上的 MAE 值, 得到 I MAE (f i ), 若
IMAE(fi)<0 说明 fi 为消极特征, 否则增加其优先级并撤回删除操作, 若 IMAE 集合中的所有值均大于 0 说明 F 中无消
极特征, 执行 Step 4, 否则迭代执行 Step 2.
Step 4. 返回预测结果最佳的特征组合 F 和各特征的重要性值集合 IMAE.
第 2 部分是基于特征重要性值的前向特征选择策略. 由于第 1 部分避免破坏特征组合导致选取出的特征数量
过多, 影响后续预测模型的简洁性和训练效率, 且许多特征仅对预测结果有轻微影响. 因此根据第 1 部分重要性值
排序前向选择特征, 以达到在所选特征数尽可能少的情况下, 保证预测的准确率. 第 1 部分得到的 F 作为待选特征
集合, 设置新的已选特征集合 NF, 初始仅包含活动. 不同特征组合作为模型输入训练、测试所得的 MAE 值记录在
集合 FCMAE 中, FCMAE(NF) 表示特征组合 NF 对应的 MAE 值. 基于特征重要性值的前向特征选择策略的具体步骤
如下.
Step 1. 采用 NF 作为输入重新训练预测模型, 计算预测模型在测试集上的 MAE 值.
Step 2. 若 F 为空执行 Step 3, 否则从 F 中选取 IMAE 最大的特征 fi 移至 NF, 采用 NF 作为输入重新训练预测模
型, 计算预测模型在测试集上的 MAE 值, 保存至 FCMAE(NF), 迭代执行 Step 2.
Step 3. 返回 FCMAE 中预测结果最佳的特征组合和 MAE 值.

软件学报 2024 年第 35 卷第 3 期

1346

3.2 特征编码
特征编码首先要将属性值转化为模型可计算的数据类型, 其次编码应能高质量地表示上下文之间的内在关
系. 针对分类特征, 简单的基于索引的编码和 one-hot 编码方式都存在一些弊端. 基于索引的编码方式仅由一位整
数组成, 难以有效表示特征值并且不利于模型参数的训练; one-hot 编码方式是由 0、1 组成的向量, 向量的位数即
为特征的分类数, 当表示某一特征值时, 表示该特征值的位上的值为 1 其余均为 0. One-hot 编码虽能有效区分和
代表不同类别, 但本质上是一个稀疏矩阵, 不能表示各类间的关系. 而且, 随着分类数的增加位数会不断增加, 影响
预测模型的训练效果. 因此需要更高质量的编码方式来表示特征.
编码方式主要来自自然语言处理领域的词嵌入技术, 包括 Word2Vec、FastText、LSA、GloVe、ELMO、
GPT 和 BERT 等 [32−34]. 其中 Word2Vec 和 FastText 基于局部语料, 优化效率高; LSA 利用全局语料特征, 但求解计
算复杂度大; GloVe 基于全局预料, 结合了 LSA 和 Word2Vec 的优点; ELMO、GPT 和 BERT 采用了动态特征, 可
以解决一词多意的问题. 在业务流程的预测性监控之中, 特征值的分类数不会像自然语言的词汇量那样庞大,
Word2Vec 即可满足特征值的表示. Word2Vec 包括 CBOW 和 Skip-gram 两种训练方式, CBOW 是采用上下文语
料训练当前词, Skip-gram 是采用当前词训练上下文.
在本文的研究场景中, 根据先前发生的事件及其属性进行业务流程剩余时间的预测. 结合 CBOW 词向量
训练方法, 本文通过先前发生事件中的活动序列训练预测下一事件的活动, 以得到活动的特征编码; 其他分类
特征难以判断其上下文关系, 则采用随机初始化向量编码的方式, 若特征值的类别小于 5 则采用基于索引的编
码方式.
活动的编码原理如图 2 所示, 输入层为最近的 n 个事件活动的 one-hot 向量表示, 例 n=3, 近 3 个事件为 a1, a2,
a4, 则 one-hot 向量表示为 [1, 1, 0, 1,…, 0, 0], 隐藏层的单元个数即为向量编码的维数, 输出层为下一事件的 onehot 向量表示. 通过计算预测的准确率调整隐含层单元个数, 使之调整到最佳表示的向量维数, 预测的准确率越高
说明该向量表示事件的能力越强, 最后将输入层与隐含层之间的参数权重作为词向量矩阵, 每行表示一个活动的
向量.
x1

x2

···

xN−1

xN

1

1

···

0

0

输入层

词向量矩阵 N×M
m1

···

mM

隐含层

0

0

···

1

0

y1

y2

···

yN−1

yN

图2

输出层

活动编码原理示意图

3.3 可解释特征分层模型
神经网络广泛应用于下一事件和剩余时间等预测任务中, 由各层神经单元的权值、偏置以及激活函数组成,
本质上可以通过各节点的权重解释特征对预测结果的影响, 但当构建的神经网络结构复杂、参数繁多时, 难以说
明输入特征与预测结果的关联关系. 本文提出一种特征分层的模型结构, 将第 3.1 节得到的最优特征组合按照重
要性值顺序输入至模型中, 并将每层的输出进行可视化展现, 以解释各特征对预测结果的影响.
特征分层输入的模型结构示例如图 3 所示, 若所选特征按重要性值排序为活动、执行时间和资源, 则依顺序

郭娜 等: 一种预测流程剩余时间的可解释特征分层方法

1347

构造分层模型, 在输入层 1 中只将活动经第 3.2 节得到的表示向量作为输入, 经隐含层 1 后得到输出层 1 预测的
剩余时间; 在输入层 2 中将输出层 1 和当前事件的执行时间共同作为输入, 由于执行时间是数值特征, 则先经过一
个全连接层 (full connection, FC) 以获取丰富的信息再输入至 LSTM 层中, 经隐含层 2 后得到输出层 2 预测的剩余
时间; 在输入层 3 中将输出层 2 和当前事件的资源共同作为输入, 资源作为分类特征输入的是其向量表示, 经隐含
层 3 后得到输出层 3 为最终预测的剩余时间.
输入层 1

活动

···

LSTM
第1层

隐含层 1

FC
FC

输出层 1
执行时间

输入层 2

FC
第2层
隐含层 2
资源
输出层 2

···

输入层 3

隐含层 3

第3层

输出层 3

图3

可解释特征分层模型的结构示例

每层的输出是采用当前已输入的特征得到的剩余时间预测结果, 可将每层结果可视化到二维平面, 观察每加
入一层特征对预测结果的影响.
需要说明模型结构中的隐含层可采用任意的网络结构进行替换, 本文通过长短期记忆网络 LSTM 和全连接
网络进行实现. LSTM 是一种特殊的循环神经网络 (recurrent neural network, RNN)[35,36], 是 RNN 的一种变体, RNN
具有记忆功能是由于其独特的结构和工作原理, 当前时刻的输出是由当前时刻的输入和上一时刻的状态共同决定
的, 因此能够对前面数据产生记忆. LSTM 能够解决 RNN 存在的长期依赖问题, 善于处理和预测时间序列中间隔
较长的重要事件, 具有长期记忆功能. 适合流程预测中序列结构的应用, 也是目前流程预测中应用最多的神经网络
单元结构. 我们可将预测节点的整个轨迹前缀作为模型输入, 以获取更丰富的信息.
LSTM 具有长期记忆取决于它的单元结构, 如图 4 所示, 每一个时序的特征输入都经过这些单元, 单元结构中
引入了门概念, 通过控制遗忘门、输入门和输出门的状态信息计算出当前时刻的预测值. 其中 t 表示当前时刻, x
表示输入, h 表示输出, C 表示单元状态, σ 表示 Sigmoid 层, tanh 表示双曲正切层.
预测值的计算过程如下.
第 1 步, 由遗忘门的 Sigmoid 层决定从上一单元状态中遗忘多少信息, ht–1 和 xt 通过 Sigmoid 激活函数得到
的 ft 为 0 到 1 之间的一个数字, 1 代表完全保留信息, 0 代表完全丢弃信息, ft 的计算公式为:
ft = σ(W f h ht−1 + W f x xt + b f )

(3)

软件学报 2024 年第 35 卷第 3 期

1348

第 2 步, 由输入门的 Sigmoid 层决定当前时刻的输入有哪些保存到单元状态, 计算 it, 由 tanh 层计算当前状态
候选值 C̃t , 然后将遗忘门的结果乘以上一时刻状态再加上输入门的结果乘以当前状态候选值得到当前时刻的状
态信息 Ct, 计算公式如下:
it = σ(Wih ht−1 + Wix xt + bi )

(4)

C̃t = tanh(WCh ht−1 + WCx xt + bC )

(5)

Ct = ft Ct−1 + it C̃t

(6)

第 3 步, 由输出门的 Sigmoid 层决定要输出哪些单元状态, 计算 ot, 将单元状态通过 tanh 函数归一化至 [−1, 1],
然后与输出门的结果相乘, 得到当前时刻的输出值 ht, 计算公式如下:
ot = σ(Woh ht−1 + Wox xt + bo )

(7)

ht = ot tanh(Ct )

(8)

以上可得出在 t 时刻的预测值 ht, 公式 (3)–(8) 中, 权重矩阵 Wfh、Wih、WCh、Woh、Wfx、Wix、WCx、Wox 和偏
置 bf、bi、bC、bo 为 LSTM 需训练的 12 组参数.
ht

Ct−1

Ct
tanh
ft

it

σ

σ

~
Ct
tanh

ot
σ

ht−1

ht

xt

图4

LSTM 单元结构

在模型的训练过程中, 每一个批次都逐层训练, 即先训练第 1 层, 计算输出层 1 的预测值与真实值的 loss, 进
行反向传播调整网络结构参数, 然后固定第 1 层的参数, 训练第 2 层, 计算输出层 2 的 loss, 以此类推, 直至每一层
都得到充分的训练.

4 实验设置和结果分析
本节介绍了测试本文方法的实验设置和所用数据集, 对本文所提方法的实验过程和结果进行了展示和分析,
并将实验结果与其他方法进行比较, 以 Helpdesk 事件日志为例进行模型预测的可视化展现. 本文方法在 Python 3.7
环境下实现, 所使用的计算机的配置为: Windows 11 操作系统, AMD Ryzen 7 5800H with Radeon Graphics @
3.20 GHz 处理器和 16.00 GB 的内存, 代码开源到 GitHub (https://github.com/gn874682003/Hierarchical-PredictionFramework).
4.1 实验设置
(1) 数据集划分: 本文为每个事件日志训练一个预测模型. 训练集和测试集根据 8:2 的比例进行划分, 将事件
日志按时间划分为 5 部分, 每一部分选取 1/5 的轨迹作为测试集, 其余轨迹为训练集.
(2) 特征自选取: 在特征选取阶段, 除了现有属性还通过时间戳拓展出 6 个数值属性, 分别为执行时间 duration、
总执行时间 allDuration、月份 month、日期 day、星期 week 和小时 hour. 其中 duration 是指上一个事件结束到当
前事件结束的时长, allDuration 是指从该案例开始到当前事件结束的时长. LightGBM 算法需设置轨迹前缀长度
为 1 的数据集训练和测试预测模型.

郭娜 等: 一种预测流程剩余时间的可解释特征分层方法

1349

(3) 特征编码: 词向量的训练需设置固定长度的上文, 本文从前缀长度分别为 1、3、5 的设置中选择在
CBOW 训练阶段预测下一事件准确率最高的设置, 从而确定特征编码阶段的前缀长度为 3.
(4) 可解释特征分层预测模型: LSTM 可将所有轨迹前缀信息循环输入至网络单元, 故选取可变长的前缀. 适
合循环神经网络的可变前缀长度有两种生成方式, 例如一条事件的执行轨迹为 [e1, e2, e4, e3, e6]. 第 1 种是以整条
轨迹作为一组数据, 即输入为 [e1, e2, e4, e3, e6], 输出为每个事件的真实剩余时间序列 [4.2, 3.1, 2.9, 1.2, 0]. 第 2 种
是切分轨迹划分数据, 可得到 5 组数据, 输入包含 [e1], [e1, e2], [e1, e2, e4], [e1, e2, e4, e3], [e1, e2, e4, e3, e6], 输出则分
别对应 [4.2], [3.1], [2.9], [1.2], [0]. 两种划分方法均可实现在轨迹的任一事件节点执行预测任务, 在事件日志轨迹
数量较少的情况下可采用第 2 种划分方式, 增加训练集的规模, 能够更充分地训练预测模型. 本文经过实验应用神
经网络构建预测模型的超参数设置如下: 学习率为 0.001, 优化算法选用 Adam, 批处理大小为 100, 迭代次数为
300, 隐含层节点数以及向量维数的设置针对不同的事件日志进行调整.
4.2 数据集
本文实验使用 8 个来自 4TU Center for Research 的公开事件日志数据集, 来验证本文所提方法. Helpdesk 事件
日志涉及一家意大利软件公司帮助台的票务管理流程 (https://doi.org/10.4121/uuid:0c60edf1-6f83-4e75-93674c63b3e9d5bb); BPIC2012 来自 2012 年 BPI 挑战赛, 是某财政机构贷款 申请流程的事件日志 (https://doi.org/
10.4121/uuid:3926db30-f712-4394-aebc-75976070e91f); BPIC2015 来自 2015 年 BPI 挑战赛, 共包含 5 个事件日志,
分别由 5 个荷兰市政府提供, 数据包含大约 4 年期间所有建筑许可证申请 (https://doi.org/10.4121/uuid:31a308efc844-48da-948c-305d167a0ec1); Production 事件日志来自某生产车间 2012 年 1–3 月部分产品的生产流程数据
(https://doi.org/10.4121/uuid:68726926-5ac5-4fab-b873-ee76ea412399).
日志的基本统计属性包括活动个数、轨迹长度、变体数量等, 表 2 给出了 8 个事件日志的统计属性. 对于数
据量较小的 Production 日志采用可变前缀长度的第 2 种切分轨迹划分数据的方式生成训练集和测试集, 该数据集
经过实验设置前缀长度范围的取值为 [1, 10], 因为过长的轨迹前缀在迭代过程中会遗忘早期信息, 对准确率的提
升不明显, 还会影响训练效率, 第 1 种划分方式采用了全部前缀, 因此无需设置前缀长度范围.
表2

事件日志的统计属性

数据集

轨迹数

事件数

活动数

属性数

变体数

Helpdesk
BPIC2012
BPIC2015_1
BPIC2015_2
BPIC2015_3
BPIC2015_4
BPIC2015_5
Production

4 580
13 087
1 199
832
1 409
1 053
1 156
225

21 348
262 200
52 217
44 354
59 681
47 293
59 083
4 543

14
36
398
410
383
356
389
55

15
6
28
27
28
28
28
14

226
4 366
1 170
828
1 349
1 049
1 153
221

事件数 (每条轨迹)
最小值
2
3
2
1
3
1
5
1

平均值
5
20
44
53
42
45
51
20

最大值
15
175
101
132
124
116
154
175

4.3 特征自选取策略的实验对比结果
为验证特征自选取策略所选特征对业务流程预测任务的有效性, 采用活动属性、全部属性以及本文方法所选
属性分别作为输入特征来进行对比实验. 实验的预测结果如表 3 所示, activity 表示仅选用“活动”作为输入特征,
all 表示选取所有属性作为输入特征, FeaSelPart1 表示经过特征自选取策略第 1 部分所选属性作为输入特征,
FeaSelPart2 表示经过整个策略所选属性作为输入特征. 本文的 MAE 值以天为单位, 表示所有流程轨迹前缀的真实
剩余时间与预测剩余时间的平均绝对误差.
由表 3 可以看出, 在 8 个事件日志中除 BPIC2015_3 日志外, 其他日志选用 All 比单一使用 Activity 活动属性
作为预测模型的输入特征会大幅提高预测的准确率, BPIC2015_3 日志中可能存在某些属性对预测具有干扰作用.

软件学报 2024 年第 35 卷第 3 期

1350

通过 FeaSelPart1 筛除消极特征后, MAE 值在各事件日志中均有显著的降低. FeaSelPart2 在尽量保证预测准确率
不大幅下降的情况下减少所选的特征数. 可以看出选取后的特征组合不仅在多数日志中 MAE 值有所下降, 还显著
减少了所选特征数量, 为后续工作提供了简洁、有效的数据支持, 另外特征选择的执行时间较短, 可以保证执行
效率.
表3
数据集
Helpdesk
BPIC2012
BPIC2015_1
BPIC2015_2
BPIC2015_3
BPIC2015_4
BPIC2015_5
Production

不同特征组合在预测任务上的表现对比

Activity

All

MAE (天)

MAE (天) 特征数

6.575
7.483
40.143
80.278
23.590
63.098
49.094
18.312

4.917
7.207
32.062
63.224
26.811
53.445
43.677
12.919

18
9
22
22
22
22
22
16

FeaSelPart1
特征选择策略的
MAE (天) 特征数
执行时间 (s)
4.684
10
16
7.192
8
17
26.364
14
38
62.405
18
53
19.001
9
26
49.451
13
41
37.613
15
44
12.550
11
11

FeaSelPart2
特征选择策略的
MAE (天) 特征数
执行时间 (s)
4.659
8
1
7.192
8
4
30.505
8
3
59.472
8
2
18.594
5
3
48.475
5
1
38.579
8
8
12.469
7
1

4.4 可解释特征分层模型的实验对比结果
为验证分层模型和编码方式对预测任务的影响, 设置了如表 4 所示的对比实验并展示了测试结果. InAct 表示
采取基于索引的编码方式并仅使用活动作为输入特征, InEFH 表示采取基于索引的编码方式并使用 FeaSelPart2
所选属性作为输入特征, EembAct 表示采取第 3.2 节所述的编码方式并仅使用活动作为输入特征, EFH 为本文所
提方法.
表4

多属性、编码和分层在预测任务上的 MAE 表现对比 (天)

数据集
Helpdesk
BPIC2012
BPIC2015_1
BPIC2015_2
BPIC2015_3
BPIC2015_4
BPIC2015_5
Production

InAct
6.587
7.332
32.186
76.240
21.564
59.308
40.397
17.584

InEFH
6.411
7.079
28.125
71.748
18.862
52.940
33.605
18.004

EembAct
4.582
7.686
26.998
64.132
17.993
50.973
35.278
13.220

EFH
4.270
6.986
25.493
66.993
17.323
50.227
33.464
11.510

由表 4 可得以下结论, 在多数日志中 InEFH 的 MAE 值均小于 InAct, 说明本文策略所选特征能够提高预测的
准确率. 在多数日志中 EembAct 的预测结果优于 InEFH 和 InAct, 说明本文应用的活动编码方式能够有效获取活
动间的关系. EFH 在除 BPIC2015_2 的日志上均取得了最好的结果, 说明分层模型的构造能够有效利用每一个特
征值, 达到较好的结果. 然而, BPIC2015_2 日志的 EFH 方法相较 EembAct 方法的 MAE 值有所升高, 并且表 3 中
依托于 LightGBM 的 FeaSelPart2 在 BPIC2015_2 和 BPIC2015_4 日志中得到的 MAE 值比 EFH 更低, 这可能是由
于某些特征组合需同时发挥作用, 而分散在不同的层次之间则产生消极影响. 本文还未考虑将具有组合作用的特
征放入同一层内, 这可作为未来的研究方向.
表 5 给出了本文所提方法与其他方法的比较结果, 其中 LSTMNN[18]是 Tax 等人提出的基于 LSTM 的业务过
程预测模型, 采用了活动和时间属性作为输入特征, LSTM 是最早应用深度学习模型到流程预测中的方法. Process
Transformer[19]是 Bukhsh 等人提出的基于 Transformer 的业务流程预测模型, 该方法添加了时间信息作为输入, 是
先进深度学习方法在流程预测领域中的应用. Auto-encoded[20]是对活动前缀进行编码以获取丰富的上下文关系,
该方法只采用活动这一关键属性.

郭娜 等: 一种预测流程剩余时间的可解释特征分层方法

表5
数据集
Helpdesk
BPIC2012
BPIC2015_1
BPIC2015_2
BPIC2015_3
BPIC2015_4
BPIC2015_5
Production

1351

可解释模型与其他算法的 MAE 表现对比 (天)

LSTMNN[18]
62.089
61.175
49.648
127.834
61.190
151.565
120.750
521.296

Process Transformer[19]
5.419
5.968
39.472
87.622
17.544
43.777
51.847
14.992

Auto-encoded[20]
8.733
8.4268
38.324
88.895
17.588
39.619
38.270
11.775

EFH
4.270
6.986
25.493
66.993
17.323
50.227
33.464
11.510

由表 5 可以看出, 本文所提方法在多数事件日志中表现最好, 这是由于选取了更多的有效属性, 为预测模型提
供了丰富的已知信息, 提高了剩余时间预测结果的准确性. 在 BPIC2012 日志中 Process Transformer 预测模型表现
最好, 这是由于该日志的规律性较强, 受其他属性影响较少, 并且数据规模较大能充分训练 Transformer 模型. 在
BPIC2015_4 日志中 Auto-encoded 预测表现最好, 通过轨迹执行时间分析可得该日志在 BPIC2015 系列日志中异
常值最少, 受其他属性影响较少, 通过自编码技术能够有效区分不同轨迹前缀, 得到较好的结果. 而在其他日志上
两种方法的表现参差不齐, 说明不同类型的事件日志适合不同的预测模型. 本文添加了其他有效特征使得在
LSTM 这一较为基础的模型上也能表现良好, 说明了特征选择和分层结构对于提升剩余时间预测准确率的有效
性. 本文提出的是一种通用的可解释特征分层预测框架, 实现方法可使用其他算法进行替换, 若采用拟合能力更强
的预测模型可能会得到更好的预测结果.
4.5 案例分析
本节以 Helpdesk 事件日志为例, 展示了特征选择过程并可视化解释了预测结果. 图 5 是经过特征选取策略第
1 部分之后根据 IMAE 画出的特征重要性图, 说明了各特征对预测结果的影响程度, 提供了全局解释. 图中只显示了

特征

对预测具有积极影响的属性, 活动作为关键标志特征, 不需要计算其特征重要性.
allDuration
seriousness_2
variant index
day
resource
product
duration
hour
responsible_section

0.739 087
0.422 930
0.099 257
0.050 192
0.028 747
0.024 746
0.021 999
0.005 007
0.003 089
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7
重要性值

图5

Helpdesk 的特征重要性值

特征选择策略的第 2 部分筛选过程如表 6 所示, 表 6 中 No. 0 是策略第 1 部分选取的特征组合和预测结果,
No. 1–10 展示了策略第 2 部分每次迭代使用的特征组合和预测结果. 策略第 2 部分返回 No.8 特征组合为最终结
果, 相比于 No. 0 去除了 hour 和 responsible_section 得到了最小的 MAE 值, 可以看出这两个属性单独去除都会增
加预测误差, 可两个属性同时去除则可以减少预测误差, 说明特征间具有组合关系.
根据特征选择策略获得的特征组合 No. 8 作为输入训练分层模型, 模型根据特征重要性值的排序逐层构建网
络, 测试结果如图 6 所示. 可以看出预测误差整体呈逐层下降的趋势, 但在 variant index 和 product 的属性层上略
有上升, 最后一层得到误差最小值 4.27.
图 7 随机抽取了测试集中来自 4 条轨迹中 4 个事件的预测结果, 并可视化出了每层的预测值. 可以看出
allDuration 这一属性对预测有较大的积极影响, 它总能得到比仅用 activity 更接近真实值的结果, 由图 7(a) 和图 7(c)

软件学报 2024 年第 35 卷第 3 期

1352

可以看出 allDuration 较大时剩余时间会相对减少, 所以它的预测值与第 1 层预测值相比会减少, 而图 7(b) 和图 7(d)
中 allDuration 非常小则预测往往会偏大. 通过图 7(c) 和图 7(d) 对比来看更能验证这一结论, 图 7(c) 和图 7(d) 中
的 activity 均为“take in charge ticket”, 预测值相同, 但真实值相差较大, 通过不同的 allDuration 值得到了更接近真
实值的预测结果. 其他属性也具有不同程度的积极影响, 每个属性根据当前特征值的不同改变预测结果和影响趋
势 (例如图中位于真实值的上下两侧). 由于该真实日志的发布者对敏感隐私数据进行了脱敏处理, 许多特征值由
代号来进行表示. 若在具体场景中知道 Value1 等代号所指的具体含义, 则可对预测结果产生更丰富的解释. 可以
看出, 我们的方法能够在事件层面上解释每一次预测的输入特征值对预测结果的影响.
表6

Helpdesk 特征组合预测结果

MAE (天)

MAE (天) 特征数
特征组合
No.
0 activity, allDuration, seriousness_2, variant index, day, resource, product, duration, hour, responsible_section 4.684
10
1
activity
6.575
1
2
activity, allDuration
5.223
2
3
activity, allDuration, seriousness_2
4.786
3
4
activity, allDuration, seriousness_2, variant index
4.744
4
5
activity, allDuration, seriousness_2, variant index, day
4.722
5
6
activity, allDuration, seriousness_2, variant index, day, resource
4.704
6
7
activity, allDuration, seriousness_2, variant index, day, resource, product
4.699
7
8
activity, allDuration, seriousness_2, variant index, day, resource, product, duration
4.659
8
9
activity, allDuration, seriousness_2, variant index, day, resource, product, duration, hour
4.687
9
10
activity, allDuration, seriousness_2, variant index, day, resource, product, duration, responsible_section
4.689
9

7.5
7.0
6.5
6.0
5.5
5.0
4.5
4.0

7.028

4.494

4.777
4.349
第1层
activity

第2层
第3层
第4层
第5层
allDuration seriousness_2 variant index
day
特征

图6

剩余时间 (天)

4.480
4.425

4.415
第6层
resource

4.270
第7层
product

第8层
duration

Helpdesk 分层模型的逐层预测结果

33
32
31
30
29
28
27
26
25

预测值
真实值

第 1 层:
activity
=Wait

第 2 层:
第 3 层:
第 4 层:
第 5 层:
第 6 层:
allDuration seriousness_2 variant
day=11
resource=
=10.0387
=Value 2
index=16
Value 12
特征
(a) “Case 1899” 第 3 个事件节点

图7

Helpdesk 日志预测结果示例

第 7 层:
product=
Value 3

第 8 层:
duration
=7.3245

郭娜 等: 一种预测流程剩余时间的可解释特征分层方法

1353

剩余时间 (天)

45
40
35
30

预测值
真实值

25
20
第 1 层:
第 2 层:
第 3 层:
第 4 层:
activity= allDuration seriousness_2 variant
resolve ticket =0.0016
=Value 2
index=1

第 5 层:
day=18

第 6 层:
resource
=Value 2

第 7 层:
product
=Value 3

第 8 层:
duration
=0.0014

剩余时间 (天)

特征
(b) “Case 390” 第 3 个事件节点

28
26
24
22
20
18
16
14

预测值
真实值

第 1 层:
第 2 层:
第 3 层:
第 4 层:
activity= allDuration seriousness_2 variant
Take in
=22.8956
=value 2
index=1
charge ticket

第 5 层:
day=11

第 6 层:
resource=
value 2

第 7 层:
product=
value 3

第 8 层:
duration=
22.8956

特征
(c) “Case 404” 第 2 个事件节点
38

预测值
真实值

剩余时间 (天)

36
34
32
30
28
26
第 1 层:
第 2 层:
第 3 层:
第 4 层:
activity= allDuration seriousness_2 variant
Take in
=0.0002
=value 1
index=1
charge ticket

第 5 层:
day=29

第 6 层:
resource=
value 2

第 7 层:
product=
value 3

第 8 层:
duration=
0.0002

特征
(d) “Case 4505” 第 2 个事件节点

图7

5 结

Helpdesk 日志预测结果示例 (续)

论

基于深度学习的业务流程剩余时间预测方法表现出了较强的拟合与泛化能力, 得到了更高的准确率. 现有的
工作大多只采用活动特征或根据领域知识选择输入特征构建预测模型, 缺少通用的特征选择方法. 另外基于深度
学习的预测模型由于其结构的复杂性难以解释输入特征与预测结果的内在联系. 为解决上述问题, 本文的主要贡

1354

软件学报 2024 年第 35 卷第 3 期

献为: (1) 提出一种通用的特征自选取策略, 在保证准确率的前提下减少选取特征的数量, 降低后续构建模型的复
杂程度; (2) 提出一种可解释特征分层预测模型结构, 每层接收不同的特征, 并将训练好的模型逐层输出进行可视
化, 以解释特征与预测结果的关联关系. 本文通过 LightGBM 和 LSTM 算法实现所提框架, 并基于 8 个真实事件日
志对所提方法进行实验对比和验证, 选用其中一个日志进行案例分析, 详述了特征选择和预测的整个过程. 实验验
证了特征自选取策略的有效性, 提供了一种可信的解释方法, 提升了剩余时间预测任务的准确率.
未来研究工作主要在所提框架中采用更先进的深度学习模型替换当前的 LSTM, 以验证是否可以进一步提升预
测准确率. 其次, 尝试确定具有组合关系的特征, 将其放入同一层中以验证特征结合对预测结果的影响. 此外, 本文通
过特征这一角度对预测的可解释性进行了探讨, 可以继续探讨其他可解释的模型或方法, 进一步提高模型的可信性.
References:
[1]

van der Aalst WMP. Process Mining: Discovery, Conformance and Enhancement of Business Processes. Berlin: Springer, 2011. [doi: 10.
1007/978-3-642-19345-3]

[2]

Liu C, Duan H, Zeng QT, Zhou MC, Lu FM, Cheng JJ. Towards comprehensive support for privacy preservation cross-organization
business process mining. IEEE Trans. on Services Computing, 2019, 12(4): 639–653. [doi: 10.1109/TSC.2016.2617331]

[3]

van der Aalst W, Weijters T, Maruster L. Workflow mining: Discovering process models from event logs. IEEE Trans. on Knowledge
and Data Engineering, 2004, 16(9): 1128–1142. [doi: 10.1109/TKDE.2004.47]

[4]

Wen LJ, Wang JM, van der Aalst WMP, Huang BQ, Sun JG. A novel approach for process mining based on event types. Journal of
Intelligent Information Systems, 2009, 32(2): 163–190. [doi: 10.1007/s10844-007-0052-1]

[5]

Buijs JCAM, van Dongen BF, van der Aalst WMP. A genetic algorithm for discovering process trees. In: Proc. of the 2012 IEEE
Congress on Evolutionary Computation. Brisbane: IEEE, 2012. 1–8. [doi: 10.1109/CEC.2012.6256458]

[6]

Harane N, Rathi S. Comprehensive survey on deep learning approaches in predictive business process monitoring. In: Gunjan VK, Zurada
JM, Raman B, Gangadharan GR, eds. Modern Approaches in Machine Learning and Cognitive Science: A Walkthrough. Cham: Springer,
2020. 115–128. [doi: 10.1007/978-3-030-38445-6_9]

[7]

Xu XR, Liu C, Li T, Guo N, Ren CG, Zeng QT. Business process remaining time prediction: An approach based on bidirectional quasi
recurrent neural network with attention. Acta Electronica Sinica, 2022, 50(8): 1975–1984 (in Chinese with English abstract). [doi: 10.
12263/DZXB.20211477]

[8]

Sun XX, Hou WJ, Ying YK, Yu DJ. Business process remaining time prediction based on two-layer machine learning. Chinese Journal of
Computers, 2021, 44(11): 2283–2294 (in Chinese with English abstract). [doi: 10.11897/SP.J.1016.2021.02283]

[9]

Márquez-Chamorro AE, Resinas M, Ruiz-Cortés A. Predictive monitoring of business processes: A survey. IEEE Trans. on Services
Computing, 2018, 11(6): 962–977. [doi: 10.1109/TSC.2017.2772256]

[10]

Di Francescomarino C, Ghidini C, Maggi FM, Milani F. Predictive process monitoring methods: Which one suits me best? In: Proc. of
the 16th Int’l Conf. on Business Process Management. Sydney: Springer, 2018. 462–479. [doi: 10.1007/978-3-319-98648-7_27]

[11]

Teinemaa I, Dumas M, La Rosa M, Maggi FM. Outcome-oriented predictive process monitoring: Review and benchmark. ACM Trans.
on Knowledge Discovery from Data, 2019, 13(2): 17. [doi: 10.1145/3301300]

[12]

Verenich I, Dumas M, La Rosa M, Maggi FM, Teinemaa I. Survey and cross-benchmark comparison of remaining time prediction
methods in business process monitoring. ACM Trans. on Intelligent Systems and Technology, 2019, 10(4): 34. [doi: 10.1145/3331449]

[13]

Neu DA, Lahann J, Fettke P. A systematic literature review on state-of-the-art deep learning methods for process prediction. Artificial
Intelligence Review, 2022, 55(2): 801–827. [doi: 10.1007/s10462-021-09960-8]

[14]

Bolt A, Sepúlveda M. Process remaining time prediction using query catalogs. In: Proc. of the 2013 Int ’l Conf. on Business Process
Management. Beijing: Springer, 2013. 54–65. [doi: 10.1007/978-3-319-06257-0_5]

[15]

Pika A, van der Aalst WMP, Fidge CJ, Ter Hofstede AHM, Wynn MT. Profiling event logs to configure risk indicators for process
delays. In: Proc. of the 25th Int’l Conf. on Advanced Information Systems Engineering. Valencia: Springer, 2013. 465–481. [doi: 10.1007/
978-3-642-38709-8_30]

[16]

van der Aalst WMP, Schonenberg MH, Song M. Time prediction based on process mining. Information Systems, 2011, 36(2): 450–475.
[doi: 10.1016/j.is.2010.09.001]

[17]

Rogge-Solti A, Weske M. Prediction of remaining service execution time using stochastic petri nets with arbitrary firing delays. In: Proc.
of the 11th Int’l Conf. on Service-oriented Computing. Berlin: Springer, 2013. 389–403. [doi: 10.1007/978-3-642-45005-1_27]

[18]

Tax N, Verenich I, La Rosa M, Dumas M. Predictive business process monitoring with LSTM neural networks. In: Proc. of the 29th Int’l

郭娜 等: 一种预测流程剩余时间的可解释特征分层方法

1355

Conf. on Advanced Information Systems Engineering. Essen: Springer, 2017. 477–492. [doi: 10.1007/978-3-319-59536-8_30]

[19]

Bukhsh ZA, Saeed A, Dijkman RM. ProcessTransformer: Predictive business process monitoring with transformer network. arXiv:2104.
00721, 2021.

[20]

Ni WJ, Yan M, Liu T, Zeng QT. Predicting remaining execution time of business process instances via auto-encoded transition system.
Intelligent Data Analysis, 2022, 26(2): 543–562. [doi: 10.3233/IDA-215755]

[21]

Camargo M, Dumas M, González-Rojas O. Learning accurate LSTM models of business processes. In: Proc. of the 17th Int’l Conf. on
Business Process Management. Vienna: Springer, 2019. 286–302. [doi: 10.1007/978-3-030-26619-6_19]

[22]

Pegoraro M, Uysal MS, Georgi DB, van der Aalst WMP. Text-aware predictive monitoring of business processes. In: Proc. of the 24th Int’l
Conf. on Business Information Systems. Hannover: BIS, 2021. 221–232.

[23]

Agrawal P, Abutarboush HF, Ganesh T, Mohamed AW. Metaheuristic algorithms on feature selection: A survey of one decade of
research (2009–2019). IEEE Access, 2021, 9: 26766–26791. [doi: 10.1109/ACCESS.2021.3056407]

[24]

Rostami M, Berahmand K, Nasiri E, Forouzandeh S. Review of swarm intelligence-based feature selection methods. Engineering
Applications of Artificial Intelligence, 2021, 100: 104210. [doi: 10.1016/j.engappai.2021.104210]

[25]

Xu X, Zhang K, Wang WJ. A feature selection method for small samples. Journal of Computer Research and Development, 2018, 55(10):
2321–2330 (in Chinese with English abstract). [doi: 10.7544/issn1000-1239.2018.20170748]

[26]

Huang NT, Peng H, Cai GW, Xu DG. Feature selection and optimal decision tree construction of complex power quality disturbances.
Proc. of the CSEE, 2017, 37(3): 776–785 (in Chinese with English abstract). [doi: 10.13334/j.0258-8013.pcsee.160108]

[27]

Li ZS, Yao X, Liu ZG, Zhang JC. Feature selection algorithm based on LightGBM. Journal of Northeastern University (Natural Science),
2021, 42(12): 1688–1695 (in Chinese with English abstract). [doi: 10.12068/j.issn.1005-3026.2021.12.003]

[28]
[29]

Sindhgatta R, Ouyang C, Moreira C, Liao Y. Interpreting predictive process monitoring benchmarks. arXiv:1912.10558, 2019.
Harl M, Weinzierl S, Stierle M, Matzner M. Explainable predictive business process monitoring using gated graph neural networks.
Journal of Decision Systems, 2020, 29(S1): 312–327. [doi: 10.1080/12460125.2020.1780780]

[30]

Hsieh C, Moreira C, Ouyang C. DiCE4EL: Interpreting process predictions using a milestone-aware counterfactual approach. In: Proc. of
the 3rd Int’l Conf. on Process Mining. Eindhoven: IEEE, 2021: 88–95. [doi: 10.1109/ICPM53251.2021.9576881]

[31]

Ke GL, Meng Q, Finley T, Wang TF, Chen W, Ma WD, Ye QW, LiuTY. LightGBM: A highly efficient gradient boosting decision tree.
In: Proc. of the 31st Int’l Conf. on Neural Information Processing Systems. Long Beach: ACM, 2017. 3149–3157. [doi: 10.5555/3294996.
3295074]

[32]

Mikolov T, Sutskever I, Chen K, et al. Distributed representations of words and phrases and their compositionality. In: Proc. of the 26th
Int’l Conf. on Neural Information Processing Systems. Lake Tahoe: ACM, 2013. 3111–3119. [doi: 10.5555/2999792.2999959]

[33]

Joulin A, Grave E, Bojanowski P, Douze M, Jégou H, Mikolov T. FastText.zip: Compressing text classification models. arXiv:1612.03651,
2016.

[34]

Devlin J, Chang MW, Lee K, Toutanova K. BERT: Pre-training of deep bidirectional transformers for language understanding. In: Proc.
of the 2019 Conf. of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies.
Minneapolis: ACL, 2018. 4171–4186. [doi: 10.18653/v1/N19-1423]

[35]
[36]

Hochreiter S, Schmidhuber J. Long short-term memory. Neural Computation, 1997, 9(8): 1735–1780. [doi: 10.1162/neco.1997.9.8.1735]
Sherstinsky A. Fundamentals of recurrent neural network (RNN) and long short-term memory (LSTM) network. Physica D: Nonlinear
Phenomena, 2020, 404: 132306. [doi: 10.1016/j.physd.2019.132306]

附中文参考文献:
[7]

徐兴荣, 刘聪, 李婷, 郭娜, 任崇广, 曾庆田. 基于双向准循环神经网络和注意力机制的业务流程剩余时间预测方法. 电子学报, 2022,
50(8): 1975–1984. [doi: 10.12263/DZXB.20211477]

[8]

孙笑笑, 侯文杰, 应钰柯, 俞东进. 基于双层机器学习的业务流程剩余时间预测. 计算机学报, 2021, 44(11): 2283–2294. [doi: 10.
11897/SP.J.1016.2021.02283]

[25]

许行, 张凯, 王文剑. 一种小样本数据的特征选择方法. 计算机研究与发展, 2018, 55(10): 2321–2330. [doi: 10.7544/issn1000-1239.
2018.20170748]

[26]

黄南天, 彭华, 蔡国伟, 徐殿国. 电能质量复合扰动特征选择与最优决策树构建. 中国电机工程学报, 2017, 37(3): 776–785. [doi: 10.
13334/j.0258-8013.pcsee.160108]

[27]

李占山, 姚鑫, 刘兆赓, 张家晨. 基于LightGBM的特征选择算法. 东北大学学报(自然科学版), 2021, 42(12): 1688–1695. [doi: 10.
12068/j.issn.1005-3026.2021.12.003]

软件学报 2024 年第 35 卷第 3 期

1356

郭娜(1996－), 女, 博士生, 主要研究领域为业务

陆婷(1990－), 女, 博士生, 主要研究领域为软件

流程预测性监控, 流程挖掘, 机器学习.

工程, 大规模的流媒体数据分析, 业务流程管理,
Petri 网.

刘聪(1990－), 男, 博士, 教授, 博士生导师, CCF

闻立杰(1977－), 男, 博士, 副教授, 博士生导师,

专业会员, 主要研究领域为业务流程管理, 流程

CCF 专业会员, 主要研究领域为自然语言处理,

挖掘, 人工智能.

大数据处理与分析, 业务过程智能.

李彩虹(1970－), 女, 博士, 教授, 博士生导师, 主

曾庆田(1976－), 男, 博士, 教授, 博士生导师, CCF

要研究领域为智能机器人, 计算智能学习算法.

高级会员, 主要研究领域为流程挖掘, Petri 网.


软件学报 ISSN 1000-9825, CODEN RUXUEW
Journal of Software [doi: 10.13328/j.cnki.jos.007023]
©中国科学院软件研究所版权所有.

E-mail: jos@iscas.ac.cn
http://www.jos.org.cn
Tel: +86-10-62562563

*

事件融合与空间注意力和时间记忆力的视频去雨网络
孙上荃 1,2, 任文琦 3, 操晓春 3
1

(中国科学院 信息工程研究所, 北京 100085)

2

(中国科学院大学 网络空间安全学院, 北京 100049)

3

(中山大学•深圳 网络空间安全学院, 广东 深圳 518107)

通信作者: 任文琦, E-mail: renwq3@mail.sysu.edu.cn

摘

要: 近年来数码视频拍摄设备不断升级, 其感光元件宽容度、快门速率的提升虽然极大程度地丰富了可拍摄

景物的多样性, 雨痕这类由于雨滴高速穿过景深范围的退化元素也更容易被记录到, 作为前景的稠密雨痕阻挡了
背景景物的有效信息, 从而影响图像的有效采集. 由此视频图像去雨成为一个亟待解决的问题, 以往的视频去雨方
法集中在利用常规图像自身的信息, 但是由于常规相机的感光元件物理极限、快门机制约束等原因, 许多光学信
息在采集时丢失, 影响后续的视频去雨效果. 由此, 利用事件数据与常规视频信息的互补性, 借助事件信息的高动
态范围、时间分辨率高等优势, 提出基于事件数据融合与空间注意力和时间记忆力的视频去雨网络, 利用三维对
齐将稀疏事件流转化为与图像大小匹配的表达形式, 叠加输入至集合了空间注意力机制的事件-图像融合处理模
块, 有效提取图像的空间信息, 并在连续帧处理时使用跨帧记忆力模块将先前帧特征利用, 最后经过三维卷积与两
个损失函数的约束. 在开源视频去雨数据集上验证所提方法的有效性, 同时达到了实时视频处理的标准.
关键词: 视频去雨; 事件数据; 多模态融合; 空间注意力; 时间记忆力
中图法分类号: TP391
中文引用格式: 孙上荃, 任文琦, 操晓春. 事件融合与空间注意力和时间记忆力的视频去雨网络. 软件学报. http://www.jos.org.cn/
1000-9825/7023.htm
英文引用格式: Sun SQ, Ren WQ, Cao XC. Event Fusion-based Spatial Attentive and Temporal Memorable Network for Video
Deraining. Ruan Jian Xue Bao/Journal of Software (in Chinese). http://www.jos.org.cn/1000-9825/7023.htm

Event Fusion-based Spatial Attentive and Temporal Memorable Network for Video Deraining
SUN Shang-Quan1,2, REN Wen-Qi3, CAO Xiao-Chun3
1

(Institute of Information Engineering, Chinese Academy of Sciences, Beijing 100085, China)

2

(School of Cyber Security, University of Chinese Academy of Sciences, Beijing 100049, China)

3

(School of Cyber Science and Technology, Shenzhen Campus of Sun Yat-sen University, Shenzhen 518107, China)

Abstract: In recent years, digital video shooting equipment has been continuously upgraded. Although the improvement of the latitude of
its image sensor and shutter rate has greatly enriched the diversity of the scene that can be photographed, the degraded factors such as rain
streaks caused by raindrops passing through the field of view at high speed are also easier to be recorded. The dense rain streaks in the
foreground block the effective information of the background scene, thus affecting the effective acquisition of images. Therefore, video
image deraining becomes an urgent problem to be solved. The previous video deraining methods focus on using the information of
conventional images themselves. However, due to the physical limit of the image sensors of conventional cameras, the constraints of the
shutter mechanism, etc., much optical information is lost during video acquisition, which affects the subsequent video deraining effect.
Therefore, taking advantage of the complementarity of event data and conventional video information, as well as the high dynamic range

*

基金项目: 国家自然科学基金 (62172409); 深圳市科技计划 (JCYJ20220530145209022)
本文由“多模态协同感知与融合技术”专题特约编辑孙立峰教授、宋新航副研究员、蒋树强教授、王莉莉教授、申恒涛教授推荐.
收稿时间: 2023-04-07; 修改时间: 2023-06-08; 采用时间: 2023-08-23; jos 在线出版时间: 2023-09-11

软件学报 2024 年第 x 卷第 x 期

2

and high temporal resolution of event information, this study proposes a video deraining network based on event data fusion, spatial
attention, and temporal memory, which uses three-dimensional alignment to convert the sparse event stream into an expression form that
matches the size of the image and superimposes the input to the event-image fusion module that integrates the spatial attention mechanism,
so as to effectively extract the spatial information of the image. In addition, in continuous frame processing, the inter-frame memory
module is used to utilize the previous frame features, which are finally constrained by the three-dimensional convolution and two loss
functions. The video deraining method is effective on the publicly available dataset and meets the standard of real-time video processing.
Key words: video deraining; event data; multi-mode fusion; spatial attention; temporal memory

降雨作为一种常见的恶劣气候, 一定程度上影响着雨天视频拍摄的效果. 密集分布、高速运动、高反射率的
雨滴在穿过镜头时, 对背景景物造成大面积的遮挡, 丢失了期望记录的视觉信息. 为解决雨痕遮挡视频图像的问
题, 视频去雨技术应运而生. 作为计算机视觉领域中底层视觉、图像复原技术的一种, 视频去雨技术辅助用户去除
采集到视频图像中的雨痕, 提升观察者视觉效果的同时, 也为其他下游如目标检测、语义分割等高层视觉算法在
降雨天气的应用 [1]铺平道路.
现有的视频去雨算法旨在利用常规相机所拍摄到的退化视频图像完成雨痕去除的目的, 然而由于此问题是欠
定的, 即受遮挡背景信息已经丢失, 仅依靠常规图像准确还原原始背景信息是困难的. 同时, 常规相机受限于许多
物理特性无法完美捕捉物理世界连续的时空信息: (1) 无论是卷帘式快门还是全局快门图像信号均以批量方式记
录传输, 两个邻接时间步之间的信息容易模糊丢失 [2,3]. (2) 感光元件受限于光度宽容极限无法捕捉超出该极限的
视觉信息, 由此造成欠曝光与过曝光问题 [4], 此研究项目中的雨痕便是雨滴高反光率导致的过曝光现象. 对此, 尽
管研究人员试图利用雨滴稀疏性 [4]、雨痕随机性等图像先验缓解欠定性问题, 因其需要具备领域知识的专家对超
参数进行人工调整, 其在复杂背景环境与多变降雨条件下的泛化性仍然不够稳定.
针对上述问题, 本文考虑到雨滴在视频中的光学与动力学性质, 引入事件数据 [3−5]作为一种极佳、可辅助视频
去雨任务的跨模态信息. 事件数据由于其时序分辨率高、时延低、动态范围大的特征, 极好地弥补了常规相机在
拍摄雨天视频图像时丢失的视觉信息, 且其具有空间稀疏性、关注运动边缘的特性极其适合捕捉降雨天气视频拍
摄过程中的高速运动雨滴. 事件数据以事件信息流的方式被记录和存储, 与常规相机拍摄照片的矩阵表达方式和
深度学习中张量特征的表达方式均不相同不适配, 由此本文拟利用空间像素堆叠的方式将稀疏信息流转换为双通
道的张量表示方式, 以使其与常规相机图像像素匹配, 同时将事件数据与常规图像进行通道串联作为输入利用事
件-图像融合处理模块进行组合编码, 最终获得去雨图像特征.
与此同时, 大部分现有的视频去雨方法无法达到实时处理视频图像的要求, 而视频去雨任务作为视频处理任
务的一种, 实时处理能力是相关技术能否落地实用的关键. 最原始的基于模型的方法对输入视频设计目标函数并
通过优化该目标获得去雨后的视频图像, 然而该过程需要多步方程运算, 由于大多无法适配 GPU 的矩阵运算加
速, 距离实时处理的目标相去甚远; 随着深度学习的发展, 借助 GPU 进行张量计算加速, 轻量级深度神经网络已经
可以达到实时处理视频的要求, 但是现有的视频去雨方法要么重度依赖三维卷积, 要么过度利用渐进式自我迭代
等技巧, 虽然去雨效果得到了一定提升, 其运算速度却大大降低. 如何使深度网络在融合事件数据、高效关注雨痕
建模与去除的同时, 降低运算成本, 提升帧内、帧间信息利用率, 都将是极其棘手而关键的问题.
针对上述问题, 本文设计了基于事件数据融合与空间注意力和时间记忆力的视频去雨网络 (event fusion-based
spatial attentive and temporal memorable network for video deraining, EFSATeM), 框架如图 1 所示. 在事件-图像融合
处理模块中利用空间注意力机制更好地融合提取单帧事件与图像中的雨痕特征, 从而使事件-图像融合处理模块
更好地关注雨痕位置已达到高效学习的效果. 同时利用长短期记忆模块将前序帧的深度注意力特征抽取融合入当
前帧的特征, 并记录传承到后序帧, 以此达到前后连续帧之间的交流, 利用前序帧的深度特征提升学习效率, 同时
增强帧间连续性. 最后利用三维卷积处理模块对初步去雨的图像串联进行进一步的处理, 以提升输出视频的平滑
度. 网络输出以残差连接的方式, 以使网络特征聚焦于形成残差的雨痕区域, 减少学习难度.
本文的主要贡献点可总结为以下 3 点.
(1) 利用事件相机数据辅助常规相机图像, 通过双通道像素堆叠的方式将事件信息流无损失地转换为可以与

孙上荃 等: 事件融合与空间注意力和时间记忆力的视频去雨网络

3

常规图像像素配对的张量表示形式, 通过与常规图像通道串联的方式形成输入, 设计事件-图像融合处理模块完成
将事件数据融合进视频去雨任务中.
(2) 设计了基于事件数据融合与空间注意力和时间记忆力的视频去雨网络, 在图像融合处理模块中利用空间
注意力机制更好地融合、提取单帧事件与图像中的雨痕特征, 利用长短期记忆模块将前序帧的深度注意力特征抽
取融合入当前帧的特征, 最后利用三维卷积处理模块对初步去雨的图像串联进行最后的处理.
(3) 在现有数据集上进行了验证性实验, 与多个现有视频图像去雨方法以实时视频处理速度达到了最好效果,
从视觉示例、客观指标、复杂度计算等多个角度验证了提出的方法的可靠性与有效性. 同时对设计模块进行了系
统的消融实验, 证明了各个模块的必要性.
Ground-truth

PReLU

PReLU

3D Conv

Inter-frame memory fusion

Video enhancer
Up
EFAT
EFAT

Up
EFAT
EFAT

Up
EFAT
EFAT

EFAT
EFAT
EFAT
EFAT

EFAT
EFAT
Down

H

EFAT
EFAT
Down

EFAT
Down

Event-image attentive fusion

3D Conv

Input images

7
Event-image attentive fusion

W

Average
Charbonnier
loss

Event-image attentive fusion

...

Input events

Event-image attentive fusion
Event-image attentive fusion

H

Event-image attentive fusion
Event-image attentive fusion

Output images

Temporal
consistency
loss

8
Sequence

W

图1

Inter-frame forward

Loss computation

Event-image fusion attentive Transformer

Concatenation

基于事件数据融合与空间注意力和时间记忆力的视频去雨网络框架图

本文第 1 节介绍视频图像去雨和事件数据融合的现有方法和科研现状. 第 2 节介绍本文相关的基础知识, 包
括事件数据、雨痕建模、注意力和长短期记忆机制. 第 3 节介绍本文提出的基于事件数据融合与空间注意力和时
间记忆力的视频去雨网络. 第 4 节通过对比实验、消融实验等方式验证所提网络的有效性. 最后总结全文.

1 融合事件数据与视频去雨相关工作
1.1 基于视频的雨痕去除
视频雨痕去除是 Garg 等人提出的单图去雨任务 [6]的扩展, 他们基于雨滴动力学和光度学分析将雨痕和非雨
区域分离, 从而解决了单图去雨问题. 而后, Barnum 等人 [7]系统分析了雨天视频的时空属性, 并在频率空间构建了
一个统计模型来检测雨痕区域. 许多工作 [8–10]利用雨痕的稀疏性解决视频图像去雨问题. 肖进胜等人提出了一个
低秩模型 [8], 用稀疏编码处理图像中的雨痕特征. 同样, Kim 等人设计了一种低秩矩阵构建方法 [9], 用于去除映射
的雨痕. Ren 等人通过多标签马尔可夫随机场域将稀疏雨痕形式化, 以区分其他移动物体和雨痕 [10]. 其他一些工作
则利用视频中雨的光度学和几何外观和动态来帮助去雨, 例如 Chen 等人将雨检测视为像素级运动分割问题 [11],
并考虑雨的光度学和色度学约束. 另一些工作中, 视频中的雨也可以在补丁级别上被视为随机的, 并被高斯分布建
模, 例如 Jiang 等人提出的一种基于张量的模型 [5], 带有全变分正则化器, 并通过多元交替方向方法来求解模型. 一
些最近的工作同时考虑了雨的稀疏性和动态特性, 比如 Li 等人基于单张图像去雨和超分辨率中使用的传统稀疏
编码方法 [12], 开发了多尺度版本的卷积稀疏编码. Jiang 等人 [13]考虑了所有已知的雨的判别特征, 并将它们公式化
为 4 个损失项. 然而, 由于复杂的背景和大运动的存在, 真实的雨天视频可能会扭曲雨痕的稀疏性和动态假设, 并
降低基于模型的方法的鲁棒性.
近年来, 例如卷积神经网络 (convolutional neural network, CNN) 和循环神经网络 (recurrent neural network,
RNN) 等基于深度学习的方法, 已经被证明在视频去雨痕任务中非常有效. Chen 等人 [14]利用超像素分割技术将帧
分割成有意义的区域, 然后将分割特征传递到 CNN 中以获得增强去雨结果. Liu 等人 [15]通过考虑雨痕的叠加重新
制定了雨痕遮挡问题, 并提出了一个循环 CNN 来对这个问题进行针对性的求解, 同时他们利用概率模型生成了两

软件学报 2024 年第 x 卷第 x 期

4

个具有时间不相关雨痕的视频去雨数据集. 根据其重新制定的雨痕遮挡模型, 动态路由残留循环网络 (dynamic
routing residue recurrent network, D3R-Net)[16]级联了一个用于空间特征提取的残差 CNN 和一个用于时间特征提取
的 RNN, 其 CNN 和 RNN 采用软动态路由机制, 其中每层的输出是多个平行单元的加权总和, 而权重又由可学习
的门控制. Yang 等人 [17]认为去雨是雨痕模拟合成的逆过程, 并引入一个逆恢复模型, 其参数由相应的神经网络负
责估计. 之后 Yang 等人 [18]设计了一个无监督的 CNN 架构, 辅助使用光流估计算法进行帧对齐. 张学锋等人利用
双注意力残差的循环网络逐阶段地去除雨痕 [19]. 在深度学习动态雨生成器的启发下, Yue 等人开发了半监督视频
去雨框架 S2VD (semi-supervised video deraining)[20], 该框架由去雨 CNN 和雨痕生成 CNN 组成, 旨在弥合实际雨
视频和合成雨视频之间的差距. 孟祥玉等人开发了一个基于运动估计与时空结合的视频去雨网络 [21], 引入基于残
差连接的编码器解码器结构处理雨痕去除问题. Zhang 等人则设计了一个简单的残差网络 [22], 利用帧间信息提取
来更好地捕捉时空特征和相邻帧之间的时序相关性. 然而, 传统相机的雨迹缺失信息以及各种迥异的雨痕模式阻
碍了现有视频去雨算法的性能, 以上基于神经网络的方法都难以高效对雨痕特征和雨滴运动信息进行建模, 相比
之下, 本文方法能借助事件编码雨痕的动态特征, 并通过空间注意力与时间记忆力提升各种雨痕建模效率.
1.2 融合事件数据的视觉任务
作为一种新型的神经形态相机, 例如 DVS (dynamic vision sensor)[3]、ATIS (asynchronous time-based image
sensor)[23]和 DAVIS (dynamic and active pixel vision sensor)[24]等事件相机可以在像素级和微秒级别异步捕捉光照
的强度变化, 这使得其相对传统相机在高动态范围和低延迟方面具有极大优势 [25]. 由于其优越性, 事件已被广泛
研究和利用于许多视觉任务, 包括立体视觉 [26]、姿态估计 [27]、视觉跟踪 [28]、光流估计 [29]、图像重建 [30]、HDR
重建 [31]等. 与此同时, 研究者也开发了许多技术以处理事件数据, 例如脉冲神经网络 [32]、确定性滤波器 [33]、脉冲
时间相关性 [34]和脉冲变压器 [35]等. 基于事件的视频图像恢复工作目前集中在视频图像去模糊 [36], 其主要利用事件
数据极高的时间分辨率来补充传统相机缺失的信息. 与处理事件数据的现有方法相比, 作者提出利用事件-图像数
据对齐融合技术、空间注意力和时间记忆力来更好地编码融合事件数据的时空特征.

2 基础知识
本文所提算法主要基于事件数据、雨痕建模、注意力机制和长短期记忆机制, 本节将就相关基础知识和有关
基本概念予以简单介绍.
2.1 事件数据流
常规相机基于快门的方式将感光元件像素批量地激活生成 RGB 三通道图像 x ∈ R3×W×H , 其中W 和H 分别是生
成图像的宽度和长度, 快门关闭导致信息记录不完全、目标运动模糊. 与常规相机记录光照的绝对光强相反, 事件
相机以光强的相对变化作为主要记录信息, 且以信息流的方式进行记录和存储, 即ei = (xi , yi , ti , pi ), 其中xi ∈ [1, W]
和yi ∈ [1, H]分别为第i个被激活像素的空间坐标, 而ti 是事件的时刻坐标, 最后 pi ∈ {−1, +1}记录事件的极化信息. 当
pi = −1时该时空坐标的像素捕捉到光强减弱, 当 pi = +1时则反之. 事件数据由此可以有极大的时间分辨率和动态

范围, 例如 DAVIS 事件相机的时间分辨率可以达到 3 μs, 其动态范围可达 130 dB 以上 [24].
事件数据流的具体生成机理展示在图 2 中. 以雨天的某个雨滴为例, 在雨滴作高速下坠运动时在T 时刻首次出
现在镜头前, 此时由于雨滴运动穿越了多个像素位置, 3 个连续像素因为雨滴的高反光导致的光强变化而被激活,
因此产生了 3 个事件数据流(x1 , y1 , T, +1)、(x1 , y2 , T, +1)和(x1 , y3 , T, +1), 其中x1 为 3 个像素的横轴坐标, 而y1 、y2 和y3
是 3 个像素的纵轴坐标, 由于此刻 3 个像素接收光强变大, 故极化信息均为+1. 当时间过了t而到了T + t时刻时, 雨
滴已经下落到了更低的像素位置, 因此先前的 3 个像素光强变小, 而此时雨滴所处 3 个像素位置的接收光强变大,
因此像素产生了 6 个事件流, 分别为 3 个负向极化信息和 3 个极化正向, 在图上分别以蓝色和红色颜色表示. 由于
事件相机采集事件数据的时间步长达到了微秒级别的小, 其在常规相机采集连续两帧之间的时间便足以产生数以
百万计的事件信息流, 且其与光强绝对强度无关, 可以做到极大动态范围, 同时没有快门关闭的约束, 可以保持不
间断持续事件数据采集.

孙上荃 等: 事件融合与空间注意力和时间记忆力的视频去雨网络

第T时刻

5

第T+t时刻

第T时刻
事件流
x1, y1,T+1
x1, y2,T+1
x1, y3,T+1
...

第T+t时刻

图2

事件流
x1, y1,T+t, −1
x1, y2,T+t, −1
x1, y3,T+t, −1
x1, y4,T+t, +1
...

事件数据流在降雨场景中的生成机理

2.2 视频去雨建模
在单图去雨任务中, 通常将雨图像O拆分为雨痕图层R与背景图层B, 即如公式 (1) 所示:
O = R+B

(1)

但是视频需要考虑到多帧图像以及雨痕图层与背景图层的帧间关系, 其物理建模需要改写为以下表达式:
Oi = Ri + Bi

(2)

其中, i表示为视频中的第i帧图像. 由于物体在前后帧图像内的移动往往连续, 可以设定一个掩码扭曲函数可以将
前后帧的图像对齐, 即Oi = Wo (Oi+1 ), 在实际中使用光流估计算法来拟合这种对齐函数. 相应地, 雨痕图层和背景图
层因其帧间连续的特点也均存在可以估计的扭曲对齐函数, 以WR 和WB 表示. 由于雨痕图层的遮挡, 背景图层的部
分内容丢失, 而雨痕去除算法就是设法仅以雨视频图像作为输入, 估计还原出原始的背景图层.
2.3 注意力机制
注意力机制由 Vaswani 等人 [37]提出, 替代了传统的循环神经网络, 使模型能够同时关注输入序列中的所有位
置编码, 从而使其更加高效和可并行化, 它也因此成为自然语言处理任务中广泛使用的架构. 而注意力机制也同样
可以应用到计算机视觉任务中, 使用注意力层代替或者辅助传统的卷积层, 使模型能够直接关注图像的不同空间
或通道区域. 其具体过程如下, 给定长度为N 的输入序列, 注意力机制需要计算出一组注意力权重矩阵A, 它表示在
预测第i个区域时需要关注第 j个区域的重要程度. 这些注意力权重是由 3 个矩阵计算而来, 即查询矩阵Q、键矩阵
K 和值矩阵V , 其均由同一输入序列经过神经网络计算获得. 注意力权重矩阵的计算式如下:
(
√ )
A = Softmax QK T / dk

(3)

其中, dK 是键向量K 的维度. 获得注意力权重矩阵后输入序列中上下文矩阵c由注意力权重矩阵和值矩阵V 的矩阵
乘法计算而成:
c = AV

(4)

无论是文本序列还是图像, 注意力机制可以捕捉输入的不同区域之间的关系, 允许模型关注相关信息并抑制
无关信息, 从而在各种任务上提高了性能.
2.4 长短期记忆模块
长短时记忆模型 (long-short term memory model, LSTM) 由 Hochreiter 等人提出 [38], 是一种常用的循环神经网
络变体. 相较于传统的 RNN, LSTM 通过引入门机制 (gate mechanism) 来有效解决了长序列依赖问题, 广泛应用于

软件学报 2024 年第 x 卷第 x 期

6

序列预测、语音识别、自然语言处理等领域. LSTM 模型中的一个关键组件是记忆单元 (memory cell), 它具有自
我更新和存储状态的能力. 同时, LSTM 通过门控单元 (gate unit) 来控制记忆单元中信息的输入、输出和遗忘, 使
得模型可以选择性地保留和遗忘先前的信息. 具体来说, LSTM 的记忆单元可以定义如下:
ct = ft ⊙ ct−1 + it ⊙ gt

(5)

其中, ct 表示时刻t的记忆单元状态, ft 表示遗忘门 (forget gate) 的输出, 控制着上一时刻的记忆单元状态被保留的程
度, it 表示输入门 (input gate) 的输出, 控制着当前信息被记忆单元更新的程度, gt 表示当前时刻的候选状态. 遗忘门、
输入门和候选状态可以通过公式 (6)–公式 (8) 计算:

(
)
ft = Sigmoid W f [ht−1 , xt ] + b f
it = Sigmoid (Wi [ht−1 , xt ] + bi )
(
)
gt = tanh Wg [ht−1 , xt ] + bg

(6)
(7)
(8)

其中, ht−1 表示上一时刻的隐藏状态, xt 表示当前时刻t的输入, W f 、Wi 和Wg 分别表示对应的权重矩阵, b f 、bi 和bg 分
别表示对应的偏置. 最后, LSTM 的输出可以通过公式 (9) 计算:
ht = ot ⊙ tanh (ct )

(9)

其中, ot 表示输出门 (output gate) 的输出, 控制着当前时刻的输出是基于哪些记忆单元状态. LSTM 模型通过引入
门机制来控制信息的输入、输出和遗忘, 从而解决了传统 RNN 中长序列依赖问题, 提高了模型的性能. 在本文中,
作者将权重矩阵和偏置的线性层计算改为了卷积层计算, 以适应视频图像数据特征.

3 基于事件数据融合与空间注意力和时间记忆力的视频去雨网络
本文利用深度神经网络的强大表征能力和特征融合能力, 提出一种基于事件数据融合与空间注意力和时间记
忆力的视频去雨网络, 该模型由以下 3 个部分组成.
首先, 针对事件数据流与常规图像的表达方式不一致的问题, 作者采用了像素极化信息双通道求和的方法. 具
体来说, 首先取出两个连续常规视频帧之间的所有事件信息流, 生成与常规视频图像长宽一致的双通道空白画布,
随后将每个事件数据流的空间位置所对应的像素处累加权值, 遇到正向极化信息的事件时对第 1 通道的对应像素
值加一, 遇到反向极化的事件时对第 2 通道的对应像素值加一, 如此一来便将稀疏事件数据流转换成了与常规图
像表征类似的图像表达形式, 为后续的事件-图像融合铺平道路.
其次, 为达到实时处理视频的目标, 利用二维图像卷积设计了事件-图像注意力融合模块. 在事件-图像注意力
融合模块中, 主要利用二维逐点卷积、逐深度卷积与注意力机制将串联输入的事件-图像数据三联体进行深度融
合, 经过 3 次下采样、上采样与多层事件-图像注意力融合 Transformer 的处理, 将雨视频图像初步复原得到去雨
图像. 同时在经过 3 次下采样得到深度特征的阶段, 利用跨帧记忆力特征融合模块将前序帧的深度特征进行记忆
融合, 此处主要利用到了长短期记忆机制, 以强化去雨效果.
最后, 使用视频增强模块对连续多帧初步处理好的图像进行几层三维卷积和 PReLU 激活层搭配的增强处理,
而后使用 Charbonnier 损失函数对去雨效果进行有力监督, 同时添加上将前后帧图像扭曲对齐的损失函数对视频
时序连续性进行增强约束, 最终达到雨痕图层的去除与背景图层的估计还原. 最终模型的运算速度与去雨效果达
到了较优平衡.
3.1 事件图像数据配对预处理
首先, 为了使事件与图像得到更好融合, 事件数据需要被转换为与图像数据相匹配的表示方式, 即将事件从稀
疏信息流的表达方式转变为二维矩阵的表达方式. 为此需要进行事件数据预处理, 其过程如图 3 所示, 将事件信息
逐个取出排列在三维空间中, 并对两帧图像之间相同像素位置的事件进行双通道合并, 正向极化信息的事件累加
并入第 1 通道, 在图例中以红色颜色表示, 负向极化信息的事件累加并入第 2 通道, 在图例中以蓝色颜色表示, 最
终形成与视频图像长宽一致的双通道事件图像表达方式. 事件数据以其高动态范围和高时间分辨率的特点, 记录

孙上荃 等: 事件融合与空间注意力和时间记忆力的视频去雨网络

7

下以雨线为代表的移动反光物体, 为后续的视频去雨任务提供雨线位置的掩码信息和尺度信息. 且事件数累加的
形式提供了视频帧间隔内像素位置的光强变化次数, 提供了反光物体的移动幅度等信息.
事件流

10

8

x1, y1,T1, +1
x2, y2,T2, +1
x3, y3,T3, −1
x4, y4,T4, +1

8

6

6

4

4

2

2

0

...

数据流

12

10

...

局部放大的散点图

时空三维分布

正反极化双通道累加

图3

正向极化通道

双通道空间表示

负向极化通道

0

累加事件数据热度图

事件数据从信息流表示转化为双通道空间表示的预处理

接着, 多帧事件图像需要与多帧视频图像进行交错合并, 形成可以输入神经网络的输入数据. 具体而言, 如图 4
所示, 一串 7 帧的视频图像有 8 帧与之相邻的事件图像, 得到这 15 帧图像之后, 可以将其交错对齐, 使相邻的事件
图像与视频图像在序列维度邻接排列, 而后在输入至基于事件数融合与空间注意力和时间记忆力的视频去雨网络
时, 为减少过多输入数据带来的信息冗余和计算负担, 仅取出单帧视频图像和与其相邻的前后两帧的事件图像, 再
将该三联体序列其在通道维度串联成通道数为 7 的二维矩阵, 为后续的事件-图像融合铺平道路.

H

H

3
8

H

W
第1帧数据

W

...

15

H

H
W

7

3

W
第7帧数据

W
事件-图像数据组合

事件数据和图像数据

图4

事件-图像逐帧数据

事件图像与视频图像的排列并对齐合并为输入形式

3.2 空间注意力与时间记忆力
获得可以输入模型的数据表达之后, 作者设计了基于事件数据融合与空间注意力和时间记忆力的视频去雨网
络, 如图 1 所示. 其由若干个事件-图像注意力融合 Transformer 模块组成, 并且利用 3 次上下采样形成 U-Net 形式.
事件-图像注意力融合 Transformer 模块的结构则在图 5 中展示, 它是由空间注意力模块和前馈模块组成的
Transformer 网络, 中间利用层标准化 (layer normalization) 将深度特征进行标准化. 注意力模块利用逐点卷积层和
逐深度卷积层分别提取特征中的空间与通道信息, 并拆分为注意力机制中所需的Q、K 和V 这 3 个张量特征, 将Q
(
)
和K 转换维度, 再进行Softmax QK T 的操作获得空间通道注意力, 注意此处的注意力在通道维度展开, 3 次下采样
与多层卷积将空间信息压缩混合至通道维度, 因此这样的通道注意力同样可以学习到空间注意力, 以此达到计算
资源与注意力范围的平衡. 而后前馈模块则同样使用逐点卷积层与逐深度卷积层的串联混合提取空间与通道特
征, 并沿通道拆分为两支, 一支通过 GeLU 激活层得到一个门控, 逐像素乘以另一支特征之后通过另一层逐点卷积
层, 最后与输入特征相加输出残差特征.
连续输入两帧数据的事件-图像注意力融合模块之间也有特征的传递与交互, 由跨帧记忆力融合模块完成. 如
图 6 所示, 该模块以卷积长短期记忆模块结构展开, 前序帧的特征[ht−1 , ct−1 ]和传至后序帧的输出特征[ht , ct ]的传递

软件学报 2024 年第 x 卷第 x 期

8

以红色箭头显示, 先将前序输出特征ht−1 和当前特征串联, 通过 4 层卷积和激活层分别获得遗忘门、输入门、记忆
单元和候选状态, 而后通过公式 (5) 和公式 (9) 获得输入至下一帧计算的深度特征.
Event-image fusion attentive Transformer

Channel-wise split

Matrix multiplication

图5

Point-wise addition

G

Point-wise multiplication

Point-wise Conv

V

Depth-wise Conv

K

Point-wise Conv

Q

Layer norm

Point-wise Conv

Feed forward layer

Softmax

Depth-wise Conv

Point-wise Conv

Layer norm

Spatial attention layer

GeLU activation G

事件-图像注意力融合 Transformer 的结构

Inter-frame memory fusion
C

Channel-wise split
Concatenation

tanh

C

Point-wise addition
Sigmoid

Sigmoid

tanh

Sigmoid

Conv

Conv

Conv

Conv

Point-wise multiplication

C

图6

跨帧记忆力融合模块的结构

3.3 视频三维卷积与损失函数
通过事件-图像融合处理模块获得每一帧图像的初步去雨图像之后, 利用视频增强模块对去雨图像帧进行三
维视频处理, 增强其去雨效果. 其内容在图 1 右侧展示, 由两层三维卷积和两层 PReLU 激活层组成, 由于其卷积通
道数均为 RGB 图像的通道维度 3, 其计算负担较小, 为实时视频处理的目标做足准备, 同时由于其三维卷积的计
算处理, 并且与下文将介绍的时序连续性损失函数相配合, 视频的帧间连续性可以得到有力保证和有效提升, 最终
达到视频实时处理与视频去雨效果的有机结合和高效均衡.
而后作者设计了两个损失函数分别约束视频图像去雨效果和视频帧间连续性. 约束去雨效果的损失函数由
{ }7
Charbonnier loss 构成, 给定输出的 7 帧去雨视频片段 B̃i i=1和已有的对应干净视频片段{Bi }7i=1, 其表达式由公式 (10)
给出:

({ }7
)
({ }7
) 1 ∑7 √ (
)
L1 B̃i i=1 , {Bi }7i=1 = LCharbonnier B̃i i=1 , {Bi }7i=1 =
B̃i − Bi 2 + ϵ
7 i=1

(10)

而后, 利用 LightFlowNet3[39]作为光流估计算法, 得到扭曲对齐函数WB , 作者可以对输出的去雨视频片段的前
后连续帧进行互相对齐, 由此进行帧间连续性增强, 其表达式由公式 (11) 给出:
({ }7 ) 1 ∑6
( )
(
)
1 ∑6
L2 B̃i i=1 =
WB B̃i − B̃i+1 +
WB B̃i+1 − B̃i
i=1
i=1
6
6

(11)

最终, 所用损失函数为前述两个损失函数的求和, 由公式 (12) 给出:
L = αL1 + βL2

其中, α和 β分别是两个损失函数权重, 其选值在后文给出. 至此, 算法的模型设计和损失函数设计均介绍完毕.

(12)

孙上荃 等: 事件融合与空间注意力和时间记忆力的视频去雨网络

9

4 实验分析
4.1 实验数据
作者在公开视频去雨数据集 NTURain[14]上进行定量实验. 表 1 给出了 NTURain 数据集所对应的详细信息,
其中, 视频帧数量×3 表示对同一段干净视频模拟了 3 种雨痕视频, 其视频图像分为模拟雨痕的训练集、模拟雨痕
的测试集以及真实雨痕的测试集, 图像长宽分别为 640 和 480, 视频雨痕模拟由视频编辑软件 Adobe Effects 完成,
在训练集上每一段干净视频均对应 3–4 种随机生成的雨痕视频, 训练集由此有共 3 123 帧图像, 模拟雨痕测试集有
1 682 帧, 真实雨痕测试集有 658 帧视频图像.
表 1 实验数据集 NTURain 的视频数据统计
数据集组别

训练集

雨痕真实性

镜头运动幅度

视频命名

视频帧数量

轻微镜头运动

t1
t7

80×3
138×4

大幅镜头运动

t2
t3
t4
t5
t6
t8

112×3
128×3
136×3
128×3
138×3
135×3

轻微镜头运动

a1
a2
a3
a4

168
116
125
298

大幅镜头运动

b1
b2
b3
b4

256
250
219
250

轻微镜头运动

ra1
ra2
ra3
ra4

60
90
80
108

大幅镜头运动

rb1
rb2
rb3

120
60
140

模拟雨痕视频

模拟雨痕视频

测试集

真实雨痕视频

4.2 评价指标及基准模型
在本文中, 作者使用两个视频图像复原任务中常用的指标: 峰值信噪比 (peak signal-to-noise ratio, PSNR) 和结
构相似性 (structural similarity, SSIM) 作为评价视频去雨算法的主要指标, 二者均为指标越大效果越好. 同时提供
视频去雨的图像样例, 作为定性评价的依据.
作者将本文视频去雨算法与其他最先进的模型进行了比较, 包括多尺度卷积稀疏编码算法 (MSCSC)[12]、渐
进式循环网络工作 (PReNet)[40]、细节计算的超像素对齐 CNN (SpacCNN)[14]、自学习去雨网络 (SLDNet)[18]、多
阶段渐进式网络 (MPRNet)[41]、半监督视频去雨 (S2VD)[20]和增强时空交互网络 (ESTINet)[22]. 其中, MSCSC 是基
于模型的方法, 其余的是基于深度学习的方法. PReNet 和 MPRNet 是单图去雨网络, 其余是视频去雨模型.
4.3 实验方法
作者将所有训练视频均匀裁剪为 7 帧128 × 128大小的图像块, batch size 选为 4. 学习率设置为 0.000 5, 当损失
函数的值在连续两个 epoch 减小幅度小于 1E–6 时, 学习率缩小 10 倍. 优化器选用 Adam[42]. 公式 (12) 中的α和 β分
别设置为 1 和 0.1. 训练过程不采用任何数据增强或后处理. 训练总 epoch 数为 50. 对于对比的现有方法, 作者保留

软件学报 2024 年第 x 卷第 x 期

10

其原始配置, 并根据它们发布的代码运行训练和测试实验. 作者使用 ESIM[43]来模拟视频序列中的事件. 所有实验
在一台搭载 Nvidia Tesla V100 PCIe 32 GB GPU 和 Intel® Silver 4214 @ 2.20 GHz CPU 的服务器上进行, 服务器系
统为 Linux Ubuntu 16.04.7 LTS. 实验使用的深度学习框架为 PyTorch 1.9.1, 使用了深度学习开源工具 Torchvision
0.10.1. 代码已发布在 https://github.com/sunsean21/EFSATem/.
4.4 实验结果与分析
作者首先对比了本文方法与现有方法的 PSNR 指标的差异, 它们在 NTURain 测试集的实验结果见表 2. 可以
看出本文方法在所有测试视频中均获得了最高的评分. 而且相较于相机运动幅度较小的前 4 个视频, 本文方法在
相机运动幅度较大的后 4 个视频的优势更为明显, 例如在 b4 视频中本文方法相较于第 2 名的 S2VD 算法取得了
3.63 dB 的显著提升, 这得益于作者对于视频图像帧间事件数据融合的有效利用, 且所提跨帧记忆力融合模块也有
一定帮助, 具体分析会在消融实验给出.
表 2 本文方法与现有视频图像去雨算法的 PSNR 性能比较 (dB)
Clip
a1
a2
a3
a4
b1
b2
b3
b4
Mean

Rainy
29.71
29.30
29.08
32.62
30.03
30.69
32.31
29.41
30.41

MSCSC
25.10
26.77
24.71
31.65
26.35
28.84
26.63
26.61
27.08

PReNet
32.13
30.41
30.73
35.77
32.66
33.74
35.34
33.17
32.99

SpacCNN
30.57
31.29
30.63
35.30
32.26
35.11
34.69
34.87
33.11

SLDNet
33.72
33.82
33.12
37.35
34.21
35.80
36.34
33.85
34.89

MPRNet
35.80
31.83
34.38
37.71
36.75
37.20
39.30
35.93
36.11

S2VD
36.39
33.06
35.75
39.53
37.34
40.55
38.83
37.53
37.37

ESTINet
36.99
34.48
36.09
40.00
37.15
40.01
38.06
36.81
37.48

Ours
37.78
34.71
36.71
41.56
38.76
40.69
41.63
41.16
39.12

作者而后由比较了现有视频去雨算法与本文方法之间 SSIM 的差距, 结果展示在表 3 中. 其取值范围为 0–1,
当前大多数方法均已接近最优值. 尽管如此, 还是可以看出本文方法仍在大多数测试视频中取得了最佳的 SSIM
指标数值, 且相机运动幅度大的视频中本文方法的优势仍然更加明显.
表 3 本文方法与现有视频图像去雨算法的 SSIM 性能比较
Clip
a1
a2
a3
a4
b1
b2
b3
b4
Mean

Rainy
0.914 9
0.928 4
0.896 4
0.938 1
0.895 6
0.887 4
0.929 9
0.893 3
0.910 8

MSCSC
0.763 5
0.824 2
0.732 6
0.932 7
0.795 4
0.886 0
0.814 2
0.802 9
0.818 9

PReNet
0.951 1
0.937 5
0.931 6
0.970 0
0.949 1
0.955 7
0.968 1
0.952 6
0.952 0

SpacCNN
0.933 4
0.935 6
0.924 7
0.962 0
0.945 4
0.967 7
0.956 6
0.953 6
0.947 5

SLDNet
0.950 8
0.951 2
0.940 4
0.972 2
0.948 2
0.959 5
0.961 4
0.946 9
0.954 0

MPRNet
0.964 9
0.949 5
0.953 9
0.975 6
0.964 6
0.965 9
0.974 0
0.961 3
0.963 7

S2VD
0.965 8
0.951 9
0.956 4
0.977 9
0.971 2
0.982 1
0.975 4
0.965 7
0.968 3

ESTINet
0.969 8
0.961 1
0.964 9
0.979 5
0.968 3
0.975 2
0.974 0
0.965 3
0.970 0

Ours
0.974 5
0.963 6
0.966 5
0.984 3
0.975 8
0.981 1
0.983 8
0.980 5
0.976 3

为了更清晰地进行视觉对比, 作者展示了几幅去雨图像案例, 其对比图在图 7 中显示. 在第 1 张对比图中, 可
以看出红色花朵处有很多密集的雨痕, 已有的 3 种方法均无法将它们完全去除, 只有本文方法可以将这些与背景
图案中的红色花朵混杂在一起的雨痕完全清除. 而在第 2 个示例对比图中, 柏油马路上有很多雨痕, 已有算法中
MPRNet 和 S2VD 均有几处明显的雨痕未能去除, 而 ESTINet 虽然能基本去除掉雨痕, 但是在原本雨痕处留下了
很多条纹状伪影, 使本该平滑的柏油路面变得质地不一, 与之对比, 本文方法达到了与参考图像最为一致的复原效
果, 将所有雨痕完全去除.
为了进一步分析视频去雨算法在真实雨痕视频上的效果, 作者展示了一张 NTURain 的真实雨痕视频帧上各

孙上荃 等: 事件融合与空间注意力和时间记忆力的视频去雨网络

11

个去雨算法的效果对比图, 在图 8 中显示. 在图 8 中可以看出草地、地砖路面和柏油马路均有深浅不一较为密集
的雨痕, PReNet 和 MPRNet 由于没有考虑到帧间时序信息, 均无法有效去除掉雨痕, 而 S2VD 虽然可以去除掉画
面中间最为明显的雨滴, 但是对于草地和路面的雨痕却没有处理, 而 ESTINet 则虽然基本去除了多数雨痕, 但是却
留有了一些浅浅的痕迹, 只有本文方法完全将所有雨痕去除, 证实了其在真实雨痕视频图像上的去雨效果.

示
例
1

示
例
2

Rainy

MPRNet

S2VD

ESTINet

Ours

Clean

Rainy

MPRNet

S2VD

ESTINet

Ours

Clean

图7

视频去雨算法的雨痕去除效果案例展示

Rainy

PReNet

MPRNet

S2VD

ESTINet

Ours

图8

视频去雨算法的真实雨痕去除效果案例展示

4.5 消融实验
为了验证设计的各种模块的有效性, 作者进行了不同网络结构和损失函数设置下的消融实验. 除了最终版本

软件学报 2024 年第 x 卷第 x 期

12

的模型, 作者定义了另外 5 个模型设置, 在 Model 0 中完全去除事件数据的编码; 在 Model 1 中将事件-图像注意力
融合 Transformer 模块全部替换成了参数量相近的卷积层模块; 在 Model 2 中将跨帧记忆力融合模块完全抛弃;
在 Model 3 中将跨帧记忆力融合模块替换成了参数量相近的卷积层模块; 在 Model 4 中将 Charbonnier 损失函数
替换成 MSE (mean square error) 损失函数. 最终结果展示在表 4 中, 根据对比结果可以发现事件-图像注意力融合
Transformer 模块最为重要, 而跨帧记忆力融合同样也有较大影响.
表 4 不同设置下本文方法性能比较
Metric
PSNR (dB)
SSIM

Model 0
36.39
0.967 1

Model 1
37.48
0.968 9

Model 2
37.66
0.969 5

Model 3
38.35
0.972 9

Model 4
39.05
0.975 5

Ours
39.12
0.976 3

4.6 复杂度对比
作者将本文方法与现有算法在 NTURain 测试集上进行了运算速度和模型复杂度的对比, 对比结果展示在表 5
之中. 由于本文方法不严重依赖三维卷积, 本方法运算速度相较于其他视频处理算法快, 同时作者使用逐点卷积层
和逐深度卷积层串联的方式 [44,45]替换常规卷积层, 最终使本文算法达到了视频实时去雨增强效果.
表 5 本文方法与现有视频图像去雨算法的每帧运行时间和模型复杂度比较
Metric
Time (s)

MSCSC
15.33

PReNet
0.18

SpacCNN
8.23

SLDNet
2.65

MPRNet
0.15

S2VD
0.14

ESTINet
0.27

Ours
0.03

Param (M)
FLOPs (G)

－
－

0.17
2 174

－
－

4.00
4 782

3.64
18 003

0.53
336

0.44
153

0.44
64

4.7 真实事件数据
作者使用一台事件图像采集设备拍摄了一段包含了真实事件数据的雨天视频, 结果如图 9 所示, 采集到的事
件数据背景色为黑色, 可以看出一簇密集的雨线和植物的茎杆重合, 由此 ESTINet 无法有效去除这种真实的雨线,
而所提方法则成功去雨.

事件采集设备

Rainy

图9

5 总

Event

ESTINet

Ours

采集到包含真实事件的一张视频帧的去雨效果展示

结

针对当前视频去雨算法仅利用常规视频图像而丢失关键信息和特征提取困难的问题, 作者使用事件数据高动
态范围、时间分辨率高的特点, 将其从信息流的表达形式转换为双通道图像形式, 并与常规视频帧进行空间像素
对齐组合, 使用它对常规视频帧之间的时序信息和空间特征进行补充辅助. 同时设计了事件-图像注意力融合
Transformer 模块和跨帧记忆力融合模块, 组合构成基于事件数据融合与空间注意力和时间记忆力的视频去雨网
络, 更加高效地提取和融合事件与视频图像帧之中的时空特征, 且这些模块主要依赖于二维卷积、逐点卷积与逐
深度卷积的串联、长短期记忆机制这些高效模块, 实现了特征提取与高速运算的平衡, 最后经过两层三维卷积后
输出去雨视频图像. 此方法的视频处理速度由此达到了实时运算的要求.

孙上荃 等: 事件融合与空间注意力和时间记忆力的视频去雨网络

13

References:
[1]

Li SY, Araujo IB, Ren WQ, Wang ZY, Tokuda EK, Junior RH, Cesar-Junior R, Zhang JW, Guo XJ, Cao XC. Single image deraining: A
comprehensive benchmark analysis. In: Proc. of the 2019 IEEE/CVF Conf. on Computer Vision and Pattern Recognition. Long Beach:
IEEE, 2019. 3838–3847. [doi: 10.1109/CVPR.2019.00396]

[2]

Le T, Le NT, Jang YM. Performance of rolling shutter and global shutter camera in optical camera communications. In: Proc. of the 2015
Int’l Conf. on Information and Communication Technology Convergence. Jeju: IEEE, 2015. 124–128. [doi: 10.1109/ICTC.2015.
7354509]

[3]

Lichtsteiner P, Posch C, Delbruck T. A 128×128 120 dB 15 μs latency asynchronous temporal contrast vision sensor. IEEE Journal of
Solid-state Circuits, 2008, 43(2): 566–576. [doi: 10.1109/JSSC.2007.914337]

[4]

Fowles GR. Introduction to Modern Optics. 2nd ed., New York: Dover Publications, 1989.

[5]

Jiang TX, Huang TZ, Zhao XL, Deng LJ, Wang Y. A novel tensor-based video rain streaks removal approach via utilizing
discriminatively intrinsic priors. In: Proc. of the 2017 IEEE Conf. on Computer Vision and Pattern Recognition. Honolulu: IEEE, 2017.
2818–2827. [doi: 10.1109/CVPR.2017.301]

[6]

Garg K, Nayar SK. Vision and rain. Int’l Journal of Computer Vision, 2007, 75(1): 3–27. [doi: 10.1007/s11263-006-0028-6]

[7]

Barnum PC, Narasimhan S, Kanade T. Analysis of rain and snow in frequency space. Int’l Journal of Computer Vision, 2010,
86(2): 256–274. [doi: 10.1007/s11263-008-0200-2]

[8]

Xiao JS, Wang W, Zou WT, Tong L, Lei JF. An image rain removal algorithm via depth of field and sparse coding. Chinese Journal of
Computers, 2019, 42(9): 2024–2034 (in Chinese with English abstract). [doi: 10.11897/SP.J.1016.2019.02024]

[9]

Kim JH, Sim JY, Kim CS. Video deraining and desnowing using temporal correlation and low-rank matrix completion. IEEE Trans. on
Image Processing, 2015, 24(9): 2658–2670. [doi: 10.1109/TIP.2015.2428933]

[10]

Ren WH, Tian JD, Han Z, Chan A, Tang YD. Video desnowing and deraining based on matrix decomposition. In: Proc. of the 2017 IEEE
Conf. on Computer Vision and Pattern Recognition. Honolulu: IEEE, 2017. 2838–2847. [doi: 10.1109/CVPR.2017.303]

[11]

Chen J, Chau LP. A rain pixel recovery algorithm for videos with highly dynamic scenes. IEEE Trans. on Image Processing, 2014,
23(3): 1097–1104. [doi: 10.1109/TIP.2013.2290595]

[12]

Li MH, Xie Q, Zhao Q, Wei W, Gu SH, Tao J, Meng DY. Video rain streak removal by multiscale convolutional sparse coding. In: Proc.
of the 2018 IEEE/CVF Conf. on Computer Vision and Pattern Recognition. Salt Lake City: IEEE, 2018. 6644–6653. [doi:
10.1109/CVPR.2018.00695]

[13]

Jiang TX, Huang TZ, Zhao XL, Deng LJ, Wang Y. FastDeRain: A novel video rain streak removal method using directional gradient
priors. IEEE Trans. on Image Processing, 2019, 28(4): 2089–2102. [doi: 10.1109/TIP.2018.2880512]

[14]

Chen J, Tan CH, Hou JH, Chau LP, Li H. Robust video content alignment and compensation for rain removal in a CNN framework. In:
Proc. of the 2018 IEEE/CVF Conf. on Computer Vision and Pattern Recognition. Salt Lake City: IEEE, 2018. 6286–6295. [doi:
10.1109/CVPR.2018.00658]

[15]

Liu JY, Yang WH, Yang S, Guo ZM. Erase or fill? Deep joint recurrent rain removal and reconstruction in videos. In: Proc. of the 2018
IEEE/CVF Conf. on Computer Vision and Pattern Recognition. Salt Lake City: IEEE, 2018. 3233–3242. [doi: 10.1109/CVPR.
2018.00341]

[16]

Liu JY, Yang WH, Yang S, Guo ZM. D3R-Net: Dynamic routing residue recurrent network for video rain removal. IEEE Trans. on
Image Processing, 2019, 28(2): 699–712. [doi: 10.1109/TIP.2018.2869722]

[17]

Yang WH, Liu JY, Feng JS. Frame-consistent recurrent video deraining with dual-level flow. In: Proc. of the 2019 IEEE/CVF Conf. on
Computer Vision and Pattern Recognition. Long Beach: IEEE, 2019. 1661–1670. [doi: 10.1109/CVPR.2019.00176]

[18]

Yang WH, Tan RT, Wang SQ, Liu JY. Self-learning video rain streak removal: When cyclic consistency meets temporal correspondence.
In: Proc. of the 2020 IEEE/CVF Conf. on Computer Vision and Pattern Recognition. Seattle: IEEE, 2020. 1717–1726. [doi: 10.1109/
CVPR42600.2020.00179]

[19]

Zhang XF, Li JJ. Single image de-raining using a recurrent dual-attention-residual ensemble network. Ruan Jian Xue Bao/Journal of
Software, 2021, 32(10): 3283–3292 (in Chinese with English abstract). http://www.jos.org.cn/1000-9825/6018.htm [doi: 10.13328/
j.cnki.jos.006018]

[20]

Yue ZS, Xie JW, Zhao Q, Meng DY. Semi-supervised video deraining with dynamical rain generator. In: Proc. of the 2021 IEEE/CVF
Conf. on Computer Vision and Pattern Recognition. Nashville: IEEE, 2021. 642–652. [doi: 10.1109/CVPR46437.2021.00070]

[21]

Meng XY, Xue XW, Li WL, Wang Y. Motion-estimation based space-temporal feature aggregation network for multi-frames rain
removal. Computer Science, 2021, 48(5): 170–176 (in Chinese with English abstract). [doi: 10.11896/jsjkx.210100104]

[22]

Zhang KH, Li DX, Luo WH, Ren WQ, Liu W. Enhanced spatio-temporal interaction learning for video deraining: Faster and better. IEEE

软件学报 2024 年第 x 卷第 x 期

14

Trans. on Pattern Analysis and Machine Intelligence, 2023, 45(1): 1287–1293. [doi: 10.1109/TPAMI.2022.3148707]

[23]

Posch C, Matolin D, Wohlgenannt R. A QVGA 143 dB dynamic range frame-free PWM image sensor with lossless pixel-level video
compression and time-domain CDS. IEEE Journal of Solid-state Circuits, 2011, 46(1): 259–275. [doi: 10.1109/JSSC.2010.2085952]

[24]

Brandli C, Berner R, Yang MH, Liu SC, Delbruck T. A 240×180 130 dB 3 µs latency global shutter spatiotemporal vision sensor. IEEE
Journal of Solid-state Circuits, 2014, 49(10): 2333–2341. [doi: 10.1109/JSSC.2014.2342715]

[25]

Gallego G, Delbrück T, Orchard G, Bartolozzi C, Taba B, Censi A, Leutenegger S, Davison AJ, Conradt J, Daniilidis K, Scaramuzza D.
Event-based vision: A survey. IEEE Trans. on Pattern Analysis and Machine Intelligence, 2022, 44(1): 154–180. [doi: 10.1109/
TPAMI.2020.3008413]

[26]

Andreopoulos A, Kashyap HJ, Nayak TK, Amir A, Flickner MD. A low power, high throughput, fully event-based stereo system. In:
Proc. of the 2018 IEEE/CVF Conf. on Computer Vision and Pattern Recognition. Salt Lake City: IEEE, 2018. 7532–7542. [doi: 10.1109/
CVPR.2018.00786]

[27]

Gallego G, Lund JEA, Mueggler E, Rebecq H, Delbruck T, Scaramuzza D. Event-based, 6-DOF camera tracking from photometric depth
maps. IEEE Trans. on Pattern Analysis and Machine Intelligence, 2018, 40(10): 2402–2412. [doi: 10.1109/TPAMI.2017.2769655]

[28]

Mitrokhin A, Fermüller C, Parameshwara C, Aloimonos Y. Event-based moving object detection and tracking. In: Proc. of the 2018
IEEE/RSJ Int’l Conf. on Intelligent Robots and Systems. Madrid: IEEE, 2018. 1–9. [doi: 10.1109/IROS.2018.8593805]

[29]

Wan ZX, Dai YC, Mao YX. Learning dense and continuous optical flow from an event camera. IEEE Trans. on Image Processing, 2022,
31: 7237–7251. [doi: 10.1109/TIP.2022.3220938]

[30]

Wang L, Kim TK, Yoon KJ. EventSR: From asynchronous events to image reconstruction, restoration, and super-resolution via end-toend adversarial learning. In: Proc. of the 2020 IEEE/CVF Conf. on Computer Vision and Pattern Recognition. Seattle: IEEE, 2020.
8312–8322. [doi: 10.1109/CVPR42600.2020.00834]

[31]

Han J, Zhou C, Duan PQ, Tang YH, Xu C, Xu C, Huang TJ, Shi BX. Neuromorphic camera guided high dynamic range imaging. In:
Proc. of the 2020 IEEE/CVF Conf. on Computer Vision and Pattern Recognition. Seattle: IEEE, 2020. 1727–1736. [doi: 10.1109/
CVPR42600.2020.00180]

[32]

Orchard G, Benosman R, Etienne-Cummings R, Thakor NV. A spiking neural network architecture for visual motion estimation. In: Proc.
of the 2013 IEEE Biomedical Circuits and Systems Conf. Rotterdam: IEEE, 2013. 298–301. [doi: 10.1109/BioCAS.2013.6679698]

[33]

Brosch T, Tschechne S, Neumann H. On event-based optical flow detection. Frontiers in Neuroscience, 2015, 9: 137. [doi:
10.3389/fnins.2015.00137]

[34]

Paredes-Vallés F, Scheper KYW, de Croon GCHE. Unsupervised learning of a hierarchical spiking neural network for optical flow
estimation: From events to global motion perception. IEEE Trans. on Pattern Analysis and Machine Intelligence, 2020, 42(8): 2051–2064.
[doi: 10.1109/TPAMI.2019.2903179]

[35]

Zhang JQ, Dong B, Zhang HW, Ding JC, Heide F, Yin BC, Yang X. Spiking transformers for event-based single object tracking. In: Proc.
of the 2022 IEEE/CVF Conf. on Computer Vision and Pattern Recognition. New Orleans: IEEE, 2022. 8791–8800. [doi: 10.1109/
CVPR52688.2022.00860]

[36]

Shang W, Ren DW, Zou DQ, Ren JS, Luo P, Zuo WM. Bringing events into video deblurring with non-consecutively blurry frames. In:
Proc. of the 2021 IEEE/CVF Int’l Conf. on Computer Vision. Montreal: IEEE, 2021. 4511–4520. [doi: 10.1109/ICCV48922.2021.00449]

[37]

Vaswani A, Shazeer N, Parmar N, Uszkoreit J, Jones L, Gomez AN, Kaiser Ł, Polosukhin I. Attention is all you need. In: Proc. of the
31st Int’l Conf. on Neural Information Processing Systems. Long Beach: Curran Associates Inc., 2017. 6000–6010.

[38]

Hochreiter S, Schmidhuber J. Long short-term memory. Neural Computation, 1997, 9(8): 1735–1780. [doi: 10.1162/neco.1997.9.8.1735]

[39]

Hui TW, Loy CC. LiteFlowNet3: Resolving correspondence ambiguity for more accurate optical flow estimation. In: Proc. of the 16th
European Conf. on Computer Vision. Glasgow: Springer, 2020. 169–184. [doi: 10.1007/978-3-030-58565-5_11]

[40]

Ren DW, Zuo WM, Hu QH, Zhu PF, Meng DY. Progressive image deraining networks: A better and simpler baseline. In: Proc. of the
2019 IEEE/CVF Conf. on Computer Vision and Pattern Recognition. Long Beach: IEEE, 2019. 3932–3941. [doi: 10.1109/CVPR.
2019.00406]

[41]

Zamir SW, Arora A, Khan S, Hayat M, Khan FS, Yang MH, Shao L. Multi-stage progressive image restoration. In: Proc. of the 2021
IEEE/CVF Conf. on Computer Vision and Pattern Recognition. Nashville: IEEE, 2021. 14816–14826. [doi: 10.1109/CVPR46437.
2021.01458]

[42]

Kingma DP, Ba JL. Adam: A method for stochastic optimization. arXiv:1412.6980, 2015.

[43]

Rebecq H, Gehrig D, Scaramuzza D. ESIM: An open event camera simulator. In: Proc. of the 2nd Annual Conf. on Robot Learning.
Zürich: PMLR, 2018. 969–982.

[44]

Tan MX, Le Q. EfficientNet: Rethinking model scaling for convolutional neural networks. In: Proc. of the 36th Int’l Conf. on Machine

孙上荃 等: 事件融合与空间注意力和时间记忆力的视频去雨网络

15

Learning. Long Beach: PMLR, 2019. 6105–6114.

[45]

Zamir SW, Arora A, Khan S, Hayat M, Khan FS, Yang MH. Restormer: Efficient transformer for high-resolution image restoration. In:
Proc. of the 2022 IEEE/CVF Conf. on Computer Vision and Pattern Recognition. New Orleans: IEEE, 2022. 5728–5739. [doi:
10.1109/CVPR52688.2022.00564]

附中文参考文献:
[8]

肖进胜, 王文, 邹文涛, 童乐, 雷俊锋. 基于景深和稀疏编码的图像去雨算法. 计算机学报, 2019, 42(9): 2024–2034. [doi: 10.11897/
SP.J.1016.2019.02024]

[19]

张学锋, 李金晶. 基于双注意力残差循环单幅图像去雨集成网络. 软件学报, 2021, 32(10): 3283–3292. http://www.jos.org.cn/10009825/6018.htm [doi: 10.13328/j.cnki.jos.006018]

[21]

孟祥玉, 薛昕惟, 李汶霖, 王祎. 基于运动估计与时空结合的多帧融合去雨网络. 计算机科学, 2021, 48(5): 170–176. [doi: 10.11896/
jsjkx.210100104]

孙上荃(1997－), 男, 博士生, CCF 学生会员, 主

操晓春(1980－), 男, 博士, 教授, 博士生导师,

要研究领域为底层视觉, 图像复原, 图像增强, 轻

CCF 杰出会员, 主要研究领域为人工智能安全,

量化模型.

网络空间内容安全.

任文琦(1987－), 男, 博士, 副教授, CCF 专业会
员, 主要研究领域为人工智能, 计算机视觉, 图像
处理, 网络空间内容安全.

